#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸŒ€ **Hilbert Transform Unified Demo V1.0** ğŸŒ€

ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›çµ±åˆè§£æã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- å…¨ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›æ‰‹æ³•ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- çµæœã®å¯è¦–åŒ–
- ä½ç›¸ãƒ»æŒ¯å¹…ãƒ»å‘¨æ³¢æ•°æˆåˆ†ã®åˆ†é›¢è©•ä¾¡
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
import yaml

from indicators.hilbert_unified import HilbertTransformUnified
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from data.data_loader import DataLoader, CSVDataSource
from data.data_processor import DataProcessor
from data.binance_data_source import BinanceDataSource

plt.style.use('dark_background')


def create_synthetic_data(n_points: int = 500) -> pd.DataFrame:
    """
    ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›è§£æç”¨ã®åˆæˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆè¤‡é›‘ãªä½ç›¸ãƒ»å‘¨æ³¢æ•°æˆåˆ†ã‚’å«ã‚€ï¼‰
    """
    np.random.seed(42)
    
    # æ™‚é–“è»¸
    t = np.linspace(0, 10, n_points)
    
    # 1. ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆä½ç›¸å›è»¢ã‚’æŒã¤ï¼‰
    main_trend = 100 + 20 * np.sin(t * 0.5) + 1.2 * t
    
    # 2. ä¸»è¦ã‚µã‚¤ã‚¯ãƒ«ï¼ˆç•°ãªã‚‹ä½ç›¸ã‚’æŒã¤ï¼‰
    primary_cycle = 12 * np.sin(t * 1.5 + np.pi/4)
    secondary_cycle = 8 * np.cos(t * 2.3 + np.pi/3)
    
    # 3. çŸ­æœŸæŒ¯å‹•ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ï¼‰
    short_oscillation1 = 5 * np.sin(t * 5.2 + np.pi/6)
    short_oscillation2 = 3 * np.cos(t * 8.1 + np.pi/2)
    
    # 4. ç¬æ™‚å‘¨æ³¢æ•°å¤‰åŒ–ï¼ˆãƒãƒ£ãƒ¼ãƒ—ä¿¡å·ï¼‰
    freq_modulation = 4 * np.sin(t * 3.0 + np.cumsum(np.sin(t * 0.8)) * 0.5)
    
    # 5. æŒ¯å¹…å¤‰èª¿
    amplitude_modulation = (1 + 0.3 * np.sin(t * 1.2)) * 6 * np.sin(t * 4.0)
    
    # 6. ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºï¼ˆã‚¬ã‚¦ã‚·ã‚¢ãƒ³ + ã‚¤ãƒ³ãƒ‘ãƒ«ã‚¹ï¼‰
    gaussian_noise = np.random.normal(0, 1.5, n_points)
    impulse_noise = np.random.choice([0, 1], n_points, p=[0.95, 0.05]) * np.random.normal(0, 5, n_points)
    
    # 7. ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ï¼ˆæ§‹é€ å¤‰åŒ–ï¼‰
    regime_switch = np.where(t > 5, 1.2, 1.0)
    
    # æœ€çµ‚ä¾¡æ ¼åˆæˆï¼ˆè¤‡é›‘ãªä½ç›¸é–¢ä¿‚ã‚’ä¿æŒï¼‰
    close_prices = (main_trend + primary_cycle + secondary_cycle + 
                   short_oscillation1 + short_oscillation2 + 
                   freq_modulation + amplitude_modulation + 
                   gaussian_noise + impulse_noise) * regime_switch
    
    # OHLCç”Ÿæˆ
    high_prices = close_prices + np.abs(np.random.normal(0, 1.0, n_points))
    low_prices = close_prices - np.abs(np.random.normal(0, 1.0, n_points))
    open_prices = np.roll(close_prices, 1)
    open_prices[0] = close_prices[0]
    
    # ãƒœãƒªãƒ¥ãƒ¼ãƒ 
    volume = np.random.lognormal(9, 0.3, n_points)
    
    # æ—¥æ™‚
    start_date = datetime.now() - timedelta(days=n_points)
    dates = [start_date + timedelta(days=i) for i in range(n_points)]
    
    df = pd.DataFrame({
        'datetime': dates,
        'open': open_prices,
        'high': high_prices,
        'low': low_prices,
        'close': close_prices,
        'volume': volume,
        # ç†è«–å€¤ï¼ˆè©•ä¾¡ç”¨ï¼‰
        'true_trend': main_trend,
        'true_primary_cycle': primary_cycle,
        'true_secondary_cycle': secondary_cycle,
        'true_total_cycle': primary_cycle + secondary_cycle,
        'true_noise': gaussian_noise + impulse_noise
    })
    
    # datetimeã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
    df.set_index('datetime', inplace=True)
    return df


def test_all_hilbert_algorithms(data: pd.DataFrame) -> dict:
    """
    å…¨ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ãƒ†ã‚¹ãƒˆã—ã¦çµæœã‚’æ¯”è¼ƒ
    """
    algorithms = HilbertTransformUnified.get_available_algorithms()
    results = {}
    
    print("ğŸŒ€ å…¨ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
    
    for algorithm_type, description in algorithms.items():
        print(f"   ğŸ“Š {algorithm_type}: {description}")
        
        try:
            # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›è§£æå™¨åˆæœŸåŒ–
            hilbert_analyzer = HilbertTransformUnified(
                algorithm_type=algorithm_type,
                src_type='close'
            )
            
            # è§£æå®Ÿè¡Œ
            result = hilbert_analyzer.calculate(data)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
            performance = evaluate_hilbert_performance(
                original=data['close'].values,
                hilbert_result=result,
                true_trend=data.get('true_trend', None),
                true_cycle=data.get('true_total_cycle', None),
                true_noise=data.get('true_noise', None)
            )
            
            # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å›ºæœ‰æƒ…å ±ã®å–å¾—
            metadata = hilbert_analyzer.get_algorithm_metadata()
            
            results[algorithm_type] = {
                'result': result,
                'performance': performance,
                'metadata': metadata,
                'description': description
            }
            
            print(f"      âœ… æˆåŠŸ - ç·åˆã‚¹ã‚³ã‚¢: {performance['total_score']:.3f}")
            
        except Exception as e:
            print(f"      âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"         è©³ç´°: {traceback.format_exc()}")
            results[algorithm_type] = None
    
    return results


def evaluate_hilbert_performance(original: np.ndarray, hilbert_result, 
                                true_trend=None, true_cycle=None, true_noise=None) -> dict:
    """
    ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›è§£æã®æ€§èƒ½ã‚’è©•ä¾¡
    """
    if len(original) < 10:
        return {'total_score': 0.0}
    
    # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    n_points = len(original)
    
    # 1. ä½ç›¸ç²¾åº¦è©•ä¾¡
    if hilbert_result.phase is not None:
        phase_values = hilbert_result.phase
        # ä½ç›¸ã®æ»‘ã‚‰ã‹ã•ï¼ˆæ€¥æ¿€ãªå¤‰åŒ–ã‚’é¿ã‘ã‚‹ï¼‰
        phase_diff = np.diff(phase_values)
        phase_continuity = 1.0 - np.mean(np.abs(phase_diff) > np.pi) if len(phase_diff) > 0 else 0
        
        # ä½ç›¸ã®ç¯„å›²é©æ­£æ€§
        phase_range_score = 1.0 if np.all((-np.pi <= phase_values) & (phase_values <= np.pi)) else 0.5
        phase_score = (phase_continuity + phase_range_score) / 2
    else:
        phase_score = 0
    
    # 2. æŒ¯å¹…ç²¾åº¦è©•ä¾¡
    if hilbert_result.amplitude is not None:
        amplitude_values = hilbert_result.amplitude
        # æŒ¯å¹…ã®éè² æ€§
        amplitude_positive = np.all(amplitude_values >= 0) if len(amplitude_values) > 0 else False
        # æŒ¯å¹…ã®åˆç†çš„ç¯„å›²
        price_std = np.std(original)
        amplitude_reasonable = np.all(amplitude_values <= 3 * price_std) if len(amplitude_values) > 0 else False
        amplitude_score = (amplitude_positive + amplitude_reasonable) / 2
    else:
        amplitude_score = 0
    
    # 3. å‘¨æ³¢æ•°ç²¾åº¦è©•ä¾¡
    if hilbert_result.frequency is not None:
        frequency_values = hilbert_result.frequency
        # å‘¨æ³¢æ•°ã®éè² æ€§ã¨åˆç†çš„ç¯„å›²
        freq_positive = np.all(frequency_values >= 0) if len(frequency_values) > 0 else False
        freq_reasonable = np.all(frequency_values <= 0.5) if len(frequency_values) > 0 else False  # ãƒŠã‚¤ã‚­ã‚¹ãƒˆå‘¨æ³¢æ•°ä»¥ä¸‹
        frequency_score = (freq_positive + freq_reasonable) / 2
    else:
        frequency_score = 0
    
    # 4. ä¿¡å·è¿½å¾“æ€§ï¼ˆåŸä¿¡å·ã¨ã®ç›¸é–¢ï¼‰
    # æŒ¯å¹…ã‚’ä½¿ç”¨ã—ã¦åŸä¿¡å·ã¨ã®è¿½å¾“æ€§ã‚’è©•ä¾¡
    hilbert_values = hilbert_result.amplitude
    valid_mask = ~np.isnan(hilbert_values)
    
    if np.sum(valid_mask) > 10:
        # æŒ¯å¹…ã®å¤‰åŒ–ã¨ä¾¡æ ¼ã®å¤‰åŒ–ã®ç›¸é–¢ã‚’è¨ˆç®—
        price_changes = np.diff(original[valid_mask])
        amplitude_changes = np.diff(hilbert_values[valid_mask])
        if len(price_changes) > 10 and len(amplitude_changes) > 10:
            correlation = np.corrcoef(price_changes, amplitude_changes)[0, 1]
            signal_tracking = max(0, correlation) if not np.isnan(correlation) else 0
        else:
            signal_tracking = 0
    else:
        signal_tracking = 0
    
    # 5. ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†è©•ä¾¡
    if hasattr(hilbert_result, 'trend_component') and hilbert_result.trend_component is not None and true_trend is not None:
        trend_component = hilbert_result.trend_component
        valid_trend_mask = ~(np.isnan(trend_component) | np.isnan(true_trend))
        if np.sum(valid_trend_mask) > 10:
            trend_correlation = np.corrcoef(trend_component[valid_trend_mask], 
                                          true_trend[valid_trend_mask])[0, 1]
            trend_accuracy = max(0, trend_correlation) if not np.isnan(trend_correlation) else 0
        else:
            trend_accuracy = 0
    else:
        trend_accuracy = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # 6. ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºèƒ½åŠ›
    if hasattr(hilbert_result, 'cycle_component') and hilbert_result.cycle_component is not None and true_cycle is not None:
        cycle_component = hilbert_result.cycle_component
        valid_cycle_mask = ~(np.isnan(cycle_component) | np.isnan(true_cycle))
        if np.sum(valid_cycle_mask) > 10:
            cycle_correlation = np.corrcoef(cycle_component[valid_cycle_mask], 
                                          true_cycle[valid_cycle_mask])[0, 1]
            cycle_accuracy = max(0, cycle_correlation) if not np.isnan(cycle_correlation) else 0
        else:
            cycle_accuracy = 0
    else:
        cycle_accuracy = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # 7. è¨ˆç®—åŠ¹ç‡æ€§ï¼ˆé…å»¶è©•ä¾¡ï¼‰
    nan_count = np.sum(np.isnan(hilbert_result.amplitude))
    delay_score = max(0, 1 - nan_count / n_points)
    
    # 8. é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹è©•ä¾¡ï¼ˆé‡å­å¼·åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç”¨ï¼‰
    if hasattr(hilbert_result, 'quantum_coherence') and hilbert_result.quantum_coherence is not None:
        coherence_score = np.nanmean(hilbert_result.quantum_coherence)
    else:
        coherence_score = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # 9. ä¿¡é ¼åº¦è©•ä¾¡
    if hasattr(hilbert_result, 'confidence') and hilbert_result.confidence is not None:
        confidence_score = np.nanmean(hilbert_result.confidence)
    else:
        confidence_score = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # 10. ç·åˆã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
    weights = [0.15, 0.15, 0.15, 0.15, 0.1, 0.1, 0.1, 0.05, 0.05]
    scores = [phase_score, amplitude_score, frequency_score, signal_tracking, 
              trend_accuracy, cycle_accuracy, delay_score, coherence_score, confidence_score]
    total_score = sum(w * s for w, s in zip(weights, scores))
    
    return {
        'phase_score': phase_score,
        'amplitude_score': amplitude_score,
        'frequency_score': frequency_score,
        'signal_tracking': signal_tracking,
        'trend_accuracy': trend_accuracy,
        'cycle_accuracy': cycle_accuracy,
        'delay_score': delay_score,
        'coherence_score': coherence_score,
        'confidence_score': confidence_score,
        'total_score': total_score
    }


def visualize_hilbert_comparison(data: pd.DataFrame, results: dict, save_path: str = None):
    """
    ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›è§£æçµæœã®æ¯”è¼ƒå¯è¦–åŒ–
    """
    # æœ‰åŠ¹ãªçµæœã®ã¿æŠ½å‡º
    valid_results = {k: v for k, v in results.items() if v is not None}
    n_algorithms = len(valid_results)
    
    if n_algorithms == 0:
        print("è¡¨ç¤ºå¯èƒ½ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # å›³ã®è¨­å®š
    fig = plt.figure(figsize=(24, 18))
    fig.patch.set_facecolor('black')
    
    # ã‚°ãƒªãƒƒãƒ‰è¨­å®š
    rows = 5
    cols = 3
    
    colors = ['cyan', 'yellow', 'lime', 'magenta', 'orange', 'red', 'pink', 'lightblue']
    
    # 1. åŸä¿¡å· vs ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›çµæœ
    ax1 = plt.subplot(rows, cols, 1)
    ax1.plot(data.index, data['close'], 'white', alpha=0.8, linewidth=1.5, label='Original Signal')
    
    for i, (algorithm_type, result_data) in enumerate(valid_results.items()):
        if result_data and result_data['result']:
            # æŒ¯å¹…ã‚’ä½¿ç”¨ã—ã¦çµæœã‚’è¡¨ç¤º
            ax1.plot(data.index, result_data['result'].amplitude, 
                    colors[i % len(colors)], alpha=0.8, linewidth=2, 
                    label=f"{algorithm_type}")
    
    ax1.set_title('ğŸŒ€ Hilbert Transform Comparison', fontsize=14, color='white', fontweight='bold')
    ax1.set_ylabel('Price', color='white')
    ax1.legend(fontsize=9, loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # 2. æ€§èƒ½ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
    ax2 = plt.subplot(rows, cols, 2)
    algorithm_names = []
    total_scores = []
    
    for algorithm_type, result_data in valid_results.items():
        if result_data and result_data['performance']:
            algorithm_names.append(algorithm_type.replace('_', '\n'))
            total_scores.append(result_data['performance']['total_score'])
    
    bars = ax2.bar(algorithm_names, total_scores, color=colors[:len(algorithm_names)], alpha=0.8)
    ax2.set_title('ğŸ“Š Performance Scores', fontsize=14, color='white', fontweight='bold')
    ax2.set_ylabel('Total Score', color='white')
    ax2.set_ylim(0, 1)
    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=10)
    
    # ã‚¹ã‚³ã‚¢å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, score in zip(bars, total_scores):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{score:.3f}', ha='center', va='bottom', color='white', fontsize=9)
    
    ax2.grid(True, alpha=0.3)
    
    # 3. æŒ¯å¹…æˆåˆ†æ¯”è¼ƒ
    ax3 = plt.subplot(rows, cols, 3)
    for i, (algorithm_type, result_data) in enumerate(valid_results.items()):
        if (result_data and result_data['result'] and 
            result_data['result'].amplitude is not None):
            ax3.plot(data.index, result_data['result'].amplitude,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{algorithm_type}")
    
    ax3.set_title('ğŸ“ˆ Amplitude Components', fontsize=14, color='white', fontweight='bold')
    ax3.set_ylabel('Amplitude', color='white')
    ax3.legend(fontsize=9)
    ax3.grid(True, alpha=0.3)
    
    # 4. ä½ç›¸æˆåˆ†æ¯”è¼ƒ
    ax4 = plt.subplot(rows, cols, 4)
    for i, (algorithm_type, result_data) in enumerate(valid_results.items()):
        if (result_data and result_data['result'] and 
            result_data['result'].phase is not None):
            ax4.plot(data.index, result_data['result'].phase,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{algorithm_type}")
    
    ax4.set_title('ğŸ”„ Phase Components', fontsize=14, color='white', fontweight='bold')
    ax4.set_ylabel('Phase (radians)', color='white')
    ax4.legend(fontsize=9)
    ax4.grid(True, alpha=0.3)
    
    # 5. å‘¨æ³¢æ•°æˆåˆ†æ¯”è¼ƒ
    ax5 = plt.subplot(rows, cols, 5)
    for i, (algorithm_type, result_data) in enumerate(valid_results.items()):
        if (result_data and result_data['result'] and 
            result_data['result'].frequency is not None):
            ax5.plot(data.index, result_data['result'].frequency,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{algorithm_type}")
    
    ax5.set_title('âš¡ Frequency Components', fontsize=14, color='white', fontweight='bold')
    ax5.set_ylabel('Frequency', color='white')
    ax5.legend(fontsize=9)
    ax5.grid(True, alpha=0.3)
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
    ranking = sorted([(k, v['performance']['total_score']) for k, v in valid_results.items() 
                     if v and v['performance']], key=lambda x: x[1], reverse=True)
    
    # 6. ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†æ¯”è¼ƒ
    ax6 = plt.subplot(rows, cols, 6)
    if 'true_trend' in data.columns:
        ax6.plot(data.index, data['true_trend'], 'white', alpha=0.8, linewidth=2, label='True Trend')
    
    for i, (algorithm_type, result_data) in enumerate(valid_results.items()):
        if (result_data and result_data['result'] and 
            hasattr(result_data['result'], 'trend_component') and
            result_data['result'].trend_component is not None):
            ax6.plot(data.index, result_data['result'].trend_component,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{algorithm_type}")
    
    ax6.set_title('ğŸ“ˆ Trend Components', fontsize=14, color='white', fontweight='bold')
    ax6.set_ylabel('Trend Value', color='white')
    ax6.legend(fontsize=9)
    ax6.grid(True, alpha=0.3)
    
    # 7. è©³ç´°æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆæœ€é«˜æ€§èƒ½ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
    ax7 = plt.subplot(rows, cols, 7)
    metrics = ['phase_score', 'amplitude_score', 'frequency_score', 'signal_tracking', 
               'trend_accuracy', 'cycle_accuracy', 'delay_score']
    metric_labels = ['Phase\nScore', 'Amplitude\nScore', 'Frequency\nScore', 'Signal\nTracking',
                    'Trend\nAccuracy', 'Cycle\nAccuracy', 'Low\nDelay']
    
    # æœ€é«˜æ€§èƒ½ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç‰¹å®š
    if ranking:
        best_algorithm = ranking[0][0]
        
        if best_algorithm and valid_results[best_algorithm]:
            perf = valid_results[best_algorithm]['performance']
            values = [perf[metric] for metric in metrics]
            
            bars = ax7.bar(metric_labels, values, color='lime', alpha=0.8)
            ax7.set_title(f'ğŸ† Best: {best_algorithm}', fontsize=14, color='white', fontweight='bold')
            ax7.set_ylabel('Score', color='white')
            ax7.set_ylim(0, 1)
            plt.setp(ax7.get_xticklabels(), rotation=45, ha='right', fontsize=9)
            
            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for bar, value in zip(bars, values):
                ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.2f}', ha='center', va='bottom', color='white', fontsize=9)
            
            ax7.grid(True, alpha=0.3)
    
    # 8. é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ï¼ˆé‡å­å¼·åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ç”¨ï¼‰
    ax8 = plt.subplot(rows, cols, 8)
    quantum_found = False
    for i, (algorithm_type, result_data) in enumerate(valid_results.items()):
        if (result_data and result_data['result'] and 
            hasattr(result_data['result'], 'quantum_coherence') and
            result_data['result'].quantum_coherence is not None):
            ax8.plot(data.index, result_data['result'].quantum_coherence,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{algorithm_type}")
            quantum_found = True
    
    if quantum_found:
        ax8.set_title('ğŸŒŒ Quantum Coherence', fontsize=14, color='white', fontweight='bold')
        ax8.set_ylabel('Coherence', color='white')
        ax8.legend(fontsize=9)
        ax8.grid(True, alpha=0.3)
    else:
        ax8.text(0.5, 0.5, 'é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹\nãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center',
                transform=ax8.transAxes, color='white', fontsize=12)
        ax8.set_title('ğŸŒŒ Quantum Coherence', fontsize=14, color='white', fontweight='bold')
    
    # 9. å€‹åˆ¥æ€§èƒ½æ¯”è¼ƒï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆé¢¨ï¼‰
    ax9 = plt.subplot(rows, cols, 9)
    metrics_short = ['Phase', 'Amplitude', 'Frequency', 'Tracking']
    n_metrics = len(metrics_short)
    
    for i, (algorithm_type, result_data) in enumerate(list(valid_results.items())[:4]):  # ä¸Šä½4ã¤ã®ã¿
        if result_data and result_data['performance']:
            perf = result_data['performance']
            values = [perf['phase_score'], perf['amplitude_score'], 
                     perf['frequency_score'], perf['signal_tracking']]
            
            x_pos = np.arange(n_metrics)
            ax9.plot(x_pos, values, 'o-', color=colors[i % len(colors)], 
                    linewidth=2, markersize=6, alpha=0.8, label=algorithm_type)
    
    ax9.set_title('âš¡ Multi-Metric Comparison', fontsize=14, color='white', fontweight='bold')
    ax9.set_xticks(range(n_metrics))
    ax9.set_xticklabels(metrics_short, fontsize=9)
    ax9.set_ylabel('Score', color='white')
    ax9.set_ylim(0, 1)
    ax9.legend(fontsize=9)
    ax9.grid(True, alpha=0.3)
    
    # 10. ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨
    ax10 = plt.subplot(rows, cols, 10)
    ax10.axis('off')
    
    ranking_text = "ğŸ† Hilbert Algorithm Ranking:\n\n"
    for i, (algorithm_name, score) in enumerate(ranking):
        emoji = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1}."
        ranking_text += f"{emoji} {algorithm_name}:\n    {score:.3f}\n"
    
    ax10.text(0.05, 0.95, ranking_text, transform=ax10.transAxes, fontsize=11, 
            color='white', verticalalignment='top', fontfamily='monospace')
    
    # 11. è¨ˆç®—æ™‚é–“ãƒ»åŠ¹ç‡æ€§ï¼ˆä»®æƒ³ãƒ‡ãƒ¼ã‚¿ï¼‰
    ax11 = plt.subplot(rows, cols, 11)
    computation_times = [np.random.uniform(0.05, 1.5) for _ in valid_results]  # ä»®æƒ³æ™‚é–“
    algorithm_names_short = [name.replace('_', '\n') for name in valid_results.keys()]
    
    bars = ax11.bar(algorithm_names_short, computation_times, 
                   color=colors[:len(computation_times)], alpha=0.8)
    ax11.set_title('â±ï¸ Computation Time', fontsize=14, color='white', fontweight='bold')
    ax11.set_ylabel('Time (seconds)', color='white')
    plt.setp(ax11.get_xticklabels(), rotation=45, ha='right', fontsize=9)
    ax11.grid(True, alpha=0.3)
    
    # 12. æ¨å¥¨ç”¨é€”
    ax12 = plt.subplot(rows, cols, 12)
    ax12.axis('off')
    
    recommendations = {
        'basic': 'é«˜é€Ÿå‡¦ç†\nåŸºæœ¬è§£æ',
        'quantum_enhanced': 'é«˜ç²¾åº¦è§£æ\né‡å­å¼·åŒ–',
        'instantaneous': 'è©³ç´°åˆ†æ\nç¬æ™‚è§£æ',
        'instantaneous_v2': 'é«˜é€Ÿåˆ†æ\nç°¡æ˜“ç‰ˆ',
        'supreme': 'æœ€é«˜ç²¾åº¦\n9ç‚¹FIR',
        'numpy_fft': 'FFTè¿‘ä¼¼\nå‘¨æ³¢æ•°ç‰¹åŒ–',
        'multiresolution': 'ãƒãƒ«ãƒè§£åƒåº¦\nã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆçµ±åˆ'
    }
    
    rec_text = "ğŸ’¡ æ¨å¥¨ç”¨é€”:\n\n"
    for i, (algorithm_type, _) in enumerate(ranking[:6]):
        rec = recommendations.get(algorithm_type, 'æ±ç”¨')
        rec_text += f"â€¢ {algorithm_type}:\n  {rec}\n\n"
    
    ax12.text(0.05, 0.95, rec_text, transform=ax12.transAxes, fontsize=10,
            color='white', verticalalignment='top')
    
    # 13. ä¿¡å·åˆ†è§£æˆåˆ†ï¼ˆä¸Šä½3ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰
    ax13 = plt.subplot(rows, cols, 13)
    for i, (algorithm_type, _) in enumerate(ranking[:3]):
        result_data = valid_results[algorithm_type]
        if (result_data and result_data['result'] and 
            hasattr(result_data['result'], 'cycle_component') and
            result_data['result'].cycle_component is not None):
            ax13.plot(data.index, result_data['result'].cycle_component,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{algorithm_type}")
    
    if 'true_total_cycle' in data.columns:
        ax13.plot(data.index, data['true_total_cycle'], 'white', alpha=0.8, 
                 linewidth=2, label='True Cycle')
    
    ax13.set_title('ğŸ”„ Cycle Decomposition', fontsize=14, color='white', fontweight='bold')
    ax13.set_ylabel('Cycle Component', color='white')
    ax13.legend(fontsize=9)
    ax13.grid(True, alpha=0.3)
    
    # 14. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    ax14 = plt.subplot(rows, cols, 14)
    confidence_found = False
    for i, (algorithm_type, result_data) in enumerate(valid_results.items()):
        if (result_data and result_data['result'] and 
            hasattr(result_data['result'], 'confidence') and
            result_data['result'].confidence is not None):
            ax14.plot(data.index, result_data['result'].confidence,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{algorithm_type}")
            confidence_found = True
    
    if confidence_found:
        ax14.set_title('ğŸ¯ Confidence Scores', fontsize=14, color='white', fontweight='bold')
        ax14.set_ylabel('Confidence', color='white')
        ax14.legend(fontsize=9)
        ax14.grid(True, alpha=0.3)
    else:
        ax14.text(0.5, 0.5, 'ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢\nãƒ‡ãƒ¼ã‚¿ãªã—', ha='center', va='center',
                 transform=ax14.transAxes, color='white', fontsize=12)
        ax14.set_title('ğŸ¯ Confidence Scores', fontsize=14, color='white', fontweight='bold')
    
    # 15. ç·åˆè©•ä¾¡ã‚µãƒãƒªãƒ¼
    ax15 = plt.subplot(rows, cols, 15)
    ax15.axis('off')
    
    if ranking:
        best_name, best_score = ranking[0]
        summary_text = f"ğŸ‰ ç·åˆè©•ä¾¡çµæœ\n\n"
        summary_text += f"ğŸ† æœ€å„ªç§€: {best_name}\n"
        summary_text += f"ğŸ“Š ã‚¹ã‚³ã‚¢: {best_score:.3f}\n\n"
        summary_text += f"ğŸ“ˆ ãƒ†ã‚¹ãƒˆæˆåŠŸ: {len(valid_results)}/{len(results)}\n"
        summary_text += f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}\n"
        summary_text += f"ğŸ’¹ ä¾¡æ ¼ç¯„å›²: ${data['close'].min():.1f}-${data['close'].max():.1f}\n"
        summary_text += f"ğŸ“… æœŸé–“: {data.index[0].strftime('%Y-%m-%d')}\n"
        summary_text += f"     ï½ {data.index[-1].strftime('%Y-%m-%d')}"
        
        ax15.text(0.05, 0.95, summary_text, transform=ax15.transAxes, fontsize=11,
                color='white', verticalalignment='top', fontfamily='monospace')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, facecolor='black', edgecolor='none', dpi=300, bbox_inches='tight')
        print(f"ğŸŒ€ ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›æ¯”è¼ƒçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    
    plt.show()


def detailed_hilbert_analysis(algorithm_type: str, data: pd.DataFrame):
    """
    ç‰¹å®šãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°åˆ†æ
    """
    print(f"\nğŸ” è©³ç´°åˆ†æ: {algorithm_type}")
    print("=" * 60)
    
    # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›å®Ÿè¡Œ
    try:
        hilbert_analyzer = HilbertTransformUnified(algorithm_type=algorithm_type, src_type='close')
        result = hilbert_analyzer.calculate(data)
        metadata = hilbert_analyzer.get_algorithm_metadata()
    except Exception as e:
        print(f"âŒ ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # åŸºæœ¬æƒ…å ±è¡¨ç¤º
    print(f"ğŸ“Š åŸºæœ¬æƒ…å ±:")
    print(f"   ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {metadata.get('algorithm', 'N/A')}")
    print(f"   èª¬æ˜: {metadata.get('description', 'N/A')}")
    print(f"   ä¾¡æ ¼ã‚½ãƒ¼ã‚¹: {metadata.get('src_type', 'N/A')}")
    print(f"   é•·ã•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {metadata.get('length', 'N/A')}")
    
    # æˆåˆ†åˆ†æ
    print(f"\nğŸ”¬ æˆåˆ†åˆ†æ:")
    if result.amplitude is not None:
        amp_mean = np.nanmean(result.amplitude)
        amp_std = np.nanstd(result.amplitude)
        print(f"   æŒ¯å¹…æˆåˆ†å¹³å‡: {amp_mean:.4f}")
        print(f"   æŒ¯å¹…æˆåˆ†æ¨™æº–åå·®: {amp_std:.4f}")
    
    if result.phase is not None:
        phase_mean = np.nanmean(result.phase)
        phase_std = np.nanstd(result.phase)
        print(f"   ä½ç›¸æˆåˆ†å¹³å‡: {phase_mean:.4f} rad")
        print(f"   ä½ç›¸æˆåˆ†æ¨™æº–åå·®: {phase_std:.4f} rad")
    
    if result.frequency is not None:
        freq_mean = np.nanmean(result.frequency)
        freq_std = np.nanstd(result.frequency)
        print(f"   å‘¨æ³¢æ•°æˆåˆ†å¹³å‡: {freq_mean:.4f}")
        print(f"   å‘¨æ³¢æ•°æˆåˆ†æ¨™æº–åå·®: {freq_std:.4f}")
    
    # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹åˆ†æï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
    if hasattr(result, 'quantum_coherence') and result.quantum_coherence is not None:
        qc_mean = np.nanmean(result.quantum_coherence)
        qc_std = np.nanstd(result.quantum_coherence)
        print(f"   é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹å¹³å‡: {qc_mean:.4f}")
        print(f"   é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ¨™æº–åå·®: {qc_std:.4f}")
    
    # æ€§èƒ½è©•ä¾¡
    performance = evaluate_hilbert_performance(
        data['close'].values, result,
        data.get('true_trend', None), data.get('true_total_cycle', None), data.get('true_noise', None)
    )
    
    print(f"\nğŸ¯ æ€§èƒ½è©•ä¾¡:")
    print(f"   ä½ç›¸ç²¾åº¦: {performance['phase_score']:.3f}")
    print(f"   æŒ¯å¹…ç²¾åº¦: {performance['amplitude_score']:.3f}")
    print(f"   å‘¨æ³¢æ•°ç²¾åº¦: {performance['frequency_score']:.3f}")
    print(f"   ä¿¡å·è¿½å¾“æ€§: {performance['signal_tracking']:.3f}")
    print(f"   ãƒˆãƒ¬ãƒ³ãƒ‰æŠ½å‡ºç²¾åº¦: {performance['trend_accuracy']:.3f}")
    print(f"   ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºèƒ½åŠ›: {performance['cycle_accuracy']:.3f}")
    print(f"   è¨ˆç®—åŠ¹ç‡æ€§: {performance['delay_score']:.3f}")
    print(f"   é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹: {performance['coherence_score']:.3f}")
    print(f"   ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {performance['confidence_score']:.3f}")
    print(f"   ç·åˆã‚¹ã‚³ã‚¢: {performance['total_score']:.3f}")


def load_real_market_data(config_path: str = 'config.yaml', n_recent: int = 500) -> pd.DataFrame:
    """
    config.yamlã‹ã‚‰å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆç›´è¿‘n_recentæœ¬ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
    """
    try:
        print(f"ğŸ“¡ {config_path} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­... (ç›´è¿‘{n_recent}æœ¬)")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        binance_config = config.get('binance_data', {})
        data_dir = binance_config.get('data_dir', 'data/binance')
        binance_data_source = BinanceDataSource(data_dir)
        
        # CSVãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¯ãƒ€ãƒŸãƒ¼ã¨ã—ã¦æ¸¡ã™ï¼ˆBinanceãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ã¿ã‚’ä½¿ç”¨ï¼‰
        dummy_csv_source = CSVDataSource("dummy")
        data_loader = DataLoader(
            data_source=dummy_csv_source,
            binance_data_source=binance_data_source
        )
        data_processor = DataProcessor()
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
        raw_data = data_loader.load_data_from_config(config)
        processed_data = {
            symbol: data_processor.process(df)
            for symbol, df in raw_data.items()
        }
        
        # æœ€åˆã®ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        first_symbol = next(iter(processed_data))
        data = processed_data[first_symbol]
        
        # ç›´è¿‘n_recentæœ¬ã«é™å®š
        if len(data) > n_recent:
            data = data.tail(n_recent).copy()
        
        print(f"âœ… å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {first_symbol}")
        print(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
        print(f"ğŸ“… æœŸé–“: {data.index[0].strftime('%Y-%m-%d')} ï½ {data.index[-1].strftime('%Y-%m-%d')}")
        return data
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"ğŸ”„ åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ (ãƒ‡ãƒ¼ã‚¿æ•°: {n_recent})")
        return create_synthetic_data(n_recent)


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸŒ€ Hilbert Transform Unified Demo V1.0")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\n1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿æº–å‚™")
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’è©¦ã—ã€å¤±æ•—ã—ãŸã‚‰åˆæˆãƒ‡ãƒ¼ã‚¿ï¼ˆç›´è¿‘500æœ¬ï¼‰
    data = load_real_market_data('config.yaml', n_recent=500)
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    print(f"   æœŸé–“: {len(data)} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
    print(f"   ä¾¡æ ¼ç¯„å›²: ${data['close'].min():.2f} - ${data['close'].max():.2f}")
    print(f"   å¹³å‡ä¾¡æ ¼: ${data['close'].mean():.2f}")
    print(f"   ä¾¡æ ¼æ¨™æº–åå·®: ${data['close'].std():.2f}")
    
    # å…¨ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ†ã‚¹ãƒˆ
    print("\n2ï¸âƒ£ å…¨ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    results = test_all_hilbert_algorithms(data)
    
    # çµæœæ¯”è¼ƒãƒ»å¯è¦–åŒ–
    print("\n3ï¸âƒ£ çµæœã®å¯è¦–åŒ–")
    output_path = os.path.join('output', 'hilbert_unified_comparison.png')
    os.makedirs('output', exist_ok=True)
    visualize_hilbert_comparison(data, results, output_path)
    
    # è©³ç´°åˆ†æ
    print("\n4ï¸âƒ£ è©³ç´°åˆ†æ")
    
    # æœ€é«˜æ€§èƒ½ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°åˆ†æ
    valid_results = {k: v for k, v in results.items() if v is not None}
    if valid_results:
        best_algorithm = max(valid_results.keys(), 
                          key=lambda k: valid_results[k]['performance']['total_score'])
        detailed_hilbert_analysis(best_algorithm, data)
    
    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
    print("\nğŸ“‹ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)
    
    valid_count = len(valid_results)
    total_algorithms = len(HilbertTransformUnified.get_available_algorithms())
    
    print(f"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: {valid_count}/{total_algorithms} ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ")
    
    if valid_results:
        # ãƒˆãƒƒãƒ—3
        ranking = sorted([(k, v['performance']['total_score']) for k, v in valid_results.items()], 
                        key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ† ãƒˆãƒƒãƒ—3ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :")
        for i, (name, score) in enumerate(ranking[:3]):
            emoji = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
            print(f"   {emoji} {name}: {score:.3f}")
        
        # æ¨å¥¨ç”¨é€”
        print(f"\nğŸ’¡ æ¨å¥¨ç”¨é€”:")
        best_algorithm_name = ranking[0][0]
        if best_algorithm_name == 'basic':
            print("   - é«˜é€Ÿå‡¦ç†ãŒå¿…è¦ãªå ´åˆã¯ basic ãŒãŠå‹§ã‚")
        elif best_algorithm_name == 'quantum_enhanced':
            print("   - é«˜ç²¾åº¦è§£æã«ã¯ quantum_enhanced ãŒãŠå‹§ã‚")
        elif best_algorithm_name == 'instantaneous':
            print("   - è©³ç´°ãªç¬æ™‚è§£æã«ã¯ instantaneous ãŒãŠå‹§ã‚")
        elif best_algorithm_name == 'supreme':
            print("   - æœ€é«˜ç²¾åº¦ã®è§£æã«ã¯ supreme (9ç‚¹FIR) ãŒãŠå‹§ã‚")
        elif best_algorithm_name == 'multiresolution':
            print("   - ãƒãƒ«ãƒè§£åƒåº¦è§£æã«ã¯ multiresolution ãŒãŠå‹§ã‚")
        else:
            print(f"   - {best_algorithm_name} ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæœ€é©ã§ã™")
    
    print(f"\nğŸ‰ ãƒ‡ãƒ¢å®Œäº†!")
    print(f"ğŸŒ€ çµæœã¯ {output_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    print(f"\nğŸ“ å„ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç‰¹å¾´:")
    for algorithm_type, description in HilbertTransformUnified.get_available_algorithms().items():
        print(f"   â€¢ {algorithm_type}: {description}")


if __name__ == "__main__":
    main() 