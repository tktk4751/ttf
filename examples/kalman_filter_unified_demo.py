#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ **Kalman Filter Unified Demo V1.0** ğŸ¯

çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- çµæœã®å¯è¦–åŒ–
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')
import yaml

from indicators.kalman_filter_unified import KalmanFilterUnified
# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from data.data_loader import DataLoader, CSVDataSource
from data.data_processor import DataProcessor
from data.binance_data_source import BinanceDataSource

plt.style.use('dark_background')


def create_synthetic_data(n_points: int = 1000) -> pd.DataFrame:
    """
    åˆæˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    """
    np.random.seed(42)
    
    # åŸºæœ¬ãƒˆãƒ¬ãƒ³ãƒ‰
    t = np.linspace(0, 10, n_points)
    trend = 100 + 10 * np.sin(t * 0.5) + 0.5 * t
    
    # ã‚µã‚¤ã‚¯ãƒ«æˆåˆ†
    cycle1 = 5 * np.sin(t * 2)
    cycle2 = 3 * np.cos(t * 3)
    
    # ãƒã‚¤ã‚º
    noise = np.random.normal(0, 2, n_points)
    
    # çªç™ºçš„ãªã‚¹ãƒ‘ã‚¤ã‚¯
    spikes = np.zeros(n_points)
    spike_indices = np.random.choice(n_points, size=20, replace=False)
    spikes[spike_indices] = np.random.normal(0, 10, 20)
    
    # æœ€çµ‚ä¾¡æ ¼
    close_prices = trend + cycle1 + cycle2 + noise + spikes
    
    # OHLCç”Ÿæˆ
    high_prices = close_prices + np.abs(np.random.normal(0, 1, n_points))
    low_prices = close_prices - np.abs(np.random.normal(0, 1, n_points))
    open_prices = np.roll(close_prices, 1)
    open_prices[0] = close_prices[0]
    
    # ãƒœãƒªãƒ¥ãƒ¼ãƒ 
    volume = np.random.lognormal(10, 0.5, n_points)
    
    # æ—¥æ™‚
    start_date = datetime.now() - timedelta(days=n_points)
    dates = [start_date + timedelta(days=i) for i in range(n_points)]
    
    df = pd.DataFrame({
        'datetime': dates,
        'open': open_prices,
        'high': high_prices,
        'low': low_prices,
        'close': close_prices,
        'volume': volume
    })
    
    # datetimeã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
    df.set_index('datetime', inplace=True)
    return df


def test_all_filters(data: pd.DataFrame) -> dict:
    """
    å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆã—ã¦çµæœã‚’æ¯”è¼ƒ
    """
    filters = KalmanFilterUnified.get_available_filters()
    results = {}
    
    print("ğŸ§ª å…¨ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
    
    for filter_type, description in filters.items():
        print(f"   ğŸ“Š {filter_type}: {description}")
        
        try:
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆæœŸåŒ–
            kalman_filter = KalmanFilterUnified(
                filter_type=filter_type,
                src_type='close',
                base_process_noise=0.001,
                base_measurement_noise=0.01,
                volatility_window=10
            )
            
            # è¨ˆç®—å®Ÿè¡Œ
            result = kalman_filter.calculate(data)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
            performance = evaluate_filter_performance(
                original=data['close'].values,
                filtered=result.filtered_values,
                confidence=result.confidence_scores
            )
            
            results[filter_type] = {
                'result': result,
                'performance': performance,
                'metadata': kalman_filter.get_filter_metadata(),
                'description': description
            }
            
            print(f"      âœ… æˆåŠŸ - æ€§èƒ½ã‚¹ã‚³ã‚¢: {performance['total_score']:.3f}")
            
        except Exception as e:
            print(f"      âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"         è©³ç´°: {traceback.format_exc()}")
            results[filter_type] = None
    
    return results


def evaluate_filter_performance(original: np.ndarray, filtered: np.ndarray, confidence: np.ndarray) -> dict:
    """
    ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æ€§èƒ½ã‚’è©•ä¾¡
    """
    if len(original) != len(filtered) or len(original) < 10:
        return {'total_score': 0.0}
    
    # 1. ãƒã‚¤ã‚ºé™¤å»åŠ¹æœï¼ˆå·®åˆ†ã®æ¨™æº–åå·®æ¯”è¼ƒï¼‰
    original_noise = np.std(np.diff(original))
    filtered_noise = np.std(np.diff(filtered))
    noise_reduction = max(0, 1 - filtered_noise / original_noise) if original_noise > 0 else 0
    
    # 2. ä¾¡æ ¼è¿½å¾“æ€§ï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰
    tracking_error = np.mean(np.abs(filtered - original))
    price_std = np.std(original)
    tracking_score = max(0, 1 - tracking_error / price_std) if price_std > 0 else 0
    
    # 3. æ»‘ã‚‰ã‹ã•ï¼ˆäºŒæ¬¡å·®åˆ†ã®åˆ†æ•£ï¼‰
    filtered_smooth = np.var(np.diff(filtered, n=2)) if len(filtered) > 2 else 1.0
    original_smooth = np.var(np.diff(original, n=2)) if len(original) > 2 else 1.0
    smoothness_score = max(0, 1 - filtered_smooth / original_smooth) if original_smooth > 0 else 0
    
    # 4. ä¿¡é ¼åº¦å¹³å‡
    confidence_score = np.nanmean(confidence) if len(confidence) > 0 else 0
    
    # 5. é…å»¶è©•ä¾¡ï¼ˆãƒ”ãƒ¼ã‚¯æ¤œå‡ºã§ã®é…å»¶æ¸¬å®šï¼‰
    delay_score = calculate_delay_score(original, filtered)
    
    # 6. ç·åˆã‚¹ã‚³ã‚¢ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
    weights = [0.25, 0.25, 0.2, 0.15, 0.15]  # [ãƒã‚¤ã‚ºé™¤å», è¿½å¾“æ€§, æ»‘ã‚‰ã‹ã•, ä¿¡é ¼åº¦, é…å»¶]
    scores = [noise_reduction, tracking_score, smoothness_score, confidence_score, delay_score]
    total_score = sum(w * s for w, s in zip(weights, scores))
    
    return {
        'noise_reduction': noise_reduction,
        'tracking_score': tracking_score,
        'smoothness_score': smoothness_score,
        'confidence_score': confidence_score,
        'delay_score': delay_score,
        'total_score': total_score
    }


def calculate_delay_score(original: np.ndarray, filtered: np.ndarray) -> float:
    """
    é…å»¶ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒ”ãƒ¼ã‚¯æ¤œå‡ºã«ã‚ˆã‚‹ï¼‰
    """
    try:
        from scipy.signal import find_peaks
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        orig_peaks, _ = find_peaks(original, height=np.percentile(original, 70))
        filt_peaks, _ = find_peaks(filtered, height=np.percentile(filtered, 70))
        
        if len(orig_peaks) < 2 or len(filt_peaks) < 2:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚³ã‚¢
        
        # æœ€ã‚‚è¿‘ã„ãƒ”ãƒ¼ã‚¯é–“ã®é…å»¶ã‚’è¨ˆç®—
        delays = []
        for op in orig_peaks[:10]:  # æœ€åˆã®10ãƒ”ãƒ¼ã‚¯ã®ã¿
            distances = np.abs(filt_peaks - op)
            min_delay = np.min(distances)
            delays.append(min_delay)
        
        avg_delay = np.mean(delays)
        max_acceptable_delay = len(original) * 0.05  # 5%ã¾ã§ã®é…å»¶ã¯è¨±å®¹
        
        delay_score = max(0, 1 - avg_delay / max_acceptable_delay)
        return min(1.0, delay_score)
        
    except ImportError:
        # scipyç„¡ã„å ´åˆã¯ç°¡æ˜“è©•ä¾¡
        correlation = np.corrcoef(original[1:], filtered[:-1])[0, 1]
        return max(0, correlation) if not np.isnan(correlation) else 0.5


def visualize_comparison(data: pd.DataFrame, results: dict, save_path: str = None):
    """
    å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æ¯”è¼ƒçµæœã‚’å¯è¦–åŒ–
    """
    # æœ‰åŠ¹ãªçµæœã®ã¿æŠ½å‡º
    valid_results = {k: v for k, v in results.items() if v is not None}
    n_filters = len(valid_results)
    
    if n_filters == 0:
        print("è¡¨ç¤ºå¯èƒ½ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # å›³ã®è¨­å®š
    fig = plt.figure(figsize=(20, 12))
    fig.patch.set_facecolor('black')
    
    # ã‚°ãƒªãƒƒãƒ‰è¨­å®š
    rows = 3
    cols = 2
    
    # 1. ä¾¡æ ¼æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
    ax1 = plt.subplot(rows, cols, 1)
    ax1.plot(data.index, data['close'], 'white', alpha=0.7, linewidth=1, label='Original')
    
    colors = ['cyan', 'yellow', 'lime', 'magenta', 'orange', 'red']
    for i, (filter_type, result_data) in enumerate(valid_results.items()):
        if result_data and result_data['result']:
            ax1.plot(data.index, result_data['result'].filtered_values, 
                    colors[i % len(colors)], alpha=0.8, linewidth=2, 
                    label=f"{filter_type}")
    
    ax1.set_title('ğŸ¯ Kalman Filters Comparison', fontsize=14, color='white', fontweight='bold')
    ax1.set_ylabel('Price', color='white')
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    
    # 2. æ€§èƒ½ã‚¹ã‚³ã‚¢æ¯”è¼ƒ
    ax2 = plt.subplot(rows, cols, 2)
    filter_names = []
    total_scores = []
    
    for filter_type, result_data in valid_results.items():
        if result_data and result_data['performance']:
            filter_names.append(filter_type)
            total_scores.append(result_data['performance']['total_score'])
    
    bars = ax2.bar(filter_names, total_scores, color=colors[:len(filter_names)], alpha=0.8)
    ax2.set_title('ğŸ“Š Performance Scores', fontsize=14, color='white', fontweight='bold')
    ax2.set_ylabel('Score', color='white')
    ax2.set_ylim(0, 1)
    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
    
    # ã‚¹ã‚³ã‚¢å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, score in zip(bars, total_scores):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{score:.3f}', ha='center', va='bottom', color='white', fontsize=10)
    
    ax2.grid(True, alpha=0.3)
    
    # 3. ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³æ¯”è¼ƒ
    ax3 = plt.subplot(rows, cols, 3)
    for i, (filter_type, result_data) in enumerate(valid_results.items()):
        if result_data and result_data['result']:
            ax3.plot(data.index, result_data['result'].kalman_gains,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{filter_type}")
    
    ax3.set_title('âš™ï¸ Kalman Gains', fontsize=14, color='white', fontweight='bold')
    ax3.set_ylabel('Kalman Gain', color='white')
    ax3.legend(fontsize=10)
    ax3.grid(True, alpha=0.3)
    
    # 4. ä¿¡é ¼åº¦æ¯”è¼ƒ
    ax4 = plt.subplot(rows, cols, 4)
    for i, (filter_type, result_data) in enumerate(valid_results.items()):
        if result_data and result_data['result']:
            ax4.plot(data.index, result_data['result'].confidence_scores,
                    colors[i % len(colors)], alpha=0.7, linewidth=1.5,
                    label=f"{filter_type}")
    
    ax4.set_title('ğŸ“ˆ Confidence Scores', fontsize=14, color='white', fontweight='bold')
    ax4.set_ylabel('Confidence', color='white')
    ax4.legend(fontsize=10)
    ax4.grid(True, alpha=0.3)
    
    # 5. è©³ç´°æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    ax5 = plt.subplot(rows, cols, 5)
    metrics = ['noise_reduction', 'tracking_score', 'smoothness_score', 'confidence_score', 'delay_score']
    metric_labels = ['Noise\nReduction', 'Price\nTracking', 'Smoothness', 'Confidence', 'Low\nDelay']
    
    # æœ€é«˜æ€§èƒ½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç‰¹å®š
    best_filter = None
    best_score = 0
    for filter_type, result_data in valid_results.items():
        if result_data and result_data['performance']['total_score'] > best_score:
            best_score = result_data['performance']['total_score']
            best_filter = filter_type
    
    if best_filter and valid_results[best_filter]:
        perf = valid_results[best_filter]['performance']
        values = [perf[metric] for metric in metrics]
        
        bars = ax5.bar(metric_labels, values, color='lime', alpha=0.8)
        ax5.set_title(f'ğŸ† Best Filter: {best_filter}', fontsize=14, color='white', fontweight='bold')
        ax5.set_ylabel('Score', color='white')
        ax5.set_ylim(0, 1)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, value in zip(bars, values):
            ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.2f}', ha='center', va='bottom', color='white', fontsize=10)
        
        ax5.grid(True, alpha=0.3)
    
    # 6. ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨
    ax6 = plt.subplot(rows, cols, 6)
    ax6.axis('off')
    
    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
    ranking = sorted([(k, v['performance']['total_score']) for k, v in valid_results.items() 
                     if v and v['performance']], key=lambda x: x[1], reverse=True)
    
    ranking_text = "ğŸ† Filter Ranking:\n\n"
    for i, (filter_name, score) in enumerate(ranking):
        emoji = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1}."
        ranking_text += f"{emoji} {filter_name}: {score:.3f}\n"
    
    ax6.text(0.1, 0.9, ranking_text, transform=ax6.transAxes, fontsize=12, 
            color='white', verticalalignment='top', fontfamily='monospace')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, facecolor='black', edgecolor='none', dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ¯”è¼ƒçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    
    plt.show()


def detailed_filter_analysis(filter_type: str, data: pd.DataFrame):
    """
    ç‰¹å®šãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è©³ç´°åˆ†æ
    """
    print(f"\nğŸ” è©³ç´°åˆ†æ: {filter_type}")
    print("=" * 50)
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Ÿè¡Œ
    try:
        kalman_filter = KalmanFilterUnified(filter_type=filter_type, src_type='close')
        result = kalman_filter.calculate(data)
        metadata = kalman_filter.get_filter_metadata()
    except Exception as e:
        print(f"âŒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # åˆ†æçµæœè¡¨ç¤º
    print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
    print(f"   ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {metadata.get('data_points', 'N/A')}")
    print(f"   å¹³å‡ä¿¡é ¼åº¦: {metadata.get('avg_confidence', 0):.3f}")
    print(f"   å¹³å‡ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³: {metadata.get('avg_kalman_gain', 0):.3f}")
    print(f"   å¹³å‡ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³: {metadata.get('avg_innovation', 0):.3f}")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®æƒ…å ±
    if metadata.get('avg_quantum_coherence'):
        print(f"   å¹³å‡é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹: {metadata['avg_quantum_coherence']:.3f}")
    if metadata.get('avg_uncertainty'):
        print(f"   å¹³å‡ä¸ç¢ºå®Ÿæ€§: {metadata['avg_uncertainty']:.3f}")
    if metadata.get('avg_trend_estimate'):
        print(f"   å¹³å‡ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®š: {metadata['avg_trend_estimate']:.3f}")
    
    # æ€§èƒ½è©•ä¾¡
    performance = evaluate_filter_performance(
        data['close'].values, result.filtered_values, result.confidence_scores
    )
    
    print(f"\nğŸ¯ æ€§èƒ½è©•ä¾¡:")
    print(f"   ãƒã‚¤ã‚ºé™¤å»: {performance['noise_reduction']:.3f}")
    print(f"   ä¾¡æ ¼è¿½å¾“æ€§: {performance['tracking_score']:.3f}")
    print(f"   æ»‘ã‚‰ã‹ã•: {performance['smoothness_score']:.3f}")
    print(f"   ä¿¡é ¼åº¦: {performance['confidence_score']:.3f}")
    print(f"   é…å»¶ã‚¹ã‚³ã‚¢: {performance['delay_score']:.3f}")
    print(f"   ç·åˆã‚¹ã‚³ã‚¢: {performance['total_score']:.3f}")


def load_data_from_config(config_path: str = 'config.yaml') -> pd.DataFrame:
    """
    config.yamlã‹ã‚‰å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆz_adaptive_trend_chart.pyå‚è€ƒï¼‰
    """
    try:
        print(f"ğŸ“¡ {config_path} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        binance_config = config.get('binance_data', {})
        data_dir = binance_config.get('data_dir', 'data/binance')
        binance_data_source = BinanceDataSource(data_dir)
        
        # CSVãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¯ãƒ€ãƒŸãƒ¼ã¨ã—ã¦æ¸¡ã™ï¼ˆBinanceãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ã¿ã‚’ä½¿ç”¨ï¼‰
        dummy_csv_source = CSVDataSource("dummy")
        data_loader = DataLoader(
            data_source=dummy_csv_source,
            binance_data_source=binance_data_source
        )
        data_processor = DataProcessor()
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
        raw_data = data_loader.load_data_from_config(config)
        processed_data = {
            symbol: data_processor.process(df)
            for symbol, df in raw_data.items()
        }
        
        # æœ€åˆã®ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        first_symbol = next(iter(processed_data))
        data = processed_data[first_symbol]
        
        print(f"âœ… å®Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {first_symbol}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
        return data
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ”„ åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")
        return create_synthetic_data(500)


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸš€ Kalman Filter Unified Demo V1.0")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    print("\n1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿æº–å‚™")
    
    # å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’è©¦ã—ã€å¤±æ•—ã—ãŸã‚‰åˆæˆãƒ‡ãƒ¼ã‚¿
    data = load_data_from_config('config.yaml')
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    print(f"   æœŸé–“: {len(data)} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
    print(f"   ä¾¡æ ¼ç¯„å›²: ${data['close'].min():.2f} - ${data['close'].max():.2f}")
    print(f"   å¹³å‡ä¾¡æ ¼: ${data['close'].mean():.2f}")
    
    # å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ†ã‚¹ãƒˆ
    print("\n2ï¸âƒ£ å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    results = test_all_filters(data)
    
    # çµæœæ¯”è¼ƒãƒ»å¯è¦–åŒ–
    print("\n3ï¸âƒ£ çµæœã®å¯è¦–åŒ–")
    output_path = os.path.join('output', 'kalman_filter_comparison.png')
    os.makedirs('output', exist_ok=True)
    visualize_comparison(data, results, output_path)
    
    # è©³ç´°åˆ†æ
    print("\n4ï¸âƒ£ è©³ç´°åˆ†æ")
    
    # æœ€é«˜æ€§èƒ½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è©³ç´°åˆ†æ
    valid_results = {k: v for k, v in results.items() if v is not None}
    if valid_results:
        best_filter = max(valid_results.keys(), 
                         key=lambda k: valid_results[k]['performance']['total_score'])
        detailed_filter_analysis(best_filter, data)
    
    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
    print("\nğŸ“‹ ç·åˆãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 50)
    
    valid_count = len(valid_results)
    total_filters = len(KalmanFilterUnified.get_available_filters())
    
    print(f"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ: {valid_count}/{total_filters} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
    
    if valid_results:
        # ãƒˆãƒƒãƒ—3
        ranking = sorted([(k, v['performance']['total_score']) for k, v in valid_results.items()], 
                        key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ† ãƒˆãƒƒãƒ—3ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:")
        for i, (name, score) in enumerate(ranking[:3]):
            emoji = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰"
            print(f"   {emoji} {name}: {score:.3f}")
        
        # æ¨å¥¨ç”¨é€”
        print(f"\nğŸ’¡ æ¨å¥¨ç”¨é€”:")
        if ranking[0][0] == 'adaptive':
            print("   - æ±ç”¨çš„ãªç”¨é€”ã«ã¯ adaptive ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒãŠå‹§ã‚")
        elif ranking[0][0] == 'quantum_adaptive':
            print("   - é«˜ç²¾åº¦ãŒå¿…è¦ãªå ´åˆã¯ quantum_adaptive ãŒãŠå‹§ã‚")
        elif ranking[0][0] == 'triple_ensemble':
            print("   - å®‰å®šæ€§é‡è¦–ãªã‚‰ triple_ensemble ãŒãŠå‹§ã‚")
        else:
            print(f"   - {ranking[0][0]} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒæœ€é©ã§ã™")
    
    print(f"\nğŸ‰ ãƒ‡ãƒ¢å®Œäº†!")
    print(f"ğŸ“Š çµæœã¯ {output_path} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")


if __name__ == "__main__":
    main() 