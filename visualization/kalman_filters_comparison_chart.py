#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import mplfinance as mpf
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import time

# ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ã®ä¾å­˜é–¢ä¿‚
from data.data_loader import DataLoader, CSVDataSource
from data.data_processor import DataProcessor
from data.binance_data_source import BinanceDataSource

# æ¯”è¼ƒã™ã‚‹ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
from indicators.ultimate_ma import UltimateMA
from indicators.ehlers_absolute_ultimate_cycle import EhlersAbsoluteUltimateCycle
from indicators.hyper_adaptive_kalman import HyperAdaptiveKalmanFilter


class KalmanFiltersComparisonChart:
    """
    3ã¤ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’æ¯”è¼ƒã™ã‚‹ãƒãƒ£ãƒ¼ãƒˆã‚¯ãƒ©ã‚¹
    
    - Ultimate MA (adaptive_kalman_filter_numba)
    - Ehlers Absolute Ultimate (ultimate_kalman_smoother) 
    - HyperAdaptiveKalmanFilter (hyper_realtime_kalman + hyper_bidirectional_kalman)
    
    æ©Ÿèƒ½:
    - 3ã¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã®åŒæ™‚è¡¨ç¤º
    - ãƒã‚¤ã‚ºé™¤å»åŠ¹æœã®æ¯”è¼ƒ
    - é…å»¶æ€§èƒ½ã®æ¯”è¼ƒ
    - è¿½å¾“æ€§ã®æ¯”è¼ƒ
    - è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.data = None
        self.ultimate_ma = None
        self.ehlers_cycle = None
        self.hyper_kalman = None
        self.fig = None
        self.axes = None
        self.performance_stats = {}
    
    def load_data_from_config(self, config_path: str) -> pd.DataFrame:
        """
        è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        binance_config = config.get('binance_data', {})
        data_dir = binance_config.get('data_dir', 'data/binance')
        binance_data_source = BinanceDataSource(data_dir)
        
        # CSVãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¯ãƒ€ãƒŸãƒ¼ã¨ã—ã¦æ¸¡ã™ï¼ˆBinanceãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ã¿ã‚’ä½¿ç”¨ï¼‰
        dummy_csv_source = CSVDataSource("dummy")
        data_loader = DataLoader(
            data_source=dummy_csv_source,
            binance_data_source=binance_data_source
        )
        data_processor = DataProcessor()
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
        print("\nãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†ä¸­...")
        raw_data = data_loader.load_data_from_config(config)
        processed_data = {
            symbol: data_processor.process(df)
            for symbol, df in raw_data.items()
        }
        
        # æœ€åˆã®ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        first_symbol = next(iter(processed_data))
        self.data = processed_data[first_symbol]
        
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {first_symbol}")
        print(f"æœŸé–“: {self.data.index.min()} â†’ {self.data.index.max()}")
        print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(self.data)}")
        
        return self.data

    def calculate_all_filters(self,
                            # Ultimate MA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                            ultimate_ma_super_smooth_period: int = 10,
                            ultimate_ma_zero_lag_period: int = 21,
                            ultimate_ma_src_type: str = 'hlc3',
                            # Ehlers Absolute Ultimate ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿  
                            ehlers_cycle_part: float = 1.0,
                            ehlers_max_output: int = 120,
                            ehlers_min_output: int = 5,
                            ehlers_period_range: Tuple[int, int] = (5, 120),
                            ehlers_src_type: str = 'hlc3',
                            # HyperAdaptiveKalman ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                            hyper_processing_mode: str = 'adaptive',
                            hyper_market_regime_window: int = 20,
                            hyper_base_process_noise: float = 1e-6,
                            hyper_base_observation_noise: float = 0.001,
                            hyper_src_type: str = 'hlc3'
                           ) -> None:
        """
        3ã¤ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            ultimate_ma_super_smooth_period: Ultimate MA ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼æœŸé–“
            ultimate_ma_zero_lag_period: Ultimate MA ã‚¼ãƒ­ãƒ©ã‚°EMAæœŸé–“
            ultimate_ma_src_type: Ultimate MA ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
            ehlers_cycle_part: Ehlers ã‚µã‚¤ã‚¯ãƒ«éƒ¨åˆ†å€ç‡
            ehlers_max_output: Ehlers æœ€å¤§å‡ºåŠ›å€¤
            ehlers_min_output: Ehlers æœ€å°å‡ºåŠ›å€¤
            ehlers_period_range: Ehlers å‘¨æœŸç¯„å›²
            ehlers_src_type: Ehlers ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
            hyper_processing_mode: Hyper å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
            hyper_market_regime_window: Hyper å¸‚å ´ä½“åˆ¶æ¤œå‡ºã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            hyper_base_process_noise: Hyper åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
            hyper_base_observation_noise: Hyper åŸºæœ¬è¦³æ¸¬ãƒã‚¤ã‚º
            hyper_src_type: Hyper ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
        """
        if self.data is None:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚load_data_from_config()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            
        print("\nğŸš€ 3ã¤ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨ˆç®—ä¸­...")
        
        # 1. Ultimate MAï¼ˆé©å¿œçš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä½¿ç”¨ï¼‰
        print("âš¡ Ultimate MA è¨ˆç®—ä¸­...")
        start_time = time.time()
        self.ultimate_ma = UltimateMA(
            super_smooth_period=ultimate_ma_super_smooth_period,
            zero_lag_period=ultimate_ma_zero_lag_period,
            src_type=ultimate_ma_src_type,
            zero_lag_period_mode='fixed',  # å›ºå®šãƒ¢ãƒ¼ãƒ‰ã§æ¯”è¼ƒ
            realtime_window_mode='fixed'
        )
        ultimate_result = self.ultimate_ma.calculate(self.data)
        ultimate_time = time.time() - start_time
        
        # 2. Ehlers Absolute Ultimateï¼ˆç©¶æ¥µã®ã‚«ãƒ«ãƒãƒ³ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ä½¿ç”¨ï¼‰
        print("ğŸŒ€ Ehlers Absolute Ultimate è¨ˆç®—ä¸­...")
        start_time = time.time()
        self.ehlers_cycle = EhlersAbsoluteUltimateCycle(
            cycle_part=ehlers_cycle_part,
            max_output=ehlers_max_output,
            min_output=ehlers_min_output,
            period_range=ehlers_period_range,
            src_type=ehlers_src_type
        )
        ehlers_result = self.ehlers_cycle.calculate(self.data)
        ehlers_time = time.time() - start_time
        
        # 3. HyperAdaptiveKalmanFilterï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼‰
        print("ğŸ¯ HyperAdaptiveKalmanFilter è¨ˆç®—ä¸­...")
        start_time = time.time()
        self.hyper_kalman = HyperAdaptiveKalmanFilter(
            processing_mode=hyper_processing_mode,
            market_regime_window=hyper_market_regime_window,
            base_process_noise=hyper_base_process_noise,
            base_observation_noise=hyper_base_observation_noise,
            src_type=hyper_src_type
        )
        hyper_result = self.hyper_kalman.calculate(self.data)
        hyper_time = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®è¨ˆç®—
        self.performance_stats = {
            'processing_times': {
                'ultimate_ma': ultimate_time,
                'ehlers_absolute': ehlers_time,
                'hyper_adaptive': hyper_time
            },
            'ultimate_ma_stats': self.ultimate_ma.get_noise_reduction_stats(),
            'hyper_kalman_stats': self.hyper_kalman.get_performance_stats(),
            'hyper_comparison': self.hyper_kalman.get_comparison_with_originals()
        }
        
        print("âœ… å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—å®Œäº†")
        print(f"å‡¦ç†æ™‚é–“ - Ultimate MA: {ultimate_time:.3f}s, Ehlers: {ehlers_time:.3f}s, Hyper: {hyper_time:.3f}s")
        
    def analyze_performance(self) -> Dict[str, Any]:
        """
        ğŸ† è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚’å®Ÿè¡Œã™ã‚‹
        
        Returns:
            è©³ç´°ãªåˆ†æçµæœã®è¾æ›¸
        """
        if not all([self.ultimate_ma, self.ehlers_cycle, self.hyper_kalman]):
            raise ValueError("å…¨ã¦ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚calculate_all_filters()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        print("\nğŸ“Š è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Ÿè¡Œä¸­...")
        
        # å…ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆHLC3ï¼‰
        raw_prices = (self.data['high'] + self.data['low'] + self.data['close']) / 3
        
        # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®çµæœå–å¾—
        ultimate_values = self.ultimate_ma.get_kalman_values()  # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ®µéšã®å€¤
        ehlers_cycles = self.ehlers_cycle.calculate(self.data)  # ã‚µã‚¤ã‚¯ãƒ«å€¤ã ãŒå†…éƒ¨ã§ã‚«ãƒ«ãƒãƒ³ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ä½¿ç”¨
        hyper_realtime = self.hyper_kalman.get_realtime_values()
        hyper_bidirectional = self.hyper_kalman.get_bidirectional_values()
        hyper_adaptive = self.hyper_kalman.get_adaptive_values()
        
        # æ­£è¦åŒ–ï¼ˆåŒã˜ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ¯”è¼ƒã®ãŸã‚ï¼‰
        ehlers_normalized = ehlers_cycles / np.nanmax(ehlers_cycles) * np.nanmax(raw_prices)
        
        # 1. ãƒã‚¤ã‚ºé™¤å»åŠ¹æœã®æ¯”è¼ƒ
        raw_volatility = np.nanstd(raw_prices)
        ultimate_volatility = np.nanstd(ultimate_values)
        hyper_rt_volatility = np.nanstd(hyper_realtime)
        hyper_bi_volatility = np.nanstd(hyper_bidirectional)
        hyper_ad_volatility = np.nanstd(hyper_adaptive)
        
        noise_reduction = {
            'raw_volatility': raw_volatility,
            'ultimate_ma_reduction': (raw_volatility - ultimate_volatility) / raw_volatility * 100,
            'hyper_realtime_reduction': (raw_volatility - hyper_rt_volatility) / raw_volatility * 100,
            'hyper_bidirectional_reduction': (raw_volatility - hyper_bi_volatility) / raw_volatility * 100,
            'hyper_adaptive_reduction': (raw_volatility - hyper_ad_volatility) / raw_volatility * 100
        }
        
        # 2. é…å»¶æ€§ã®æ¯”è¼ƒï¼ˆç›¸é–¢åˆ†æï¼‰
        def calculate_lag_correlation(original, filtered, max_lag=10):
            """é…å»¶ç›¸é–¢ã‚’è¨ˆç®—"""
            correlations = []
            for lag in range(max_lag + 1):
                if lag == 0:
                    corr = np.corrcoef(original[:-1], filtered[:-1])[0, 1]
                else:
                    corr = np.corrcoef(original[:-lag-1], filtered[lag:-1])[0, 1]
                correlations.append(corr)
            return correlations
        
        # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®é…å»¶åˆ†æ
        ultimate_lag_corr = calculate_lag_correlation(raw_prices, ultimate_values)
        hyper_rt_lag_corr = calculate_lag_correlation(raw_prices, hyper_realtime)
        hyper_bi_lag_corr = calculate_lag_correlation(raw_prices, hyper_bidirectional)
        hyper_ad_lag_corr = calculate_lag_correlation(raw_prices, hyper_adaptive)
        
        lag_analysis = {
            'ultimate_ma_peak_correlation': max(ultimate_lag_corr),
            'ultimate_ma_optimal_lag': ultimate_lag_corr.index(max(ultimate_lag_corr)),
            'hyper_realtime_peak_correlation': max(hyper_rt_lag_corr),
            'hyper_realtime_optimal_lag': hyper_rt_lag_corr.index(max(hyper_rt_lag_corr)),
            'hyper_bidirectional_peak_correlation': max(hyper_bi_lag_corr),
            'hyper_bidirectional_optimal_lag': hyper_bi_lag_corr.index(max(hyper_bi_lag_corr)),
            'hyper_adaptive_peak_correlation': max(hyper_ad_lag_corr),
            'hyper_adaptive_optimal_lag': hyper_ad_lag_corr.index(max(hyper_ad_lag_corr))
        }
        
        # 3. è¿½å¾“æ€§ã®æ¯”è¼ƒï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“èƒ½åŠ›ï¼‰
        def calculate_trend_following(original, filtered):
            """ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“æ€§ã‚’è¨ˆç®—"""
            original_diff = np.diff(original)
            filtered_diff = np.diff(filtered)
            
            # æ–¹å‘ä¸€è‡´ç‡
            direction_matches = np.sum((original_diff > 0) == (filtered_diff > 0))
            direction_accuracy = direction_matches / len(original_diff) * 100
            
            # å¤‰åŒ–é‡ã®ç›¸é–¢
            change_correlation = np.corrcoef(original_diff, filtered_diff)[0, 1]
            
            return {
                'direction_accuracy': direction_accuracy,
                'change_correlation': change_correlation
            }
        
        ultimate_trend = calculate_trend_following(raw_prices, ultimate_values)
        hyper_rt_trend = calculate_trend_following(raw_prices, hyper_realtime)
        hyper_bi_trend = calculate_trend_following(raw_prices, hyper_bidirectional)
        hyper_ad_trend = calculate_trend_following(raw_prices, hyper_adaptive)
        
        trend_following = {
            'ultimate_ma': ultimate_trend,
            'hyper_realtime': hyper_rt_trend,
            'hyper_bidirectional': hyper_bi_trend,
            'hyper_adaptive': hyper_ad_trend
        }
        
        # 4. å…¨ä½“çš„ãªå“è³ªæŒ‡æ¨™
        def calculate_quality_score(noise_reduction_pct, lag, direction_accuracy, change_correlation):
            """ç·åˆå“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
            # é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢ï¼ˆ0-100ç‚¹ï¼‰
            noise_score = min(noise_reduction_pct, 50) * 2  # æœ€å¤§100ç‚¹
            lag_score = max(0, 100 - lag * 10)  # é…å»¶ãƒšãƒŠãƒ«ãƒ†ã‚£
            direction_score = direction_accuracy  # æ–¹å‘ä¸€è‡´ç‡
            correlation_score = change_correlation * 100  # ç›¸é–¢ã‚¹ã‚³ã‚¢
            
            total_score = (noise_score * 0.3 + lag_score * 0.3 + 
                          direction_score * 0.2 + correlation_score * 0.2)
            
            return {
                'total_score': total_score,
                'noise_score': noise_score,
                'lag_score': lag_score,
                'direction_score': direction_score,
                'correlation_score': correlation_score
            }
        
        quality_scores = {
            'ultimate_ma': calculate_quality_score(
                noise_reduction['ultimate_ma_reduction'],
                lag_analysis['ultimate_ma_optimal_lag'],
                ultimate_trend['direction_accuracy'],
                ultimate_trend['change_correlation']
            ),
            'hyper_realtime': calculate_quality_score(
                noise_reduction['hyper_realtime_reduction'],
                lag_analysis['hyper_realtime_optimal_lag'],
                hyper_rt_trend['direction_accuracy'],
                hyper_rt_trend['change_correlation']
            ),
            'hyper_bidirectional': calculate_quality_score(
                noise_reduction['hyper_bidirectional_reduction'],
                lag_analysis['hyper_bidirectional_optimal_lag'],
                hyper_bi_trend['direction_accuracy'],
                hyper_bi_trend['change_correlation']
            ),
            'hyper_adaptive': calculate_quality_score(
                noise_reduction['hyper_adaptive_reduction'],
                lag_analysis['hyper_adaptive_optimal_lag'],
                hyper_ad_trend['direction_accuracy'],
                hyper_ad_trend['change_correlation']
            )
        }
        
        # ç·åˆåˆ†æçµæœ
        analysis_result = {
            'noise_reduction': noise_reduction,
            'lag_analysis': lag_analysis,
            'trend_following': trend_following,
            'quality_scores': quality_scores,
            'processing_times': self.performance_stats['processing_times'],
            'winner_analysis': self._determine_winners(quality_scores, noise_reduction, lag_analysis, trend_following)
        }
        
        return analysis_result
    
    def _determine_winners(self, quality_scores, noise_reduction, lag_analysis, trend_following):
        """å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å‹è€…ã‚’æ±ºå®š"""
        winners = {}
        
        # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        best_overall = max(quality_scores.keys(), key=lambda k: quality_scores[k]['total_score'])
        winners['overall'] = best_overall
        
        # ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ
        noise_keys = ['ultimate_ma_reduction', 'hyper_realtime_reduction', 
                     'hyper_bidirectional_reduction', 'hyper_adaptive_reduction']
        best_noise = max(noise_keys, key=lambda k: noise_reduction[k])
        winners['noise_reduction'] = best_noise.replace('_reduction', '')
        
        # ä½é…å»¶æ€§
        lag_keys = ['ultimate_ma_optimal_lag', 'hyper_realtime_optimal_lag',
                   'hyper_bidirectional_optimal_lag', 'hyper_adaptive_optimal_lag']
        best_lag = min(lag_keys, key=lambda k: lag_analysis[k])
        winners['low_latency'] = best_lag.replace('_optimal_lag', '')
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“æ€§
        trend_keys = ['ultimate_ma', 'hyper_realtime', 'hyper_bidirectional', 'hyper_adaptive']
        best_trend = max(trend_keys, key=lambda k: trend_following[k]['direction_accuracy'])
        winners['trend_following'] = best_trend
        
        return winners

    def print_detailed_analysis(self, analysis_result: Dict[str, Any]) -> None:
        """è©³ç´°åˆ†æçµæœã‚’ç¾ã—ãå‡ºåŠ›ã™ã‚‹"""
        print("\n" + "="*80)
        print("ğŸ† ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è©³ç´°æ¯”è¼ƒåˆ†æçµæœ")
        print("="*80)
        
        # 1. å‡¦ç†æ™‚é–“
        print("\nâš¡ å‡¦ç†é€Ÿåº¦æ¯”è¼ƒ:")
        times = analysis_result['processing_times']
        for name, time_val in times.items():
            print(f"  {name:20}: {time_val:.4f}ç§’")
        fastest = min(times.keys(), key=lambda k: times[k])
        print(f"  ğŸ¥‡ æœ€é€Ÿ: {fastest}")
        
        # 2. ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ
        print("\nğŸ”‡ ãƒã‚¤ã‚ºé™¤å»åŠ¹æœæ¯”è¼ƒ:")
        noise = analysis_result['noise_reduction']
        print(f"  å…ƒãƒ‡ãƒ¼ã‚¿æ¨™æº–åå·®: {noise['raw_volatility']:.6f}")
        for key, value in noise.items():
            if 'reduction' in key:
                name = key.replace('_reduction', '').replace('_', ' ').title()
                print(f"  {name:20}: {value:.2f}%")
        
        # 3. é…å»¶æ€§åˆ†æ
        print("\nâš¡ é…å»¶æ€§åˆ†æ:")
        lag = analysis_result['lag_analysis']
        filters = ['ultimate_ma', 'hyper_realtime', 'hyper_bidirectional', 'hyper_adaptive']
        for filter_name in filters:
            lag_key = f"{filter_name}_optimal_lag"
            corr_key = f"{filter_name}_peak_correlation"
            if lag_key in lag and corr_key in lag:
                name = filter_name.replace('_', ' ').title()
                print(f"  {name:20}: æœ€é©é…å»¶ {lag[lag_key]}æœŸé–“, ç›¸é–¢ {lag[corr_key]:.4f}")
        
        # 4. ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“æ€§
        print("\nğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“æ€§:")
        trend = analysis_result['trend_following']
        for filter_name, stats in trend.items():
            name = filter_name.replace('_', ' ').title()
            print(f"  {name:20}: æ–¹å‘ä¸€è‡´ç‡ {stats['direction_accuracy']:.2f}%, "
                  f"å¤‰åŒ–ç›¸é–¢ {stats['change_correlation']:.4f}")
        
        # 5. ç·åˆå“è³ªã‚¹ã‚³ã‚¢
        print("\nğŸ† ç·åˆå“è³ªã‚¹ã‚³ã‚¢ (0-100ç‚¹):")
        quality = analysis_result['quality_scores']
        sorted_scores = sorted(quality.items(), key=lambda x: x[1]['total_score'], reverse=True)
        
        for i, (filter_name, scores) in enumerate(sorted_scores):
            rank = "ğŸ¥‡" if i == 0 else "ğŸ¥ˆ" if i == 1 else "ğŸ¥‰" if i == 2 else f"{i+1}ä½"
            name = filter_name.replace('_', ' ').title()
            print(f"  {rank} {name:18}: {scores['total_score']:.1f}ç‚¹")
            print(f"      â”œ ãƒã‚¤ã‚ºé™¤å»: {scores['noise_score']:.1f}ç‚¹")
            print(f"      â”œ ä½é…å»¶æ€§: {scores['lag_score']:.1f}ç‚¹")
            print(f"      â”œ æ–¹å‘ä¸€è‡´: {scores['direction_score']:.1f}ç‚¹")
            print(f"      â”” å¤‰åŒ–ç›¸é–¢: {scores['correlation_score']:.1f}ç‚¹")
        
        # 6. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å‹è€…
        print("\nğŸ… ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å‹è€…:")
        winners = analysis_result['winner_analysis']
        categories = {
            'overall': 'ç·åˆå“è³ª',
            'noise_reduction': 'ãƒã‚¤ã‚ºé™¤å»',
            'low_latency': 'ä½é…å»¶æ€§',
            'trend_following': 'ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“'
        }
        
        for category, japanese_name in categories.items():
            winner = winners[category].replace('_', ' ').title()
            print(f"  {japanese_name:12}: ğŸ† {winner}")
        
        # 7. æ¨å¥¨äº‹é …
        print("\nğŸ’¡ ç”¨é€”åˆ¥æ¨å¥¨:")
        overall_winner = winners['overall'].replace('_', ' ').title()
        latency_winner = winners['low_latency'].replace('_', ' ').title()
        quality_winner = winners['noise_reduction'].replace('_', ' ').title()
        
        print(f"  ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¼•: {latency_winner}")
        print(f"  ğŸ“Š é«˜å“è³ªåˆ†æ: {quality_winner}")
        print(f"  âš–ï¸  ãƒãƒ©ãƒ³ã‚¹é‡è¦–: {overall_winner}")
        
        print("\n" + "="*80)

    def plot_comparison(self, 
                       title: str = "ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¯”è¼ƒ", 
                       start_date: Optional[str] = None,
                       end_date: Optional[str] = None,
                       show_volume: bool = True,
                       figsize: Tuple[int, int] = (16, 14),
                       style: str = 'yahoo',
                       savefig: Optional[str] = None) -> None:
        """
        3ã¤ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’æç”»ã™ã‚‹
        
        Args:
            title: ãƒãƒ£ãƒ¼ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«
            start_date: è¡¨ç¤ºé–‹å§‹æ—¥ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: YYYY-MM-DDï¼‰
            end_date: è¡¨ç¤ºçµ‚äº†æ—¥ï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: YYYY-MM-DDï¼‰
            show_volume: å‡ºæ¥é«˜ã‚’è¡¨ç¤ºã™ã‚‹ã‹
            figsize: å›³ã®ã‚µã‚¤ã‚º
            style: mplfinanceã®ã‚¹ã‚¿ã‚¤ãƒ«
            savefig: ä¿å­˜å…ˆã®ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è¡¨ç¤ºã®ã¿ï¼‰
        """
        if not all([self.ultimate_ma, self.ehlers_cycle, self.hyper_kalman]):
            raise ValueError("å…¨ã¦ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒè¨ˆç®—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚calculate_all_filters()ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ‡ãƒ¼ã‚¿ã®æœŸé–“çµã‚Šè¾¼ã¿
        df = self.data.copy()
        if start_date:
            df = df[df.index >= pd.to_datetime(start_date)]
        if end_date:
            df = df[df.index <= pd.to_datetime(end_date)]
        
        # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å€¤ã‚’å–å¾—
        ultimate_kalman = self.ultimate_ma.get_kalman_values()
        ultimate_final = self.ultimate_ma.get_values()
        
        hyper_realtime = self.hyper_kalman.get_realtime_values()
        hyper_bidirectional = self.hyper_kalman.get_bidirectional_values()
        hyper_adaptive = self.hyper_kalman.get_adaptive_values()
        hyper_regimes = self.hyper_kalman.get_market_regimes()
        hyper_confidence = self.hyper_kalman.get_confidence_scores()
        
        # å…ƒä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        raw_hlc3 = (self.data['high'] + self.data['low'] + self.data['close']) / 3
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
        full_df = pd.DataFrame(
            index=self.data.index,
            data={
                'raw_hlc3': raw_hlc3,
                'ultimate_kalman': ultimate_kalman,
                'ultimate_final': ultimate_final,
                'hyper_realtime': hyper_realtime,
                'hyper_bidirectional': hyper_bidirectional,
                'hyper_adaptive': hyper_adaptive,
                'market_regimes': hyper_regimes,
                'confidence_scores': hyper_confidence
            }
        )
        
        # çµã‚Šè¾¼ã¿å¾Œã®ãƒ‡ãƒ¼ã‚¿ã«çµåˆ
        df = df.join(full_df)
        
        print(f"ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº† - è¡Œæ•°: {len(df)}")
        
        # mplfinanceã§ãƒ—ãƒ­ãƒƒãƒˆç”¨ã®è¨­å®š
        main_plots = []
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆè–„ã„ã‚°ãƒ¬ãƒ¼ï¼‰
        main_plots.append(mpf.make_addplot(df['raw_hlc3'], color='lightgray', width=1, alpha=0.5, label='Raw HLC3'))
        
        # Ultimate MAï¼ˆé’ç³»ï¼‰
        main_plots.append(mpf.make_addplot(df['ultimate_kalman'], color='blue', width=1.5, label='Ultimate MA (Kalman)'))
        main_plots.append(mpf.make_addplot(df['ultimate_final'], color='darkblue', width=1.0, alpha=0.7, label='Ultimate MA (Final)'))
        
        # HyperAdaptiveKalmanï¼ˆç·‘ãƒ»èµ¤ãƒ»ç´«ç³»ï¼‰
        main_plots.append(mpf.make_addplot(df['hyper_realtime'], color='green', width=1.5, label='Hyper (Realtime)'))
        main_plots.append(mpf.make_addplot(df['hyper_bidirectional'], color='red', width=1.5, label='Hyper (Bidirectional)'))
        main_plots.append(mpf.make_addplot(df['hyper_adaptive'], color='purple', width=2, label='Hyper (Adaptive)'))
        
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆï¼šå¸‚å ´ä½“åˆ¶ã¨ä¿¡é ¼åº¦
        regime_panel = mpf.make_addplot(df['market_regimes'], panel=1, color='orange', width=1.2, 
                                       ylabel='Market Regime', secondary_y=False, label='Regime')
        
        confidence_panel = mpf.make_addplot(df['confidence_scores'], panel=2, color='brown', width=1.2, 
                                          ylabel='Confidence', secondary_y=False, label='Confidence')
        
        # mplfinanceã®è¨­å®š
        kwargs = dict(
            type='candle',
            figsize=figsize,
            title=title,
            style=style,
            datetime_format='%Y-%m-%d',
            xrotation=45,
            returnfig=True
        )
        
        # å‡ºæ¥é«˜ã¨ãƒ‘ãƒãƒ«è¨­å®š
        if show_volume:
            kwargs['volume'] = True
            kwargs['panel_ratios'] = (5, 1, 1, 1)  # ãƒ¡ã‚¤ãƒ³:å‡ºæ¥é«˜:ä½“åˆ¶:ä¿¡é ¼åº¦
            # ãƒ‘ãƒãƒ«ç•ªå·ã‚’èª¿æ•´
            regime_panel = mpf.make_addplot(df['market_regimes'], panel=2, color='orange', width=1.2, 
                                           ylabel='Market Regime', secondary_y=False, label='Regime')
            confidence_panel = mpf.make_addplot(df['confidence_scores'], panel=3, color='brown', width=1.2, 
                                              ylabel='Confidence', secondary_y=False, label='Confidence')
        else:
            kwargs['volume'] = False
            kwargs['panel_ratios'] = (5, 1, 1)  # ãƒ¡ã‚¤ãƒ³:ä½“åˆ¶:ä¿¡é ¼åº¦
        
        # å…¨ãƒ—ãƒ­ãƒƒãƒˆã‚’çµåˆ
        all_plots = main_plots + [regime_panel, confidence_panel]
        kwargs['addplot'] = all_plots
        
        # ãƒ—ãƒ­ãƒƒãƒˆå®Ÿè¡Œ
        fig, axes = mpf.plot(df, **kwargs)
        
        # å‡¡ä¾‹ã®è¿½åŠ 
        axes[0].legend(['Raw HLC3', 'Ultimate MA (Kalman)', 'Ultimate MA (Final)', 
                       'Hyper (Realtime)', 'Hyper (Bidirectional)', 'Hyper (Adaptive)'], 
                      loc='upper left', fontsize=8)
        
        self.fig = fig
        self.axes = axes
        
        # å‚ç…§ç·šã®è¿½åŠ 
        if show_volume:
            # å¸‚å ´ä½“åˆ¶ãƒ‘ãƒãƒ«ï¼ˆ0=ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°, 1=ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°, 2=é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
            axes[2].axhline(y=0.5, color='blue', linestyle='--', alpha=0.5)
            axes[2].axhline(y=1.5, color='orange', linestyle='--', alpha=0.5)
            axes[2].set_ylim(-0.5, 2.5)
            
            # ä¿¡é ¼åº¦ãƒ‘ãƒãƒ«
            axes[3].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)
            axes[3].axhline(y=0.8, color='green', linestyle='--', alpha=0.5)
            axes[3].set_ylim(0, 1)
        else:
            # å¸‚å ´ä½“åˆ¶ãƒ‘ãƒãƒ«
            axes[1].axhline(y=0.5, color='blue', linestyle='--', alpha=0.5)
            axes[1].axhline(y=1.5, color='orange', linestyle='--', alpha=0.5)
            axes[1].set_ylim(-0.5, 2.5)
            
            # ä¿¡é ¼åº¦ãƒ‘ãƒãƒ«
            axes[2].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)
            axes[2].axhline(y=0.8, color='green', linestyle='--', alpha=0.5)
            axes[2].set_ylim(0, 1)
        
        # ä¿å­˜ã¾ãŸã¯è¡¨ç¤º
        if savefig:
            plt.savefig(savefig, dpi=150, bbox_inches='tight')
            print(f"ãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {savefig}")
        else:
            plt.tight_layout()
            plt.show()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®æç”»')
    parser.add_argument('--config', '-c', type=str, default='config.yaml', help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--start', '-s', type=str, help='è¡¨ç¤ºé–‹å§‹æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--end', '-e', type=str, help='è¡¨ç¤ºçµ‚äº†æ—¥ (YYYY-MM-DD)')
    parser.add_argument('--output', '-o', type=str, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--hyper-mode', type=str, default='adaptive', 
                       choices=['realtime', 'high_quality', 'adaptive'], 
                       help='HyperAdaptiveKalmanã®å‡¦ç†ãƒ¢ãƒ¼ãƒ‰')
    args = parser.parse_args()
    
    # æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ
    chart = KalmanFiltersComparisonChart()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        chart.load_data_from_config(args.config)
        
        # å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—
        chart.calculate_all_filters(hyper_processing_mode=args.hyper_mode)
        
        # è©³ç´°åˆ†æå®Ÿè¡Œ
        analysis_result = chart.analyze_performance()
        
        # åˆ†æçµæœå‡ºåŠ›
        chart.print_detailed_analysis(analysis_result)
        
        # ãƒãƒ£ãƒ¼ãƒˆæç”»
        chart.plot_comparison(
            start_date=args.start,
            end_date=args.end,
            savefig=args.output
        )
        
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 