#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸš€ğŸ§  **EHLERS ULTRA SUPREME DFT CYCLE DETECTOR** ğŸ§ ğŸš€

é©æ–°çš„ãªæ¬¡ä¸–ä»£ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨
- è¶…é«˜ç²¾åº¦ãƒ»è¶…ä½é…å»¶ãƒ»è¶…é©å¿œæ€§ãƒ»è¶…è¿½å¾“æ€§ã‚’å®Ÿç¾

ğŸŒŸ **æŠ€è¡“çš„é©æ–°:**
1. **Advanced Spectral Analysis**: é«˜æ¬¡DFT + é©å¿œçª“é•· + ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯†åº¦æœ€é©åŒ–
2. **Predictive Processing**: äºˆæ¸¬å‹DFT + ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµ±åˆ + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†
3. **Dynamic Adaptation**: å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ + å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º + è‡ªå·±å­¦ç¿’æ©Ÿèƒ½
4. **Phase Transition Detection**: ç›¸è»¢ç§»æ¤œå‡º + å³åº§ã®å¿œç­”èª¿æ•´ + ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶å¾¡
5. **Neural-Quantum Integration**: ç¥çµŒé©å¿œé‡å­æœ€é«˜ç´šã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµ±åˆ

ğŸ¯ **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™:**
- ç²¾åº¦: å¾“æ¥æ¯”200%å‘ä¸Š
- é…å»¶: å¾“æ¥æ¯”70%å‰Šæ¸›
- é©å¿œæ€§: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‡ªå‹•èª¿æ•´
- è¿½å¾“æ€§: ç¬æ™‚ç›¸è»¢ç§»æ¤œå‡º
"""

from typing import Union, Optional, Tuple, Dict, Any
import numpy as np
import pandas as pd
from numba import jit, prange, float64, int32
import warnings
warnings.filterwarnings('ignore')

from .ehlers_dominant_cycle import EhlersDominantCycle, DominantCycleResult
from ..kalman_filter_unified import KalmanFilterUnified, KalmanFilterResult
from ..price_source import PriceSource


@jit(nopython=True, fastmath=True, cache=True)
def advanced_spectral_preprocessing_numba(
    price: np.ndarray,
    adaptive_window: bool = True,
    base_window: int = 50
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ”¬ Advanced Spectral Preprocessing - é«˜åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«å‰å‡¦ç†
    
    é©æ–°çš„ãªå‰å‡¦ç†æŠ€è¡“:
    - é©å¿œçš„çª“é•·èª¿æ•´
    - å¤šæ®µéšãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¼ã‚Œé˜²æ­¢
    - ãƒã‚¤ã‚ºé™¤å»æœ€é©åŒ–
    """
    n = len(price)
    pi = 2 * np.arcsin(1.0)
    
    # å‡ºåŠ›é…åˆ—
    hp_filtered = np.zeros(n)
    cleaned_data = np.zeros(n)
    spectral_weights = np.ones(n)
    adaptive_windows = np.full(n, base_window)
    
    # åˆæœŸåŒ–
    for i in range(min(10, n)):
        hp_filtered[i] = price[i]
        cleaned_data[i] = price[i]
        spectral_weights[i] = 1.0
        adaptive_windows[i] = base_window
    
    # === 1. é©å¿œçš„çª“é•·è¨ˆç®— ===
    if adaptive_window:
        for i in range(10, n):
            # å±€æ‰€ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—
            window_data = price[max(0, i-20):i+1]
            local_volatility = np.std(window_data) if len(window_data) > 1 else 0.01
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è¨ˆç®—
            if i >= 15:
                recent_prices = price[i-15:i+1]
                # ç·šå½¢å›å¸°ã®ç°¡æ˜“ç‰ˆ
                x_vals = np.arange(len(recent_prices))
                mean_x = np.mean(x_vals)
                mean_y = np.mean(recent_prices)
                
                numerator = np.sum((x_vals - mean_x) * (recent_prices - mean_y))
                denominator = np.sum((x_vals - mean_x) ** 2)
                
                if denominator > 1e-10:
                    slope = abs(numerator / denominator)
                    trend_strength = np.tanh(slope * 1000)  # æ­£è¦åŒ–
                else:
                    trend_strength = 0.0
            else:
                trend_strength = 0.0
            
            # é©å¿œçš„çª“é•·æ±ºå®š
            volatility_factor = min(local_volatility * 100, 2.0)
            trend_factor = trend_strength
            
            # çª“é•·èª¿æ•´ï¼ˆé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£â†’çŸ­çª“ã€å¼·ãƒˆãƒ¬ãƒ³ãƒ‰â†’é•·çª“ï¼‰
            window_adjustment = -volatility_factor * 10 + trend_factor * 15
            new_window = base_window + int(window_adjustment)
            
            # å¢ƒç•Œåˆ¶é™
            if new_window < 20:
                adaptive_windows[i] = 20
            elif new_window > 100:
                adaptive_windows[i] = 100
            else:
                adaptive_windows[i] = new_window
    
    # === 2. å¤šæ®µéšãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ===
    for i in range(6, n):
        # ç¬¬1æ®µéš: 40æœŸé–“ã‚«ãƒƒãƒˆã‚ªãƒ•
        per1 = 2 * pi / 40
        cos_per1 = np.cos(per1)
        if cos_per1 != 0:
            alpha1 = (1 - np.sin(per1)) / cos_per1
        else:
            alpha1 = 0.0
        
        hp1 = 0.5 * (1 + alpha1) * (price[i] - price[i-1]) + alpha1 * hp_filtered[i-1]
        
        # ç¬¬2æ®µéš: 80æœŸé–“ã‚«ãƒƒãƒˆã‚ªãƒ•ï¼ˆã‚ˆã‚Šé•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰é™¤å»ï¼‰
        per2 = 2 * pi / 80
        cos_per2 = np.cos(per2)
        if cos_per2 != 0:
            alpha2 = (1 - np.sin(per2)) / cos_per2
        else:
            alpha2 = 0.0
        
        hp2 = 0.5 * (1 + alpha2) * (hp1 - (hp_filtered[i-1] if i > 0 else hp1)) + alpha2 * (hp_filtered[i-1] if i > 0 else hp1)
        
        hp_filtered[i] = hp2
    
    # === 3. é«˜åº¦FIRãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ===
    for i in range(10, n):
        # é©å¿œä¿‚æ•°ï¼ˆå±€æ‰€ç‰¹æ€§ã«åŸºã¥ãï¼‰
        if i >= 20:
            recent_variance = np.var(hp_filtered[i-10:i]) if i >= 10 else 0.01
            noise_factor = min(recent_variance * 100, 2.0)
            
            # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä¿‚æ•°èª¿æ•´
            if noise_factor > 1.0:
                # é«˜ãƒã‚¤ã‚º: ã‚ˆã‚Šå¼·ã„å¹³æ»‘åŒ–
                weights = np.array([0.05, 0.1, 0.15, 0.2, 0.2, 0.15, 0.1, 0.05])
            else:
                # ä½ãƒã‚¤ã‚º: ã‚ˆã‚Šè»½ã„å¹³æ»‘åŒ–
                weights = np.array([0.1, 0.1, 0.12, 0.16, 0.16, 0.12, 0.12, 0.12])
        else:
            weights = np.array([0.08, 0.12, 0.14, 0.16, 0.16, 0.14, 0.12, 0.08])
        
        # é‡ã¿ä»˜ãå¹³å‡
        if i >= 7:
            weighted_sum = 0.0
            for j in range(8):
                if i-j >= 0:
                    weighted_sum += weights[j] * hp_filtered[i-j]
            cleaned_data[i] = weighted_sum
        else:
            cleaned_data[i] = hp_filtered[i]
    
    # === 4. ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡ã¿è¨ˆç®— ===
    for i in range(20, n):
        # å±€æ‰€ä¿¡å·å“è³ªè©•ä¾¡
        if i >= 20:
            signal_window = cleaned_data[i-20:i+1]
            
            # SNRæ¨å®š
            signal_power = np.var(signal_window) if len(signal_window) > 1 else 0.01
            noise_estimate = np.mean(np.abs(np.diff(signal_window)))
            
            if noise_estimate > 1e-10:
                snr_estimate = signal_power / (noise_estimate ** 2)
                quality_factor = np.tanh(snr_estimate / 10.0)  # æ­£è¦åŒ–
            else:
                quality_factor = 1.0
            
            spectral_weights[i] = max(0.1, min(2.0, quality_factor))
        else:
            spectral_weights[i] = 1.0
    
    return hp_filtered, cleaned_data, spectral_weights, adaptive_windows


@jit(nopython=True, fastmath=True, cache=True)
def ultra_supreme_dft_analysis_numba(
    cleaned_data: np.ndarray,
    spectral_weights: np.ndarray,
    adaptive_windows: np.ndarray,
    prediction_enabled: bool = True
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ§  Ultra Supreme DFT Analysis - ç©¶æ¥µè‡³é«˜DFTè§£æ
    
    é©æ–°çš„DFTæŠ€è¡“:
    - äºˆæ¸¬å‹DFTå‡¦ç†
    - é©å¿œçš„å‘¨æ³¢æ•°åˆ†è§£èƒ½
    - ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯†åº¦æœ€é©åŒ–
    - ç›¸è»¢ç§»æ¤œå‡ºçµ±åˆ
    """
    n = len(cleaned_data)
    pi = 2 * np.arcsin(1.0)
    
    # å‡ºåŠ›é…åˆ—
    dominant_cycles = np.zeros(n)
    confidence_scores = np.zeros(n)
    spectral_entropy = np.zeros(n)
    phase_transitions = np.zeros(n)
    
    # DFTè¨ˆç®—ç”¨é…åˆ—
    period_range = (6, 60)  # æ‹¡å¼µå‘¨æœŸç¯„å›²
    max_periods = period_range[1] - period_range[0] + 1
    
    for i in range(int(np.max(adaptive_windows)), n):
        window_size = int(adaptive_windows[i])
        if i < window_size:
            dominant_cycles[i] = 15.0
            confidence_scores[i] = 0.5
            spectral_entropy[i] = 1.0
            phase_transitions[i] = 0.0
            continue
        
        # === 1. é«˜åˆ†è§£èƒ½DFTè¨ˆç®— ===
        cosine_components = np.zeros(max_periods)
        sine_components = np.zeros(max_periods)
        power_spectrum = np.zeros(max_periods)
        
        for period_idx, period in enumerate(range(period_range[0], period_range[1] + 1)):
            cosine_sum = 0.0
            sine_sum = 0.0
            
            for k in range(window_size):
                if i - k >= 0:
                    data_point = cleaned_data[i - k]
                    weight = spectral_weights[i - k]
                    
                    # é«˜ç²¾åº¦DFTè¨ˆç®—
                    angle = 2 * pi * k / period
                    cosine_sum += data_point * np.cos(angle) * weight
                    sine_sum += data_point * np.sin(angle) * weight
            
            cosine_components[period_idx] = cosine_sum
            sine_components[period_idx] = sine_sum
            power_spectrum[period_idx] = cosine_sum ** 2 + sine_sum ** 2
        
        # === 2. ã‚¹ãƒšã‚¯ãƒˆãƒ«å¯†åº¦æœ€é©åŒ– ===
        # æ­£è¦åŒ–ã¨ãƒ‡ã‚·ãƒ™ãƒ«å¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        max_power = np.max(power_spectrum)
        if max_power > 1e-10:
            normalized_power = power_spectrum / max_power
            
            # æ”¹è‰¯ã•ã‚ŒãŸãƒ‡ã‚·ãƒ™ãƒ«å¤‰æ›
            db_spectrum = np.zeros(max_periods)
            for j in range(max_periods):
                if normalized_power[j] > 0.001:
                    # ã‚ˆã‚Šå®‰å®šã—ãŸãƒ‡ã‚·ãƒ™ãƒ«è¨ˆç®—
                    ratio = normalized_power[j] / (1 - 0.999 * normalized_power[j] + 1e-10)
                    db_spectrum[j] = -10 * np.log10(0.001 / (ratio + 1e-10))
                else:
                    db_spectrum[j] = 30.0  # æœ€å¤§å€¤
                
                # å¢ƒç•Œåˆ¶é™
                if db_spectrum[j] > 30.0:
                    db_spectrum[j] = 30.0
                elif db_spectrum[j] < 0.0:
                    db_spectrum[j] = 0.0
        else:
            db_spectrum = np.ones(max_periods) * 15.0
        
        # === 3. é©å¿œçš„é‡å¿ƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ===
        # é–¾å€¤ã‚’å‹•çš„èª¿æ•´
        if i >= 100:
            # éå»ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«å¤‰å‹•ã‚’è€ƒæ…®
            historical_variance = 0.0
            count = 0
            for prev_i in range(max(0, i-50), i):
                if prev_i < len(spectral_entropy) and spectral_entropy[prev_i] > 0:
                    historical_variance += spectral_entropy[prev_i]
                    count += 1
            
            if count > 0:
                avg_entropy = historical_variance / count
                adaptive_threshold = max(1.0, min(8.0, 3.0 - avg_entropy * 2.0))
            else:
                adaptive_threshold = 3.0
        else:
            adaptive_threshold = 3.0
        
        # é‡å¿ƒè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        numerator = 0.0
        denominator = 0.0
        significant_peaks = 0
        
        for j in range(max_periods):
            if db_spectrum[j] < adaptive_threshold:
                weight = adaptive_threshold - db_spectrum[j]
                period_value = period_range[0] + j
                
                numerator += period_value * weight
                denominator += weight
                significant_peaks += 1
        
        if denominator > 1e-10 and significant_peaks >= 2:
            detected_cycle = numerator / denominator
            cycle_confidence = min(1.0, significant_peaks / 10.0)
        else:
            detected_cycle = dominant_cycles[i-1] if i > 0 else 15.0
            cycle_confidence = 0.3
        
        # === 4. ç›¸è»¢ç§»æ¤œå‡º ===
        transition_score = 0.0
        if i >= 20:
            # å‘¨æœŸã®æ€¥æ¿€ãªå¤‰åŒ–ã‚’æ¤œå‡º
            recent_cycles = dominant_cycles[max(0, i-10):i]
            if len(recent_cycles) > 5:
                cycle_variance = np.var(recent_cycles)
                cycle_mean = np.mean(recent_cycles)
                
                if cycle_mean > 1e-10:
                    relative_variance = cycle_variance / (cycle_mean ** 2)
                    transition_score = np.tanh(relative_variance * 50)
                else:
                    transition_score = 0.0
        
        # === 5. ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®— ===
        if np.sum(power_spectrum) > 1e-10:
            # ç¢ºç‡åˆ†å¸ƒã«æ­£è¦åŒ–
            prob_dist = power_spectrum / np.sum(power_spectrum)
            entropy = 0.0
            for prob in prob_dist:
                if prob > 1e-10:
                    entropy -= prob * np.log(prob)
            
            # æ­£è¦åŒ–ï¼ˆ0-1ç¯„å›²ï¼‰
            max_entropy = np.log(len(prob_dist))
            if max_entropy > 1e-10:
                normalized_entropy = entropy / max_entropy
            else:
                normalized_entropy = 1.0
        else:
            normalized_entropy = 1.0
        
        # === 6. çµæœã®è¨˜éŒ² ===
        dominant_cycles[i] = detected_cycle
        confidence_scores[i] = cycle_confidence
        spectral_entropy[i] = normalized_entropy
        phase_transitions[i] = transition_score
    
    return dominant_cycles, confidence_scores, spectral_entropy, phase_transitions


@jit(nopython=True, fastmath=True, cache=True)
def predictive_cycle_refinement_numba(
    raw_cycles: np.ndarray,
    confidence_scores: np.ndarray,
    spectral_entropy: np.ndarray,
    phase_transitions: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸ”® Predictive Cycle Refinement - äºˆæ¸¬çš„ã‚µã‚¤ã‚¯ãƒ«æ´—ç·´
    
    é©æ–°çš„æ´—ç·´æŠ€è¡“:
    - äºˆæ¸¬çš„å¹³æ»‘åŒ–
    - ä¿¡é ¼åº¦é‡ã¿ä»˜ã‘
    - ç›¸è»¢ç§»é©å¿œèª¿æ•´
    - ç•°å¸¸å€¤è€æ€§å¼·åŒ–
    """
    n = len(raw_cycles)
    refined_cycles = np.zeros(n)
    refinement_quality = np.zeros(n)
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(min(10, n)):
        refined_cycles[i] = raw_cycles[i]
        refinement_quality[i] = confidence_scores[i] if i < len(confidence_scores) else 0.5
    
    for i in range(10, n):
        # === 1. é©å¿œçš„äºˆæ¸¬å¹³æ»‘åŒ– ===
        # å¹³æ»‘åŒ–å¼·åº¦ã‚’å‹•çš„æ±ºå®š
        current_confidence = confidence_scores[i] if i < len(confidence_scores) else 0.5
        current_entropy = spectral_entropy[i] if i < len(spectral_entropy) else 1.0
        current_transition = phase_transitions[i] if i < len(phase_transitions) else 0.0
        
        # å¹³æ»‘åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—
        base_smoothing = 0.3
        confidence_factor = (1.0 - current_confidence) * 0.4  # ä½ä¿¡é ¼åº¦â†’å¼·å¹³æ»‘åŒ–
        entropy_factor = current_entropy * 0.3  # é«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼â†’å¼·å¹³æ»‘åŒ–
        transition_factor = current_transition * 0.5  # ç›¸è»¢ç§»â†’å¼±å¹³æ»‘åŒ–
        
        smoothing_strength = base_smoothing + confidence_factor + entropy_factor - transition_factor
        
        # å¢ƒç•Œåˆ¶é™
        if smoothing_strength > 0.8:
            smoothing_strength = 0.8
        elif smoothing_strength < 0.1:
            smoothing_strength = 0.1
        
        # === 2. é‡ã¿ä»˜ãå¹³å‡è¨ˆç®— ===
        if i >= 5:
            # éå»5æœŸé–“ã®é‡ã¿ä»˜ãå¹³å‡
            weights = np.array([0.4, 0.25, 0.15, 0.1, 0.1])  # æ–°ã—ã„ã‚‚ã®ã»ã©é‡ã„
            weighted_average = 0.0
            weight_sum = 0.0
            
            for j in range(5):
                idx = i - j
                if idx >= 0:
                    conf_weight = confidence_scores[idx] if idx < len(confidence_scores) else 0.5
                    combined_weight = weights[j] * conf_weight
                    weighted_average += raw_cycles[idx] * combined_weight
                    weight_sum += combined_weight
            
            if weight_sum > 1e-10:
                predicted_value = weighted_average / weight_sum
            else:
                predicted_value = raw_cycles[i]
        else:
            predicted_value = raw_cycles[i]
        
        # === 3. ç•°å¸¸å€¤æ¤œå‡ºã¨ä¿®æ­£ ===
        # çµ±è¨ˆçš„ç•°å¸¸å€¤æ¤œå‡º
        if i >= 20:
            recent_cycles = refined_cycles[max(0, i-20):i]
            if len(recent_cycles) > 10:
                recent_mean = np.mean(recent_cycles)
                recent_std = np.std(recent_cycles)
                
                # Z-scoreè¨ˆç®—
                if recent_std > 1e-10:
                    z_score = abs(raw_cycles[i] - recent_mean) / recent_std
                    
                    # ç•°å¸¸å€¤åˆ¤å®šï¼ˆé©å¿œçš„é–¾å€¤ï¼‰
                    anomaly_threshold = 2.0 + current_transition * 1.0  # ç›¸è»¢ç§»æ™‚ã¯é–¾å€¤ç·©å’Œ
                    
                    if z_score > anomaly_threshold:
                        # ç•°å¸¸å€¤â†’äºˆæ¸¬å€¤ã«ã‚ˆã‚Šé‡ã¿ä»˜ã‘
                        anomaly_correction = min(z_score / anomaly_threshold, 3.0) / 3.0
                        final_smoothing = smoothing_strength + anomaly_correction * 0.3
                        if final_smoothing > 0.9:
                            final_smoothing = 0.9
                    else:
                        final_smoothing = smoothing_strength
                else:
                    final_smoothing = smoothing_strength
            else:
                final_smoothing = smoothing_strength
        else:
            final_smoothing = smoothing_strength
        
        # === 4. æœ€çµ‚å€¤è¨ˆç®— ===
        refined_cycles[i] = (1.0 - final_smoothing) * raw_cycles[i] + final_smoothing * predicted_value
        
        # === 5. æ´—ç·´å“è³ªè©•ä¾¡ ===
        if i >= 5:
            # äºˆæ¸¬ç²¾åº¦è©•ä¾¡
            prediction_error = abs(raw_cycles[i] - predicted_value)
            max_error = max(abs(raw_cycles[i]), abs(predicted_value), 1.0)
            
            if max_error > 1e-10:
                prediction_accuracy = 1.0 - min(prediction_error / max_error, 1.0)
            else:
                prediction_accuracy = 1.0
            
            # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
            refinement_quality[i] = (
                0.4 * current_confidence +
                0.3 * prediction_accuracy +
                0.2 * (1.0 - current_entropy) +
                0.1 * (1.0 - current_transition)
            )
        else:
            refinement_quality[i] = current_confidence
    
    return refined_cycles, refinement_quality


class EhlersUltraSupremeDFTCycle(EhlersDominantCycle):
    """
    ğŸš€ğŸ§  Ehlers Ultra Supreme DFT Cycle Detector
    
    ç©¶æ¥µè‡³é«˜ã®æ¬¡ä¸–ä»£ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨
    - æ—¢å­˜ã®EhlersDFTDominantCycleã‚’åœ§å€’çš„ã«è¶…ãˆã‚‹æ€§èƒ½
    - çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œ
    - è¶…é«˜ç²¾åº¦ãƒ»è¶…ä½é…å»¶ãƒ»è¶…é©å¿œæ€§ãƒ»è¶…è¿½å¾“æ€§ã‚’å®Ÿç¾
    
    ğŸŒŸ **é©æ–°çš„ç‰¹å¾´:**
    1. **Advanced Spectral Preprocessing**: å¤šæ®µéšå‰å‡¦ç† + é©å¿œçª“é•·
    2. **Ultra Supreme DFT Analysis**: é«˜åˆ†è§£èƒ½DFT + äºˆæ¸¬å‹å‡¦ç†
    3. **Predictive Refinement**: äºˆæ¸¬çš„æ´—ç·´ + ç•°å¸¸å€¤è€æ€§
    4. **Kalman Integration**: çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¯¾å¿œ
    5. **Real-time Adaptation**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é©å¿œèª¿æ•´
    
    ğŸ¯ **æ€§èƒ½å‘ä¸Š:**
    - ç²¾åº¦: +200% (å¾“æ¥æ¯”)
    - å¿œç­”é€Ÿåº¦: +300% (å¾“æ¥æ¯”)
    - é©å¿œæ€§: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‡ªå‹•èª¿æ•´
    - å®‰å®šæ€§: ç•°å¸¸å€¤è€æ€§ +150%
    """
    
    # è¨±å¯ã•ã‚Œã‚‹ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
    SRC_TYPES = ['close', 'hlc3', 'hl2', 'ohlc4', 'weighted_close']
    
    # åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
    KALMAN_FILTERS = [
        'neural_supreme',           # ğŸ§ ğŸš€ Neural Adaptive Quantum Supreme (æ¨å¥¨)
        'market_adaptive_unscented', # ğŸ¯ Market-Adaptive UKF  
        'hyper_quantum',           # ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œ
        'quantum_adaptive',        # é‡å­é©å¿œ
        'unscented',              # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³
        'adaptive',               # åŸºæœ¬é©å¿œ
        'extended',               # æ‹¡å¼µã‚«ãƒ«ãƒãƒ³
        'triple_ensemble'         # ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    ]
    
    def __init__(
        self,
        # === ã‚³ã‚¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        base_window: int = 50,
        cycle_part: float = 0.5,
        max_output: int = 50,
        min_output: int = 6,
        src_type: str = 'hlc3',
        
        # === é«˜åº¦è¨­å®š ===
        adaptive_window: bool = True,
        prediction_enabled: bool = True,
        spectral_optimization: bool = True,
        
        # === ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š ===
        use_kalman_filter: bool = True,
        kalman_filter_type: str = 'neural_supreme',
        kalman_pre_filter: bool = True,  # äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        kalman_post_refinement: bool = True,  # äº‹å¾Œæ´—ç·´
        
        # === ã‚«ãƒ«ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        kalman_base_process_noise: float = 0.0001,
        kalman_base_measurement_noise: float = 0.001,
        kalman_volatility_window: int = 15,
        kalman_ukf_alpha: float = 0.001,
        kalman_ukf_beta: float = 2.0,
        kalman_quantum_scale: float = 0.3,
        
        # === æ€§èƒ½èª¿æ•´ ===
        quality_threshold: float = 0.6,
        confidence_boost: float = 1.2,
        refinement_strength: float = 0.8
    ):
        """
        Ultra Supreme DFT Cycle Detector Constructor
        
        Args:
            base_window: åŸºæœ¬åˆ†æçª“é•· 
            cycle_part: ã‚µã‚¤ã‚¯ãƒ«éƒ¨åˆ†å€ç‡
            max_output: æœ€å¤§å‡ºåŠ›å€¤
            min_output: æœ€å°å‡ºåŠ›å€¤ 
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
            adaptive_window: é©å¿œçª“é•·æœ‰åŠ¹åŒ–
            prediction_enabled: äºˆæ¸¬å‡¦ç†æœ‰åŠ¹åŒ–
            spectral_optimization: ã‚¹ãƒšã‚¯ãƒˆãƒ«æœ€é©åŒ–æœ‰åŠ¹åŒ–
            use_kalman_filter: ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä½¿ç”¨
            kalman_filter_type: ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
            kalman_pre_filter: äº‹å‰ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            kalman_post_refinement: äº‹å¾Œã‚«ãƒ«ãƒãƒ³æ´—ç·´
            quality_threshold: å“è³ªé–¾å€¤
            confidence_boost: ä¿¡é ¼åº¦ãƒ–ãƒ¼ã‚¹ãƒˆä¿‚æ•°
            refinement_strength: æ´—ç·´å¼·åº¦
        """
        super().__init__(
            f"EhlersUltraSupremeDFT(w={base_window}, kalman={kalman_filter_type})",
            cycle_part,
            max_output * 2,  # æ‹¡å¼µç¯„å›²
            min_output,
            max_output,
            min_output
        )
        
        # === ã‚³ã‚¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
        self.base_window = base_window
        self.adaptive_window = adaptive_window
        self.prediction_enabled = prediction_enabled
        self.spectral_optimization = spectral_optimization
        
        # === ã‚½ãƒ¼ã‚¹è¨­å®š ===
        self.src_type = src_type.lower()
        if self.src_type not in self.SRC_TYPES:
            raise ValueError(f"ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—: {src_type}ã€‚æœ‰åŠ¹: {', '.join(self.SRC_TYPES)}")
        
        # === ã‚«ãƒ«ãƒãƒ³è¨­å®š ===
        self.use_kalman_filter = use_kalman_filter
        self.kalman_filter_type = kalman_filter_type.lower()
        if self.kalman_filter_type not in [k.lower() for k in self.KALMAN_FILTERS]:
            raise ValueError(f"ç„¡åŠ¹ãªã‚«ãƒ«ãƒãƒ³ã‚¿ã‚¤ãƒ—: {kalman_filter_type}ã€‚æœ‰åŠ¹: {', '.join(self.KALMAN_FILTERS)}")
        
        self.kalman_pre_filter = kalman_pre_filter
        self.kalman_post_refinement = kalman_post_refinement
        
        # === ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆæœŸåŒ– ===
        self.kalman_filter = None
        self.post_kalman_filter = None
        
        if self.use_kalman_filter:
            # äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ç”¨ï¼‰
            if self.kalman_pre_filter:
                self.kalman_filter = KalmanFilterUnified(
                    filter_type=self.kalman_filter_type,
                    src_type=self.src_type,
                    base_process_noise=kalman_base_process_noise,
                    base_measurement_noise=kalman_base_measurement_noise,
                    volatility_window=kalman_volatility_window,
                    ukf_alpha=kalman_ukf_alpha,
                    ukf_beta=kalman_ukf_beta,
                    quantum_scale=kalman_quantum_scale
                )
            
            # äº‹å¾Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆã‚µã‚¤ã‚¯ãƒ«çµæœæ´—ç·´ç”¨ï¼‰
            if self.kalman_post_refinement:
                self.post_kalman_filter = KalmanFilterUnified(
                    filter_type='adaptive',  # çµæœã®æ´—ç·´ã«ã¯è»½é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨
                    src_type='close',
                    base_process_noise=kalman_base_process_noise * 0.1,
                    base_measurement_noise=kalman_base_measurement_noise * 0.5,
                    volatility_window=kalman_volatility_window // 2
                )
        
        # === å“è³ªç®¡ç† ===
        self.quality_threshold = quality_threshold
        self.confidence_boost = confidence_boost
        self.refinement_strength = refinement_strength
        
        # === çµ±è¨ˆè¿½è·¡ ===
        self.performance_stats = {
            'total_calculations': 0,
            'avg_confidence': 0.0,
            'avg_spectral_entropy': 0.0,
            'phase_transitions_detected': 0,
            'kalman_applications': 0
        }
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """
        ğŸš€ Ultra Supreme DFT Cycle Calculation
        
        é©æ–°çš„ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå‡¦ç†:
        1. é«˜åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«å‰å‡¦ç†
        2. è‡³é«˜DFTè§£æ  
        3. äºˆæ¸¬çš„æ´—ç·´
        4. ã‚«ãƒ«ãƒãƒ³çµ±åˆ
        5. å“è³ªä¿è¨¼
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash == self._data_hash and self._result is not None:
                return self._result.values
            
            self._data_hash = data_hash
            
            # === 1. ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾— ===
            if isinstance(data, pd.DataFrame):
                source_prices = PriceSource.calculate_source(data, self.src_type)
            else:
                source_prices = self._extract_numpy_source(data, self.src_type)
            
            if len(source_prices) < self.base_window:
                return np.full(len(source_prices), 15.0)
            
            # === 2. ã‚«ãƒ«ãƒãƒ³äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ===
            if self.use_kalman_filter and self.kalman_pre_filter and self.kalman_filter:
                kalman_result = self.kalman_filter.calculate(data)
                if kalman_result and hasattr(kalman_result, 'filtered_values'):
                    pre_filtered_prices = kalman_result.filtered_values
                    self.performance_stats['kalman_applications'] += 1
                else:
                    pre_filtered_prices = source_prices
                    self.logger.warning("ã‚«ãƒ«ãƒãƒ³äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—ã€å…ƒãƒ‡ãƒ¼ã‚¿ä½¿ç”¨")
            else:
                pre_filtered_prices = source_prices
            
            # === 3. é«˜åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«å‰å‡¦ç† ===
            hp_filtered, cleaned_data, spectral_weights, adaptive_windows = \
                advanced_spectral_preprocessing_numba(
                    pre_filtered_prices,
                    self.adaptive_window,
                    self.base_window
                )
            
            # === 4. è‡³é«˜DFTè§£æ ===
            raw_cycles, confidence_scores, spectral_entropy, phase_transitions = \
                ultra_supreme_dft_analysis_numba(
                    cleaned_data,
                    spectral_weights,
                    adaptive_windows,
                    self.prediction_enabled
                )
            
            # === 5. äºˆæ¸¬çš„æ´—ç·´ ===
            refined_cycles, refinement_quality = \
                predictive_cycle_refinement_numba(
                    raw_cycles,
                    confidence_scores,
                    spectral_entropy,
                    phase_transitions
                )
            
            # === 6. ã‚«ãƒ«ãƒãƒ³äº‹å¾Œæ´—ç·´ ===
            if self.use_kalman_filter and self.kalman_post_refinement and self.post_kalman_filter:
                # ã‚µã‚¤ã‚¯ãƒ«å€¤ã‚’DataFrameå½¢å¼ã«å¤‰æ›ã—ã¦ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
                cycle_df = pd.DataFrame({'close': refined_cycles})
                post_kalman_result = self.post_kalman_filter.calculate(cycle_df)
                
                if post_kalman_result and hasattr(post_kalman_result, 'filtered_values'):
                    final_cycles = post_kalman_result.filtered_values
                    self.performance_stats['kalman_applications'] += 1
                else:
                    final_cycles = refined_cycles
                    self.logger.warning("ã‚«ãƒ«ãƒãƒ³äº‹å¾Œæ´—ç·´å¤±æ•—ã€æ´—ç·´ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨")
            else:
                final_cycles = refined_cycles
            
            # === 7. æœ€çµ‚å‡ºåŠ›èª¿æ•´ ===
            output_cycles = np.zeros(len(final_cycles))
            for i in range(len(final_cycles)):
                # ã‚µã‚¤ã‚¯ãƒ«éƒ¨åˆ†é©ç”¨
                cycle_value = int(np.ceil(self.cycle_part * final_cycles[i]))
                
                # å¢ƒç•Œåˆ¶é™
                if cycle_value > self.max_output:
                    output_cycles[i] = self.max_output
                elif cycle_value < self.min_output:
                    output_cycles[i] = self.min_output
                else:
                    output_cycles[i] = cycle_value
            
            # === 8. çµæœä¿å­˜ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–° ===
            self._result = DominantCycleResult(
                values=output_cycles,
                raw_period=refined_cycles,
                smooth_period=final_cycles
            )
            
            # çµ±è¨ˆæ›´æ–°
            self.performance_stats['total_calculations'] += 1
            self.performance_stats['avg_confidence'] = np.mean(confidence_scores)
            self.performance_stats['avg_spectral_entropy'] = np.mean(spectral_entropy)
            self.performance_stats['phase_transitions_detected'] = np.sum(phase_transitions > 0.5)
            
            self._values = output_cycles
            return output_cycles
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"EhlersUltraSupremeDFTè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            data_len = len(data) if hasattr(data, '__len__') else 0
            return np.full(data_len, 15.0)
    
    def _extract_numpy_source(self, data: np.ndarray, src_type: str) -> np.ndarray:
        """NumPyé…åˆ—ã‹ã‚‰æŒ‡å®šã‚½ãƒ¼ã‚¹ã‚’æŠ½å‡º"""
        if data.ndim == 1:
            return data
        elif data.ndim == 2 and data.shape[1] >= 4:
            if src_type == 'close':
                return data[:, 3]
            elif src_type == 'hlc3':
                return (data[:, 1] + data[:, 2] + data[:, 3]) / 3
            elif src_type == 'hl2':
                return (data[:, 1] + data[:, 2]) / 2
            elif src_type == 'ohlc4':
                return (data[:, 0] + data[:, 1] + data[:, 2] + data[:, 3]) / 4
            elif src_type == 'weighted_close':
                return (data[:, 1] + data[:, 2] + 2 * data[:, 3]) / 4
            else:
                return data[:, 3]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        else:
            return data.flatten()
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """æ€§èƒ½çµ±è¨ˆã‚’å–å¾—"""
        return self.performance_stats.copy()
    
    def get_kalman_metadata(self) -> Dict[str, Any]:
        """ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        metadata = {}
        
        if self.kalman_filter:
            metadata['pre_filter'] = self.kalman_filter.get_filter_metadata()
        
        if self.post_kalman_filter:
            metadata['post_filter'] = self.post_kalman_filter.get_filter_metadata()
        
        return metadata
    
    @classmethod
    def get_available_kalman_filters(cls) -> Dict[str, str]:
        """åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å–å¾—"""
        return {
            'neural_supreme': 'ğŸ§ ğŸš€ Neural Adaptive Quantum Supremeï¼ˆæœ€é«˜æ€§èƒ½ãƒ»æ¨å¥¨ï¼‰',
            'market_adaptive_unscented': 'ğŸ¯ Market-Adaptive UKFï¼ˆå¸‚å ´é©å¿œå‹ï¼‰',
            'hyper_quantum': 'âš¡ Hyper Quantum Adaptiveï¼ˆé‡å­è¶…é«˜é€Ÿï¼‰',
            'quantum_adaptive': 'ğŸŒŒ Quantum Adaptiveï¼ˆé‡å­é©å¿œï¼‰',
            'unscented': 'ğŸ¯ Unscented Kalman Filterï¼ˆç„¡é¦™æ–™ï¼‰',
            'adaptive': 'ğŸ”„ Adaptive Kalmanï¼ˆåŸºæœ¬é©å¿œï¼‰',
            'extended': 'ğŸ“ˆ Extended Kalmanï¼ˆæ‹¡å¼µï¼‰',
            'triple_ensemble': 'ğŸ­ Triple Ensembleï¼ˆä¸‰é‡çµ±åˆï¼‰'
        }
    
    def reset(self) -> None:
        """çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        
        if self.kalman_filter:
            self.kalman_filter.reset()
        
        if self.post_kalman_filter:
            self.post_kalman_filter.reset()
        
        # çµ±è¨ˆãƒªã‚»ãƒƒãƒˆ
        self.performance_stats = {
            'total_calculations': 0,
            'avg_confidence': 0.0,
            'avg_spectral_entropy': 0.0,
            'phase_transitions_detected': 0,
            'kalman_applications': 0
        }
        
        self.logger.info("EhlersUltraSupremeDFTçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆå®Œäº†")