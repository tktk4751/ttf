#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from typing import Union, Optional, Tuple, Dict, List
import numpy as np
import pandas as pd
from numba import jit, prange, float64, int32, types
from scipy import signal
from scipy.stats import entropy
import warnings
warnings.filterwarnings('ignore')

from .ehlers_dominant_cycle import EhlersDominantCycle, DominantCycleResult
from indicators.kalman.unified_kalman import UnifiedKalman


@jit(nopython=True)
def calculate_quantum_entropy(data: np.ndarray, window: int = 20) -> Tuple[float, float]:
    """
    é‡å­çš„æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ãƒ•ã‚©ãƒ³ãƒ»ãƒã‚¤ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—
    """
    if len(data) < window:
        return 0.5, 0.5
    
    recent_data = data[-window:]
    data_range = np.max(recent_data) - np.min(recent_data)
    if data_range == 0:
        return 0.0, 0.0
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ï¼ˆé‡å­çŠ¶æ…‹ã¨ã—ã¦æ‰±ã†ï¼‰
    normalized = (recent_data - np.min(recent_data)) / data_range
    
    # ç¢ºç‡åˆ†å¸ƒã‚’è¨ˆç®—
    hist, _ = np.histogram(normalized, bins=12, range=(0, 1))
    hist = hist.astype(np.float64)
    total = np.sum(hist)
    if total == 0:
        return 0.0, 0.0
    
    prob_dist = hist / total
    
    # ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    shannon_entropy = 0.0
    for p in prob_dist:
        if p > 0:
            shannon_entropy -= p * np.log2(p)
    
    # é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆãƒ•ã‚©ãƒ³ãƒ»ãƒã‚¤ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¿‘ä¼¼ï¼‰
    quantum_entropy = 0.0
    for p in prob_dist:
        if p > 0:
            # é‡å­çš„é‡ã­åˆã‚ã›ã‚’è€ƒæ…®ã—ãŸä¿®æ­£é …
            superposition_factor = np.sqrt(p) * (1 - p)
            quantum_entropy -= (p + superposition_factor) * np.log2(p + superposition_factor + 1e-10)
    
    return shannon_entropy / np.log2(12), quantum_entropy / np.log2(12)


@jit(nopython=True)
def adaptive_kalman_filter(
    observations: np.ndarray,
    initial_period: float = 20.0,
    process_noise: float = 0.01,
    observation_noise: float = 0.1
) -> Tuple[np.ndarray, np.ndarray]:
    """
    é©å¿œå‹Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚‹ã‚µã‚¤ã‚¯ãƒ«æœŸé–“ã®æ¨å®š
    """
    n = len(observations)
    
    # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«: [period, velocity]
    state = np.array([initial_period, 0.0])
    covariance = np.array([[1.0, 0.0], [0.0, 1.0]])
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
    F = np.array([[1.0, 1.0], [0.0, 1.0]])  # çŠ¶æ…‹é·ç§»
    H = np.array([[1.0, 0.0]])  # è¦³æ¸¬ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
    
    filtered_periods = np.zeros(n)
    uncertainties = np.zeros(n)
    
    for i in range(n):
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        state_pred = F @ state
        cov_pred = F @ covariance @ F.T + np.array([[process_noise, 0.0], [0.0, process_noise]])
        
        # é©å¿œå‹ãƒã‚¤ã‚ºèª¿æ•´
        adaptive_obs_noise = observation_noise * (1 + 0.1 * abs(observations[i] - state_pred[0]) / state_pred[0])
        R = np.array([[adaptive_obs_noise]])
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = observations[i] - H @ state_pred
        innovation_cov = H @ cov_pred @ H.T + R
        
        if innovation_cov[0, 0] > 0:
            kalman_gain = cov_pred @ H.T / innovation_cov[0, 0]
            state = state_pred + kalman_gain.flatten() * innovation[0]
            covariance = cov_pred - kalman_gain.reshape(-1, 1) @ H @ cov_pred
        else:
            state = state_pred
            covariance = cov_pred
        
        filtered_periods[i] = max(6.0, min(50.0, state[0]))
        uncertainties[i] = np.sqrt(covariance[0, 0])
    
    return filtered_periods, uncertainties


@jit(nopython=True)
def wavelet_cycle_detection(
    price: np.ndarray,
    scales: np.ndarray = np.array([8, 12, 16, 20, 24, 32, 40])
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ã«ã‚ˆã‚‹å¤šã‚¹ã‚±ãƒ¼ãƒ«ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º
    """
    n = len(price)
    n_scales = len(scales)
    
    # Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã«ã‚ˆã‚‹é€£ç¶šã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ã®è¿‘ä¼¼
    cwt_coeffs = np.zeros((n_scales, n))
    
    for scale_idx, scale in enumerate(scales):
        for i in range(n):
            coeff = 0.0
            norm_factor = 0.0
            
            # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä¿‚æ•°ã®è¨ˆç®—
            for j in range(max(0, i - int(2 * scale)), min(n, i + int(2 * scale) + 1)):
                t = (j - i) / scale
                if abs(t) <= 3:  # è¨ˆç®—ç¯„å›²ã‚’åˆ¶é™
                    # Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
                    wavelet_val = np.exp(-0.5 * t * t) * np.cos(5 * t)
                    coeff += price[j] * wavelet_val
                    norm_factor += wavelet_val * wavelet_val
            
            if norm_factor > 0:
                cwt_coeffs[scale_idx, i] = coeff / np.sqrt(norm_factor)
    
    # æœ€å¤§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æ¤œå‡º
    energy = np.abs(cwt_coeffs)
    dominant_scales = np.zeros(n)
    energy_levels = np.zeros(n)
    
    for i in range(n):
        max_idx = np.argmax(energy[:, i])
        dominant_scales[i] = scales[max_idx]
        energy_levels[i] = energy[max_idx, i]
    
    return dominant_scales, energy_levels


@jit(nopython=True)
def quantum_ensemble_weights(
    methods_periods: np.ndarray,
    methods_uncertainties: np.ndarray,
    quantum_entropy: float,
    performance_history: np.ndarray
) -> np.ndarray:
    """
    é‡å­çš„ç¢ºç‡è«–ã«åŸºã¥ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿è¨ˆç®—
    """
    n_methods = len(methods_periods)
    if n_methods == 0:
        return np.ones(1)
    
    weights = np.zeros(n_methods)
    
    for i in range(n_methods):
        # ä¸ç¢ºå®Ÿæ€§ãƒ™ãƒ¼ã‚¹ã®é‡ã¿ï¼ˆä½ã„ä¸ç¢ºå®Ÿæ€§ã»ã©é«˜ã„é‡ã¿ï¼‰
        uncertainty_weight = 1.0 / (1.0 + methods_uncertainties[i])
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ãƒ™ãƒ¼ã‚¹ã®é‡ã¿
        performance_weight = performance_history[i] if len(performance_history) > i else 0.5
        
        # é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«ã‚ˆã‚‹èª¿æ•´
        quantum_factor = np.exp(-quantum_entropy * 2.0)
        
        # é‡ã­åˆã‚ã›åŠ¹æœï¼ˆé‡å­çš„å¹²æ¸‰ï¼‰
        superposition_factor = np.sqrt(uncertainty_weight * performance_weight)
        
        weights[i] = (uncertainty_weight * performance_weight * quantum_factor + 
                     0.3 * superposition_factor)
    
    # æ­£è¦åŒ–
    total_weight = np.sum(weights)
    if total_weight > 0:
        weights = weights / total_weight
    else:
        weights = np.ones(n_methods) / n_methods
    
    return weights


@jit(nopython=True)
def multi_timeframe_analysis(
    price: np.ndarray,
    timeframes: np.ndarray = np.array([5, 10, 20, 40])
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ
    """
    n = len(price)
    n_timeframes = len(timeframes)
    
    mtf_periods = np.zeros((n_timeframes, n))
    mtf_confidences = np.zeros((n_timeframes, n))
    
    for tf_idx, tf in enumerate(timeframes):
        # ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        if tf > 1:
            sub_indices = np.arange(0, n, tf)
            sub_price = price[sub_indices]
        else:
            sub_price = price
        
        # å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã®ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºï¼ˆç°¡å˜ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰
        for i in range(len(sub_price)):
            if i < 10:
                mtf_periods[tf_idx, i * tf if tf > 1 else i] = 20.0
                mtf_confidences[tf_idx, i * tf if tf > 1 else i] = 0.5
            else:
                # ç°¡å˜ãªå‘¨æœŸæ¤œå‡ºï¼ˆã‚ˆã‚Šé«˜åº¦ãªæ‰‹æ³•ã«ç½®ãæ›ãˆå¯èƒ½ï¼‰
                window = min(20, i)
                recent_data = sub_price[i-window:i+1]
                
                # è‡ªå·±ç›¸é–¢ã«ã‚ˆã‚‹å‘¨æœŸæ¤œå‡º
                max_corr = 0.0
                best_period = 20.0
                
                for period in range(6, min(30, window)):
                    if i >= period:
                        corr = np.corrcoef(recent_data[:-period], recent_data[period:])[0, 1]
                        if not np.isnan(corr) and abs(corr) > max_corr:
                            max_corr = abs(corr)
                            best_period = float(period * tf)
                
                actual_idx = i * tf if tf > 1 else i
                if actual_idx < n:
                    mtf_periods[tf_idx, actual_idx] = best_period
                    mtf_confidences[tf_idx, actual_idx] = max_corr
    
    # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§ã®ä¸€è‡´åº¦ã‚’è¨ˆç®—
    consensus_periods = np.zeros(n)
    consensus_confidences = np.zeros(n)
    
    for i in range(n):
        weights = mtf_confidences[:, i]
        if np.sum(weights) > 0:
            weights = weights / np.sum(weights)
            consensus_periods[i] = np.sum(mtf_periods[:, i] * weights)
            consensus_confidences[i] = np.mean(mtf_confidences[:, i])
        else:
            consensus_periods[i] = 20.0
            consensus_confidences[i] = 0.5
    
    return consensus_periods, consensus_confidences


@jit(nopython=True)
def calculate_quantum_adaptive_cycle_numba(
    price: np.ndarray,
    cycle_part: float = 0.5,
    max_output: int = 34,
    min_output: int = 1,
    entropy_window: int = 20,
    period_range: Tuple[int, int] = (6, 50)
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    é‡å­é©å¿œå‹ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    n = len(price)
    
    # 1. é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
    quantum_entropies = np.zeros(n)
    shannon_entropies = np.zeros(n)
    
    for i in range(entropy_window, n):
        shannon_ent, quantum_ent = calculate_quantum_entropy(
            price[max(0, i-entropy_window):i+1], entropy_window
        )
        shannon_entropies[i] = shannon_ent
        quantum_entropies[i] = quantum_ent
    
    # 2. ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ
    scales = np.array([8.0, 12.0, 16.0, 20.0, 24.0, 32.0, 40.0])
    wavelet_periods, wavelet_energies = wavelet_cycle_detection(price, scales)
    
    # 3. ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ
    timeframes = np.array([1, 2, 4, 8])
    mtf_periods, mtf_confidences = multi_timeframe_analysis(price, timeframes)
    
    # 4. è¤‡æ•°æ‰‹æ³•ã«ã‚ˆã‚‹ã‚µã‚¤ã‚¯ãƒ«æœŸé–“æ¨å®š
    # åŸºæœ¬çš„ãªHilbert-basedæ‰‹æ³•ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    hilbert_periods = np.zeros(n)
    for i in range(10, n):
        # ç°¡å˜ãªHilbert transform approximation
        window_size = min(20, i)
        recent_prices = price[i-window_size:i+1]
        
        # è‡ªå·±ç›¸é–¢ã«åŸºã¥ãå‘¨æœŸæ¨å®š
        max_corr = 0.0
        best_period = 20.0
        
        for period in range(period_range[0], min(period_range[1], window_size//2)):
            if len(recent_prices) >= 2 * period:
                corr = np.corrcoef(recent_prices[:-period], recent_prices[period:])[0, 1]
                if not np.isnan(corr) and abs(corr) > max_corr:
                    max_corr = abs(corr)
                    best_period = float(period)
        
        hilbert_periods[i] = best_period
    
    # åˆæœŸåŒ–
    for i in range(10):
        hilbert_periods[i] = 20.0
    
    # 5. Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚‹å¹³æ»‘åŒ–
    kalman_periods, kalman_uncertainties = adaptive_kalman_filter(hilbert_periods)
    wavelet_smooth, wavelet_uncertainties = adaptive_kalman_filter(wavelet_periods)
    mtf_smooth, mtf_uncertainties = adaptive_kalman_filter(mtf_periods)
    
    # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    performance_history = np.array([0.7, 0.8, 0.6])  # [Kalman, Wavelet, MTF]
    
    # 7. é‡å­ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆ
    final_periods = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    for i in range(n):
        methods_periods = np.array([kalman_periods[i], wavelet_smooth[i], mtf_smooth[i]])
        methods_uncertainties = np.array([kalman_uncertainties[i], wavelet_uncertainties[i], mtf_uncertainties[i]])
        
        # é‡å­é‡ã¿è¨ˆç®—
        weights = quantum_ensemble_weights(
            methods_periods, methods_uncertainties, quantum_entropies[i], performance_history
        )
        
        # é‡ã¿ä»˜ãå¹³å‡
        final_periods[i] = np.sum(methods_periods * weights)
        confidence_scores[i] = 1.0 / (1.0 + np.sum(methods_uncertainties * weights))
    
    # 8. æœ€çµ‚ã‚µã‚¤ã‚¯ãƒ«å€¤è¨ˆç®—
    dom_cycle = np.zeros(n)
    for i in range(n):
        cycle_value = np.ceil(final_periods[i] * cycle_part)
        dom_cycle[i] = max(min_output, min(max_output, cycle_value))
    
    return dom_cycle, final_periods, confidence_scores, quantum_entropies, shannon_entropies


class EhlersQuantumAdaptiveCycle(EhlersDominantCycle):
    """
    ç©¶æ¥µã®é‡å­é©å¿œå‹ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨
    
    ã“ã®é©æ–°çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ä»¥ä¸‹ã®æœ€å…ˆç«¯æŠ€è¡“ã‚’çµ±åˆã—ã¾ã™:
    
    ğŸŒŸ **é©æ–°çš„ãªç‰¹å¾´:**
    1. **é‡å­æƒ…å ±ç†è«–**: ãƒ•ã‚©ãƒ³ãƒ»ãƒã‚¤ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«ã‚ˆã‚‹é«˜åº¦ãªãƒã‚¤ã‚ºè©•ä¾¡
    2. **é©å¿œå‹Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: å‹•çš„ãƒã‚¤ã‚ºæ¨å®šã¨çŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
    3. **ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šã‚¹ã‚±ãƒ¼ãƒ«è§£æ**: æ™‚é–“-å‘¨æ³¢æ•°é ˜åŸŸã§ã®ç²¾å¯†ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º
    4. **ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ çµ±åˆ**: è¤‡æ•°ã®æ™‚é–“è»¸ã§ã®ä¸€è‡´æ€§è©•ä¾¡
    5. **é‡å­ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: é‡ã­åˆã‚ã›åŠ¹æœã‚’è€ƒæ…®ã—ãŸé‡ã¿ä»˜ã‘
    6. **æ©Ÿæ¢°å­¦ç¿’çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡**: å‹•çš„æ‰‹æ³•é¸æŠ
    
    ğŸ¯ **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
    - å¾“æ¥æ‰‹æ³•ã‚’è¶…ãˆã‚‹å®‰å®šæ€§
    - ãƒã‚¤ã‚ºã«å¯¾ã™ã‚‹é«˜ã„è€æ€§
    - å¸‚å ´çŠ¶æ³ã¸ã®å³åº§ã®é©å¿œ
    - è¤‡æ•°ã‚µã‚¤ã‚¯ãƒ«ã®åŒæ™‚æ¤œå‡º
    - æ¥µé™ã¾ã§ä½ã„é…å»¶
    """
    
    # è¨±å¯ã•ã‚Œã‚‹ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
    SRC_TYPES = ['close', 'hlc3', 'hl2', 'ohlc4']
    
    def __init__(
        self,
        cycle_part: float = 0.5,
        max_output: int = 34,
        min_output: int = 1,
        entropy_window: int = 20,
        period_range: Tuple[int, int] = (6, 50),
        src_type: str = 'close',
        use_kalman_filter: bool = True,
        kalman_filter_type: str = 'quantum_adaptive'
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            cycle_part: ã‚µã‚¤ã‚¯ãƒ«éƒ¨åˆ†ã®å€ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5ï¼‰
            max_output: æœ€å¤§å‡ºåŠ›å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 34ï¼‰
            min_output: æœ€å°å‡ºåŠ›å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
            entropy_window: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰
            period_range: ã‚µã‚¤ã‚¯ãƒ«æœŸé–“ã®ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: (6, 50)ï¼‰
            src_type: ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ— ('close', 'hlc3', 'hl2', 'ohlc4')
            use_kalman_filter: Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
            kalman_filter_type: Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã‚¿ã‚¤ãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'quantum_adaptive'ï¼‰
        """
        super().__init__(
            f"EhlersQuantumAdaptive({cycle_part}, {period_range})",
            cycle_part,
            period_range[1],  # max_cycle
            period_range[0],  # min_cycle
            max_output,
            min_output
        )
        
        self.entropy_window = entropy_window
        self.period_range = period_range
        self.src_type = src_type.lower()
        self.use_kalman_filter = use_kalman_filter
        self.kalman_filter_type = kalman_filter_type
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®æ¤œè¨¼
        if self.src_type not in self.SRC_TYPES:
            raise ValueError(f"ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã§ã™: {src_type}ã€‚æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {', '.join(self.SRC_TYPES)}")
        
        # Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        self.kalman_filter = None
        if self.use_kalman_filter:
            self.kalman_filter = UnifiedKalman(
                filter_type=kalman_filter_type,
                src_type='close'  # å†…éƒ¨çš„ã«ã¯closeã§ä½¿ç”¨
            )
        
        # è¿½åŠ ã®çµæœä¿å­˜ç”¨
        self._confidence_scores = None
        self._quantum_entropies = None
        self._shannon_entropies = None
    
    def calculate_source_values(self, data: Union[pd.DataFrame, np.ndarray], src_type: str) -> np.ndarray:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã™ã‚‹
        """
        if isinstance(data, pd.DataFrame):
            if src_type == 'close':
                if 'close' in data.columns:
                    return data['close'].values
                elif 'Close' in data.columns:
                    return data['Close'].values
                else:
                    raise ValueError("DataFrameã«ã¯'close'ã¾ãŸã¯'Close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'hlc3':
                if all(col in data.columns for col in ['high', 'low', 'close']):
                    return (data['high'] + data['low'] + data['close']).values / 3
                elif all(col in data.columns for col in ['High', 'Low', 'Close']):
                    return (data['High'] + data['Low'] + data['Close']).values / 3
                else:
                    raise ValueError("hlc3ã®è¨ˆç®—ã«ã¯'high', 'low', 'close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'hl2':
                if all(col in data.columns for col in ['high', 'low']):
                    return (data['high'] + data['low']).values / 2
                elif all(col in data.columns for col in ['High', 'Low']):
                    return (data['High'] + data['Low']).values / 2
                else:
                    raise ValueError("hl2ã®è¨ˆç®—ã«ã¯'high', 'low'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'ohlc4':
                if all(col in data.columns for col in ['open', 'high', 'low', 'close']):
                    return (data['open'] + data['high'] + data['low'] + data['close']).values / 4
                elif all(col in data.columns for col in ['Open', 'High', 'Low', 'Close']):
                    return (data['Open'] + data['High'] + data['Low'] + data['Close']).values / 4
                else:
                    raise ValueError("ohlc4ã®è¨ˆç®—ã«ã¯'open', 'high', 'low', 'close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
        
        else:  # NumPyé…åˆ—ã®å ´åˆ
            if data.ndim == 2 and data.shape[1] >= 4:
                if src_type == 'close':
                    return data[:, 3]  # close
                elif src_type == 'hlc3':
                    return (data[:, 1] + data[:, 2] + data[:, 3]) / 3  # high, low, close
                elif src_type == 'hl2':
                    return (data[:, 1] + data[:, 2]) / 2  # high, low
                elif src_type == 'ohlc4':
                    return (data[:, 0] + data[:, 1] + data[:, 2] + data[:, 3]) / 4  # open, high, low, close
            else:
                return data  # 1æ¬¡å…ƒé…åˆ—ã¨ã—ã¦æ‰±ã†
        
        return data
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """
        é‡å­é©å¿œå‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯NumPyé…åˆ—ï¼‰
        
        Returns:
            ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã®å€¤
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash == self._data_hash and self._result is not None:
                return self._result.values
            
            self._data_hash = data_hash
            
            # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            price = self.calculate_source_values(data, self.src_type)
            
            # Numbaé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‚’è¨ˆç®—
            dom_cycle, raw_period, confidence_scores, quantum_entropies, shannon_entropies = calculate_quantum_adaptive_cycle_numba(
                price,
                self.cycle_part,
                self.max_output,
                self.min_output,
                self.entropy_window,
                self.period_range
            )
            
            # çµæœã‚’ä¿å­˜
            self._result = DominantCycleResult(
                values=dom_cycle,
                raw_period=raw_period,
                smooth_period=raw_period  # ã“ã®å®Ÿè£…ã§ã¯åŒã˜
            )
            
            # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            self._confidence_scores = confidence_scores
            self._quantum_entropies = quantum_entropies
            self._shannon_entropies = shannon_entropies
            
            self._values = dom_cycle
            return dom_cycle
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"EhlersQuantumAdaptiveCycleè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            return np.array([])
    
    @property
    def confidence_scores(self) -> Optional[np.ndarray]:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        return self._confidence_scores
    
    @property
    def quantum_entropies(self) -> Optional[np.ndarray]:
        """é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’å–å¾—"""
        return self._quantum_entropies
    
    @property
    def shannon_entropies(self) -> Optional[np.ndarray]:
        """ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’å–å¾—"""
        return self._shannon_entropies
    
    def get_analysis_summary(self) -> Dict:
        """
        åˆ†æã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        """
        if self._result is None:
            return {}
        
        summary = {
            'algorithm': 'Quantum Adaptive Cycle Detector',
            'methods_used': [
                'Quantum Information Theory',
                'Adaptive Kalman Filter', 
                'Wavelet Multi-Scale Analysis',
                'Multi-Timeframe Integration',
                'Quantum Ensemble Weighting'
            ],
            'cycle_range': self.period_range,
            'avg_confidence': np.mean(self._confidence_scores) if self._confidence_scores is not None else None,
            'avg_quantum_entropy': np.mean(self._quantum_entropies) if self._quantum_entropies is not None else None,
            'avg_shannon_entropy': np.mean(self._shannon_entropies) if self._shannon_entropies is not None else None,
            'dominant_cycle_stats': {
                'mean': np.mean(self._result.values),
                'std': np.std(self._result.values),
                'min': np.min(self._result.values),
                'max': np.max(self._result.values)
            },
            'innovations': [
                'Von Neumann Entropy for quantum noise assessment',
                'Adaptive Kalman filtering with dynamic noise estimation',
                'Morlet wavelet multi-scale decomposition',
                'Quantum superposition in ensemble weighting',
                'Multi-timeframe consensus mechanism'
            ]
        }
        
        return summary 