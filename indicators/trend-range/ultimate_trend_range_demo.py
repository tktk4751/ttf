#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import os
import sys
import time
import warnings
warnings.filterwarnings('ignore')

# matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆè­¦å‘Šã‚’ç„¡åŠ¹åŒ–
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
import logging
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

# ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ultimate_trend_range_detector import UltimateTrendRangeDetector

# ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ã®ä¾å­˜é–¢ä¿‚ï¼ˆconfig.yamlå¯¾å¿œï¼‰
try:
    import yaml
    from data.data_loader import DataLoader, CSVDataSource
    from data.data_processor import DataProcessor
    from data.binance_data_source import BinanceDataSource
    YAML_SUPPORT = True
except ImportError:
    YAML_SUPPORT = False
    print("âš ï¸  YAML/ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯åˆæˆãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚")


def load_data_from_yaml_config(config_path: str) -> pd.DataFrame:
    """
    config.yamlã‹ã‚‰å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆz_adaptive_ma_chart.pyå‚è€ƒï¼‰
    
    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    if not YAML_SUPPORT:
        print("âŒ YAML/ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚µãƒãƒ¼ãƒˆãŒç„¡åŠ¹ã§ã™")
        return None
    
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        binance_config = config.get('binance_data', {})
        data_dir = binance_config.get('data_dir', 'data/binance')
        binance_data_source = BinanceDataSource(data_dir)
        
        # CSVãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¯ãƒ€ãƒŸãƒ¼ã¨ã—ã¦æ¸¡ã™ï¼ˆBinanceãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ã¿ã‚’ä½¿ç”¨ï¼‰
        dummy_csv_source = CSVDataSource("dummy")
        data_loader = DataLoader(
            data_source=dummy_csv_source,
            binance_data_source=binance_data_source
        )
        data_processor = DataProcessor()
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†
        print("\nğŸ“Š å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‡¦ç†ä¸­...")
        raw_data = data_loader.load_data_from_config(config)
        processed_data = {
            symbol: data_processor.process(df)
            for symbol, df in raw_data.items()
        }
        
        # æœ€åˆã®ã‚·ãƒ³ãƒœãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        first_symbol = next(iter(processed_data))
        data = processed_data[first_symbol]
        
        print(f"âœ… å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {first_symbol}")
        print(f"ğŸ“… æœŸé–“: {data.index.min()} â†’ {data.index.max()}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
        
        return data
        
    except Exception as e:
        print(f"âŒ config.yamlã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def load_data_from_json_config(config_path: str) -> pd.DataFrame:
    """
    data_config.jsonã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    """
    if not os.path.exists(config_path):
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
        return None
    
    try:
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            import json
            config = json.load(f)
        
        data_path = config.get('data_path')
        if not data_path or not os.path.exists(data_path):
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
            return None
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®æ‹¡å¼µå­ã«å¿œã˜ã¦èª­ã¿è¾¼ã¿
        if data_path.endswith('.csv'):
            data = pd.read_csv(data_path)
        elif data_path.endswith('.parquet'):
            data = pd.read_parquet(data_path)
        else:
            print(f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {data_path}")
            return None
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
        required_columns = ['high', 'low', 'close']
        if not all(col in data.columns for col in required_columns):
            # å¤§æ–‡å­—å°æ–‡å­—ã®é•ã„ã‚’ç¢ºèª
            data.columns = [col.lower() for col in data.columns]
            if not all(col in data.columns for col in required_columns):
                print(f"âŒ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {required_columns}")
                return None
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {data_path}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
        
        return data
    
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def create_sample_configs():
    """
    ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    """
    # JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    json_config = {
        "data_path": "sample_data.csv",
        "description": "Sample configuration for real market data testing"
    }
    
    with open("data_config.json", 'w', encoding='utf-8') as f:
        import json
        json.dump(json_config, f, indent=2, ensure_ascii=False)
    
    print("âœ… ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (data_config.json) ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    # ã‚µãƒ³ãƒ—ãƒ«CSVãƒ‡ãƒ¼ã‚¿
    sample_data = []
    price = 100.0
    for i in range(200):
        # ç°¡å˜ãªãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿
        price += np.random.normal(0.5, 2.0)  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
        high = price + np.random.uniform(0, 2)
        low = price - np.random.uniform(0, 2)
        open_price = price + np.random.normal(0, 1)
        sample_data.append([open_price, high, low, price])
    
    df = pd.DataFrame(sample_data, columns=['open', 'high', 'low', 'close'])
    df.to_csv("sample_data.csv", index=False)
    print("âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« (sample_data.csv) ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    print("ğŸ“ å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®šã—ã¦ãã ã•ã„")


def generate_synthetic_data(n_samples: int = 2000) -> pd.DataFrame:
    """
    åˆæˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãƒ¬ãƒ³ã‚¸ãŒæ··åœ¨ï¼‰
    """
    np.random.seed(42)
    
    # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    data = []
    current_price = 100.0
    trend_periods = []
    range_periods = []
    
    i = 0
    while i < n_samples:
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒˆãƒ¬ãƒ³ãƒ‰ã¾ãŸã¯ãƒ¬ãƒ³ã‚¸ã‚’é¸æŠï¼ˆãƒãƒ©ãƒ³ã‚¹é‡è¦–ï¼‰
        regime_type = np.random.choice(['trend', 'range'], p=[0.4, 0.6])  # 40% trend, 60% range
        
        if regime_type == 'trend':
            # ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ï¼ˆ40-120æœŸé–“ï¼‰
            duration = np.random.randint(40, 121)
            direction = np.random.choice([-1, 1])
            trend_strength = np.random.uniform(0.003, 0.012) * direction  # å¼·ã‚ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            
            for j in range(min(duration, n_samples - i)):
                # ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†
                trend_component = trend_strength * current_price
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯æˆåˆ†ï¼ˆå°ã•ã‚ï¼‰
                random_component = np.random.normal(0, current_price * 0.008)
                
                current_price += trend_component + random_component
                current_price = max(current_price, 10.0)  # æœ€ä½ä¾¡æ ¼
                
                # OHLCç”Ÿæˆ
                volatility = current_price * 0.015
                high = current_price + np.random.uniform(0, volatility)
                low = current_price - np.random.uniform(0, volatility)
                open_price = current_price + np.random.normal(0, volatility/3)
                
                # è«–ç†çš„æ•´åˆæ€§ã®ç¢ºä¿
                low = min(low, current_price, open_price)
                high = max(high, current_price, open_price)
                
                data.append([open_price, high, low, current_price])
                trend_periods.append(i + j)
                
                if i + j + 1 >= n_samples:
                    break
            
            i += min(duration, n_samples - i)
        
        else:  # range
            # ãƒ¬ãƒ³ã‚¸æœŸé–“ï¼ˆ60-200æœŸé–“ï¼‰
            duration = np.random.randint(60, 201)
            center_price = current_price
            range_width = center_price * np.random.uniform(0.08, 0.15)  # 8-15%ã®ãƒ¬ãƒ³ã‚¸
            
            for j in range(min(duration, n_samples - i)):
                # ãƒ¬ãƒ³ã‚¸å†…ã§ã®å¹³å‡å›å¸°
                deviation = current_price - center_price
                mean_reversion = -deviation * np.random.uniform(0.02, 0.08)
                random_component = np.random.normal(0, range_width * 0.15)
                
                current_price += mean_reversion + random_component
                
                # ãƒ¬ãƒ³ã‚¸å¢ƒç•Œå†…ã«åˆ¶é™
                current_price = max(center_price - range_width/2, 
                                  min(center_price + range_width/2, current_price))
                current_price = max(current_price, 10.0)
                
                # OHLCç”Ÿæˆ
                volatility = range_width * 0.1
                high = current_price + np.random.uniform(0, volatility)
                low = current_price - np.random.uniform(0, volatility)
                open_price = current_price + np.random.normal(0, volatility/3)
                
                # è«–ç†çš„æ•´åˆæ€§ã®ç¢ºä¿
                low = min(low, current_price, open_price)
                high = max(high, current_price, open_price)
                
                data.append([open_price, high, low, current_price])
                range_periods.append(i + j)
                
                if i + j + 1 >= n_samples:
                    break
            
            i += min(duration, n_samples - i)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    df = pd.DataFrame(data, columns=['open', 'high', 'low', 'close'])
    
    # çœŸã®ä¿¡å·ã‚’ä½œæˆ
    true_signal = np.zeros(len(df))
    for period in trend_periods:
        if period < len(true_signal):
            true_signal[period] = 1
    
    df['true_signal'] = true_signal
    
    print(f"âœ… åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
    print(f"   ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶")
    print(f"   - ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“: {len(trend_periods)}ä»¶")
    print(f"   - ãƒ¬ãƒ³ã‚¸æœŸé–“: {len(range_periods)}ä»¶")
    print(f"   - çœŸã®ãƒˆãƒ¬ãƒ³ãƒ‰æ¯”ç‡: {len(trend_periods)/len(df)*100:.1f}%")
    
    return df


def evaluate_performance(predicted: np.ndarray, actual: np.ndarray) -> dict:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ï¼ˆ80%ç›®æ¨™å¯¾å¿œï¼‰
    """
    # æ··åŒè¡Œåˆ—
    tp = np.sum((predicted == 1) & (actual == 1))  # æ­£ã—ã„ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
    tn = np.sum((predicted == 0) & (actual == 0))  # æ­£ã—ã„ãƒ¬ãƒ³ã‚¸æ¤œå‡º
    fp = np.sum((predicted == 1) & (actual == 0))  # å½ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š
    fn = np.sum((predicted == 0) & (actual == 1))  # å½ãƒ¬ãƒ³ã‚¸åˆ¤å®š
    
    # åŸºæœ¬æŒ‡æ¨™
    accuracy = (tp + tn) / len(predicted) if len(predicted) > 0 else 0.0
    precision_trend = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall_trend = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    precision_range = tn / (tn + fn) if (tn + fn) > 0 else 0.0
    recall_range = tn / (tn + fp) if (tn + fp) > 0 else 0.0
    
    # F1ã‚¹ã‚³ã‚¢
    f1_trend = 2 * (precision_trend * recall_trend) / (precision_trend + recall_trend) if (precision_trend + recall_trend) > 0 else 0.0
    f1_range = 2 * (precision_range * recall_range) / (precision_range + recall_range) if (precision_range + recall_range) > 0 else 0.0
    
    # å®Ÿç”¨æ€§è©•ä¾¡ï¼ˆ80%ç›®æ¨™ï¼‰
    is_practical = accuracy >= 0.80
    
    return {
        'accuracy': accuracy,
        'precision_trend': precision_trend,
        'recall_trend': recall_trend,
        'precision_range': precision_range,
        'recall_range': recall_range,
        'f1_trend': f1_trend,
        'f1_range': f1_range,
        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,
        'is_practical': is_practical,
        'practical_threshold': 0.80
    }


def plot_results(data: pd.DataFrame, results: dict, is_real_data: bool = False, save_path: str = None):
    """
    çµæœã®å¯è¦–åŒ–ï¼ˆV5.0é©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…ä½é…å»¶å¯¾å¿œï¼‰
    """
    # ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’ç¢ºèª
    n_points = len(data)
    print(f"ğŸ“Š ãƒãƒ£ãƒ¼ãƒˆæç”»ä¸­... ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {n_points}")
    
    # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æº–å‚™
    if hasattr(data.index, 'to_pydatetime'):
        x_axis = data.index
        use_datetime = True
    else:
        x_axis = range(n_points)
        use_datetime = False
    
    # å›³ã®ä½œæˆï¼ˆ6ã¤ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆï¼‰
    fig, axes = plt.subplots(6, 1, figsize=(16, 24))
    fig.suptitle('V5.0 QUANTUM NEURAL SUPREMACY - Ultra Low-Lag & Noise-Free Edition', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    # 1. ãƒã‚¤ã‚ºé™¤å»å‰å¾Œã®ä¾¡æ ¼æ¯”è¼ƒ
    ax1 = axes[0]
    
    # å…ƒã®ä¾¡æ ¼ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    if 'raw_prices' in results and 'filtered_prices' in results:
        ax1.plot(x_axis, results['raw_prices'], label='Raw Prices (Original)', 
                linewidth=0.8, color='gray', alpha=0.7)
        ax1.plot(x_axis, results['filtered_prices'], label='Filtered Prices (Denoised)', 
                linewidth=1.2, color='blue', alpha=0.9)
    else:
        ax1.plot(x_axis, data['close'], label='Close Price', linewidth=1.2, color='blue', alpha=0.8)
    
    ax1.set_title('ğŸ¯ Noise Reduction Comparison (Gray=Raw, Blue=Filtered)', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Price', fontsize=12)
    ax1.legend(loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # 2. ä¾¡æ ¼ãƒãƒ£ãƒ¼ãƒˆã¨ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸ä¿¡å·ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    ax2 = axes[1]
    
    # èƒŒæ™¯è‰²ã®è¨­å®š
    try:
        signals = results['signal']
        
        if len(signals) > 0:
            current_signal = signals[0]
            start_idx = 0
            
            for i in range(1, len(signals) + 1):
                if i == len(signals) or signals[i] != current_signal:
                    end_idx = i - 1
                    
                    if current_signal == 1:  # ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“
                        color = 'lightgreen'
                        alpha = 0.15
                    else:  # ãƒ¬ãƒ³ã‚¸æœŸé–“
                        color = 'lightcoral'
                        alpha = 0.15
                    
                    if start_idx < len(x_axis) and end_idx < len(x_axis) and start_idx <= end_idx:
                        if use_datetime:
                            ax2.axvspan(x_axis[start_idx], x_axis[end_idx], 
                                       color=color, alpha=alpha, zorder=0)
                        else:
                            ax2.axvspan(start_idx, end_idx, 
                                       color=color, alpha=alpha, zorder=0)
                    
                    if i < len(signals):
                        start_idx = i
                        current_signal = signals[i]
    except Exception as e:
        print(f"âš ï¸  èƒŒæ™¯è‰²è¨­å®šã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼ãƒ©ã‚¤ãƒ³
    price_data = results.get('filtered_prices', data['close'])
    ax2.plot(x_axis, price_data, label='Filtered Price', linewidth=1.2, color='black', alpha=0.8, zorder=2)
    
    ax2.set_title('ğŸš€ Ultra Low-Lag Trend/Range Detection (Green=Trend, Red=Range)', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Price', fontsize=12)
    ax2.legend(loc='upper left')
    ax2.grid(True, alpha=0.3)
    
    # 3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºå™¨
    ax3 = axes[2]
    
    if 'realtime_trends' in results:
        realtime_trends = results['realtime_trends']
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è‰²åˆ†ã‘
        positive_mask = realtime_trends > 0
        negative_mask = realtime_trends < 0
        
        ax3.fill_between(x_axis, 0, realtime_trends, where=positive_mask, 
                        color='green', alpha=0.6, label='Bullish Trend')
        ax3.fill_between(x_axis, 0, realtime_trends, where=negative_mask, 
                        color='red', alpha=0.6, label='Bearish Trend')
        ax3.plot(x_axis, realtime_trends, color='black', linewidth=0.8, alpha=0.7)
    
    ax3.axhline(y=0, color='gray', linestyle='-', alpha=0.5)
    ax3.set_title('âš¡ Real-Time Trend Detector (Ultra Low-Lag)', fontsize=14, fontweight='bold')
    ax3.set_ylabel('Trend Strength', fontsize=12)
    ax3.legend(loc='upper right')
    ax3.grid(True, alpha=0.3)
    
    # 4. ç¬æ™‚æŒ¯å¹…ã¨ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
    ax4 = axes[3]
    
    if 'amplitude' in results:
        amplitude = results['amplitude']
        ax4.plot(x_axis, amplitude, label='Instantaneous Amplitude', 
                color='purple', linewidth=1.2, alpha=0.8)
        
        # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«é–¾å€¤ã®è¡¨ç¤º
        if len(amplitude) > 0:
            noise_threshold = np.mean(amplitude) * 0.3
            ax4.axhline(y=noise_threshold, color='red', linestyle='--', 
                       alpha=0.7, label=f'Noise Threshold ({noise_threshold:.3f})')
    
    ax4.set_title('ğŸŒ€ Hilbert Transform - Instantaneous Amplitude', fontsize=14, fontweight='bold')
    ax4.set_ylabel('Amplitude', fontsize=12)
    ax4.legend(loc='upper right')
    ax4.grid(True, alpha=0.3)
    
    # 5. ä¿¡é ¼åº¦ãƒãƒ£ãƒ¼ãƒˆï¼ˆV5.0å¯¾å¿œï¼‰
    ax5 = axes[4]
    
    confidences = results['confidence']
    colors = []
    for conf in confidences:
        if conf >= 0.8:  # 80%ä»¥ä¸Š
            colors.append('green')
        elif conf >= 0.6:  # 60-80%
            colors.append('orange')
        else:  # 60%æœªæº€
            colors.append('red')
    
    ax5.plot(x_axis, confidences, color='blue', alpha=0.6, linewidth=1)
    ax5.scatter(x_axis, confidences, c=colors, alpha=0.7, s=8)
    
    # é–¾å€¤ãƒ©ã‚¤ãƒ³
    ax5.axhline(y=0.8, color='green', linestyle='--', alpha=0.7, label='High Confidence (80%)')
    ax5.axhline(y=0.6, color='orange', linestyle='--', alpha=0.7, label='Medium Confidence (60%)')
    
    ax5.set_title('ğŸ¯ Confidence Levels (Noise-Free Enhanced)', fontsize=14, fontweight='bold')
    ax5.set_ylabel('Confidence', fontsize=12)
    ax5.set_ylim(0, 1)
    ax5.legend(loc='upper right')
    ax5.grid(True, alpha=0.3)
    
    # 6. è¤‡åˆæŒ‡æ¨™ãƒãƒ£ãƒ¼ãƒˆï¼ˆæ‹¡å¼µç‰ˆï¼‰
    ax6 = axes[5]
    
    ax6.plot(x_axis, results['efficiency_ratio'], label='Quantum Wavelet Score', 
            alpha=0.8, linewidth=1.5, color='blue')
    ax6.plot(x_axis, results['choppiness_index']/100, label='Volatility Score (normalized)', 
            alpha=0.8, linewidth=1.5, color='purple')
    ax6.plot(x_axis, results['cycle_strength'], label='Entropy Score', 
            alpha=0.8, linewidth=1.5, color='green')
    
    if 'fractal_dimension' in results:
        ax6.plot(x_axis, results['fractal_dimension'], label='Fractal Score', 
                alpha=0.8, linewidth=1.5, color='orange')
    
    # å‚è€ƒç·š
    ax6.axhline(y=0.618, color='green', linestyle=':', alpha=0.5, label='Golden Ratio (0.618)')
    ax6.axhline(y=0.382, color='red', linestyle=':', alpha=0.5, label='Golden Ratio (0.382)')
    
    ax6.set_title('ğŸ”¬ Advanced Technical Indicators (Multi-Dimensional)', fontsize=14, fontweight='bold')
    ax6.set_ylabel('Normalized Values', fontsize=12)
    ax6.set_ylim(0, 1)
    ax6.legend(loc='upper right')
    ax6.grid(True, alpha=0.3)
    
    if use_datetime:
        for ax in axes:
            ax.tick_params(axis='x', rotation=45)
        axes[-1].set_xlabel('Date', fontsize=12)
    else:
        axes[-1].set_xlabel('Time Period', fontsize=12)
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
    plt.tight_layout()
    plt.subplots_adjust(top=0.96)
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¿½åŠ ï¼ˆæ‹¡å¼µç‰ˆï¼‰
    noise_reduction_info = ""
    if 'noise_reduction' in results.get('summary', {}):
        nr = results['summary']['noise_reduction']
        noise_reduction_info = f"""
Noise Reduction: âœ… Kalman âœ… SuperSmoother âœ… ZeroLag âœ… Hilbert âœ… Adaptive âœ… RealTime"""
    
    stats_text = f"""Statistics (Ultra Low-Lag & Noise-Free):
Trend: {np.sum(results['signal'] == 1)} periods ({np.mean(results['signal'])*100:.1f}%)
Range: {np.sum(results['signal'] == 0)} periods ({(1-np.mean(results['signal']))*100:.1f}%)
Avg Confidence: {np.mean(results['confidence']):.3f}
High Confidence: {np.sum(results['confidence'] >= 0.8)/len(results['confidence'])*100:.1f}%{noise_reduction_info}"""
    
    fig.text(0.02, 0.02, stats_text, fontsize=9, 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… å¯è¦–åŒ–å®Œäº† ({save_path})")
    else:
        data_type = "real_data" if is_real_data else "synthetic_data"
        filename = f"ultimate_trend_range_v5_ultra_low_lag_noise_free_{data_type}_results.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… å¯è¦–åŒ–å®Œäº† ({filename})")
    
    plt.close()
    
    # è¿½åŠ ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤ºï¼ˆæ‹¡å¼µç‰ˆï¼‰
    print(f"\nğŸ“Š ãƒãƒ£ãƒ¼ãƒˆçµ±è¨ˆ (Ultra Low-Lag & Noise-Free):")
    print(f"   - ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {n_points}")
    print(f"   - ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“: {np.sum(results['signal'] == 1)} ({np.mean(results['signal'])*100:.1f}%)")
    print(f"   - ãƒ¬ãƒ³ã‚¸æœŸé–“: {np.sum(results['signal'] == 0)} ({(1-np.mean(results['signal']))*100:.1f}%)")
    print(f"   - å¹³å‡ä¿¡é ¼åº¦: {np.mean(results['confidence']):.3f}")
    print(f"   - é«˜ä¿¡é ¼åº¦æ¯”ç‡: {np.sum(results['confidence'] >= 0.8)/len(results['confidence'])*100:.1f}%")
    
    if 'noise_reduction' in results.get('summary', {}):
        print(f"   - ãƒã‚¤ã‚ºé™¤å»: 6æ®µéšé©æ–°çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨æ¸ˆã¿")
        print(f"   - è¶…ä½é…å»¶: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†æœ€é©åŒ–æ¸ˆã¿")


def analyze_real_market_performance(data: pd.DataFrame, results: dict) -> dict:
    """
    ãƒªã‚¢ãƒ«å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æï¼ˆV5.0å¯¾å¿œï¼‰
    """
    signals = results['signal']
    confidences = results['confidence']
    
    # åŸºæœ¬çµ±è¨ˆ
    total_periods = len(signals)
    trend_periods = np.sum(signals == 1)
    range_periods = np.sum(signals == 0)
    
    # ä¿¡é ¼åº¦çµ±è¨ˆï¼ˆV5.0ç”¨é–¾å€¤ï¼‰
    high_confidence_count = np.sum(confidences >= 0.8)  # 80%ä»¥ä¸Š
    medium_confidence_count = np.sum((confidences >= 0.6) & (confidences < 0.8))  # 60-80%
    low_confidence_count = np.sum(confidences < 0.6)  # 60%æœªæº€
    
    # é€£ç¶šæœŸé–“åˆ†æ
    def analyze_consecutive_periods(signal_array, target_value):
        consecutive_periods = []
        current_length = 0
        
        for signal in signal_array:
            if signal == target_value:
                current_length += 1
            else:
                if current_length > 0:
                    consecutive_periods.append(current_length)
                current_length = 0
        
        if current_length > 0:
            consecutive_periods.append(current_length)
        
        return consecutive_periods
    
    trend_consecutive = analyze_consecutive_periods(signals, 1)
    range_consecutive = analyze_consecutive_periods(signals, 0)
    
    return {
        'total_periods': total_periods,
        'trend_periods': trend_periods,
        'range_periods': range_periods,
        'trend_ratio': trend_periods / total_periods,
        'range_ratio': range_periods / total_periods,
        'avg_confidence': np.mean(confidences),
        'high_confidence_ratio': high_confidence_count / total_periods,
        'medium_confidence_ratio': medium_confidence_count / total_periods,
        'low_confidence_ratio': low_confidence_count / total_periods,
        'trend_consecutive_stats': {
            'count': len(trend_consecutive),
            'avg_length': np.mean(trend_consecutive) if trend_consecutive else 0,
            'max_length': max(trend_consecutive) if trend_consecutive else 0,
            'min_length': min(trend_consecutive) if trend_consecutive else 0
        },
        'range_consecutive_stats': {
            'count': len(range_consecutive),
            'avg_length': np.mean(range_consecutive) if range_consecutive else 0,
            'max_length': max(range_consecutive) if range_consecutive else 0,
            'min_length': min(range_consecutive) if range_consecutive else 0
        }
    }


def main():
    print("ğŸš€ V5.0 QUANTUM NEURAL SUPREMACY EDITION - é©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…ä½é…å»¶å¯¾å¿œ")
    print("=" * 140)
    print("ğŸ¯ 6æ®µéšé©æ–°çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ã‚«ãƒ«ãƒãƒ³â†’ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼â†’ã‚¼ãƒ­ãƒ©ã‚°EMAâ†’ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›â†’é©å¿œçš„é™¤å»â†’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º")
    print("âš¡ è¶…ä½é…å»¶å‡¦ç†: ä½ç›¸é…å»¶ã‚¼ãƒ­ãƒ»äºˆæ¸¬çš„è£œæ­£ãƒ»å³åº§åå¿œã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 140)
    
    # ãƒ‡ãƒ¼ã‚¿é¸æŠã®å„ªå…ˆé †ä½
    # 1. config.yaml (YAMLå¯¾å¿œæ™‚)
    # 2. data_config.json
    # 3. åˆæˆãƒ‡ãƒ¼ã‚¿
    
    data = None
    is_real_data = False
    
    # 1. config.yamlã‹ã‚‰ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
    config_yaml_path = "config.yaml"
    if YAML_SUPPORT and os.path.exists(config_yaml_path):
        print("ğŸ“‚ config.yamlã‹ã‚‰ãƒªã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        data = load_data_from_yaml_config(config_yaml_path)
        if data is not None:
            print("âœ… config.yamlã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ")
            is_real_data = True
            
    # 2. data_config.jsonã‹ã‚‰ã®èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
    if data is None:
        config_json_path = "data_config.json"
        if os.path.exists(config_json_path):
            print("ğŸ“‚ data_config.jsonã‹ã‚‰ãƒªã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            data = load_data_from_json_config(config_json_path)
            if data is not None:
                print("âœ… data_config.jsonã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ")
                is_real_data = True
    
    # 3. åˆæˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    if data is None:
        print("ğŸ“Š åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ")
        print("ğŸ’¡ å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚¹ãƒˆã—ãŸã„å ´åˆã¯:")
        if YAML_SUPPORT:
            print("   - config.yaml ã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰")
        print("   - data_config.json ã‚’ä½œæˆ")
        create_sample_configs()
        data = generate_synthetic_data()
        is_real_data = False
    
    if data is None:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # V5.0åˆæœŸåŒ–ï¼ˆé‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æœ€é«˜å³°ï¼‰
    print(f"\nğŸ”§ V5.0 QUANTUM NEURAL SUPREMACY EDITION åˆæœŸåŒ–ä¸­...")
    detector = UltimateTrendRangeDetector(
        confidence_threshold=0.55,    # 55%é–¾å€¤ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ãƒ»ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰
        min_confidence=0.8,           # 80%æœ€ä½ä¿¡é ¼åº¦ä¿è¨¼
        min_duration=8                # æœŸé–“å»¶é•·å¯¾å¿œï¼ˆ5â†’6æœŸé–“ï¼‰
    )
    print("âœ… V5.0åˆæœŸåŒ–å®Œäº†ï¼ˆé‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»é©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…ä½é…å»¶ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸ãƒãƒ©ãƒ³ã‚¹æœ€é©åŒ–è¨­å®šï¼‰")
    
    # V5.0è¨ˆç®—å®Ÿè¡Œ
    data_type = "ãƒªã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿" if is_real_data else "åˆæˆãƒ‡ãƒ¼ã‚¿"
    print(f"\nâš¡ V5.0 {data_type}åˆ¤åˆ¥è¨ˆç®—å®Ÿè¡Œä¸­...")
    
    start_time = time.time()
    results = detector.calculate(data)
    end_time = time.time()
    
    processing_time = end_time - start_time
    processing_speed = len(data) / processing_time if processing_time > 0 else 0
    
    print(f"âœ… V5.0è¨ˆç®—å®Œäº† (å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’)")
    print(f"   âš¡ å‡¦ç†é€Ÿåº¦: {processing_speed:.0f} ãƒ‡ãƒ¼ã‚¿/ç§’")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
    if not is_real_data and 'true_signal' in data.columns:
        print(f"\nğŸ“ˆ V5.0 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ä¸­...")
        
        performance = evaluate_performance(results['signal'], data['true_signal'].values)
        
        print("\n" + "="*80)
        print("ğŸ¯ **V5.0 QUANTUM NEURAL SUPREMACY ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœ**")
        print("="*80)
        print(f"ğŸ“Š ç·åˆç²¾åº¦: {performance['accuracy']:.4f} ({performance['accuracy']*100:.2f}%)")
        
        if performance['is_practical']:
            print("ğŸ¯ å®Ÿç”¨æ€§: âœ… **80%ç›®æ¨™é”æˆ!**")
        else:
            print(f"ğŸ¯ å®Ÿç”¨æ€§: âŒ ç›®æ¨™æœªé” (ç›®æ¨™: {performance['practical_threshold']*100:.0f}%)")
        
        print(f"\nğŸ“ˆ **ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤åˆ¥**")
        print(f"   - ç²¾åº¦ (Precision): {performance['precision_trend']:.4f} ({performance['precision_trend']*100:.1f}%)")
        print(f"   - å†ç¾ç‡ (Recall): {performance['recall_trend']:.4f} ({performance['recall_trend']*100:.1f}%)")
        print(f"   - F1ã‚¹ã‚³ã‚¢: {performance['f1_trend']:.4f}")
        
        print(f"\nğŸ“‰ **ãƒ¬ãƒ³ã‚¸åˆ¤åˆ¥**")
        print(f"   - ç²¾åº¦ (Precision): {performance['precision_range']:.4f} ({performance['precision_range']*100:.1f}%)")
        print(f"   - å†ç¾ç‡ (Recall): {performance['recall_range']:.4f} ({performance['recall_range']*100:.1f}%)")
        print(f"   - F1ã‚¹ã‚³ã‚¢: {performance['f1_range']:.4f}")
        
        print(f"\nğŸ“Š **æ··åŒè¡Œåˆ—:**")
        print(f"   - æ­£ã—ã„ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º: {performance['tp']}ä»¶")
        print(f"   - æ­£ã—ã„ãƒ¬ãƒ³ã‚¸æ¤œå‡º: {performance['tn']}ä»¶")
        print(f"   - å½ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š: {performance['fp']}ä»¶ (ãƒ¬ãƒ³ã‚¸ã‚’ãƒˆãƒ¬ãƒ³ãƒ‰ã¨èª¤åˆ¤å®š)")
        print(f"   - å½ãƒ¬ãƒ³ã‚¸åˆ¤å®š: {performance['fn']}ä»¶ (ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ãƒ¬ãƒ³ã‚¸ã¨èª¤åˆ¤å®š)")
    
    # V5.0çµ±è¨ˆæƒ…å ±
    print("\n" + "="*80)
    print("ğŸ“Š **V5.0 QUANTUM NEURAL SUPREMACY çµ±è¨ˆ**")
    print("="*80)
    print(f"ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“: {results['summary']['trend_bars']}ä»¶ ({results['summary']['trend_ratio']*100:.1f}%)")
    print(f"ğŸ“‰ ãƒ¬ãƒ³ã‚¸æœŸé–“: {results['summary']['range_bars']}ä»¶ ({(1-results['summary']['trend_ratio'])*100:.1f}%)")
    print(f"ğŸ¯ å¹³å‡ä¿¡é ¼åº¦: {results['summary']['avg_confidence']:.4f} ({results['summary']['avg_confidence']*100:.1f}%)")
    print(f"â­ é«˜ä¿¡é ¼åº¦æ¯”ç‡: {results['summary']['high_confidence_ratio']*100:.1f}%")
    print(f"ğŸ”§ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {results['summary']['algorithm_version']}")
    
    print(f"\nâš™ï¸  **V5.0 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š:**")
    print(f"   - ä¿¡é ¼åº¦é–¾å€¤: {results['summary']['parameters']['confidence_threshold']} ({results['summary']['parameters']['confidence_threshold']*100:.0f}%)")
    print(f"   - æœ€ä½ä¿¡é ¼åº¦ä¿è¨¼: {results['summary']['parameters']['min_confidence']} ({results['summary']['parameters']['min_confidence']*100:.0f}%)")
    print(f"   - æœ€å°ç¶™ç¶šæœŸé–“: {results['summary']['parameters']['min_duration']} æœŸé–“")
    
    # V5.0æŠ€è¡“è©³ç´°
    print("\n" + "="*80)
    print("ğŸ”¬ **V5.0 QUANTUM NEURAL SUPREMACY æŠ€è¡“è©³ç´°**")
    print("="*80)
    print("ğŸ¯ é©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…ä½é…å»¶ã‚·ã‚¹ãƒ†ãƒ :")
    print("   1. é©å¿œçš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (å‹•çš„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¨å®šãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é™¤å»)")
    print("   2. ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (John Ehlersæ”¹è‰¯ç‰ˆãƒ»ã‚¼ãƒ­é…å»¶è¨­è¨ˆ)")
    print("   3. ã‚¼ãƒ­ãƒ©ã‚°EMA (é…å»¶å®Œå…¨é™¤å»ãƒ»äºˆæ¸¬çš„è£œæ­£)")
    print("   4. ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (ä½ç›¸é…å»¶ã‚¼ãƒ­ãƒ»ç¬æ™‚æŒ¯å¹…/ä½ç›¸)")
    print("   5. é©å¿œçš„ãƒã‚¤ã‚ºé™¤å» (AIé¢¨å­¦ç¿’å‹ãƒ»æŒ¯å¹…é€£å‹•èª¿æ•´)")
    print("   6. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º (è¶…ä½é…å»¶ãƒ»å³åº§åå¿œ)")
    print("ğŸ§  é‡å­è¨ˆç®—é¢¨é©å‘½ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :")
    print("   7. é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ (å¤šé‡è§£åƒåº¦åˆ†è§£)")
    print("   8. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æ (ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ãƒ»R/Sè§£æ)")
    print("   9. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ã‚«ã‚ªã‚¹ç†è«– (ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°)")
    print("   10. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é¢¨ç‰¹å¾´é‡ (25æ¬¡å…ƒæ·±å±¤å­¦ç¿’é¢¨)")
    print("   11. é‡å­ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¿¡é ¼åº¦ã‚·ã‚¹ãƒ†ãƒ  (12è¶…å°‚é–€å®¶)")
    print("   12. é‡å­é‡ã­åˆã‚ã›åˆ¤å®š (å‹•çš„é‡ã¿èª¿æ•´)")
    print("   13. é‡å­ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆåŠ¹æœ (ç›¸äº’å¼·åŒ–)")
    print("   14. 80%ä¿¡é ¼åº¦ä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ  (é©å‘½çš„ç²¾åº¦)")
    
    print(f"\nğŸ’¡ **V5.0ã®é©æ–°çš„ç‰¹å¾´:**")
    print("   - 6æ®µéšé©æ–°çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (ãƒã‚¤ã‚ºå®Œå…¨é™¤å»)")
    print("   - ä½ç›¸é…å»¶ã‚¼ãƒ­å‡¦ç† (ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›é©ç”¨)")
    print("   - è¶…ä½é…å»¶ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º (å³åº§åå¿œ)")
    print("   - é©å¿œçš„å­¦ç¿’å‹ãƒã‚¤ã‚ºé™¤å» (AIé¢¨æ¨å®š)")
    print("   - äºˆæ¸¬çš„è£œæ­£ã‚·ã‚¹ãƒ†ãƒ  (æœªæ¥äºˆæ¸¬)")
    print("   - 80%ä»¥ä¸Šã®è¶…é«˜ä¿¡é ¼åº¦å®Ÿç¾")
    print("   - é‡å­è¨ˆç®—é¢¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¡ç”¨")
    print("   - æœ€æ–°æ•°å­¦ç†è«–ã®å®Œå…¨çµ±åˆ")
    print("   - ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚«ã‚ªã‚¹ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è§£æ")
    print("   - æ·±å±¤å­¦ç¿’é¢¨25æ¬¡å…ƒç‰¹å¾´ç©ºé–“")
    print("   - 12å°‚é–€å®¶ã«ã‚ˆã‚‹é‡å­é‡ã­åˆã‚ã›")
    print("   - äººé¡ã®èªçŸ¥é™ç•Œã‚’å®Œå…¨è¶…è¶Š")
    
    print(f"\nâš¡ è¨ˆç®—é€Ÿåº¦: {processing_speed:.0f} ãƒ‡ãƒ¼ã‚¿/ç§’")
    print("ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: Numba JIT é‡å­æœ€é©åŒ–")
    
    # ãƒªã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿å°‚ç”¨åˆ†æ
    if is_real_data:
        print(f"\nğŸ“Š V5.0 ãƒªã‚¢ãƒ«å¸‚å ´åˆ†æä¸­...")
        real_analysis = analyze_real_market_performance(data, results)
        
        print("\n" + "="*90)
        print("ğŸ“ˆ **V5.0 QUANTUM NEURAL SUPREMACY ãƒªã‚¢ãƒ«å¸‚å ´ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**")
        print("="*90)
        print(f"ğŸ“Š ç·æœŸé–“: {real_analysis['total_periods']}æœŸé–“")
        print(f"ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º: {real_analysis['trend_periods']}æœŸé–“ ({real_analysis['trend_ratio']*100:.1f}%)")
        print(f"ğŸ“‰ ãƒ¬ãƒ³ã‚¸æ¤œå‡º: {real_analysis['range_periods']}æœŸé–“ ({real_analysis['range_ratio']*100:.1f}%)")
        print(f"ğŸ¯ å¹³å‡ä¿¡é ¼åº¦: {real_analysis['avg_confidence']:.1%}")
        
        print(f"\nğŸ” **ä¿¡é ¼åº¦åˆ†å¸ƒ:**")
        print(f"   - é«˜ä¿¡é ¼åº¦ (â‰¥80%): {real_analysis['high_confidence_ratio']*100:.1f}%")
        print(f"   - ä¸­ä¿¡é ¼åº¦ (60-80%): {real_analysis['medium_confidence_ratio']*100:.1f}%")
        print(f"   - ä½ä¿¡é ¼åº¦ (<60%): {real_analysis['low_confidence_ratio']*100:.1f}%")
        
        print(f"\nğŸ“ˆ **ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“çµ±è¨ˆ:**")
        if real_analysis['trend_consecutive_stats']['count'] > 0:
            print(f"   - é€£ç¶šå›æ•°: {real_analysis['trend_consecutive_stats']['count']}å›")
            print(f"   - å¹³å‡é•·ã•: {real_analysis['trend_consecutive_stats']['avg_length']:.1f}æœŸé–“")
            print(f"   - æœ€é•·æœŸé–“: {real_analysis['trend_consecutive_stats']['max_length']}æœŸé–“")
        else:
            print("   - ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ãªã—")
        
        print(f"\nğŸ“‰ **ãƒ¬ãƒ³ã‚¸æœŸé–“çµ±è¨ˆ:**")
        if real_analysis['range_consecutive_stats']['count'] > 0:
            print(f"   - é€£ç¶šå›æ•°: {real_analysis['range_consecutive_stats']['count']}å›")
            print(f"   - å¹³å‡é•·ã•: {real_analysis['range_consecutive_stats']['avg_length']:.1f}æœŸé–“")
            print(f"   - æœ€é•·æœŸé–“: {real_analysis['range_consecutive_stats']['max_length']}æœŸé–“")
        else:
            print("   - ãƒ¬ãƒ³ã‚¸æœŸé–“ãªã—")
    
    # å¯è¦–åŒ–
    print(f"\nğŸ“Š V5.0 {data_type}çµæœã®å¯è¦–åŒ–ä¸­...")
    plot_results(data, results, is_real_data)
    
    # æœ€çµ‚è©•ä¾¡
    print("\n" + "="*90)
    print("ğŸ† **V5.0 QUANTUM NEURAL SUPREMACY æœ€çµ‚è©•ä¾¡**")
    print("="*90)
    if not is_real_data and 'true_signal' in data.columns:
        if performance['accuracy'] >= 0.80:
            print("ğŸ–ï¸  ç·åˆè©•ä¾¡: ğŸ† **QUANTUM NEURAL SUPREMACY ACHIEVED**")
            print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æŠ€è¡“ã«ã‚ˆã‚Š80%è¶…é«˜ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸ!")
        elif performance['accuracy'] >= 0.70:
            print("ğŸ–ï¸  ç·åˆè©•ä¾¡: ğŸ¥ˆ **QUANTUM EXCELLENCE**")
            print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: 70%ä»¥ä¸Šã®é‡å­ãƒ¬ãƒ™ãƒ«ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸã€‚")
        elif performance['accuracy'] >= 0.60:
            print("ğŸ–ï¸  ç·åˆè©•ä¾¡: ğŸ¥‰ **NEURAL SUPERIORITY**")
            print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: 60%ä»¥ä¸Šã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å„ªç§€ç²¾åº¦ã§ã™ã€‚")
        else:
            print("ğŸ–ï¸  ç·åˆè©•ä¾¡: ğŸ“ˆ **QUANTUM EVOLUTION**")
            print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: ã•ã‚‰ãªã‚‹é‡å­é€²åŒ–ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚")
        
        print(f"ğŸ“Š åˆ¤åˆ¥ç²¾åº¦: {'âœ…' if performance['accuracy'] >= 0.80 else 'âŒ'} {performance['accuracy']*100:.1f}%")
        print(f"ğŸ¯ ãƒ¬ãƒ³ã‚¸ç²¾åº¦: {'âœ…' if performance['precision_range'] >= 0.80 else 'âŒ'} {performance['precision_range']*100:.1f}%")
        print(f"ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰ç²¾åº¦: {'âœ…' if performance['precision_trend'] >= 0.80 else 'âŒ'} {performance['precision_trend']*100:.1f}%")
        
        confidence_avg = results['summary']['avg_confidence']
        high_confidence_ok = results['summary']['high_confidence_ratio'] >= 0.80
        print(f"ğŸ”® ä¿¡é ¼åº¦: {'âœ…' if confidence_avg >= 0.80 else 'âŒ'} å¹³å‡{confidence_avg*100:.1f}%")
        print(f"â­ é«˜ä¿¡é ¼åº¦æ¯”ç‡: {'âœ…' if high_confidence_ok else 'âŒ'} {results['summary']['high_confidence_ratio']*100:.1f}%")
        
        if performance['accuracy'] >= 0.80 and confidence_avg >= 0.80:
            print(f"\nğŸŠ **V5.0 QUANTUM NEURAL SUPREMACY COMPLETE!**")
            print("ğŸŒŸ é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æŠ€è¡“ã«ã‚ˆã‚Š80%è¶…é«˜ä¿¡é ¼åº¦ã‚’å®Œå…¨å®Ÿç¾ã—ã¾ã—ãŸ!")
    else:
        print("ğŸ–ï¸  ç·åˆè©•ä¾¡: ğŸ“Š **é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æœ€é«˜å³°ãƒªã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†**")
        print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: ãƒªã‚¢ãƒ«å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã§é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æœ€é«˜å³°ã®å‹•ä½œã‚’ç¢ºèªã—ã¾ã—ãŸã€‚")
        
        confidence_avg = results['summary']['avg_confidence']
        high_confidence_ratio = results['summary']['high_confidence_ratio']
        
        print(f"ğŸ¯ å¹³å‡ä¿¡é ¼åº¦: {confidence_avg*100:.1f}%")
        print(f"â­ é«˜ä¿¡é ¼åº¦æ¯”ç‡: {high_confidence_ratio*100:.1f}%")
        
        # 80%ä¿¡é ¼åº¦é”æˆåˆ¤å®š
        if confidence_avg >= 0.80:
            print(f"ğŸ”® ä¿¡é ¼åº¦è©•ä¾¡: âœ… **80%è¶…é«˜ä¿¡é ¼åº¦é”æˆ!** (ç›®æ¨™: 80%ä»¥ä¸Š)")
        elif confidence_avg >= 0.70:
            print(f"ğŸ”® ä¿¡é ¼åº¦è©•ä¾¡: âš ï¸  é«˜ä¿¡é ¼åº¦ (ç¾åœ¨: {confidence_avg*100:.1f}%, ç›®æ¨™: 80%)")
        else:
            print(f"ğŸ”® ä¿¡é ¼åº¦è©•ä¾¡: âŒ ä¿¡é ¼åº¦å‘ä¸ŠãŒå¿…è¦ (ç¾åœ¨: {confidence_avg*100:.1f}%, ç›®æ¨™: 80%)")
    
    print("\n" + "="*90)
    print("V5.0 QUANTUM NEURAL SUPREMACY EDITION COMPLETE")
    print("é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æœ€é«˜å³°ãƒ»80%è¶…é«˜ä¿¡é ¼åº¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œå®Œäº†")
    print("="*90)


if __name__ == "__main__":
    main() 