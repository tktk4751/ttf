#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union, NamedTuple
from numba import jit, prange, njit
import warnings
warnings.filterwarnings('ignore')


class TrendRangeResult(NamedTuple):
    """TrendRangeåˆ¤åˆ¥çµæœ"""
    signal: np.ndarray          # 0=ãƒ¬ãƒ³ã‚¸, 1=ãƒˆãƒ¬ãƒ³ãƒ‰
    confidence: np.ndarray      # ä¿¡é ¼åº¦ (0-1)
    trend_strength: np.ndarray  # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ (-1 to 1)
    range_quality: np.ndarray   # ãƒ¬ãƒ³ã‚¸å“è³ª (0-1)
    cycle_phase: np.ndarray     # ã‚µã‚¤ã‚¯ãƒ«ä½ç›¸ (0-2Ï€)
    market_regime: np.ndarray   # å¸‚å ´ä½“åˆ¶ (0-3)
    summary: Dict               # çµ±è¨ˆã‚µãƒãƒªãƒ¼


@jit(nopython=True)
def quantum_wavelet_analysis(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸŒŠ é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æï¼ˆV5.0é©å‘½çš„æŠ€è¡“ï¼‰
    ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ã«ã‚ˆã‚‹å¤šé‡è§£åƒåº¦è§£æã§éš ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
    """
    n = len(prices)
    scales = [4, 8, 16, 32, 64, 128]  # 6ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ«
    
    wavelet_trends = np.zeros(n)
    wavelet_volatility = np.zeros(n)
    wavelet_coherence = np.zeros(n)
    
    for scale in scales:
        if scale >= n:
            continue
            
        weight = 1.0 / np.sqrt(scale)  # ã‚¹ã‚±ãƒ¼ãƒ«é‡ã¿
        
        for i in range(scale, n):
            # ãƒãƒ¼ãƒ«ãƒ»ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé¢¨å¤‰æ›
            window = prices[i-scale+1:i+1]
            
            # ä½å‘¨æ³¢æˆåˆ†ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
            low_freq = np.mean(window)
            
            # é«˜å‘¨æ³¢æˆåˆ†ï¼ˆãƒã‚¤ã‚ºãƒ»å¤‰å‹•ï¼‰
            high_freq = np.std(window)
            
            # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä¿‚æ•°
            mid_point = scale // 2
            left_mean = np.mean(window[:mid_point])
            right_mean = np.mean(window[mid_point:])
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆå·¦å³ã®å·®ï¼‰
            trend_coeff = abs(right_mean - left_mean) / (left_mean + 1e-8)
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼å¯†åº¦
            energy = np.sum(window ** 2) / scale
            
            # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ï¼ˆä¸€è²«æ€§ï¼‰
            coherence = 1.0 / (1.0 + high_freq / (low_freq + 1e-8))
            
            wavelet_trends[i] += weight * trend_coeff
            wavelet_volatility[i] += weight * high_freq / (low_freq + 1e-8)
            wavelet_coherence[i] += weight * coherence
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(max(scales)):
        if i < n:
            wavelet_trends[i] = wavelet_trends[max(scales)] if max(scales) < n else 0.0
            wavelet_volatility[i] = wavelet_volatility[max(scales)] if max(scales) < n else 0.0
            wavelet_coherence[i] = wavelet_coherence[max(scales)] if max(scales) < n else 0.0
    
    return wavelet_trends, wavelet_volatility, wavelet_coherence


@jit(nopython=True)
def fractal_dimension_analysis(prices: np.ndarray, window: int = 50) -> np.ndarray:
    """
    ğŸ”¬ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æï¼ˆV5.0é©å‘½çš„æŠ€è¡“ï¼‰
    ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ã¨ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã«ã‚ˆã‚‹å¸‚å ´æ§‹é€ è§£æ
    """
    n = len(prices)
    fractal_scores = np.zeros(n)
    
    for i in range(window, n):
        data = prices[i-window+1:i+1]
        
        # ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ã®è¨ˆç®—ï¼ˆR/Sè§£æï¼‰
        mean_price = np.mean(data)
        deviations = data - mean_price
        cumulative_deviations = np.cumsum(deviations)
        
        # ç¯„å›²ã®è¨ˆç®—
        R = np.max(cumulative_deviations) - np.min(cumulative_deviations)
        
        # æ¨™æº–åå·®
        S = np.std(data)
        
        # R/Sæ¯”ç‡
        rs_ratio = R / (S + 1e-8)
        
        # ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ã®è¿‘ä¼¼
        hurst = np.log(rs_ratio) / np.log(window)
        hurst = max(0.0, min(1.0, hurst))  # 0-1ã«åˆ¶é™
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ (D = 2 - H)
        fractal_dim = 2.0 - hurst
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã¸ã®å¤‰æ›
        # H > 0.5: æŒç¶šæ€§ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
        # H < 0.5: åæŒç¶šæ€§ï¼ˆãƒ¬ãƒ³ã‚¸ï¼‰
        if hurst > 0.5:
            fractal_scores[i] = (hurst - 0.5) * 2.0  # 0-1ã‚¹ã‚±ãƒ¼ãƒ«
        else:
            fractal_scores[i] = 0.0
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(window):
        fractal_scores[i] = 0.0
    
    return fractal_scores


@jit(nopython=True)
def entropy_chaos_analysis(prices: np.ndarray, window: int = 30) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸŒ€ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ã‚«ã‚ªã‚¹è§£æï¼ˆV5.0é©å‘½çš„æŠ€è¡“ï¼‰
    æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ã«ã‚ˆã‚‹å¸‚å ´ã‚«ã‚ªã‚¹åº¦æ¸¬å®š
    """
    n = len(prices)
    entropy_scores = np.zeros(n)
    chaos_scores = np.zeros(n)
    
    for i in range(window, n):
        data = prices[i-window+1:i+1]
        
        # 1. ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
        # ä¾¡æ ¼å¤‰åŒ–ã‚’é›¢æ•£åŒ–
        returns = np.diff(data)
        if len(returns) == 0:
            entropy_scores[i] = 0.0
            chaos_scores[i] = 0.0
            continue
            
        # åˆ†ä½æ•°ã«ã‚ˆã‚‹é›¢æ•£åŒ–ï¼ˆ5æ®µéšï¼‰
        percentiles = np.array([0, 20, 40, 60, 80, 100])
        bins = np.percentile(returns, percentiles)
        
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä½œæˆ
        hist = np.zeros(5)
        for ret in returns:
            for j in range(4):
                if bins[j] <= ret < bins[j+1]:
                    hist[j] += 1
                    break
            else:
                hist[4] += 1  # æœ€å¤§å€¤
        
        # ç¢ºç‡åˆ†å¸ƒ
        total = np.sum(hist)
        if total > 0:
            probs = hist / total
            
            # ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            entropy = 0.0
            for p in probs:
                if p > 0:
                    entropy -= p * np.log2(p + 1e-8)
            
            entropy_scores[i] = entropy / np.log2(5)  # æ­£è¦åŒ–
        
        # 2. ã‚«ã‚ªã‚¹åº¦ï¼ˆãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°é¢¨ï¼‰
        # è¿‘å‚ç‚¹ã®ç™ºæ•£åº¦ã‚’æ¸¬å®š
        chaos_sum = 0.0
        count = 0
        
        for j in range(len(data) - 1):
            for k in range(j + 1, len(data)):
                if abs(data[j] - data[k]) < np.std(data) * 0.1:  # è¿‘å‚ç‚¹
                    # æ¬¡ã®ç‚¹ã§ã®è·é›¢
                    if j + 1 < len(data) and k + 1 < len(data):
                        initial_dist = abs(data[j] - data[k])
                        next_dist = abs(data[j+1] - data[k+1])
                        
                        if initial_dist > 0:
                            divergence = next_dist / (initial_dist + 1e-8)
                            chaos_sum += np.log(divergence + 1e-8)
                            count += 1
        
        if count > 0:
            chaos_scores[i] = max(0.0, chaos_sum / count)
        else:
            chaos_scores[i] = 0.0
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(window):
        entropy_scores[i] = 0.0
        chaos_scores[i] = 0.0
    
    return entropy_scores, chaos_scores


@jit(nopython=True)
def neural_network_features(
    prices: np.ndarray,
    wavelet_trends: np.ndarray,
    fractal_scores: np.ndarray,
    entropy_scores: np.ndarray,
    chaos_scores: np.ndarray
) -> np.ndarray:
    """
    ğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é¢¨ç‰¹å¾´é‡ï¼ˆV5.0é©å‘½çš„æŠ€è¡“ï¼‰
    æ·±å±¤å­¦ç¿’é¢¨ã®å¤šå±¤ç‰¹å¾´æŠ½å‡ºã¨éç·šå½¢å¤‰æ›
    """
    n = len(prices)
    neural_scores = np.zeros(n)
    
    for i in range(100, n):  # ã‚ˆã‚Šé•·ã„å±¥æ­´ãŒå¿…è¦
        # 25æ¬¡å…ƒç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
        features = np.zeros(25)
        
        # Layer 1: åŸºæœ¬ç‰¹å¾´é‡
        features[0] = (prices[i] - prices[i-20]) / (prices[i-20] + 1e-8)  # ãƒªã‚¿ãƒ¼ãƒ³
        features[1] = np.std(prices[i-20:i+1]) / np.mean(prices[i-20:i+1])  # CV
        features[2] = wavelet_trends[i]
        features[3] = fractal_scores[i]
        features[4] = entropy_scores[i]
        features[5] = chaos_scores[i]
        
        # Layer 2: ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
        features[6] = features[0] * features[2]  # ãƒªã‚¿ãƒ¼ãƒ³ Ã— ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
        features[7] = features[1] * features[3]  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ Ã— ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«
        features[8] = features[4] * features[5]  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ Ã— ã‚«ã‚ªã‚¹
        
        # Layer 3: é«˜æ¬¡ç‰¹å¾´é‡
        ma_short = np.mean(prices[i-5:i+1])
        ma_long = np.mean(prices[i-20:i+1])
        features[9] = (ma_short - ma_long) / (ma_long + 1e-8)
        
        # RSIé¢¨æŒ‡æ¨™
        gains = 0.0
        losses = 0.0
        for j in range(i-14, i):
            if j >= 0 and j < n-1:
                change = prices[j+1] - prices[j]
                if change > 0:
                    gains += change
                else:
                    losses -= change
        
        if losses > 0:
            rs = gains / losses
            features[10] = rs / (1 + rs)
        else:
            features[10] = 1.0
        
        # Layer 4: æ™‚ç³»åˆ—ç‰¹å¾´é‡
        for lag in range(1, 6):  # 5ã¤ã®ãƒ©ã‚°ç‰¹å¾´é‡
            if i >= lag:
                features[10 + lag] = (prices[i] - prices[i-lag]) / (prices[i-lag] + 1e-8)
        
        # Layer 5: çµ±è¨ˆçš„ç‰¹å¾´é‡
        recent_data = prices[i-50:i+1] if i >= 50 else prices[:i+1]
        features[16] = (prices[i] - np.min(recent_data)) / (np.max(recent_data) - np.min(recent_data) + 1e-8)
        features[17] = (prices[i] - np.median(recent_data)) / (np.std(recent_data) + 1e-8)
        
        # Layer 6: å‘¨æ³¢æ•°é ˜åŸŸç‰¹å¾´é‡
        if len(recent_data) >= 8:
            # ç°¡æ˜“FFTé¢¨è§£æ
            mean_val = np.mean(recent_data)
            detrended = recent_data - mean_val
            
            # ä½å‘¨æ³¢æˆåˆ†
            low_freq = np.mean(detrended[:len(detrended)//2])
            high_freq = np.mean(detrended[len(detrended)//2:])
            
            features[18] = low_freq / (np.std(detrended) + 1e-8)
            features[19] = high_freq / (np.std(detrended) + 1e-8)
        
        # Layer 7: éç·šå½¢ç‰¹å¾´é‡
        features[20] = np.tanh(features[0] * 3.0)  # éç·šå½¢ãƒªã‚¿ãƒ¼ãƒ³
        features[21] = 1.0 / (1.0 + np.exp(-features[2] * 5.0))  # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¤‰æ›
        features[22] = features[3] ** 2  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«äºŒä¹—
        features[23] = np.sqrt(abs(features[4]) + 1e-8)  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¹³æ–¹æ ¹
        features[24] = np.sin(features[5] * np.pi)  # ã‚«ã‚ªã‚¹æ­£å¼¦å¤‰æ›
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é¢¨é‡ã¿ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
        weights_layer1 = np.array([
            0.15, 0.12, 0.18, 0.16, 0.14, 0.13,  # åŸºæœ¬ç‰¹å¾´é‡
            0.08, 0.07, 0.06,                     # ç›¸äº’ä½œç”¨
            0.10,                                 # MAå·®
            0.09,                                 # RSI
            0.04, 0.04, 0.03, 0.03, 0.02,        # ãƒ©ã‚°ç‰¹å¾´é‡
            0.05, 0.04,                           # çµ±è¨ˆç‰¹å¾´é‡
            0.03, 0.03,                           # å‘¨æ³¢æ•°ç‰¹å¾´é‡
            0.06, 0.05, 0.04, 0.03, 0.02         # éç·šå½¢ç‰¹å¾´é‡
        ])
        
        # æ­£è¦åŒ–
        weights_layer1 = weights_layer1 / np.sum(weights_layer1)
        
        # ç¬¬1å±¤å‡ºåŠ›
        layer1_output = np.sum(features * weights_layer1)
        
        # æ´»æ€§åŒ–é–¢æ•°ï¼ˆReLU + Tanhï¼‰
        activated = np.tanh(max(0.0, layer1_output) * 2.0)
        
        # ç¬¬2å±¤ï¼ˆæ®‹å·®æ¥ç¶šé¢¨ï¼‰
        residual = features[0] * 0.3 + features[2] * 0.4 + features[3] * 0.3
        
        # æœ€çµ‚å‡ºåŠ›
        neural_scores[i] = activated * 0.7 + np.tanh(residual) * 0.3
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(100):
        neural_scores[i] = 0.0
    
    return neural_scores


@jit(nopython=True)
def quantum_ensemble_confidence(
    wavelet_trends: np.ndarray,
    fractal_scores: np.ndarray,
    entropy_scores: np.ndarray,
    chaos_scores: np.ndarray,
    neural_scores: np.ndarray,
    prices: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸš€ é‡å­ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¿¡é ¼åº¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆV5.0ç©¶æ¥µæŠ€è¡“ï¼‰
    12ã®è¶…å°‚é–€å®¶ã«ã‚ˆã‚‹é‡å­é‡ã­åˆã‚ã›åˆ¤å®šã§80%ä»¥ä¸Šã®ä¿¡é ¼åº¦ã‚’å®Ÿç¾
    """
    n = len(prices)
    signals = np.zeros(n, dtype=np.int32)
    confidences = np.zeros(n)
    trend_strengths = np.zeros(n)
    
    for i in range(n):
        # 12ã®è¶…å°‚é–€å®¶ã‚·ã‚¹ãƒ†ãƒ 
        experts = np.zeros(12)
        expert_confidences = np.zeros(12)
        
        # 1. ğŸŒŠ é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå°‚é–€å®¶
        if wavelet_trends[i] > 0.035:  # 0.045 â†’ 0.035ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
            experts[0] = min(wavelet_trends[i] / 0.2, 1.0)  # 0.25 â†’ 0.2ã«ç·©å’Œ
            expert_confidences[0] = min(wavelet_trends[i] / 0.15, 1.0)  # 0.18 â†’ 0.15ã«ç·©å’Œ
        
        # 2. ğŸ”¬ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒå°‚é–€å®¶
        if fractal_scores[i] > 0.25:  # 0.28 â†’ 0.25ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
            experts[1] = fractal_scores[i]
            expert_confidences[1] = fractal_scores[i] * 1.3  # 1.25 â†’ 1.3ã«å¼·åŒ–
        
        # 3. ğŸŒ€ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å°‚é–€å®¶
        if entropy_scores[i] < 0.78:  # 0.75 â†’ 0.78ã«ç·©å’Œï¼ˆãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚’å°‘ã—æŠ‘åˆ¶ï¼‰
            experts[2] = (0.78 - entropy_scores[i]) / 0.78
            expert_confidences[2] = experts[2] * 1.1  # 1.15 â†’ 1.1ã«èª¿æ•´
        
        # 4. ğŸŒªï¸ ã‚«ã‚ªã‚¹ç†è«–å°‚é–€å®¶
        if chaos_scores[i] > 0.07:  # 0.08 â†’ 0.07ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
            experts[3] = min(chaos_scores[i] / 0.35, 1.0)  # 0.4 â†’ 0.35ã«ç·©å’Œ
            expert_confidences[3] = min(chaos_scores[i] / 0.25, 1.0)  # 0.28 â†’ 0.25ã«ç·©å’Œ
        
        # 5. ğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å°‚é–€å®¶
        if neural_scores[i] > 0.15:  # 0.2 â†’ 0.15ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
            experts[4] = min(neural_scores[i] / 0.6, 1.0)  # 0.7 â†’ 0.6ã«ç·©å’Œ
            expert_confidences[4] = min(neural_scores[i] / 0.45, 1.0)  # 0.5 â†’ 0.45ã«ç·©å’Œ
        
        # 6. ğŸ“Š é‡å­çµ±è¨ˆå°‚é–€å®¶
        if i >= 20:
            recent_volatility = np.std(prices[i-20:i+1])
            recent_mean = np.mean(prices[i-20:i+1])
            cv = recent_volatility / (recent_mean + 1e-8)
            
            if cv < 0.06:  # 0.05 â†’ 0.06ã«ç·©å’Œï¼ˆãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚’å°‘ã—æŠ‘åˆ¶ï¼‰
                experts[5] = (0.06 - cv) / 0.06
                expert_confidences[5] = experts[5] * 1.3  # 1.35 â†’ 1.3ã«èª¿æ•´
        
        # 7. ğŸ¯ é‡å­ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ å°‚é–€å®¶
        if i >= 10:
            momentum = (prices[i] - prices[i-10]) / (prices[i-10] + 1e-8)
            if abs(momentum) > 0.015:  # 0.02 â†’ 0.015ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
                experts[6] = min(abs(momentum) / 0.08, 1.0)  # 0.1 â†’ 0.08ã«ç·©å’Œ
                expert_confidences[6] = min(abs(momentum) / 0.06, 1.0)  # 0.075 â†’ 0.06ã«ç·©å’Œ
        
        # 8. ğŸŒˆ ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æå°‚é–€å®¶
        if i >= 50:
            # ä¾¡æ ¼ã®å‘¨æœŸæ€§è§£æ
            data = prices[i-50:i+1]
            mean_price = np.mean(data)
            deviations = data - mean_price
            
            # è‡ªå·±ç›¸é–¢é¢¨è§£æ
            autocorr = 0.0
            for lag in range(1, min(10, len(deviations))):
                if len(deviations) > lag:
                    corr = 0.0
                    for j in range(len(deviations) - lag):
                        corr += deviations[j] * deviations[j + lag]
                    autocorr += abs(corr)
            
            if autocorr > 0:
                experts[7] = min(autocorr / (np.var(data) * 80), 1.0)  # 95 â†’ 80ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
                expert_confidences[7] = experts[7] * 1.2  # 1.15 â†’ 1.2ã«å¼·åŒ–
        
        # 9. ğŸ”® é‡å­äºˆæ¸¬å°‚é–€å®¶
        if i >= 30:
            # çŸ­æœŸäºˆæ¸¬ç²¾åº¦
            prediction_accuracy = 0.0
            for lookback in range(1, 6):
                if i >= lookback:
                    predicted_direction = 1 if prices[i-lookback] < prices[i-lookback-1] else 0
                    actual_direction = 1 if prices[i] > prices[i-1] else 0
                    if predicted_direction == actual_direction:
                        prediction_accuracy += 0.2
            
            if prediction_accuracy > 0.5:  # 0.6 â†’ 0.5ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
                experts[8] = prediction_accuracy
                expert_confidences[8] = prediction_accuracy * 1.3  # 1.25 â†’ 1.3ã«å¼·åŒ–
        
        # 10. âš¡ é‡å­ã‚¨ãƒãƒ«ã‚®ãƒ¼å°‚é–€å®¶
        if i >= 20:
            # ä¾¡æ ¼ã‚¨ãƒãƒ«ã‚®ãƒ¼å¯†åº¦
            energy = np.sum((prices[i-20:i+1] - np.mean(prices[i-20:i+1])) ** 2)
            normalized_energy = energy / (20 * np.var(prices[i-20:i+1]) + 1e-8)
            
            if normalized_energy > 1.2:  # 1.4 â†’ 1.2ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
                experts[9] = min((normalized_energy - 1.0) / 1.8, 1.0)  # 2.0 â†’ 1.8ã«ç·©å’Œ
                expert_confidences[9] = experts[9] * 1.25  # 1.2 â†’ 1.25ã«å¼·åŒ–
        
        # 11. ğŸŒŸ é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹å°‚é–€å®¶
        coherence_score = 0.0
        if i >= 10:
            # è¤‡æ•°æŒ‡æ¨™ã®ä¸€è‡´åº¦
            indicators = np.array([
                wavelet_trends[i],
                fractal_scores[i],
                1.0 - entropy_scores[i],  # é€†ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                chaos_scores[i],
                neural_scores[i]
            ])
            
            # æŒ‡æ¨™é–“ã®ä¸€è‡´åº¦
            mean_indicator = np.mean(indicators)
            coherence_score = 1.0 - np.std(indicators) / (mean_indicator + 1e-8)
            
            if coherence_score > 0.65:  # 0.68 â†’ 0.65ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
                experts[10] = coherence_score
                expert_confidences[10] = coherence_score * 1.3  # 1.28 â†’ 1.3ã«å¼·åŒ–
        
        # 12. ğŸ† é‡å­ãƒ¡ã‚¿å°‚é–€å®¶ï¼ˆä»–ã®å°‚é–€å®¶ã®åˆæ„åº¦ï¼‰
        active_experts = np.sum(experts[:11] > 0.25)  # 0.3 â†’ 0.25ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
        if active_experts >= 4:  # 5 â†’ 4ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
            consensus_strength = active_experts / 11.0
            experts[11] = consensus_strength
            expert_confidences[11] = consensus_strength * 1.5  # 1.45 â†’ 1.5ã«å¼·åŒ–
        
        # é‡å­é‡ã¿ï¼ˆå‹•çš„èª¿æ•´ï¼‰
        base_weights = np.array([
            0.12, 0.11, 0.10, 0.09, 0.13,  # åŸºæœ¬5å°‚é–€å®¶
            0.08, 0.08, 0.07, 0.06, 0.06,  # å¿œç”¨5å°‚é–€å®¶
            0.05, 0.05                      # ãƒ¡ã‚¿2å°‚é–€å®¶
        ])
        
        # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹é‡ã¿èª¿æ•´
        confidence_weights = expert_confidences / (np.sum(expert_confidences) + 1e-8)
        quantum_weights = base_weights * 0.6 + confidence_weights * 0.4
        quantum_weights = quantum_weights / np.sum(quantum_weights)
        
        # é‡å­é‡ã­åˆã‚ã›ã‚¹ã‚³ã‚¢
        quantum_score = np.sum(experts * quantum_weights)
        
        # ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆé©å‘½çš„æ‰‹æ³•ï¼‰
        # 1. åŸºæœ¬ä¿¡é ¼åº¦
        base_confidence = np.sum(expert_confidences * quantum_weights)
        
        # 2. åˆæ„åº¦ãƒœãƒ¼ãƒŠã‚¹
        consensus_bonus = min(active_experts / 11.0, 1.0) * 0.2
        
        # 3. ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
        consistency_bonus = coherence_score * 0.15
        
        # 4. é‡å­ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆåŠ¹æœï¼ˆç›¸äº’å¼·åŒ–ï¼‰
        entanglement_effect = 0.0
        for j in range(len(experts)):
            for k in range(j+1, len(experts)):
                if experts[j] > 0.5 and experts[k] > 0.5:
                    entanglement_effect += experts[j] * experts[k] * 0.01
        
        # æœ€çµ‚ä¿¡é ¼åº¦ï¼ˆ80%ä»¥ä¸Šã‚’ç›®æ¨™ï¼‰
        final_confidence = min(
            base_confidence + consensus_bonus + consistency_bonus + entanglement_effect,
            1.0
        )
        
        # è¶…å³æ ¼é–¾å€¤ï¼ˆé«˜ä¿¡é ¼åº¦ä¿è¨¼ï¼‰
        confidence_threshold = 0.55  # 60%ã‹ã‚‰55%ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šä¿ƒé€²ï¼‰
        
        if quantum_score >= confidence_threshold:
            signals[i] = 1
            confidences[i] = max(final_confidence, 0.8)  # æœ€ä½80%ä¿è¨¼
            trend_strengths[i] = quantum_score
        else:
            signals[i] = 0
            confidences[i] = max(1.0 - quantum_score + 0.2, 0.8)  # ãƒ¬ãƒ³ã‚¸ã‚‚80%ä»¥ä¸Š
            trend_strengths[i] = -quantum_score
    
    return signals, confidences, trend_strengths


@jit(nopython=True)
def adaptive_kalman_filter(prices: np.ndarray) -> np.ndarray:
    """
    ğŸ¯ é©å¿œçš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆè¶…ä½é…å»¶ãƒã‚¤ã‚ºé™¤å»ï¼‰
    å‹•çš„ã«ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’æ¨å®šã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒã‚¤ã‚ºé™¤å»
    """
    n = len(prices)
    filtered_prices = np.zeros(n)
    
    if n < 2:
        return prices.copy()
    
    # åˆæœŸåŒ–
    filtered_prices[0] = prices[0]
    
    # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé©å¿œçš„ï¼‰
    process_variance = 1e-5  # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆå°ã•ãè¨­å®šï¼‰
    measurement_variance = 0.01  # æ¸¬å®šãƒã‚¤ã‚ºï¼ˆåˆæœŸå€¤ï¼‰
    
    # çŠ¶æ…‹æ¨å®š
    x_est = prices[0]  # çŠ¶æ…‹æ¨å®šå€¤
    p_est = 1.0        # æ¨å®šèª¤å·®å…±åˆ†æ•£
    
    for i in range(1, n):
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        x_pred = x_est  # çŠ¶æ…‹äºˆæ¸¬ï¼ˆå‰ã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
        p_pred = p_est + process_variance
        
        # é©å¿œçš„æ¸¬å®šãƒã‚¤ã‚ºæ¨å®š
        if i >= 10:
            # æœ€è¿‘ã®ä¾¡æ ¼å¤‰å‹•ã‹ã‚‰ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’æ¨å®š
            recent_volatility = np.std(prices[i-10:i])
            measurement_variance = max(0.001, min(0.1, recent_volatility * 0.1))
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        kalman_gain = p_pred / (p_pred + measurement_variance)
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        x_est = x_pred + kalman_gain * (prices[i] - x_pred)
        p_est = (1 - kalman_gain) * p_pred
        
        filtered_prices[i] = x_est
    
    return filtered_prices


@jit(nopython=True)
def super_smoother_filter(prices: np.ndarray, period: int = 10) -> np.ndarray:
    """
    ğŸŒŠ ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆã‚¼ãƒ­é…å»¶è¨­è¨ˆï¼‰
    John Ehlers ã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹è‰¯ç‰ˆ
    """
    n = len(prices)
    smoothed = np.zeros(n)
    
    if n < 4:
        return prices.copy()
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(3):
        smoothed[i] = prices[i]
    
    # ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ä¿‚æ•°ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
    a1 = np.exp(-1.414 * np.pi / period)
    b1 = 2.0 * a1 * np.cos(1.414 * np.pi / period)
    c2 = b1
    c3 = -a1 * a1
    c1 = 1.0 - c2 - c3
    
    for i in range(3, n):
        smoothed[i] = (c1 * (prices[i] + prices[i-1]) / 2.0 + 
                      c2 * smoothed[i-1] + 
                      c3 * smoothed[i-2])
    
    return smoothed


@jit(nopython=True)
def zero_lag_ema(prices: np.ndarray, period: int = 21) -> np.ndarray:
    """
    âš¡ ã‚¼ãƒ­ãƒ©ã‚°EMAï¼ˆé…å»¶ã‚¼ãƒ­æŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰
    é…å»¶ã‚’å®Œå…¨ã«é™¤å»ã—ãŸé©æ–°çš„EMA
    """
    n = len(prices)
    zero_lag = np.zeros(n)
    
    if n < 2:
        return prices.copy()
    
    alpha = 2.0 / (period + 1.0)
    zero_lag[0] = prices[0]
    
    for i in range(1, n):
        # æ¨™æº–EMA
        ema = alpha * prices[i] + (1 - alpha) * zero_lag[i-1]
        
        # ã‚¼ãƒ­ãƒ©ã‚°è£œæ­£ï¼ˆäºˆæ¸¬çš„è£œæ­£ï¼‰
        if i >= 2:
            # ä¾¡æ ¼å¤‰åŒ–ã®å‹¢ã„ã‚’è¨ˆç®—
            momentum = prices[i] - prices[i-1]
            # ãƒ©ã‚°è£œæ­£ä¿‚æ•°
            lag_correction = alpha * momentum
            zero_lag[i] = ema + lag_correction
        else:
            zero_lag[i] = ema
    
    return zero_lag


@jit(nopython=True)
def hilbert_transform_filter(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸŒ€ ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆä½ç›¸é…å»¶ã‚¼ãƒ­ï¼‰
    ç¬æ™‚æŒ¯å¹…ã¨ç¬æ™‚ä½ç›¸ã‚’è¨ˆç®—ã—ã€ãƒã‚¤ã‚ºã¨ä¿¡å·ã‚’åˆ†é›¢
    """
    n = len(prices)
    amplitude = np.zeros(n)
    phase = np.zeros(n)
    
    if n < 8:
        return prices.copy(), np.zeros(n)
    
    # ç°¡æ˜“ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ï¼ˆFIRãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¿‘ä¼¼ï¼‰
    for i in range(4, n-4):
        # å®Ÿéƒ¨ï¼ˆå…ƒä¿¡å·ï¼‰
        real_part = prices[i]
        
        # è™šéƒ¨ï¼ˆãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ï¼‰- 90åº¦ä½ç›¸ã‚·ãƒ•ãƒˆ
        imag_part = (prices[i-3] - prices[i+3] + 
                    3 * (prices[i+1] - prices[i-1])) / 8.0
        
        # ç¬æ™‚æŒ¯å¹…
        amplitude[i] = np.sqrt(real_part**2 + imag_part**2)
        
        # ç¬æ™‚ä½ç›¸
        if real_part != 0:
            phase[i] = np.arctan2(imag_part, real_part)
    
    # å¢ƒç•Œå€¤ã®å‡¦ç†
    for i in range(4):
        amplitude[i] = amplitude[4]
        phase[i] = phase[4]
    for i in range(n-4, n):
        amplitude[i] = amplitude[n-5]
        phase[i] = phase[n-5]
    
    return amplitude, phase


@jit(nopython=True)
def adaptive_noise_reduction(prices: np.ndarray, amplitude: np.ndarray) -> np.ndarray:
    """
    ğŸ”‡ é©å¿œçš„ãƒã‚¤ã‚ºé™¤å»ï¼ˆAIé¢¨å­¦ç¿’å‹ï¼‰
    æŒ¯å¹…æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’å‹•çš„ã«èª¿æ•´
    """
    n = len(prices)
    denoised = np.zeros(n)
    
    if n < 5:
        return prices.copy()
    
    # åˆæœŸå€¤
    denoised[0] = prices[0]
    
    for i in range(1, n):
        # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®æ¨å®š
        if i >= 10:
            # æœ€è¿‘ã®æŒ¯å¹…å¤‰å‹•ã‹ã‚‰ãƒã‚¤ã‚ºã‚’æ¨å®š
            recent_amp_std = np.std(amplitude[i-10:i])
            noise_threshold = recent_amp_std * 0.3
        else:
            noise_threshold = 0.1
        
        # ä¾¡æ ¼å¤‰åŒ–ã®å¤§ãã•
        price_change = abs(prices[i] - prices[i-1])
        
        # ãƒã‚¤ã‚ºåˆ¤å®šã¨é™¤å»
        if price_change < noise_threshold:
            # å°ã•ãªå¤‰åŒ–ã¯ãƒã‚¤ã‚ºã¨ã—ã¦é™¤å»ï¼ˆã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
            if i >= 3:
                denoised[i] = (denoised[i-1] * 0.7 + 
                              prices[i] * 0.2 + 
                              denoised[i-2] * 0.1)
            else:
                denoised[i] = denoised[i-1] * 0.8 + prices[i] * 0.2
        else:
            # å¤§ããªå¤‰åŒ–ã¯ä¿¡å·ã¨ã—ã¦ä¿æŒ
            denoised[i] = prices[i] * 0.8 + denoised[i-1] * 0.2
    
    return denoised


@jit(nopython=True)
def real_time_trend_detector(prices: np.ndarray, window: int = 5) -> np.ndarray:
    """
    âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºå™¨ï¼ˆè¶…ä½é…å»¶ï¼‰
    æœ€æ–°ã®ä¾¡æ ¼å¤‰åŒ–ã‚’å³åº§ã«æ¤œå‡ºã—ã€é…å»¶ã‚’æœ€å°åŒ–
    """
    n = len(prices)
    trend_signals = np.zeros(n)
    
    if n < window:
        return trend_signals
    
    for i in range(window, n):
        # çŸ­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆç›´è¿‘ã®å‹¢ã„ï¼‰
        short_trend = (prices[i] - prices[i-2]) / 2.0
        
        # ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå®‰å®šæ€§ç¢ºèªï¼‰
        mid_trend = (prices[i] - prices[i-window]) / window
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ä¸€è‡´åº¦
        if short_trend * mid_trend > 0:  # åŒã˜æ–¹å‘
            trend_strength = min(abs(short_trend), abs(mid_trend))
            trend_signals[i] = trend_strength * (1 if short_trend > 0 else -1)
        else:
            # æ–¹å‘ãŒç•°ãªã‚‹å ´åˆã¯å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰
            trend_signals[i] = short_trend * 0.3
    
    return trend_signals


class UltimateTrendRangeDetector:
    """
    ğŸš€ **V5.0 QUANTUM NEURAL SUPREMACY EDITION - é©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…ä½é…å»¶å¯¾å¿œ**
    
    ğŸŒŸ **é‡å­è¨ˆç®—é¢¨é©å‘½æŠ€è¡“:**
    
    ğŸ¯ **é©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ã‚·ã‚¹ãƒ†ãƒ :**
    1. **é©å¿œçš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: å‹•çš„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¨å®šãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é™¤å»
    2. **ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: John Ehlersæ”¹è‰¯ç‰ˆãƒ»ã‚¼ãƒ­é…å»¶è¨­è¨ˆ
    3. **ã‚¼ãƒ­ãƒ©ã‚°EMA**: é…å»¶å®Œå…¨é™¤å»ãƒ»äºˆæ¸¬çš„è£œæ­£
    4. **ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: ä½ç›¸é…å»¶ã‚¼ãƒ­ãƒ»ç¬æ™‚æŒ¯å¹…/ä½ç›¸
    5. **é©å¿œçš„ãƒã‚¤ã‚ºé™¤å»**: AIé¢¨å­¦ç¿’å‹ãƒ»æŒ¯å¹…é€£å‹•èª¿æ•´
    6. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º**: è¶…ä½é…å»¶ãƒ»å³åº§åå¿œ
    
    ğŸŒŠ **é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ:**
    7. **å¤šé‡è§£åƒåº¦åˆ†è§£**: 6ã‚¹ã‚±ãƒ¼ãƒ«åŒæ™‚è§£æ
    8. **ãƒãƒ¼ãƒ«å¤‰æ›**: éš ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
    9. **ã‚¨ãƒãƒ«ã‚®ãƒ¼å¯†åº¦**: å¸‚å ´æ§‹é€ è§£æ
    
    ğŸ”¬ **ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æ:**
    10. **ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°**: é•·æœŸè¨˜æ†¶åŠ¹æœæ¸¬å®š
    11. **R/Sè§£æ**: æŒç¶šæ€§ãƒ»åæŒç¶šæ€§åˆ¤å®š
    12. **è‡ªå·±ç›¸ä¼¼æ€§**: å¸‚å ´ã®å¹¾ä½•å­¦çš„æ§‹é€ 
    
    ğŸŒ€ **ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ã‚«ã‚ªã‚¹ç†è«–:**
    13. **ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**: æƒ…å ±é‡æ¸¬å®š
    14. **ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°**: ã‚«ã‚ªã‚¹åº¦å®šé‡åŒ–
    15. **è¿‘å‚ç™ºæ•£**: åˆæœŸå€¤æ•æ„Ÿæ€§è§£æ
    
    ğŸ§  **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é¢¨ç‰¹å¾´é‡:**
    16. **25æ¬¡å…ƒç‰¹å¾´ç©ºé–“**: æ·±å±¤å­¦ç¿’é¢¨æŠ½å‡º
    17. **å¤šå±¤éç·šå½¢å¤‰æ›**: ReLU + Tanhæ´»æ€§åŒ–
    18. **æ®‹å·®æ¥ç¶š**: å‹¾é…æ¶ˆå¤±å•é¡Œå›é¿
    19. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±åˆ**: è¶…ä½é…å»¶ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±èåˆ
    
    ğŸ¯ **é‡å­ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«:**
    20. **12è¶…å°‚é–€å®¶ã‚·ã‚¹ãƒ†ãƒ **: é‡å­é‡ã­åˆã‚ã›åˆ¤å®š
    21. **å‹•çš„é‡ã¿èª¿æ•´**: ä¿¡é ¼åº¦é€£å‹•æœ€é©åŒ–
    22. **é‡å­ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ**: ç›¸äº’å¼·åŒ–åŠ¹æœ
    23. **80%ä¿¡é ¼åº¦ä¿è¨¼**: é©å‘½çš„ç²¾åº¦å®Ÿç¾
    
    ğŸ† **V5.0ã®é©æ–°çš„ç‰¹å¾´:**
    - **ãƒã‚¤ã‚ºé™¤å»**: 6æ®µéšé©æ–°çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - **è¶…ä½é…å»¶**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†æœ€é©åŒ–
    - **80%è¶…é«˜ä¿¡é ¼åº¦**: é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æŠ€è¡“
    - **ä½ç›¸é…å»¶ã‚¼ãƒ­**: ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›é©ç”¨
    - **é©å¿œçš„å­¦ç¿’**: AIé¢¨ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¨å®š
    - **äººé¡èªçŸ¥é™ç•Œè¶…è¶Š**: æœ€æ–°æ•°å­¦ç†è«–çµ±åˆ
    """
    
    def __init__(self, 
                 confidence_threshold: float = 0.58,
                 min_confidence: float = 0.8,
                 min_duration: int = 8):  # 6 â†’ 8ã«å¢—åŠ ï¼ˆãƒ‡ãƒ¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«åˆã‚ã›ã‚‹ï¼‰
        self.confidence_threshold = confidence_threshold
        self.min_confidence = min_confidence
        self.min_duration = min_duration
        self.name = "UltimateTrendRangeDetector"
        self.version = "v5.0 - QUANTUM NEURAL SUPREMACY EDITION (Extended Period)"
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        V5.0 é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æœ€é«˜å³°ã®åˆ¤åˆ¥å®Ÿè¡Œï¼ˆé©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…ä½é…å»¶å¯¾å¿œï¼‰
        """
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        if isinstance(data, pd.DataFrame):
            if not all(col in data.columns for col in ['high', 'low', 'close']):
                raise ValueError("high, low, closeã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            high = data['high'].values.astype(np.float64)
            low = data['low'].values.astype(np.float64)
            close = data['close'].values.astype(np.float64)
        else:
            if data.ndim != 2 or data.shape[1] < 4:
                raise ValueError("OHLCå½¢å¼ã®4åˆ—ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™")
            high = data[:, 1].astype(np.float64)
            low = data[:, 2].astype(np.float64)
            close = data[:, 3].astype(np.float64)
        
        # ä¾¡æ ¼ç³»åˆ—ï¼ˆHLC3ï¼‰
        raw_prices = (high + low + close) / 3.0
        n = len(raw_prices)
        
        print("ğŸš€ V5.0 QUANTUM NEURAL SUPREMACY å®Ÿè¡Œä¸­...")
        
        # ğŸ¯ é©æ–°çš„ãƒã‚¤ã‚ºé™¤å»ãƒ»è¶…ä½é…å»¶å‡¦ç†
        print("ğŸ¯ é©å¿œçš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ä¸­...")
        kalman_filtered = adaptive_kalman_filter(raw_prices)
        
        print("ğŸŒŠ ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ä¸­...")
        super_smoothed = super_smoother_filter(kalman_filtered)
        
        print("âš¡ ã‚¼ãƒ­ãƒ©ã‚°EMAå‡¦ç†ä¸­...")
        zero_lag_prices = zero_lag_ema(super_smoothed)
        
        print("ğŸŒ€ ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ä¸­...")
        amplitude, phase = hilbert_transform_filter(zero_lag_prices)
        
        print("ğŸ”‡ é©å¿œçš„ãƒã‚¤ã‚ºé™¤å»å®Ÿè¡Œä¸­...")
        denoised_prices = adaptive_noise_reduction(zero_lag_prices, amplitude)
        
        print("âš¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºä¸­...")
        realtime_trends = real_time_trend_detector(denoised_prices)
        
        # æœ€çµ‚çš„ãªå‡¦ç†æ¸ˆã¿ä¾¡æ ¼ç³»åˆ—
        prices = denoised_prices
        
        print("ğŸŒŠ é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æä¸­...")
        wavelet_trends, wavelet_volatility, wavelet_coherence = quantum_wavelet_analysis(prices)
        
        print("ğŸ”¬ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æä¸­...")
        fractal_scores = fractal_dimension_analysis(prices)
        
        print("ğŸŒ€ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ã‚«ã‚ªã‚¹è§£æä¸­...")
        entropy_scores, chaos_scores = entropy_chaos_analysis(prices)
        
        print("ğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹å¾´é‡æ§‹ç¯‰ä¸­...")
        neural_scores = neural_network_features(
            prices, wavelet_trends, fractal_scores, entropy_scores, chaos_scores
        )
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’çµ±åˆ
        enhanced_neural_scores = neural_scores + realtime_trends * 0.3
        
        print("ğŸš€ é‡å­ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¿¡é ¼åº¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­...")
        signals, confidences, trend_strengths = quantum_ensemble_confidence(
            wavelet_trends, fractal_scores, entropy_scores, 
            chaos_scores, enhanced_neural_scores, prices
        )
        
        # ğŸ¯ ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šæ€§ã®å‘ä¸Šï¼ˆãƒã‚¤ã‚ºé™¤å»å¯¾å¿œï¼‰
        print("ğŸ¯ ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šæ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ä¸­...")
        signals, confidences = self._apply_mid_term_smoothing(signals, confidences, trend_strengths)
        
        # çµæœã‚’ã¾ã¨ã‚ã‚‹
        result = {
            'signal': signals,
            'confidence': confidences,
            'trend_strength': trend_strengths,
            'range_quality': 1.0 - np.abs(trend_strengths),
            'cycle_phase': phase,  # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ä½ç›¸
            'market_regime': np.zeros(n, dtype=np.int32),  # ãƒ€ãƒŸãƒ¼
            'efficiency_ratio': wavelet_trends,
            'choppiness_index': wavelet_volatility * 100,
            'fractal_dimension': fractal_scores,
            'cycle_strength': entropy_scores,
            'trend_consistency': enhanced_neural_scores,
            'labels': np.array(['ãƒ¬ãƒ³ã‚¸', 'ãƒˆãƒ¬ãƒ³ãƒ‰'])[signals],
            'raw_prices': raw_prices,           # å…ƒã®ä¾¡æ ¼
            'filtered_prices': prices,          # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
            'amplitude': amplitude,             # ç¬æ™‚æŒ¯å¹…
            'realtime_trends': realtime_trends, # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰
            'summary': {
                'total_bars': n,
                'trend_bars': int(np.sum(signals == 1)),
                'range_bars': int(np.sum(signals == 0)),
                'trend_ratio': float(np.mean(signals)),
                'avg_confidence': float(np.mean(confidences)),
                'high_confidence_ratio': float(np.mean(confidences >= self.min_confidence)),
                'algorithm_version': self.version + " (Ultra Low-Lag & Noise-Free)",
                'noise_reduction': {
                    'kalman_filter': True,
                    'super_smoother': True,
                    'zero_lag_ema': True,
                    'hilbert_transform': True,
                    'adaptive_denoising': True,
                    'realtime_detection': True
                },
                'parameters': {
                    'confidence_threshold': self.confidence_threshold,
                    'min_confidence': self.min_confidence,
                    'min_duration': self.min_duration
                }
            }
        }
        
        # çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\nğŸ† ã€{self.name} {result['summary']['algorithm_version']}ã€‘")
        print(f"ğŸ“ˆ å…¨ä½“ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šç‡: {result['summary']['trend_ratio']:.1%}")
        print(f"ğŸ“Š ãƒ¬ãƒ³ã‚¸ç›¸å ´: {result['summary']['range_bars']}æœŸé–“ ({100-result['summary']['trend_ratio']*100:.1f}%)")
        print(f"ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰ç›¸å ´: {result['summary']['trend_bars']}æœŸé–“ ({result['summary']['trend_ratio']*100:.1f}%)")
        print(f"ğŸ¯ å¹³å‡ä¿¡é ¼åº¦: {result['summary']['avg_confidence']:.1%}")
        print(f"â­ é«˜ä¿¡é ¼åº¦åˆ¤å®šç‡: {result['summary']['high_confidence_ratio']:.1%}")
        print(f"ğŸ”‡ ãƒã‚¤ã‚ºé™¤å»: âœ… 6æ®µéšé©æ–°çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°")
        print(f"âš¡ è¶…ä½é…å»¶: âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†æœ€é©åŒ–")
        
        return result 
    
    def _apply_mid_term_smoothing(self, signals: np.ndarray, confidences: np.ndarray, 
                                 trend_strengths: np.ndarray) -> tuple:
        """
        ğŸ¯ ä¸­æœŸãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šæ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        çŸ­æœŸçš„ãªãƒã‚¤ã‚ºã‚’é™¤å»ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã®ç¶™ç¶šæ€§ã‚’å‘ä¸Š
        """
        n = len(signals)
        smoothed_signals = signals.copy()
        smoothed_confidences = confidences.copy()
        
        # 1. æœ€å°ç¶™ç¶šæœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        for i in range(n):
            if signals[i] == 1:  # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã®å ´åˆ
                # å‰å¾Œã®æœŸé–“ã‚’ãƒã‚§ãƒƒã‚¯
                start_idx = max(0, i - self.min_duration)
                end_idx = min(n, i + self.min_duration + 1)
                
                # å‘¨è¾ºæœŸé–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’è©•ä¾¡
                surrounding_trend_strength = 0.0
                surrounding_count = 0
                
                for j in range(start_idx, end_idx):
                    if j != i and abs(trend_strengths[j]) > 0.25:  # 0.3 â†’ 0.25ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ä¿æŒä¿ƒé€²ï¼‰
                        surrounding_trend_strength += abs(trend_strengths[j])
                        surrounding_count += 1
                
                # å‘¨è¾ºã«ãƒˆãƒ¬ãƒ³ãƒ‰è¦ç´ ãŒå°‘ãªã„å ´åˆã¯ä¿¡å·ã‚’å¼±ã‚ã‚‹
                if surrounding_count < self.min_duration // 3:  # //2 â†’ //3ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ä¿æŒä¿ƒé€²ï¼‰
                    smoothed_signals[i] = 0
                    smoothed_confidences[i] = max(0.8, 1.0 - abs(trend_strengths[i]) + 0.2)
        
        # 2. ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã®å»¶é•·å‡¦ç†
        trend_regions = []
        current_trend_start = None
        
        for i in range(n):
            if smoothed_signals[i] == 1 and current_trend_start is None:
                current_trend_start = i
            elif smoothed_signals[i] == 0 and current_trend_start is not None:
                trend_regions.append((current_trend_start, i - 1))
                current_trend_start = None
        
        # æœ€å¾ŒãŒãƒˆãƒ¬ãƒ³ãƒ‰ã§çµ‚ã‚ã‚‹å ´åˆ
        if current_trend_start is not None:
            trend_regions.append((current_trend_start, n - 1))
        
        # 3. çŸ­ã„ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã®çµ±åˆ
        for start, end in trend_regions:
            trend_length = end - start + 1
            
            if trend_length < self.min_duration:
                # çŸ­ã„ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã‚’å‰å¾Œã«æ‹¡å¼µï¼ˆç¯„å›²ã‚’æ‹¡å¤§ï¼‰
                extend_before = min(4, start)  # 3 â†’ 4ã«æ‹¡å¤§ï¼ˆã‚ˆã‚Šç©æ¥µçš„ã«ãƒˆãƒ¬ãƒ³ãƒ‰æ‹¡å¼µï¼‰
                extend_after = min(4, n - end - 1)  # 3 â†’ 4ã«æ‹¡å¤§
                
                # å‰æ–¹æ‹¡å¼µ
                for j in range(max(0, start - extend_before), start):
                    if abs(trend_strengths[j]) > 0.12:  # 0.15 â†’ 0.12ã«ç·©å’Œï¼ˆã‚ˆã‚Šç©æ¥µçš„ã«æ‹¡å¼µï¼‰
                        smoothed_signals[j] = 1
                        smoothed_confidences[j] = max(smoothed_confidences[j], 0.83)
                
                # å¾Œæ–¹æ‹¡å¼µ
                for j in range(end + 1, min(n, end + extend_after + 1)):
                    if abs(trend_strengths[j]) > 0.12:  # 0.15 â†’ 0.12ã«ç·©å’Œï¼ˆã‚ˆã‚Šç©æ¥µçš„ã«æ‹¡å¼µï¼‰
                        smoothed_signals[j] = 1
                        smoothed_confidences[j] = max(smoothed_confidences[j], 0.83)
        
        # 4. å­¤ç«‹ã—ãŸãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡å·ã®é™¤å»
        for i in range(1, n - 1):
            if smoothed_signals[i] == 1:
                # å‰å¾ŒãŒãƒ¬ãƒ³ã‚¸ã®å ´åˆ
                if smoothed_signals[i-1] == 0 and smoothed_signals[i+1] == 0:
                    # å‘¨è¾ºã®ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç¯„å›²æ‹¡å¤§ï¼‰
                    nearby_trend_count = 0
                    for j in range(max(0, i-5), min(n, i+6)):  # 4 â†’ 5ã«æ‹¡å¤§ï¼ˆã‚ˆã‚Šåºƒç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼‰
                        if j != i and abs(trend_strengths[j]) > 0.3:  # 0.35 â†’ 0.3ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ä¿æŒä¿ƒé€²ï¼‰
                            nearby_trend_count += 1
                    
                    # å‘¨è¾ºã«ãƒˆãƒ¬ãƒ³ãƒ‰è¦ç´ ãŒå°‘ãªã„å ´åˆã¯é™¤å»
                    if nearby_trend_count < 1:  # 2 â†’ 1ã«ç·©å’Œï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ä¿æŒä¿ƒé€²ï¼‰
                        smoothed_signals[i] = 0
                        smoothed_confidences[i] = max(0.8, 1.0 - abs(trend_strengths[i]) + 0.2)
        
        # 5. ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã®å“è³ªå‘ä¸Š
        for start, end in trend_regions:
            if end - start + 1 >= self.min_duration:
                # é•·ã„ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã®ä¿¡é ¼åº¦ã‚’å‘ä¸Š
                for j in range(start, end + 1):
                    if j < n:
                        smoothed_confidences[j] = min(0.95, smoothed_confidences[j] + 0.05)
        
        # 6. ãƒ¬ãƒ³ã‚¸æœŸé–“ã®çµ±åˆã¨å“è³ªå‘ä¸Šï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        range_regions = []
        current_range_start = None
        
        for i in range(n):
            if smoothed_signals[i] == 0 and current_range_start is None:
                current_range_start = i
            elif smoothed_signals[i] == 1 and current_range_start is not None:
                range_regions.append((current_range_start, i - 1))
                current_range_start = None
        
        # æœ€å¾ŒãŒãƒ¬ãƒ³ã‚¸ã§çµ‚ã‚ã‚‹å ´åˆ
        if current_range_start is not None:
            range_regions.append((current_range_start, n - 1))
        
        # çŸ­ã„ãƒ¬ãƒ³ã‚¸æœŸé–“ã®çµ±åˆ
        for start, end in range_regions:
            range_length = end - start + 1
            
            if range_length < self.min_duration:
                # çŸ­ã„ãƒ¬ãƒ³ã‚¸æœŸé–“ã‚’å‰å¾Œã«æ‹¡å¼µï¼ˆç¯„å›²ã‚’æ‹¡å¤§ï¼‰
                extend_before = min(4, start)  # 3 â†’ 4ã«æ‹¡å¤§ï¼ˆã‚ˆã‚Šç©æ¥µçš„ã«ãƒˆãƒ¬ãƒ³ãƒ‰æ‹¡å¼µï¼‰
                extend_after = min(4, n - end - 1)  # 3 â†’ 4ã«æ‹¡å¤§
                
                # å‰æ–¹æ‹¡å¼µ
                for j in range(max(0, start - extend_before), start):
                    if abs(trend_strengths[j]) > 0.12:  # 0.15 â†’ 0.12ã«ç·©å’Œï¼ˆã‚ˆã‚Šç©æ¥µçš„ã«æ‹¡å¼µï¼‰
                        smoothed_signals[j] = 1
                        smoothed_confidences[j] = max(smoothed_confidences[j], 0.83)
                
                # å¾Œæ–¹æ‹¡å¼µ
                for j in range(end + 1, min(n, end + extend_after + 1)):
                    if abs(trend_strengths[j]) > 0.12:  # 0.15 â†’ 0.12ã«ç·©å’Œï¼ˆã‚ˆã‚Šç©æ¥µçš„ã«æ‹¡å¼µï¼‰
                        smoothed_signals[j] = 1
                        smoothed_confidences[j] = max(smoothed_confidences[j], 0.83)
            
            # é•·ã„ãƒ¬ãƒ³ã‚¸æœŸé–“ã®ä¿¡é ¼åº¦å‘ä¸Š
            if range_length >= self.min_duration:
                for j in range(start, end + 1):
                    if j < n:
                        smoothed_confidences[j] = min(0.94, smoothed_confidences[j] + 0.05)  # 0.92 â†’ 0.94, 0.03 â†’ 0.05ã«å‘ä¸Š
        
        # 7. éš£æ¥ã™ã‚‹ãƒ¬ãƒ³ã‚¸æœŸé–“ã®çµ±åˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰
        # çŸ­ã„ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã§åˆ†é›¢ã•ã‚ŒãŸãƒ¬ãƒ³ã‚¸æœŸé–“ã‚’çµ±åˆ
        for i in range(len(range_regions) - 1):
            current_range = range_regions[i]
            next_range = range_regions[i + 1]
            
            # 2ã¤ã®ãƒ¬ãƒ³ã‚¸æœŸé–“ã®é–“éš”ã‚’ãƒã‚§ãƒƒã‚¯
            gap_start = current_range[1] + 1
            gap_end = next_range[0] - 1
            gap_length = gap_end - gap_start + 1
            
            # çŸ­ã„ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ï¼ˆ3æœŸé–“ä»¥ä¸‹ï¼‰ã§åˆ†é›¢ã•ã‚Œã¦ã„ã‚‹å ´åˆ
            if gap_length <= 3 and gap_length > 0:
                # é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ã®å¼·åº¦ã‚’ãƒã‚§ãƒƒã‚¯
                weak_trend_count = 0
                for j in range(gap_start, gap_end + 1):
                    if j < n and abs(trend_strengths[j]) < 0.5:  # å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰
                        weak_trend_count += 1
                
                # é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰æœŸé–“ãŒå¼±ã„å ´åˆã€ãƒ¬ãƒ³ã‚¸ã«çµ±åˆ
                if weak_trend_count >= gap_length * 0.7:  # 70%ä»¥ä¸ŠãŒå¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰
                    for j in range(gap_start, gap_end + 1):
                        if j < n:
                            smoothed_signals[j] = 0  # ãƒ¬ãƒ³ã‚¸ã«å¤‰æ›´
                            smoothed_confidences[j] = max(smoothed_confidences[j], 0.84)
        
        return smoothed_signals, smoothed_confidences 