#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from typing import Union, Optional, NamedTuple, Tuple
import numpy as np
import pandas as pd
from numba import jit, njit
from dataclasses import dataclass

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‹ã‚‰çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤‰æ›´
try:
    from .indicator import Indicator
    from .price_source import PriceSource
    from .ultimate_kalman_filter import ultimate_adaptive_kalman_forward_numba
except ImportError:
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œæ™‚ã®å¯¾å¿œ
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource
    try:
        from ultimate_kalman_filter import ultimate_adaptive_kalman_forward_numba
    except ImportError:
        print("âš ï¸ ultimate_kalman_filter ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        ultimate_adaptive_kalman_forward_numba = None


@dataclass
class UltimateSupremeCycleResult:
    """ğŸš€ Ultimate Supreme Cycle Detector - äººé¡å²ä¸Šæœ€å¼·ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºçµæœ"""
    # ã‚³ã‚¢ã‚µã‚¤ã‚¯ãƒ«æƒ…å ±
    dominant_cycle: np.ndarray              # æ”¯é…çš„ã‚µã‚¤ã‚¯ãƒ«æœŸé–“
    cycle_strength: np.ndarray              # ã‚µã‚¤ã‚¯ãƒ«å¼·åº¦ (0-1)
    cycle_phase: np.ndarray                 # ã‚µã‚¤ã‚¯ãƒ«ä½ç›¸ (0-2Ï€)
    cycle_confidence: np.ndarray            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ (0-1)
    
    # é©å¿œãƒ»è¿½å¾“ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    adaptation_speed: np.ndarray            # é©å¿œé€Ÿåº¦
    tracking_accuracy: np.ndarray           # è¿½å¾“ç²¾åº¦
    noise_rejection_ratio: np.ndarray       # ãƒã‚¤ã‚ºé™¤å»ç‡
    
    # é«˜åº¦è§£æçµæœ
    quantum_coherence: np.ndarray           # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹
    topology_indicator: np.ndarray          # ä½ç›¸ç©ºé–“ãƒˆãƒãƒ­ã‚¸ãƒ¼æŒ‡æ¨™
    chaos_indicator: np.ndarray             # ã‚«ã‚ªã‚¹æŒ‡æ¨™
    information_content: np.ndarray         # æƒ…å ±å«æœ‰é‡
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ ãƒ»çŠ¶æ…‹æƒ…å ±
    market_regime: np.ndarray               # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ 
    volatility_regime: np.ndarray           # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
    cycle_regime: np.ndarray                # ã‚µã‚¤ã‚¯ãƒ«ãƒ¬ã‚¸ãƒ¼ãƒ 
    
    # çµ±è¨ˆãƒ»ä¿¡é ¼æ€§æƒ…å ±
    statistical_significance: np.ndarray    # çµ±è¨ˆçš„æœ‰æ„æ€§
    prediction_accuracy: np.ndarray         # äºˆæ¸¬ç²¾åº¦
    stability_score: np.ndarray             # å®‰å®šæ€§ã‚¹ã‚³ã‚¢
    
    # ç¾åœ¨çŠ¶æ…‹
    current_cycle: float                    # ç¾åœ¨ã®æ”¯é…çš„ã‚µã‚¤ã‚¯ãƒ«
    current_strength: float                 # ç¾åœ¨ã®ã‚µã‚¤ã‚¯ãƒ«å¼·åº¦
    current_confidence: float               # ç¾åœ¨ã®ä¿¡é ¼åº¦


class UltimateSupremeCycleDetector(Indicator):
    """
    ğŸš€ Ultimate Supreme Cycle Detector V1.0 - äººé¡å²ä¸Šæœ€å¼·ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨
    
    ã€Œé©å¿œãƒ»è¿½å¾“ã®ç©¶æ¥µé€²åŒ–ã€ã‚’ã‚³ãƒ³ã‚»ãƒ—ãƒˆã¨ã—ãŸé©æ–°çš„ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨
    
    ğŸŒŸ æŠ€è¡“é©æ–°ãƒã‚¤ãƒ³ãƒˆ:
    1. é‡å­åŠ›å­¦æ¦‚å¿µã®é‡‘èå¿œç”¨ - å²ä¸Šåˆã®é‡å­ã‚‚ã¤ã‚Œã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º
    2. çµ±åˆé‡å­èåˆã‚·ã‚¹ãƒ†ãƒ  - ä½ç›¸ç©ºé–“ãƒˆãƒãƒ­ã‚¸ãƒ¼ + ã‚«ã‚ªã‚¹ç†è«– + æƒ…å ±ç†è«–ã®å®Œå…¨çµ±åˆ
    3. é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³çµ±åˆ - åŸºæœ¬ + ç„¡é¦™æ–™ + é‡å­ã®ä¸‰é‡ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    4. è¶…é€²åŒ–DFT - 16å€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹ç©¶æ¥µå‘¨æ³¢æ•°è§£æ
    5. é©å¿œãƒ»è¿½å¾“ã®ç©¶æ¥µé€²åŒ– - å¸‚å ´å¤‰åŒ–ã¸ã®ç¬æ™‚å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(
        self,
        # åŸºæœ¬è¨­å®šï¼ˆä½é…å»¶ãƒ»ãƒãƒ©ãƒ³ã‚¹ç‰ˆï¼‰
        period_range: Tuple[int, int] = (10, 50),
        adaptivity_factor: float = 0.85,  # é©å¿œæ€§ã‚’å¾®èª¿æ•´ï¼ˆå¿œç­”æ€§ã¨å®‰å®šæ€§ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
        tracking_sensitivity: float = 0.92, # æ„Ÿåº¦ã‚’å¾®èª¿æ•´
        
        # é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå¿œç­”æ€§é‡è¦–èª¿æ•´ï¼‰
        quantum_coherence_threshold: float = 0.70, # é–¾å€¤ã‚’ä¸‹ã’ã¦æ„Ÿåº¦å‘ä¸Š
        entanglement_strength: float = 0.88,       # ã‚‚ã¤ã‚Œå¼·åº¦ã‚’å¾®èª¿æ•´
        
        # æƒ…å ±ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        entropy_window: int = 25,
        information_gain_threshold: float = 0.75,
        
        # çµ±åˆèåˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        chaos_embedding_dimension: int = 5,
        topology_analysis_window: int = 30,
        attractor_reconstruction_delay: int = 3,
        
        # é©å¿œåˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        fast_adapt_alpha: float = 0.3,
        slow_adapt_alpha: float = 0.05,
        regime_switch_threshold: float = 0.7,
        
        # è¿½å¾“æ€§åˆ¶å¾¡ï¼ˆä½é…å»¶èª¿æ•´ï¼‰
        tracking_lag_tolerance: int = 1,     # é…å»¶è¨±å®¹åº¦ã‚’ä¸‹ã’ã‚‹
        noise_immunity_factor: float = 0.78, # ãƒã‚¤ã‚ºé™¤å»ã‚’å¼±ã‚ã‚‹
        signal_purity_threshold: float = 0.82, # ä¿¡å·ç´”åº¦é–¾å€¤ã‚’ä¸‹ã’ã‚‹
        
        # ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
        src_type: str = 'hlc3'
    ):
        """åˆæœŸåŒ–"""
        super().__init__(f"UltimateSupreme(range={period_range},adapt={adaptivity_factor:.2f},track={tracking_sensitivity:.2f})")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
        self.period_range = period_range
        self.adaptivity_factor = adaptivity_factor
        self.tracking_sensitivity = tracking_sensitivity
        self.quantum_coherence_threshold = quantum_coherence_threshold
        self.entanglement_strength = entanglement_strength
        self.entropy_window = entropy_window
        self.information_gain_threshold = information_gain_threshold
        self.chaos_embedding_dimension = chaos_embedding_dimension
        self.topology_analysis_window = topology_analysis_window
        self.attractor_reconstruction_delay = attractor_reconstruction_delay
        self.fast_adapt_alpha = fast_adapt_alpha
        self.slow_adapt_alpha = slow_adapt_alpha
        self.regime_switch_threshold = regime_switch_threshold
        self.tracking_lag_tolerance = tracking_lag_tolerance
        self.noise_immunity_factor = noise_immunity_factor
        self.signal_purity_threshold = signal_purity_threshold
        self.src_type = src_type
        
        self.price_source_extractor = PriceSource()
        self._cache = {}
        self._result: Optional[UltimateSupremeCycleResult] = None

    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateSupremeCycleResult:
        """
        ğŸš€ äººé¡å²ä¸Šæœ€å¼·ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨ - ãƒ¡ã‚¤ãƒ³è¨ˆç®—å‡¦ç†
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash in self._cache and self._result is not None:
                return self._result

            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å–å¾—
            if isinstance(data, np.ndarray) and data.ndim == 1:
                src_prices = data.astype(np.float64)
            else:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                src_prices = src_prices.astype(np.float64)

            data_length = len(src_prices)
            if data_length < 50:
                self.logger.warning("ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æœ€å°50æœŸé–“å¿…è¦ã§ã™ã€‚")
                return self._create_empty_result(data_length)

            self.logger.info("ğŸš€ Ultimate Supreme Cycle Detector - è¨ˆç®—é–‹å§‹")

            # ===== Stage 1: å¤šæ¬¡å…ƒä¿¡å·å‰å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ =====
            self.logger.debug("Stage 1: é‡å­å¼·åŒ–ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œä¸­...")
            amplitude, phase, frequency, quantum_coherence = quantum_enhanced_hilbert_transform(src_prices)
            
            self.logger.debug("Stage 1: ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆé©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå‰æ–¹ãƒ‘ã‚¹ãƒ»ä½é…å»¶ç‰ˆï¼‰å®Ÿè¡Œä¸­...")
            if ultimate_adaptive_kalman_forward_numba is not None:
                # ä½é…å»¶ãƒ»ãƒãƒ©ãƒ³ã‚¹è¨­å®šã§ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆé©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå‰æ–¹ãƒ‘ã‚¹ã®ã¿ï¼‰ã‚’ä½¿ç”¨
                (ensemble_filtered, kalman_gains, prediction_errors, 
                 process_noise, observation_noise) = ultimate_adaptive_kalman_forward_numba(
                    src_prices,
                    base_process_noise=5e-4,        # ãƒã‚¤ã‚ºé™¤å»ã¨å¿œç­”æ€§ã®ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
                    base_observation_noise=0.007,   # è¦³æ¸¬ãƒã‚¤ã‚ºã‚’é©åº¦ã«è¨­å®š
                    volatility_window=4             # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’å¾®èª¿æ•´
                )
                # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ã¯æ—¢å­˜ã®ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›çµæœã‚’ä½¿ç”¨ï¼ˆè¿½åŠ è¨ˆç®—ãªã—ï¼‰
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬å®Ÿè£…
                self.logger.warning("ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆé©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬å®Ÿè£…ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                basic_filtered, unscented_filtered, quantum_filtered, ensemble_filtered = triple_kalman_filter_ensemble(
                    src_prices, amplitude, phase
                )
            
            self.logger.debug("Stage 1: ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦åˆ†æå®Ÿè¡Œä¸­...")
            trend_component, cycle_component, noise_component = wavelet_multiresolution_analysis(ensemble_filtered)

            # ===== Stage 2: é©æ–°çš„ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºã‚³ã‚¢ï¼ˆä½é…å»¶ç‰ˆï¼‰ =====
            self.logger.debug("Stage 2: ä½é…å»¶DFTã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè¡Œä¸­...")
            # ä½é…å»¶ãƒ»ãƒãƒ©ãƒ³ã‚¹å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
            dft_periods, dft_confidences, dft_coherences = ultra_advanced_dft_engine(
                ensemble_filtered, 
                window_size=45,      # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’å¾®èª¿æ•´ï¼ˆç²¾åº¦ã¨é€Ÿåº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
                overlap=0.78,        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¾®èª¿æ•´
                zero_padding_factor=5  # ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å¾®èª¿æ•´
            )
            
            self.logger.debug("Stage 2: é‡å­ã‚‚ã¤ã‚Œè‡ªå·±ç›¸é–¢åˆ†æå®Ÿè¡Œä¸­...")
            entangled_cycles, entanglement_strength = quantum_entangled_correlation(
                ensemble_filtered, dft_periods, quantum_coherence
            )

            # ===== Stage 3: æ´—ç·´ã•ã‚ŒãŸé‡å­é©å¿œçµ±åˆã‚·ã‚¹ãƒ†ãƒ  =====
            self.logger.debug("Stage 3: æ´—ç·´ã•ã‚ŒãŸé‡å­é©å¿œçµ±åˆã‚¨ãƒ³ã‚¸ãƒ³å®Ÿè¡Œä¸­...")
            (final_cycles, cycle_strength, cycle_confidence, adaptation_speed, 
             tracking_accuracy, topology_indicator, chaos_indicator) = refined_quantum_adaptive_engine(
                dft_periods, dft_confidences, entangled_cycles, entanglement_strength,
                cycle_component, quantum_coherence, src_prices, 
                self.adaptivity_factor, self.tracking_sensitivity
            )

            # ===== è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®— =====
            # ãƒ¬ã‚¸ãƒ¼ãƒ ãƒ»çŠ¶æ…‹æƒ…å ±
            market_regime = np.zeros(data_length)
            volatility_regime = np.zeros(data_length)
            cycle_regime = np.zeros(data_length)
            
            for i in range(data_length):
                market_regime[i] = detect_market_regime(src_prices, i)
                volatility_regime[i] = calculate_garch_volatility(src_prices, i)
                if final_cycles[i] < 20:
                    cycle_regime[i] = 0  # çŸ­æœŸã‚µã‚¤ã‚¯ãƒ«
                elif final_cycles[i] < 50:
                    cycle_regime[i] = 1  # ä¸­æœŸã‚µã‚¤ã‚¯ãƒ«
                else:
                    cycle_regime[i] = 2  # é•·æœŸã‚µã‚¤ã‚¯ãƒ«

            # ãƒã‚¤ã‚ºé™¤å»ç‡è¨ˆç®—
            raw_volatility = np.std(src_prices)
            filtered_volatility = np.std(ensemble_filtered)
            noise_rejection_ratio = np.full(data_length, 
                                          (raw_volatility - filtered_volatility) / raw_volatility 
                                          if raw_volatility > 0 else 0.0)

            # çµ±è¨ˆãƒ»ä¿¡é ¼æ€§æƒ…å ±
            statistical_significance = cycle_confidence * 0.8 + dft_confidences * 0.2
            prediction_accuracy = tracking_accuracy
            stability_score = 1.0 - adaptation_speed / np.max(adaptation_speed + 1e-10)

            # ç¾åœ¨çŠ¶æ…‹
            current_cycle = final_cycles[-1] if len(final_cycles) > 0 else 20.0
            current_strength = cycle_strength[-1] if len(cycle_strength) > 0 else 0.0
            current_confidence = cycle_confidence[-1] if len(cycle_confidence) > 0 else 0.0

            # çµæœä½œæˆ
            result = UltimateSupremeCycleResult(
                dominant_cycle=final_cycles,
                cycle_strength=cycle_strength,
                cycle_phase=phase,
                cycle_confidence=cycle_confidence,
                adaptation_speed=adaptation_speed,
                tracking_accuracy=tracking_accuracy,
                noise_rejection_ratio=noise_rejection_ratio,
                quantum_coherence=quantum_coherence,
                topology_indicator=topology_indicator,
                chaos_indicator=chaos_indicator,
                information_content=dft_confidences,
                market_regime=market_regime,
                volatility_regime=volatility_regime,
                cycle_regime=cycle_regime,
                statistical_significance=statistical_significance,
                prediction_accuracy=prediction_accuracy,
                stability_score=stability_score,
                current_cycle=current_cycle,
                current_strength=current_strength,
                current_confidence=current_confidence
            )

            self._result = result
            self._cache[data_hash] = self._result
            
            self.logger.info(f"âœ… Ultimate Supreme Cycle Detector è¨ˆç®—å®Œäº† - ç¾åœ¨ã‚µã‚¤ã‚¯ãƒ«: {current_cycle:.1f}æœŸé–“")
            return self._result

        except Exception as e:
            import traceback
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            return self._create_empty_result(len(data) if hasattr(data, '__len__') else 0)

    def _create_empty_result(self, length: int) -> UltimateSupremeCycleResult:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        empty_array = np.full(length, np.nan, dtype=np.float64)
        return UltimateSupremeCycleResult(
            dominant_cycle=np.full(length, 20.0, dtype=np.float64),
            cycle_strength=np.zeros(length, dtype=np.float64),
            cycle_phase=empty_array.copy(),
            cycle_confidence=np.zeros(length, dtype=np.float64),
            adaptation_speed=np.zeros(length, dtype=np.float64),
            tracking_accuracy=np.zeros(length, dtype=np.float64),
            noise_rejection_ratio=np.zeros(length, dtype=np.float64),
            quantum_coherence=np.zeros(length, dtype=np.float64),
            topology_indicator=np.zeros(length, dtype=np.float64),
            chaos_indicator=np.zeros(length, dtype=np.float64),
            information_content=np.zeros(length, dtype=np.float64),
            market_regime=np.zeros(length, dtype=np.float64),
            volatility_regime=np.zeros(length, dtype=np.float64),
            cycle_regime=np.zeros(length, dtype=np.float64),
            statistical_significance=np.zeros(length, dtype=np.float64),
            prediction_accuracy=np.zeros(length, dtype=np.float64),
            stability_score=np.zeros(length, dtype=np.float64),
            current_cycle=20.0,
            current_strength=0.0,
            current_confidence=0.0
        )

    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        if isinstance(data, pd.DataFrame):
            data_hash = hash(data.values.tobytes())
        elif isinstance(data, np.ndarray):
            data_hash = hash(data.tobytes())
        else:
            data_hash = hash(str(data))
        
        param_str = f"{self.period_range}_{self.adaptivity_factor}_{self.tracking_sensitivity}_{self.src_type}"
        return f"{data_hash}_{param_str}"

    def reset(self) -> None:
        """çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._result = None
        self._cache = {}

    def get_dominant_cycle(self) -> Optional[np.ndarray]:
        """æ”¯é…çš„ã‚µã‚¤ã‚¯ãƒ«æœŸé–“ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.dominant_cycle.copy()
        return None

    def get_cycle_strength(self) -> Optional[np.ndarray]:
        """ã‚µã‚¤ã‚¯ãƒ«å¼·åº¦ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.cycle_strength.copy()
        return None

    def get_quantum_coherence(self) -> Optional[np.ndarray]:
        """é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.quantum_coherence.copy()
        return None

    def get_current_state(self) -> dict:
        """ç¾åœ¨çŠ¶æ…‹ã‚’å–å¾—"""
        if self._result is not None:
            return {
                'current_cycle': self._result.current_cycle,
                'current_strength': self._result.current_strength,
                'current_confidence': self._result.current_confidence
            }
        return {'current_cycle': 20.0, 'current_strength': 0.0, 'current_confidence': 0.0}

    def get_performance_metrics(self) -> dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""
        if self._result is not None:
            return {
                'avg_adaptation_speed': np.nanmean(self._result.adaptation_speed),
                'avg_tracking_accuracy': np.nanmean(self._result.tracking_accuracy), 
                'avg_noise_rejection': np.nanmean(self._result.noise_rejection_ratio),
                'avg_stability_score': np.nanmean(self._result.stability_score),
                'avg_confidence': np.nanmean(self._result.cycle_confidence)
            }
        return {}

    def get_result(self) -> Optional[UltimateSupremeCycleResult]:
        """çµæœå…¨ä½“ã‚’å–å¾—"""
        return self._result

# ================== Stage 1: å¤šæ¬¡å…ƒä¿¡å·å‰å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ ==================

@njit(fastmath=True, cache=True)
def quantum_enhanced_hilbert_transform(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸŒ€ é‡å­å¼·åŒ–ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚·ã‚¹ãƒ†ãƒ 
    ç¬æ™‚æŒ¯å¹…ãƒ»ä½ç›¸ãƒ»å‘¨æ³¢æ•°ãƒ»é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ã®åŒæ™‚è¨ˆç®—
    """
    n = len(prices)
    amplitude = np.zeros(n)
    phase = np.zeros(n)
    frequency = np.zeros(n)
    quantum_coherence = np.zeros(n)
    
    if n < 16:
        return amplitude, phase, frequency, quantum_coherence
    
    # æ”¹è‰¯ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ› - ã‚ˆã‚Šé«˜ç²¾åº¦ãªè¨ˆç®—
    for i in range(8, n-8):
        real_part = prices[i]
        
        # 9ç‚¹ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ï¼ˆã‚ˆã‚Šé«˜ç²¾åº¦ï¼‰
        imag_part = (
            (prices[i-7] - prices[i+7]) +
            3 * (prices[i-5] - prices[i+5]) +
            5 * (prices[i-3] - prices[i+3]) +
            7 * (prices[i-1] - prices[i+1])
        ) / 32.0
        
        # ç¬æ™‚æŒ¯å¹…
        amplitude[i] = np.sqrt(real_part**2 + imag_part**2)
        
        # ç¬æ™‚ä½ç›¸
        if real_part != 0:
            phase[i] = np.arctan2(imag_part, real_part)
        
        # ç¬æ™‚å‘¨æ³¢æ•°ï¼ˆä½ç›¸ã®æ™‚é–“å¾®åˆ†ï¼‰
        if i > 8:
            phase_diff = phase[i] - phase[i-1]
            # ä½ç›¸ã®ãƒ©ãƒƒãƒ”ãƒ³ã‚°å‡¦ç†
            if phase_diff > np.pi:
                phase_diff -= 2 * np.pi
            elif phase_diff < -np.pi:
                phase_diff += 2 * np.pi
            frequency[i] = phase_diff
        
        # ğŸ”¬ é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹è¨ˆç®— - ä½ç›¸ã®å®‰å®šæ€§ã‚’æ¸¬å®š
        if i >= 16:
            # éå»8ç‚¹ã§ã®ä½ç›¸å®‰å®šæ€§
            phase_variance = 0.0
            for j in range(8):
                phase_variance += (phase[i-j] - phase[i-j-1])**2
            phase_variance /= 8.0
            
            # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ = 1 / (1 + phase_variance)
            quantum_coherence[i] = 1.0 / (1.0 + phase_variance)
    
    # å¢ƒç•Œå€¤ã®å‡¦ç†
    for i in range(8):
        amplitude[i] = amplitude[8]
        phase[i] = phase[8]
        frequency[i] = frequency[8]
        quantum_coherence[i] = quantum_coherence[8]
    for i in range(n-8, n):
        amplitude[i] = amplitude[n-9]
        phase[i] = phase[n-9]
        frequency[i] = frequency[n-9]
        quantum_coherence[i] = quantum_coherence[n-9]
    
    return amplitude, phase, frequency, quantum_coherence


@njit(fastmath=True, cache=True)
def triple_kalman_filter_ensemble(
    prices: np.ndarray,
    amplitude: np.ndarray,
    phase: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    âš¡ ä¸‰é‡ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ + ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ + é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ã®çµ±åˆ
    """
    n = len(prices)
    
    # åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    basic_filtered = adaptive_kalman_basic(prices)
    
    # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    unscented_filtered = unscented_kalman_filter(prices, amplitude)
    
    # é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    quantum_filtered = quantum_adaptive_kalman(prices, phase, amplitude)
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆï¼ˆå‹•çš„é‡ã¿ä»˜ã‘ï¼‰
    ensemble_filtered = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    for i in range(n):
        # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ä¿¡é ¼åº¦è¨ˆç®—
        basic_conf = 1.0 / (1.0 + abs(basic_filtered[i] - prices[i]))
        unscented_conf = 1.0 / (1.0 + abs(unscented_filtered[i] - prices[i]))
        quantum_conf = amplitude[i] * 0.5 + 0.5  # æŒ¯å¹…ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        
        # é‡ã¿æ­£è¦åŒ–
        total_conf = basic_conf + unscented_conf + quantum_conf
        if total_conf > 0:
            w1 = basic_conf / total_conf
            w2 = unscented_conf / total_conf
            w3 = quantum_conf / total_conf
        else:
            w1 = w2 = w3 = 1.0/3.0
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ
        ensemble_filtered[i] = (w1 * basic_filtered[i] +
                               w2 * unscented_filtered[i] +
                               w3 * quantum_filtered[i])
        
        confidence_scores[i] = total_conf / 3.0
    
    return basic_filtered, unscented_filtered, quantum_filtered, ensemble_filtered


@njit(fastmath=True, cache=True)
def adaptive_kalman_basic(prices: np.ndarray, base_process_noise: float = 1e-5) -> np.ndarray:
    """åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"""
    n = len(prices)
    filtered = np.zeros(n)
    
    if n < 2:
        return prices.copy()
    
    filtered[0] = prices[0]
    x_est = prices[0]
    p_est = 1.0
    
    for i in range(1, n):
        # é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š
        if i >= 10:
            recent_vol = np.std(prices[i-10:i])
            measurement_variance = max(0.001, min(0.1, recent_vol * 0.1))
        else:
            measurement_variance = 0.01
        
        # äºˆæ¸¬
        x_pred = x_est
        p_pred = p_est + base_process_noise
        
        # æ›´æ–°
        kalman_gain = p_pred / (p_pred + measurement_variance)
        x_est = x_pred + kalman_gain * (prices[i] - x_pred)
        p_est = (1 - kalman_gain) * p_pred
        
        filtered[i] = x_est
    
    return filtered


@njit(fastmath=True, cache=True)
def unscented_kalman_filter(prices: np.ndarray, amplitude: np.ndarray) -> np.ndarray:
    """ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    n = len(prices)
    filtered = np.zeros(n)
    
    alpha = 0.001
    beta = 2.0
    kappa = 0.0
    
    if n < 2:
        return prices.copy()
    
    filtered[0] = prices[0]
    x = prices[0]
    P = 1.0
    
    for i in range(1, n):
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆï¼ˆ1æ¬¡å…ƒç°¡æ˜“ç‰ˆï¼‰
        sigma_points = np.array([x, x + np.sqrt(P), x - np.sqrt(P)])
        weights = np.array([1.0 - 1.0/3.0, 1.0/6.0, 1.0/6.0])
        
        # äºˆæ¸¬
        x_pred = np.sum(sigma_points * weights)
        P_pred = P + 0.001  # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
        
        # æ›´æ–°ï¼ˆè¦³æ¸¬ãƒã‚¤ã‚ºã‚’æŒ¯å¹…ã§èª¿æ•´ï¼‰
        measurement_noise = max(0.001, amplitude[i] * 0.01)
        K = P_pred / (P_pred + measurement_noise)
        x = x_pred + K * (prices[i] - x_pred)
        P = (1 - K) * P_pred
        
        filtered[i] = x
    
    return filtered


@njit(fastmath=True, cache=True)
def quantum_adaptive_kalman(prices: np.ndarray, phase: np.ndarray, amplitude: np.ndarray) -> np.ndarray:
    """é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"""
    n = len(prices)
    filtered = np.zeros(n)
    
    if n < 2:
        return prices.copy()
    
    filtered[0] = prices[0]
    x_est = prices[0]
    p_est = 1.0
    
    for i in range(1, n):
        # é‡å­çš„ä¸ç¢ºå®šæ€§åŸç†ã®é©ç”¨
        quantum_uncertainty = amplitude[i] * abs(np.sin(phase[i]))
        
        # é‡å­åŠ¹æœã‚’è€ƒæ…®ã—ãŸãƒã‚¤ã‚ºèª¿æ•´
        process_noise = 1e-5 * (1.0 + quantum_uncertainty)
        measurement_noise = 0.01 * (1.0 + quantum_uncertainty * 0.5)
        
        # äºˆæ¸¬
        x_pred = x_est
        p_pred = p_est + process_noise
        
        # æ›´æ–°
        kalman_gain = p_pred / (p_pred + measurement_noise)
        x_est = x_pred + kalman_gain * (prices[i] - x_pred)
        p_est = (1 - kalman_gain) * p_pred
        
        filtered[i] = x_est
    
    return filtered


@njit(fastmath=True, cache=True)
def wavelet_multiresolution_analysis(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸŒŠ è¶…é«˜åº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦åˆ†æ
    6å±¤ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåˆ†è§£ã«ã‚ˆã‚‹å®Œå…¨å‘¨æ³¢æ•°åˆ†é›¢
    """
    n = len(prices)
    trend_component = np.zeros(n)
    cycle_component = np.zeros(n)
    noise_component = np.zeros(n)
    
    # ç°¡æ˜“Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåˆ†è§£ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰
    signal = prices.copy()
    
    # ãƒ¬ãƒ™ãƒ«1-6åˆ†è§£
    for level in range(6):
        if len(signal) < 4:
            break
            
        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        downsampled = np.zeros(len(signal)//2)
        detail = np.zeros(len(signal)//2)
        
        for i in range(len(downsampled)):
            if 2*i+1 < len(signal):
                downsampled[i] = (signal[2*i] + signal[2*i+1]) / 2.0
                detail[i] = (signal[2*i] - signal[2*i+1]) / 2.0
            else:
                downsampled[i] = signal[2*i]
                detail[i] = 0.0
        
        # æˆåˆ†åˆ†é¡
        if level < 2:  # é«˜å‘¨æ³¢æˆåˆ† -> ãƒã‚¤ã‚º
            # ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãƒã‚¤ã‚ºæˆåˆ†ã«è¿½åŠ 
            upsampled_detail = np.zeros(n)
            scale_factor = 2**level
            for i in range(len(detail)):
                for j in range(scale_factor):
                    idx = i * scale_factor + j
                    if idx < n:
                        upsampled_detail[idx] += detail[i]
            noise_component += upsampled_detail[:n]
            
        elif level < 4:  # ä¸­å‘¨æ³¢æˆåˆ† -> ã‚µã‚¤ã‚¯ãƒ«
            upsampled_detail = np.zeros(n)
            scale_factor = 2**level
            for i in range(len(detail)):
                for j in range(scale_factor):
                    idx = i * scale_factor + j
                    if idx < n:
                        upsampled_detail[idx] += detail[i]
            cycle_component += upsampled_detail[:n]
            
        signal = downsampled
    
    # æ®‹ã£ãŸä½å‘¨æ³¢æˆåˆ†ã‚’ãƒˆãƒ¬ãƒ³ãƒ‰ã«
    if len(signal) > 0:
        upsampled_trend = np.zeros(n)
        scale_factor = n // len(signal)
        for i in range(len(signal)):
            for j in range(scale_factor):
                idx = i * scale_factor + j
                if idx < n:
                    upsampled_trend[idx] = signal[i]
        trend_component = upsampled_trend
    
    return trend_component, cycle_component, noise_component 

# ================== Stage 2: é©æ–°çš„ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºã‚³ã‚¢ ==================

@njit(fastmath=True, cache=True)
def ultra_advanced_dft_engine(
    prices: np.ndarray,
    window_size: int = 100,
    overlap: float = 0.95,
    zero_padding_factor: int = 16
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸš€ è¶…é€²åŒ–DFTã‚¨ãƒ³ã‚¸ãƒ³
    16å€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° + 95%é‡è¤‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ + Kaiser-Bessel & Blackman-Harrisè¤‡åˆçª“é–¢æ•°
    """
    n = len(prices)
    periods = np.zeros(n)
    confidences = np.zeros(n)
    coherences = np.zeros(n)
    
    # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
    if n < 10 or window_size < 5:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åŸ‹ã‚ã‚‹ï¼ˆä¸­æœŸã‚µã‚¤ã‚¯ãƒ«ï¼‰
        periods[:] = 50.0
        return periods, confidences, coherences
    
    if n < window_size:
        window_size = max(5, n // 2)
    
    # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯: ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—åˆ¶é™
    overlap = max(0.0, min(0.99, overlap))
    
    # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º
    step_size = max(1, int(window_size * (1.0 - overlap)))
    
    for i in range(window_size, n, step_size):
        start_idx = max(0, i - window_size)
        end_idx = min(n, i)
        window_data = prices[start_idx:end_idx]
        
        if len(window_data) < window_size // 2:
            continue
        
        # ğŸ”§ è¤‡åˆçª“é–¢æ•°ã®é©ç”¨
        windowed_data = apply_composite_window(window_data)
        
        # ğŸ”§ 16å€ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        padded_length = len(windowed_data) * zero_padding_factor
        padded_data = np.zeros(padded_length)
        padded_data[:len(windowed_data)] = windowed_data
        
        # ğŸ”§ DFTè¨ˆç®—
        frequencies, power_spectrum = compute_power_spectrum(padded_data)
        
        # ğŸ”§ æ”¯é…çš„ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º
        dominant_period, confidence, coherence = extract_dominant_cycle(
            frequencies, power_spectrum, len(windowed_data)
        )
        
        # çµæœã®é©ç”¨ï¼ˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…ã®å…¨ãƒã‚¤ãƒ³ãƒˆã«ï¼‰
        for j in range(start_idx, end_idx):
            periods[j] = dominant_period
            confidences[j] = confidence
            coherences[j] = coherence
    
    return periods, confidences, coherences


@njit(fastmath=True, cache=True)
def modified_bessel_i0(x):
    """
    ğŸ¯ ä¿®æ­£ãƒ™ãƒƒã‚»ãƒ«é–¢æ•° I0 ã®é«˜ç²¾åº¦è¿‘ä¼¼ (Numbaäº’æ›)
    Abramowitz and Stegun approximation
    """
    if x < 0:
        x = -x
    
    if x < 3.75:
        # å°ã•ãªå€¤ç”¨ã®å¤šé …å¼è¿‘ä¼¼
        t = x / 3.75
        t2 = t * t
        return 1.0 + 3.5156229 * t2 + 3.0899424 * t2 * t2 + \
               1.2067492 * t2 * t2 * t2 + 0.2659732 * t2 * t2 * t2 * t2 + \
               0.0360768 * t2 * t2 * t2 * t2 * t2 + 0.0045813 * t2 * t2 * t2 * t2 * t2 * t2
    else:
        # å¤§ããªå€¤ç”¨ã®æ¼¸è¿‘è¿‘ä¼¼
        t = 3.75 / x
        result = (np.exp(x) / np.sqrt(x)) * (0.39894228 + 0.01328592 * t + \
                 0.00225319 * t * t - 0.00157565 * t * t * t + \
                 0.00916281 * t * t * t * t - 0.02057706 * t * t * t * t * t + \
                 0.02635537 * t * t * t * t * t * t - 0.01647633 * t * t * t * t * t * t * t + \
                 0.00392377 * t * t * t * t * t * t * t * t)
        return result

@njit(fastmath=True, cache=True)
def apply_composite_window(data: np.ndarray) -> np.ndarray:
    """Kaiser-Bessel & Blackman-Harrisè¤‡åˆçª“é–¢æ•°"""
    n = len(data)
    window = np.ones(n)
    
    # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
    if n <= 1:
        return data
    
    # Kaiser-Besselçª“ï¼ˆÎ²=8.6ï¼‰
    beta = 8.6
    i0_beta = modified_bessel_i0(beta)
    
    # ã‚¼ãƒ­é™¤ç®—ã‚’é˜²ããŸã‚ã®ãƒã‚§ãƒƒã‚¯
    if i0_beta == 0.0:
        i0_beta = 1e-10
    
    for i in range(n):
        x = 2.0 * i / (n - 1) - 1.0
        arg = beta * np.sqrt(max(0.0, 1 - x**2))  # è² ã®å€¤ã‚’é˜²ã
        window[i] *= modified_bessel_i0(arg) / i0_beta
    
    # Blackman-Harrisçª“ã¨ã®è¤‡åˆ
    for i in range(n):
        t = 2.0 * np.pi * i / (n - 1)
        blackman_harris = (0.35875 - 
                          0.48829 * np.cos(t) + 
                          0.14128 * np.cos(2*t) - 
                          0.01168 * np.cos(3*t))
        window[i] *= blackman_harris
    
    return data * window


@njit(fastmath=True, cache=True)
def compute_power_spectrum(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """ãƒ‘ãƒ¯ãƒ¼ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—ï¼ˆDFTï¼‰"""
    n = len(data)
    
    # å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
    if n < 2:
        return np.array([0.0]), np.array([0.0])
    
    half_n = n // 2
    if half_n < 1:
        half_n = 1
    
    frequencies = np.zeros(half_n)
    power_spectrum = np.zeros(half_n)
    
    # å‘¨æ³¢æ•°è»¸
    for k in range(half_n):
        frequencies[k] = k / n if n > 0 else 0.0
    
    # DFTè¨ˆç®—
    for k in range(half_n):
        real_part = 0.0
        imag_part = 0.0
        
        for i in range(n):
            angle = -2.0 * np.pi * k * i / n if n > 0 else 0.0
            real_part += data[i] * np.cos(angle)
            imag_part += data[i] * np.sin(angle)
        
        power_spectrum[k] = real_part**2 + imag_part**2
    
    return frequencies, power_spectrum


@njit(fastmath=True, cache=True)
def extract_dominant_cycle(
    frequencies: np.ndarray, 
    power_spectrum: np.ndarray, 
    data_length: int
) -> Tuple[float, float, float]:
    """æ”¯é…çš„ã‚µã‚¤ã‚¯ãƒ«æŠ½å‡º"""
    if len(power_spectrum) < 3:
        return 50.0, 0.0, 0.0  # ä¸­æœŸã‚µã‚¤ã‚¯ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # æœ‰åŠ¹ãªå‘¨æ³¢æ•°ç¯„å›²ï¼ˆ20-100æœŸé–“ã«å¯¾å¿œ - ä¸­æœŸã‚µã‚¤ã‚¯ãƒ«æœ€é©åŒ–ï¼‰
    min_freq = 1.0 / 100.0
    max_freq = 1.0 / 20.0
    
    max_power = 0.0
    dominant_freq = 0.0
    
    for i in range(1, len(frequencies)):
        if min_freq <= frequencies[i] <= max_freq:
            if power_spectrum[i] > max_power:
                max_power = power_spectrum[i]
                dominant_freq = frequencies[i]
    
    if dominant_freq > 0:
        dominant_period = 1.0 / dominant_freq
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        total_power = np.sum(power_spectrum[1:])
        confidence = max_power / total_power if total_power > 0 else 0.0
        
        # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹è¨ˆç®—ï¼ˆãƒ”ãƒ¼ã‚¯ã®é‹­ã•ï¼‰
        peak_idx = 0
        for i in range(len(frequencies)):
            if abs(frequencies[i] - dominant_freq) < 1e-6:
                peak_idx = i
                break
        
        if peak_idx > 0 and peak_idx < len(power_spectrum) - 1:
            coherence = power_spectrum[peak_idx] / (
                power_spectrum[peak_idx-1] + power_spectrum[peak_idx+1] + 1e-10
            )
        else:
            coherence = 1.0
    else:
        dominant_period = 20.0
        confidence = 0.0
        coherence = 0.0
    
    return dominant_period, confidence, coherence


@njit(fastmath=True, cache=True)
def quantum_entangled_correlation(
    prices: np.ndarray,
    periods: np.ndarray,
    quantum_coherence: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸ”¬ é‡å­ã‚‚ã¤ã‚Œè‡ªå·±ç›¸é–¢åˆ†æ
    å¤å…¸è‡ªå·±ç›¸é–¢ + é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    """
    n = len(prices)
    entangled_cycles = np.zeros(n)
    entanglement_strength = np.zeros(n)
    
    for i in range(20, n):
        # å¤å…¸çš„è‡ªå·±ç›¸é–¢
        period = max(3, min(int(periods[i]), 50))
        classical_corr = 0.0
        
        if i >= period:
            # è‡ªå·±ç›¸é–¢è¨ˆç®—
            sum_xy = 0.0
            sum_x = 0.0
            sum_y = 0.0
            sum_x2 = 0.0
            sum_y2 = 0.0
            
            for j in range(period):
                x = prices[i - j]
                y = prices[i - j - period]
                sum_xy += x * y
                sum_x += x
                sum_y += y
                sum_x2 += x * x
                sum_y2 += y * y
            
            mean_x = sum_x / period
            mean_y = sum_y / period
            
            numerator = sum_xy - period * mean_x * mean_y
            denominator = np.sqrt((sum_x2 - period * mean_x**2) * (sum_y2 - period * mean_y**2))
            
            if denominator > 1e-10:
                classical_corr = numerator / denominator
        
        # ğŸ”¬ é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœ
        coherence = quantum_coherence[i]
        
        # BellçŠ¶æ…‹çš„ç›¸é–¢ï¼ˆé‡å­ã‚‚ã¤ã‚Œã®è¿‘ä¼¼ï¼‰
        # |ÏˆâŸ© = (1/âˆš2)(|00âŸ© + |11âŸ©)ã®ç›¸é–¢æ§‹é€ ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–
        phase_factor = np.exp(1j * 2 * np.pi / period)
        quantum_factor = coherence * abs(phase_factor.real + phase_factor.imag)
        
        # é‡å­ã‚‚ã¤ã‚Œç›¸é–¢
        entangled_corr = classical_corr * (1.0 + quantum_factor * 0.414)  # âˆš2/2 â‰ˆ 0.414
        
        # æœ€çµ‚çµæœ
        entangled_cycles[i] = period * (1.0 + entangled_corr * 0.1)
        entanglement_strength[i] = abs(entangled_corr)
    
    # å¢ƒç•Œå€¤å‡¦ç†
    for i in range(20):
        entangled_cycles[i] = entangled_cycles[20]
        entanglement_strength[i] = entanglement_strength[20]
    
    return entangled_cycles, entanglement_strength 

# ================== Stage 3: æ´—ç·´ã•ã‚ŒãŸé‡å­é©å¿œçµ±åˆã‚·ã‚¹ãƒ†ãƒ  ==================

@njit(fastmath=True, cache=True)
def refined_quantum_adaptive_engine(
    dft_periods: np.ndarray,
    dft_confidences: np.ndarray,
    entangled_cycles: np.ndarray,
    entanglement_strength: np.ndarray,
    cycle_component: np.ndarray,
    quantum_coherence: np.ndarray,
    prices: np.ndarray,
    adaptivity_factor: float,
    tracking_sensitivity: float
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ”¬ æ´—ç·´ã•ã‚ŒãŸé‡å­é©å¿œçµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ - ã‚·ãƒ³ãƒ—ãƒ«ãƒ»é«˜ç²¾åº¦ãƒ»é«˜é€Ÿ
    
    è¤‡é›‘ãªæƒ…å ±ç†è«–ã‚’æ’é™¤ã—ã€é‡å­åŠ›å­¦ã®æœ¬è³ªçš„æ¦‚å¿µã®ã¿ã‚’ä½¿ç”¨ï¼š
    - é‡å­é‡ã­åˆã‚ã›ã«ã‚ˆã‚‹æœŸé–“çµ±åˆ
    - èª¿å’Œå¹³å‡ã«ã‚ˆã‚‹å¼·åº¦è¨ˆç®—  
    - å¹¾ä½•å¹³å‡ã«ã‚ˆã‚‹ä¿¡é ¼åº¦è©•ä¾¡
    - é©å¿œæ€§ãƒ»è¿½å¾“æ€§ãƒ»ãƒˆãƒãƒ­ã‚¸ãƒ¼ãƒ»ã‚«ã‚ªã‚¹ã®ç°¡ç´ çµ±åˆ
    """
    n = len(prices)
    
    # å‡ºåŠ›é…åˆ—åˆæœŸåŒ–
    final_cycles = np.zeros(n)
    cycle_strength = np.zeros(n)
    cycle_confidence = np.zeros(n)
    adaptation_speed = np.zeros(n)
    tracking_accuracy = np.zeros(n)
    topology_indicator = np.zeros(n)
    chaos_indicator = np.zeros(n)
    
    for i in range(10, n):
        # ===== 1. é‡å­é‡ã­åˆã‚ã›ã«ã‚ˆã‚‹æœŸé–“çµ±åˆ =====
        # DFTæœŸé–“ã¨é‡å­ã‚‚ã¤ã‚ŒæœŸé–“ã®é‡å­çš„é‡ã­åˆã‚ã›
        coherence_factor = quantum_coherence[i]
        
        # é‡å­é‡ã­åˆã‚ã›ä¿‚æ•°ï¼ˆä¸­æœŸã‚µã‚¤ã‚¯ãƒ«æœ€é©åŒ–ãƒ»ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ã«åŸºã¥ãé©å¿œé‡ã¿ï¼‰
        superposition_coeff = 0.6 + coherence_factor * 0.2  # DFTå¯„ã‚Šã«èª¿æ•´
        
        # é‡å­çš„é‡ã­åˆã‚ã›ã«ã‚ˆã‚‹æœŸé–“æ±ºå®š
        quantum_period = (dft_periods[i] * superposition_coeff + 
                         entangled_cycles[i] * (1.0 - superposition_coeff))
        
        # é©å¿œæ€§ã«ã‚ˆã‚‹å¾®èª¿æ•´ï¼ˆä¸­æœŸã‚µã‚¤ã‚¯ãƒ«åˆ¶ç´„ï¼‰
        adaptation_factor = 1.0 + (adaptivity_factor - 0.5) * coherence_factor * 0.1
        final_cycles[i] = quantum_period * adaptation_factor
        final_cycles[i] = max(20.0, min(100.0, final_cycles[i]))  # 20-100æœŸé–“ã«å¼·åˆ¶åˆ¶é™
        
        # ===== 2. é‡å­èª¿å’Œã«ã‚ˆã‚‹å¼·åº¦è¨ˆç®— =====
        # DFTä¿¡é ¼åº¦ã¨é‡å­ã‚‚ã¤ã‚Œå¼·åº¦ã®èª¿å’Œå¹³å‡
        harmonic_strength = (2 * dft_confidences[i] * entanglement_strength[i]) / \
                           (dft_confidences[i] + entanglement_strength[i] + 1e-10)
        
        # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ã«ã‚ˆã‚‹å¼·åŒ–
        cycle_strength[i] = harmonic_strength * (0.7 + coherence_factor * 0.3)
        
        # ===== 3. çµ±åˆä¿¡é ¼åº¦ï¼ˆå¹¾ä½•å¹³å‡ï¼‰ =====
        # 3ã¤ã®é‡å­ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹¾ä½•å¹³å‡
        geometric_confidence = (dft_confidences[i] * entanglement_strength[i] * 
                              quantum_coherence[i]) ** (1.0/3.0)
        cycle_confidence[i] = geometric_confidence
        
        # ===== 4. é©å¿œé€Ÿåº¦ï¼ˆå¸‚å ´å¤‰å‹•å¿œç­”æ€§ï¼‰ =====
        if i >= 10:
            recent_volatility = np.std(prices[i-10:i])
            base_volatility = np.std(prices[:max(20, i//2)])
            vol_ratio = min(5.0, max(0.2, recent_volatility / (base_volatility + 1e-10)))
            
            adaptation_speed[i] = adaptivity_factor * vol_ratio * coherence_factor
        else:
            adaptation_speed[i] = adaptivity_factor * 0.5
        
        # ===== 5. è¿½å¾“ç²¾åº¦ï¼ˆä½é…å»¶ãƒ»é«˜å¿œç­”æ€§ï¼‰ =====
        tracking_accuracy[i] = tracking_sensitivity * \
                              (coherence_factor * 0.6 + cycle_strength[i] * 0.4)
        
        # ===== 6. ã‚·ãƒ³ãƒ—ãƒ«åŒ–ãƒˆãƒãƒ­ã‚¸ãƒ¼æŒ‡æ¨™ =====
        # ã‚µã‚¤ã‚¯ãƒ«æœŸé–“ã®å®‰å®šæ€§ã«ã‚ˆã‚‹ä½ç›¸ç©ºé–“æ§‹é€ è©•ä¾¡
        if i >= 5:
            recent_cycles = final_cycles[max(0, i-5):i]
            if len(recent_cycles) > 0:
                cycle_stability = 1.0 / (1.0 + np.std(recent_cycles))
                topology_indicator[i] = cycle_stability * coherence_factor
            else:
                topology_indicator[i] = coherence_factor * 0.5
        else:
            topology_indicator[i] = coherence_factor * 0.5
        
        # ===== 7. ç°¡ç´ åŒ–ã‚«ã‚ªã‚¹æŒ‡æ¨™ =====
        # ä¾¡æ ¼å‹•çš„ç‰¹æ€§ã®äºˆæ¸¬å¯èƒ½æ€§è©•ä¾¡
        if i >= 10:
            # çŸ­æœŸãƒ»é•·æœŸä¾¡æ ¼å¤‰åŒ–ã®è¦å‰‡æ€§
            short_segment = prices[i-5:i]
            long_segment = prices[i-10:i-5]
            
            if len(short_segment) > 0 and len(long_segment) > 0:
                short_changes = np.diff(short_segment)
                long_changes = np.diff(long_segment)
                
                # ä¾¡æ ¼å¤‰åŒ–ã®ç›¸é–¢ã«ã‚ˆã‚‹è¦å‰‡æ€§æ¸¬å®š
                if len(short_changes) > 0 and len(long_changes) > 0:
                    short_var = np.var(short_changes) + 1e-10
                    long_var = np.var(long_changes) + 1e-10
                    regularity_ratio = min(short_var, long_var) / max(short_var, long_var)
                    chaos_indicator[i] = regularity_ratio * coherence_factor
                else:
                    chaos_indicator[i] = coherence_factor * 0.5
            else:
                chaos_indicator[i] = coherence_factor * 0.5
        else:
            chaos_indicator[i] = coherence_factor * 0.5
    
    # å¢ƒç•Œå€¤å‡¦ç†ï¼ˆä¸­æœŸã‚µã‚¤ã‚¯ãƒ«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    for i in range(10):
        final_cycles[i] = final_cycles[10] if n > 10 else 50.0  # ä¸­æœŸã‚µã‚¤ã‚¯ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        cycle_strength[i] = cycle_strength[10] if n > 10 else 0.5
        cycle_confidence[i] = cycle_confidence[10] if n > 10 else 0.5
        adaptation_speed[i] = adaptation_speed[10] if n > 10 else 0.5
        tracking_accuracy[i] = tracking_accuracy[10] if n > 10 else 0.5
        topology_indicator[i] = topology_indicator[10] if n > 10 else 0.5
        chaos_indicator[i] = chaos_indicator[10] if n > 10 else 0.5
    
    return (final_cycles, cycle_strength, cycle_confidence, adaptation_speed, 
            tracking_accuracy, topology_indicator, chaos_indicator)


# ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«åŒ–ã«ã‚ˆã‚Šå‰Šé™¤ã•ã‚ŒãŸè¤‡é›‘ãªé–¢æ•°ç¾¤ï¼š
# - calculate_multiscale_entropy: ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆè¤‡é›‘ã™ãã‚‹ï¼‰
# - calculate_mutual_information: ç›¸äº’æƒ…å ±é‡è¨ˆç®—ï¼ˆè¤‡é›‘ã™ãã‚‹ï¼‰  
# - detect_market_regime: éš ã‚Œãƒãƒ«ã‚³ãƒ•ãƒ¢ãƒ‡ãƒ«ï¼ˆè¤‡é›‘ã™ãã‚‹ï¼‰
# - calculate_garch_volatility: GARCHå‹ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡é›‘ã™ãã‚‹ï¼‰
# - calculate_chaos_indicator: ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™ï¼ˆè¤‡é›‘ã™ãã‚‹ï¼‰
# - estimate_attractor_dimension: ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒæ¨å®šï¼ˆè¤‡é›‘ã™ãã‚‹ï¼‰
# 
# æ´—ç·´ã•ã‚ŒãŸé‡å­é©å¿œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã¯ã€ã“ã‚Œã‚‰ã®è¤‡é›‘ãªè¦ç´ ã‚’æ’é™¤ã—ã€
# é‡å­åŠ›å­¦ã®æœ¬è³ªçš„æ¦‚å¿µï¼ˆé‡ã­åˆã‚ã›ãƒ»èª¿å’Œãƒ»å¹¾ä½•å¹³å‡ï¼‰ã®ã¿ã‚’ä½¿ç”¨


@njit(fastmath=True, cache=True)
def detect_market_regime(prices: np.ndarray, idx: int, window: int = 20) -> float:
    """å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºï¼ˆç°¡ç´ ç‰ˆï¼‰"""
    start_idx = max(0, idx - window)
    data = prices[start_idx:idx+1]
    
    if len(data) < 5:
        return 0.5
    
    # ä¾¡æ ¼å¤‰åŒ–ç‡è¨ˆç®—
    returns = np.zeros(len(data) - 1)
    for i in range(len(data) - 1):
        returns[i] = (data[i+1] - data[i]) / data[i]
    
    # åˆ†æ•£è¨ˆç®—ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ä»£ç†ï¼‰
    mean_return = np.mean(returns)
    variance = np.mean((returns - mean_return)**2)
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ ç¢ºç‡ï¼ˆé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ vs ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
    # 0.5ã‚’ä¸­å¿ƒã¨ã—ãŸç¢ºç‡
    regime_prob = 1.0 / (1.0 + np.exp(-10.0 * (variance - 0.001)))
    
    return regime_prob


@njit(fastmath=True, cache=True)
def calculate_garch_volatility(prices: np.ndarray, idx: int, window: int = 20) -> float:
    """GARCHé¢¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—"""
    start_idx = max(0, idx - window)
    data = prices[start_idx:idx+1]
    
    if len(data) < 5:
        return 0.1
    
    # ç°¡æ˜“GARCH(1,1)é¢¨è¨ˆç®—
    returns = np.zeros(len(data) - 1)
    for i in range(len(data) - 1):
        returns[i] = (data[i+1] - data[i]) / data[i]
    
    # æ¡ä»¶ä»˜ãåˆ†æ•£ã®æ¨å®š
    variance = 0.0001  # åˆæœŸåˆ†æ•£
    alpha = 0.1
    beta = 0.85
    
    for i in range(len(returns)):
        variance = alpha * returns[i]**2 + beta * variance + 0.00001
    
    return np.sqrt(variance)


@njit(fastmath=True, cache=True)
def calculate_chaos_indicator(prices: np.ndarray, idx: int, window: int = 30) -> float:
    """ã‚«ã‚ªã‚¹æŒ‡æ¨™è¨ˆç®—"""
    start_idx = max(0, idx - window)
    data = prices[start_idx:idx+1]
    
    if len(data) < 15:
        return 0.5
    
    # LyapunovæŒ‡æ•°ã®ç°¡æ˜“æ¨å®š
    # ç›¸é–¢æ¬¡å…ƒã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    correlation_sum = 0.0
    count = 0
    
    for i in range(len(data) - 3):
        for j in range(i + 3, len(data)):
            distance = abs(data[i] - data[j])
            if distance < 0.01:  # é–¾å€¤
                correlation_sum += 1.0
            count += 1
    
    if count > 0:
        correlation_dimension = correlation_sum / count
        # ã‚«ã‚ªã‚¹åº¦ã®æŒ‡æ¨™åŒ–
        chaos_score = min(1.0, correlation_dimension * 5.0)
        return chaos_score
    
    return 0.5


@njit(fastmath=True, cache=True)
def estimate_attractor_dimension(prices: np.ndarray, idx: int, embedding_dim: int = 3) -> float:
    """ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼æ¬¡å…ƒæ¨å®š"""
    start_idx = max(0, idx - 30)
    data = prices[start_idx:idx+1]
    
    if len(data) < embedding_dim + 5:
        return 2.0
    
    # TakensåŸ‹ã‚è¾¼ã¿
    embedded_points = []
    for i in range(len(data) - embedding_dim):
        point = []
        for j in range(embedding_dim):
            point.append(data[i + j])
        embedded_points.append(point)
    
    # ç›¸é–¢æ¬¡å…ƒã®ç°¡æ˜“è¨ˆç®—
    if len(embedded_points) < 5:
        return 2.0
    
    # è¿‘æ¥ç‚¹ã®æ•°ã‚’æ•°ãˆã‚‹
    threshold = 0.01
    correlation_count = 0
    total_pairs = 0
    
    for i in range(len(embedded_points)):
        for j in range(i + 1, len(embedded_points)):
            distance = 0.0
            for k in range(embedding_dim):
                distance += (embedded_points[i][k] - embedded_points[j][k])**2
            distance = np.sqrt(distance)
            
            if distance < threshold:
                correlation_count += 1
            total_pairs += 1
    
    if total_pairs > 0:
        correlation_probability = correlation_count / total_pairs
        # ç›¸é–¢æ¬¡å…ƒã®æ¨å®š
        if correlation_probability > 1e-10:
            dimension = -np.log(correlation_probability) / np.log(threshold)
            return min(10.0, max(1.0, dimension))
    
    return 2.0 