#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Quantum-Fractal Adaptive Volatility Channel (QFAVC)
é‡å­ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒãƒ£ãƒãƒ«

é©æ–°çš„ãªå‹•çš„ãƒãƒ£ãƒãƒ«ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ - æ—¢å­˜ã®æ¦‚å¿µã‚’å®Œå…¨ã«è¶…è¶Šã—ãŸæ–°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

æ ¸ã¨ãªã‚‹é©æ–°è¦ç´ :
ğŸŒŒ é‡å­ã‚‚ã¤ã‚Œç†è«–ã«ã‚ˆã‚‹ä¾¡æ ¼ç›¸é–¢è§£æ
ğŸ”® ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã«ã‚ˆã‚‹market complexityè¨ˆæ¸¬
ğŸŒŠ æ¶²ä½“åŠ›å­¦ã«ã‚ˆã‚‹å¸‚å ´ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹åˆ¤å®š
ğŸ¯ é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚‹å‹•çš„ãƒã‚¤ã‚ºãƒ¢ãƒ‡ãƒªãƒ³ã‚°
ğŸ“Š GARCH-Hurstçµ±åˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
âš¡ è¶…ä½é…å»¶ãƒ»è¶…é«˜è¿½å¾“æ€§ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Optional, List, NamedTuple
import numpy as np
import pandas as pd
from numba import jit, njit, prange, vectorize, float64, int64, boolean
import math

from .indicator import Indicator
from .price_source import PriceSource
from .cycle.ehlers_unified_dc import EhlersUnifiedDC
from .kalman_filter import KalmanFilter


@dataclass
class QFAVCResult:
    """QFAVCè¨ˆç®—çµæœ"""
    # ãƒãƒ£ãƒãƒ«è¦ç´ 
    centerline: np.ndarray          # é‡å­é©å¿œã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³
    upper_channel: np.ndarray       # ä¸Šéƒ¨ãƒãƒ£ãƒãƒ«
    lower_channel: np.ndarray       # ä¸‹éƒ¨ãƒãƒ£ãƒãƒ«
    dynamic_width: np.ndarray       # å‹•çš„ãƒãƒ£ãƒãƒ«å¹…
    
    # é‡å­è§£ææˆåˆ†
    quantum_entanglement: np.ndarray    # é‡å­ã‚‚ã¤ã‚Œå¼·åº¦
    coherence_factor: np.ndarray        # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹å› å­
    superposition_state: np.ndarray     # é‡ã­åˆã‚ã›çŠ¶æ…‹
    
    # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è§£ææˆåˆ†
    fractal_dimension: np.ndarray       # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
    hurst_exponent: np.ndarray          # ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°
    complexity_index: np.ndarray        # è¤‡é›‘æ€§æŒ‡æ•°
    
    # æµä½“åŠ›å­¦æˆåˆ†
    flow_regime: np.ndarray             # ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ (å±¤æµ=1, ä¹±æµ=-1)
    reynolds_number: np.ndarray         # ãƒ¬ã‚¤ãƒãƒ«ã‚ºæ•°
    turbulence_intensity: np.ndarray    # ä¹±æµå¼·åº¦
    
    # é©å¿œãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æˆåˆ†
    garch_volatility: np.ndarray        # GARCHæ¨å®šãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
    adaptive_variance: np.ndarray       # é©å¿œåˆ†æ•£
    volatility_regime: np.ndarray       # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
    
    # çµ±åˆæŒ‡æ¨™
    market_phase: np.ndarray            # å¸‚å ´ãƒ•ã‚§ãƒ¼ã‚ºåˆ¤å®š
    adaptation_factor: np.ndarray       # é©å¿œå› å­
    confidence_score: np.ndarray        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢


# === é‡å­è§£æé–¢æ•°ç¾¤ ===

@njit(fastmath=True, parallel=True, cache=True)
def calculate_quantum_entanglement(prices: np.ndarray, window: int = 21) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    é‡å­ã‚‚ã¤ã‚Œç†è«–ã«ã‚ˆã‚‹ä¾¡æ ¼ç›¸é–¢è§£æ
    
    ä¾¡æ ¼ç³»åˆ—é–“ã®éç·šå½¢ç›¸é–¢ã‚’é‡å­ã‚‚ã¤ã‚ŒçŠ¶æ…‹ã¨ã—ã¦è§£é‡ˆã—ã€
    å¸‚å ´ã®éš ã‚ŒãŸç›¸é–¢æ§‹é€ ã‚’æ¤œå‡ºã™ã‚‹é©æ–°çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    """
    n = len(prices)
    entanglement = np.full(n, np.nan)
    coherence = np.full(n, np.nan)
    superposition = np.full(n, np.nan)
    
    for i in prange(window, n):
        # å±€æ‰€ä¾¡æ ¼ãƒ™ã‚¯ãƒˆãƒ«ã®æŠ½å‡º
        local_prices = prices[i-window:i]
        
        # é‡å­çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®æ§‹æˆ
        returns = np.diff(local_prices)
        if len(returns) == 0:
            continue
            
        # é‡å­ã‚‚ã¤ã‚Œã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
        normalized_returns = returns / (np.std(returns) + 1e-10)
        
        # ãƒ™ãƒ«çŠ¶æ…‹é¡ä¼¼åº¦ã®è¨ˆç®—
        bell_correlation = 0.0
        coherence_sum = 0.0
        
        for j in range(len(normalized_returns) - 1):
            for k in range(j + 1, len(normalized_returns)):
                # é‡å­ã‚‚ã¤ã‚Œæ¸¬åº¦
                correlation = normalized_returns[j] * normalized_returns[k]
                bell_correlation += math.exp(-abs(correlation))
                
                # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ¸¬åº¦
                phase_diff = math.atan2(normalized_returns[k], normalized_returns[j])
                coherence_sum += math.cos(phase_diff)
        
        # æ­£è¦åŒ–
        pairs_count = len(normalized_returns) * (len(normalized_returns) - 1) / 2
        if pairs_count > 0:
            entanglement[i] = bell_correlation / pairs_count
            coherence[i] = abs(coherence_sum) / pairs_count
        
        # é‡ã­åˆã‚ã›çŠ¶æ…‹ã®è¨ˆç®—
        price_momentum = (prices[i] - prices[i-window//2]) / (prices[i-window//2] + 1e-10)
        superposition[i] = math.tanh(price_momentum) * coherence[i]
    
    return entanglement, coherence, superposition


@njit(fastmath=True, parallel=True, cache=True)
def calculate_fractal_dimension(prices: np.ndarray, window: int = 34) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã¨ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ã«ã‚ˆã‚‹è¤‡é›‘æ€§è§£æ
    
    å¸‚å ´ã®è‡ªå·±ç›¸ä¼¼æ€§ã¨ãƒˆãƒ¬ãƒ³ãƒ‰æŒç¶šæ€§ã‚’å®šé‡åŒ–ã—ã€
    price actionã®æœ¬è³ªçš„ãªè¤‡é›‘ã•ã‚’æ¸¬å®š
    """
    n = len(prices)
    fractal_dim = np.full(n, np.nan)
    hurst_exp = np.full(n, np.nan)
    complexity = np.full(n, np.nan)
    
    for i in prange(window, n):
        local_prices = prices[i-window:i]
        
        # Box-countingæ³•ã«ã‚ˆã‚‹ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
        price_range = np.max(local_prices) - np.min(local_prices)
        if price_range == 0:
            continue
            
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£æ
        scales = np.array([2, 4, 8, 16])
        log_scales = np.log(scales)
        log_variations = np.zeros(len(scales))
        
        for j, scale in enumerate(scales):
            if scale >= len(local_prices):
                continue
                
            # å„ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å¤‰å‹•è¨ˆç®—
            variations = 0.0
            count = 0
            for k in range(0, len(local_prices) - scale, scale):
                segment = local_prices[k:k+scale]
                variations += np.max(segment) - np.min(segment)
                count += 1
            
            if count > 0:
                log_variations[j] = math.log(variations / count + 1e-10)
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒæ¨å®š (ç·šå½¢å›å¸°ã®å‚¾ã)ï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
        std_scales = np.std(log_scales)
        std_variations = np.std(log_variations)
        if std_scales > 1e-10 and std_variations > 1e-10:
            correlation = np.corrcoef(log_scales, log_variations)[0, 1]
            if not np.isnan(correlation):
                slope = correlation * std_variations / std_scales
                fractal_dim[i] = 2.0 - slope  # ç†è«–çš„ãªèª¿æ•´
            else:
                fractal_dim[i] = 1.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        else:
            fractal_dim[i] = 1.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ã®è¨ˆç®— (R/Sè§£æ)
        returns = np.diff(local_prices)
        if len(returns) > 1:
            mean_return = np.mean(returns)
            deviations = returns - mean_return
            cumulative_deviations = np.cumsum(deviations)
            
            R = np.max(cumulative_deviations) - np.min(cumulative_deviations)
            S = max(np.std(returns), 1e-10)
            
            if S > 1e-10 and R > 1e-10 and len(returns) > 1:
                log_rs = math.log(R/S)
                log_n = math.log(len(returns))
                if abs(log_n) > 1e-10:
                    hurst_exp[i] = log_rs / log_n
                else:
                    hurst_exp[i] = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            else:
                hurst_exp[i] = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # è¤‡é›‘æ€§æŒ‡æ•° (ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã¨ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ã®çµ±åˆ)
        if not np.isnan(fractal_dim[i]) and not np.isnan(hurst_exp[i]):
            complexity[i] = fractal_dim[i] * (1.0 - abs(hurst_exp[i] - 0.5))
    
    return fractal_dim, hurst_exp, complexity


@njit(fastmath=True, parallel=True, cache=True)
def calculate_fluid_dynamics(prices: np.ndarray, volume: np.ndarray, window: int = 21) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    æ¶²ä½“åŠ›å­¦ã«ã‚ˆã‚‹å¸‚å ´ãƒ•ãƒ­ãƒ¼è§£æ
    
    ä¾¡æ ¼ã®å‹•ãã‚’æµä½“ã®å‹•ãã¨ã—ã¦è§£é‡ˆã—ã€
    å±¤æµï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰ã¨ä¹±æµï¼ˆãƒ¬ãƒ³ã‚¸ï¼‰ã®çŠ¶æ…‹ã‚’åˆ¤å®š
    """
    n = len(prices)
    flow_regime = np.full(n, np.nan)
    reynolds_number = np.full(n, np.nan)
    turbulence = np.full(n, np.nan)
    
    for i in prange(window, n):
        local_prices = prices[i-window:i]
        local_volume = volume[i-window:i] if len(volume) > i else np.ones(window)
        
        # é€Ÿåº¦å ´ã®è¨ˆç®— (ä¾¡æ ¼å¤‰åŒ–ç‡)
        velocity = np.diff(local_prices)
        if len(velocity) == 0:
            continue
            
        # å¯†åº¦å ´ã®è¨ˆç®— (æ­£è¦åŒ–ãƒœãƒªãƒ¥ãƒ¼ãƒ )
        density = local_volume / (np.mean(local_volume) + 1e-10)
        
        # ãƒ¬ã‚¤ãƒãƒ«ã‚ºæ•°ã®è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–å¼·åŒ–ï¼‰
        characteristic_velocity = max(np.std(velocity), 1e-10)
        characteristic_length = window
        mean_density = max(np.mean(density), 1e-10)
        viscosity = 1.0 / mean_density  # é€†ç²˜æ€§
        
        if viscosity > 1e-10:
            reynolds = (characteristic_velocity * characteristic_length) / viscosity
        else:
            reynolds = 1000.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        reynolds_number[i] = reynolds
        
        # ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã®åˆ¤å®š
        critical_reynolds = 2300  # è‡¨ç•Œãƒ¬ã‚¤ãƒãƒ«ã‚ºæ•°
        if reynolds > critical_reynolds:
            flow_regime[i] = -1  # ä¹±æµ (ãƒ¬ãƒ³ã‚¸ç›¸å ´)
        else:
            flow_regime[i] = 1   # å±¤æµ (ãƒˆãƒ¬ãƒ³ãƒ‰ç›¸å ´)
        
        # ä¹±æµå¼·åº¦ã®è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
        velocity_fluctuations = velocity - np.mean(velocity)
        turbulence_kinetic_energy = np.mean(velocity_fluctuations ** 2)
        velocity_squared = max(characteristic_velocity ** 2, 1e-10)
        turbulence[i] = turbulence_kinetic_energy / velocity_squared
        
        # æ­£è¦åŒ–
        turbulence[i] = min(max(turbulence[i], 0.0), 1.0)
    
    return flow_regime, reynolds_number, turbulence


@njit(fastmath=True, cache=True)
def garch_volatility_estimation(returns: np.ndarray, alpha: float = 0.1, beta: float = 0.85) -> np.ndarray:
    """
    GARCH(1,1)ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹é©å¿œãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®š
    
    æ¡ä»¶ä»˜ãåˆ†æ•£ã®å‹•çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šã€
    æ™‚å¤‰ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’é«˜ç²¾åº¦ã§æ¨å®š
    """
    n = len(returns)
    volatility = np.full(n, np.nan)
    
    if n < 2:
        return volatility
    
    # åˆæœŸå€¤è¨­å®š
    long_run_variance = np.var(returns)
    conditional_variance = long_run_variance
    
    for i in range(1, n):
        # GARCH(1,1)æ›´æ–°å¼
        lagged_return_squared = returns[i-1] ** 2
        conditional_variance = (
            (1 - alpha - beta) * long_run_variance +
            alpha * lagged_return_squared +
            beta * conditional_variance
        )
        
        volatility[i] = math.sqrt(max(conditional_variance, 1e-10))
    
    return volatility


@njit(fastmath=True, parallel=True, cache=True)
def adaptive_kalman_centerline(prices: np.ndarray, quantum_coherence: np.ndarray, 
                              process_noise_factor: float = 0.01) -> np.ndarray:
    """
    é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã‚‹é‡å­é©å¿œã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³
    
    é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹å› å­ã«åŸºã¥ã„ã¦ãƒã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„èª¿æ•´ã—ã€
    å¸‚å ´çŠ¶æ³ã«å¿œã˜ã¦æœ€é©ãªå¹³æ»‘åŒ–ã‚’å®Ÿç¾
    """
    n = len(prices)
    filtered_prices = np.full(n, np.nan)
    
    if n < 2:
        return filtered_prices
    
    # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆæœŸåŒ–
    state_estimate = prices[0]
    error_covariance = 1.0
    
    filtered_prices[0] = state_estimate
    
    for i in range(1, n):
        # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ã«åŸºã¥ãé©å¿œçš„ãƒã‚¤ã‚ºèª¿æ•´
        coherence = quantum_coherence[i] if not np.isnan(quantum_coherence[i]) else 0.5
        
        # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºã¨observationãƒã‚¤ã‚ºã®å‹•çš„èª¿æ•´
        process_noise = process_noise_factor * (1.0 - coherence)
        observation_noise = 0.1 * (1.0 + coherence)
        
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        state_prediction = state_estimate
        error_prediction = error_covariance + process_noise
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
        denominator = error_prediction + observation_noise
        if denominator > 1e-10:
            kalman_gain = error_prediction / denominator
        else:
            kalman_gain = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
        state_estimate = state_prediction + kalman_gain * (prices[i] - state_prediction)
        error_covariance = (1 - kalman_gain) * error_prediction
        
        filtered_prices[i] = state_estimate
    
    return filtered_prices


@njit(fastmath=True, parallel=True, cache=True)
def calculate_dynamic_channel_width(
    garch_vol: np.ndarray,
    hurst_exp: np.ndarray,
    flow_regime: np.ndarray,
    complexity: np.ndarray,
    base_multiplier: float = 2.0
) -> np.ndarray:
    """
    çµ±åˆçš„å‹•çš„ãƒãƒ£ãƒãƒ«å¹…è¨ˆç®—
    
    GARCH-Hurstçµ±åˆãƒ¢ãƒ‡ãƒ«ã€ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã€è¤‡é›‘æ€§æŒ‡æ•°ã‚’çµ±åˆã—ã€
    å¸‚å ´çŠ¶æ³ã«å®Œå…¨é©å¿œã™ã‚‹é©æ–°çš„ãƒãƒ£ãƒãƒ«å¹…ã‚’ç®—å‡º
    """
    n = len(garch_vol)
    dynamic_width = np.full(n, np.nan)
    
    for i in prange(n):
        if (np.isnan(garch_vol[i]) or np.isnan(hurst_exp[i]) or 
            np.isnan(flow_regime[i]) or np.isnan(complexity[i])):
            continue
        
        # ãƒ™ãƒ¼ã‚¹ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (GARCH)
        base_volatility = garch_vol[i]
        
        # ãƒãƒ¼ã‚¹ãƒˆæŒ‡æ•°ã«ã‚ˆã‚‹èª¿æ•´
        # H > 0.5: ãƒˆãƒ¬ãƒ³ãƒ‰æŒç¶šæ€§ â†’ ãƒãƒ£ãƒãƒ«å¹…ç¸®å°
        # H < 0.5: å¹³å‡å›å¸°æ€§ â†’ ãƒãƒ£ãƒãƒ«å¹…æ‹¡å¤§
        hurst_factor = 1.0 + 2.0 * abs(hurst_exp[i] - 0.5)
        if hurst_exp[i] > 0.5:
            hurst_factor = 1.0 / hurst_factor  # ãƒˆãƒ¬ãƒ³ãƒ‰æ™‚ã¯ç¸®å°
        
        # ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹ã«ã‚ˆã‚‹èª¿æ•´
        flow_factor = 1.0
        if flow_regime[i] > 0:  # å±¤æµ (ãƒˆãƒ¬ãƒ³ãƒ‰)
            flow_factor = 0.7  # ãƒãƒ£ãƒãƒ«å¹…ç¸®å°
        else:  # ä¹±æµ (ãƒ¬ãƒ³ã‚¸)
            flow_factor = 1.5  # ãƒãƒ£ãƒãƒ«å¹…æ‹¡å¤§
        
        # è¤‡é›‘æ€§ã«ã‚ˆã‚‹å¾®èª¿æ•´
        complexity_factor = 1.0 + 0.5 * complexity[i]
        
        # çµ±åˆçš„ãƒãƒ£ãƒãƒ«å¹…
        dynamic_width[i] = (
            base_multiplier * 
            base_volatility * 
            hurst_factor * 
            flow_factor * 
            complexity_factor
        )
        
        # å®‰å…¨ãªç¯„å›²ã«åˆ¶é™
        dynamic_width[i] = max(min(dynamic_width[i], base_multiplier * 3.0), base_multiplier * 0.3)
    
    return dynamic_width


class QuantumFractalAdaptiveChannel(Indicator):
    """
    Quantum-Fractal Adaptive Volatility Channel (QFAVC)
    
    é©æ–°çš„ãªå‹•çš„ãƒãƒ£ãƒãƒ«ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼
    - é‡å­ã‚‚ã¤ã‚Œç†è«–ã«ã‚ˆã‚‹ä¾¡æ ¼ç›¸é–¢è§£æ
    - ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã«ã‚ˆã‚‹è¤‡é›‘æ€§è©•ä¾¡
    - æ¶²ä½“åŠ›å­¦ã«ã‚ˆã‚‹å¸‚å ´ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹åˆ¤å®š
    - é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³
    - GARCH-Hurstçµ±åˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
    """
    
    def __init__(
        self,
        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        quantum_window: int = 21,
        fractal_window: int = 34,
        flow_window: int = 21,
        garch_window: int = 55,
        
        # å‹•çš„æœŸé–“æ±ºå®šç”¨
        use_dynamic_periods: bool = True,
        dc_detector_type: str = 'dudi_e',
        dc_cycle_part: float = 0.4,
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        base_multiplier: float = 2.0,
        garch_alpha: float = 0.1,
        garch_beta: float = 0.85,
        
        # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        kalman_process_noise: float = 0.01,
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        src_type: str = 'hlc3',
        volume_src: str = 'volume'
    ):
        """
        Quantum-Fractal Adaptive Channel ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        """
        super().__init__(f"QFAVC(qw={quantum_window},fw={fractal_window})")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
        self.quantum_window = quantum_window
        self.fractal_window = fractal_window
        self.flow_window = flow_window
        self.garch_window = garch_window
        
        self.use_dynamic_periods = use_dynamic_periods
        self.base_multiplier = base_multiplier
        self.garch_alpha = garch_alpha
        self.garch_beta = garch_beta
        self.kalman_process_noise = kalman_process_noise
        
        self.src_type = src_type
        self.volume_src = volume_src
        
        # ä¾å­˜ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.price_source = PriceSource()
        
        # å‹•çš„æœŸé–“æ±ºå®šç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.use_dynamic_periods:
            self.dominant_cycle = EhlersUnifiedDC(
                detector_type=dc_detector_type,
                cycle_part=dc_cycle_part,
                src_type=src_type
            )
        else:
            self.dominant_cycle = None
        
        # çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._result_cache = {}
        self._cache_keys = []
        self._max_cache_size = 5
    
    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆé«˜é€ŸåŒ–ï¼‰"""
        try:
            if isinstance(data, pd.DataFrame):
                shape_signature = data.shape
                first_close = float(data.iloc[0].get('close', data.iloc[0, -1])) if len(data) > 0 else 0.0
                last_close = float(data.iloc[-1].get('close', data.iloc[-1, -1])) if len(data) > 0 else 0.0
                data_signature = (shape_signature, first_close, last_close)
            else:
                shape_signature = data.shape
                first_val = float(data[0, -1]) if len(data) > 0 and data.ndim > 1 else float(data[0]) if len(data) > 0 else 0.0
                last_val = float(data[-1, -1]) if len(data) > 0 and data.ndim > 1 else float(data[-1]) if len(data) > 0 else 0.0
                data_signature = (shape_signature, first_val, last_val)
            
            params_signature = (
                self.quantum_window, self.fractal_window, self.flow_window,
                self.base_multiplier, self.src_type
            )
            
            return f"{hash(data_signature)}_{hash(params_signature)}"
        except:
            return f"{id(data)}_{self.quantum_window}"
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> QFAVCResult:
        """
        QFAVCè¨ˆç®—ãƒ¡ã‚¤ãƒ³é–¢æ•°
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash in self._result_cache:
                return self._result_cache[data_hash]
            
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            if isinstance(data, pd.DataFrame):
                price_series = self.price_source.get_source(data, self.src_type)
                if hasattr(price_series, 'values'):
                    prices = price_series.values
                else:
                    prices = price_series
                volume = data.get(self.volume_src, pd.Series(np.ones(len(data)))).values
            else:
                prices = data[:, 3] if data.ndim > 1 else data  # closeä¾¡æ ¼
                volume = np.ones(len(prices))  # ãƒ€ãƒŸãƒ¼ãƒœãƒªãƒ¥ãƒ¼ãƒ 
            
            n = len(prices)
            
            # å‹•çš„æœŸé–“æ±ºå®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.use_dynamic_periods and self.dominant_cycle:
                try:
                    dc_values = self.dominant_cycle.calculate(data)
                    avg_period = int(np.nanmean(dc_values))
                    
                    # æœŸé–“ã‚’å‹•çš„èª¿æ•´
                    self.quantum_window = max(min(avg_period, 55), 8)
                    self.fractal_window = max(min(int(avg_period * 1.6), 89), 13)
                    self.flow_window = max(min(avg_period, 34), 8)
                except:
                    pass  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨
            
            # === æ®µéš1: é‡å­è§£æ ===
            quantum_entanglement, coherence_factor, superposition_state = calculate_quantum_entanglement(
                prices, self.quantum_window
            )
            
            # === æ®µéš2: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è§£æ ===
            fractal_dimension, hurst_exponent, complexity_index = calculate_fractal_dimension(
                prices, self.fractal_window
            )
            
            # === æ®µéš3: æµä½“åŠ›å­¦è§£æ ===
            flow_regime, reynolds_number, turbulence_intensity = calculate_fluid_dynamics(
                prices, volume, self.flow_window
            )
            
            # === æ®µéš4: GARCH ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®š ===  
            returns = np.diff(prices)
            garch_volatility = garch_volatility_estimation(returns, self.garch_alpha, self.garch_beta)
            # é•·ã•ã‚’ä¾¡æ ¼ã¨åˆã‚ã›ã‚‹
            garch_vol_padded = np.full(n, np.nan)
            garch_vol_padded[1:] = garch_volatility
            
            # === æ®µéš5: é©å¿œã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³ ===
            centerline = adaptive_kalman_centerline(
                prices, coherence_factor, self.kalman_process_noise
            )
            
            # === æ®µéš6: å‹•çš„ãƒãƒ£ãƒãƒ«å¹…è¨ˆç®— ===
            dynamic_width = calculate_dynamic_channel_width(
                garch_vol_padded, hurst_exponent, flow_regime, 
                complexity_index, self.base_multiplier
            )
            
            # === æ®µéš7: ãƒãƒ£ãƒãƒ«æ§‹ç¯‰ ===
            upper_channel = centerline + dynamic_width
            lower_channel = centerline - dynamic_width
            
            # === æ®µéš8: çµ±åˆæŒ‡æ¨™è¨ˆç®— ===
            market_phase = np.full(n, np.nan)
            adaptation_factor = np.full(n, np.nan)
            confidence_score = np.full(n, np.nan)
            adaptive_variance = np.full(n, np.nan)
            volatility_regime = np.full(n, np.nan)
            
            for i in range(n):
                if not np.isnan(flow_regime[i]) and not np.isnan(hurst_exponent[i]):
                    # å¸‚å ´ãƒ•ã‚§ãƒ¼ã‚ºåˆ¤å®š
                    if flow_regime[i] > 0 and hurst_exponent[i] > 0.5:
                        market_phase[i] = 1  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰
                    elif flow_regime[i] < 0 and hurst_exponent[i] < 0.5:
                        market_phase[i] = -1  # å¼·ã„ãƒ¬ãƒ³ã‚¸
                    else:
                        market_phase[i] = 0  # ä¸­é–“çŠ¶æ…‹
                    
                    # é©å¿œå› å­
                    adaptation_factor[i] = abs(hurst_exponent[i] - 0.5) * (1.0 - turbulence_intensity[i])
                    
                    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
                    confidence_score[i] = (
                        coherence_factor[i] * 0.4 +
                        (1.0 - turbulence_intensity[i]) * 0.3 +
                        abs(flow_regime[i]) * 0.3
                    ) if not np.isnan(coherence_factor[i]) and not np.isnan(turbulence_intensity[i]) else 0.5
                
                # é©å¿œåˆ†æ•£
                if not np.isnan(garch_vol_padded[i]):
                    adaptive_variance[i] = garch_vol_padded[i] ** 2
                
                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
                if not np.isnan(garch_vol_padded[i]) and i > 20:
                    recent_vol = np.nanmean(garch_vol_padded[max(0, i-20):i])
                    if garch_vol_padded[i] > recent_vol * 1.5:
                        volatility_regime[i] = 1  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                    elif garch_vol_padded[i] < recent_vol * 0.7:
                        volatility_regime[i] = -1  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                    else:
                        volatility_regime[i] = 0  # æ­£å¸¸ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            
            # çµæœæ§‹ç¯‰
            result = QFAVCResult(
                # ãƒãƒ£ãƒãƒ«è¦ç´ 
                centerline=centerline,
                upper_channel=upper_channel,
                lower_channel=lower_channel,
                dynamic_width=dynamic_width,
                
                # é‡å­è§£ææˆåˆ†
                quantum_entanglement=quantum_entanglement,
                coherence_factor=coherence_factor,
                superposition_state=superposition_state,
                
                # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è§£ææˆåˆ†
                fractal_dimension=fractal_dimension,
                hurst_exponent=hurst_exponent,
                complexity_index=complexity_index,
                
                # æµä½“åŠ›å­¦æˆåˆ†
                flow_regime=flow_regime,
                reynolds_number=reynolds_number,
                turbulence_intensity=turbulence_intensity,
                
                # é©å¿œãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æˆåˆ†
                garch_volatility=garch_vol_padded,
                adaptive_variance=adaptive_variance,
                volatility_regime=volatility_regime,
                
                # çµ±åˆæŒ‡æ¨™
                market_phase=market_phase,
                adaptation_factor=adaptation_factor,
                confidence_score=confidence_score
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
            if len(self._result_cache) >= self._max_cache_size and self._cache_keys:
                oldest_key = self._cache_keys.pop(0)
                if oldest_key in self._result_cache:
                    del self._result_cache[oldest_key]
            
            self._result_cache[data_hash] = result
            self._cache_keys.append(data_hash)
            
            return result
            
        except Exception as e:
            self.logger.error(f"QFAVCè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¿”ã™
            n = len(data) if hasattr(data, '__len__') else 0
            empty_array = np.full(n, np.nan)
            return QFAVCResult(
                centerline=empty_array, upper_channel=empty_array, lower_channel=empty_array,
                dynamic_width=empty_array, quantum_entanglement=empty_array, coherence_factor=empty_array,
                superposition_state=empty_array, fractal_dimension=empty_array, hurst_exponent=empty_array,
                complexity_index=empty_array, flow_regime=empty_array, reynolds_number=empty_array,
                turbulence_intensity=empty_array, garch_volatility=empty_array, adaptive_variance=empty_array,
                volatility_regime=empty_array, market_phase=empty_array, adaptation_factor=empty_array,
                confidence_score=empty_array
            )
    
    def get_channel_bands(self, data: Union[pd.DataFrame, np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """ãƒãƒ£ãƒãƒ«ãƒãƒ³ãƒ‰å–å¾—"""
        if data is not None:
            result = self.calculate(data)
        elif self._result_cache:
            result = list(self._result_cache.values())[-1]
        else:
            empty_array = np.array([])
            return empty_array, empty_array, empty_array
        return result.centerline, result.upper_channel, result.lower_channel
    
    def get_market_intelligence_report(self, data: Union[pd.DataFrame, np.ndarray] = None) -> Dict:
        """å¸‚å ´çŸ¥èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if data is not None:
            result = self.calculate(data)
        elif self._result_cache:
            result = list(self._result_cache.values())[-1]
        else:
            return {"status": "no_data"}
        
        # æœ€æ–°å€¤ã®å–å¾—
        latest_idx = -1
        while latest_idx >= -len(result.market_phase) and np.isnan(result.market_phase[latest_idx]):
            latest_idx -= 1
        
        if abs(latest_idx) >= len(result.market_phase):
            return {"status": "insufficient_data"}
        
        return {
            "current_market_phase": int(result.market_phase[latest_idx]) if not np.isnan(result.market_phase[latest_idx]) else 0,
            "hurst_exponent": float(result.hurst_exponent[latest_idx]) if not np.isnan(result.hurst_exponent[latest_idx]) else 0.5,
            "flow_regime": "layer_flow" if result.flow_regime[latest_idx] > 0 else "turbulent_flow",
            "quantum_coherence": float(result.coherence_factor[latest_idx]) if not np.isnan(result.coherence_factor[latest_idx]) else 0.5,
            "complexity_index": float(result.complexity_index[latest_idx]) if not np.isnan(result.complexity_index[latest_idx]) else 0.5,
            "confidence_score": float(result.confidence_score[latest_idx]) if not np.isnan(result.confidence_score[latest_idx]) else 0.5,
            "volatility_regime": int(result.volatility_regime[latest_idx]) if not np.isnan(result.volatility_regime[latest_idx]) else 0
        } 