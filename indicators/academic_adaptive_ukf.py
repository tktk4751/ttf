#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ **Academic Adaptive UKF (Ge et al. 2019)** ğŸ¯

è«–æ–‡ã€ŒAdaptive Unscented Kalman Filter for Target Tracking with Unknown Time-Varying Noise Covarianceã€
by Baoshuang Ge et al., Sensors 2019 ã®å³å¯†å®Ÿè£…

ğŸŒŸ **è«–æ–‡ã®é©æ–°çš„æ‰‹æ³•:**
1. **ç›¸äº’ç›¸é–¢ç†è«–**: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ®‹å·®ã®æ•°å­¦çš„å³å¯†ãªç›¸äº’ç›¸é–¢è¨¼æ˜
2. **ç·šå½¢è¡Œåˆ—æ–¹ç¨‹å¼**: å·®åˆ†ç³»åˆ—å…±åˆ†æ•£ã‹ã‚‰ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºQæ¨å®š
3. **RMNCE**: å†—é•·è¨ˆæ¸¬ãƒã‚¤ã‚ºå…±åˆ†æ•£æ¨å®šï¼ˆRedundant Measurement Noise Covariance Estimationï¼‰
4. **æ•°å­¦çš„å³å¯†æ€§**: ç†è«–çš„ã«è¨¼æ˜ã•ã‚ŒãŸæ‰‹æ³•
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Optional
import numpy as np
import pandas as pd
from numba import njit
import warnings

try:
    from .indicator import Indicator
    from .price_source import PriceSource
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource


@dataclass
class AcademicAUKFResult:
    """å­¦è¡“ç‰ˆé©å¿œUKFã®è¨ˆç®—çµæœ"""
    filtered_values: np.ndarray          # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    velocity_estimates: np.ndarray       # é€Ÿåº¦æ¨å®šå€¤
    acceleration_estimates: np.ndarray   # åŠ é€Ÿåº¦æ¨å®šå€¤
    uncertainty: np.ndarray              # æ¨å®šä¸ç¢ºå®Ÿæ€§
    kalman_gains: np.ndarray             # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
    innovations: np.ndarray              # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç³»åˆ—
    residuals: np.ndarray                # æ®‹å·®ç³»åˆ—
    confidence_scores: np.ndarray        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    raw_values: np.ndarray              # å…ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
    
    # è«–æ–‡ç‰¹æœ‰ã®çµæœ
    adaptive_process_noise: np.ndarray   # é©å¿œçš„ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆQæ¨å®šï¼‰
    adaptive_observation_noise: np.ndarray  # é©å¿œçš„è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆRæ¨å®šï¼‰
    cross_correlation: np.ndarray        # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³-æ®‹å·®ç›¸äº’ç›¸é–¢
    innovation_covariance: np.ndarray    # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å…±åˆ†æ•£
    residual_covariance: np.ndarray      # æ®‹å·®å…±åˆ†æ•£
    difference_covariance: np.ndarray    # å·®åˆ†ç³»åˆ—å…±åˆ†æ•£


@njit(fastmath=True, cache=True)
def generate_sigma_points_academic(
    mean: np.ndarray, 
    covariance: np.ndarray, 
    alpha: float = 0.001, 
    beta: float = 2.0, 
    kappa: float = 0.0
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """å­¦è¡“ç‰ˆã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ"""
    L = len(mean)
    lambda_param = alpha * alpha * (L + kappa) - L
    
    n_sigma = 2 * L + 1
    sigma_points = np.zeros((n_sigma, L))
    Wm = np.zeros(n_sigma)
    Wc = np.zeros(n_sigma)
    
    # é‡ã¿è¨ˆç®—
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    for i in range(1, n_sigma):
        Wm[i] = 0.5 / (L + lambda_param)
        Wc[i] = 0.5 / (L + lambda_param)
    
    # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
    sigma_points[0] = mean
    
    try:
        sqrt_matrix = np.linalg.cholesky((L + lambda_param) * covariance)
    except:
        # Choleskyåˆ†è§£å¤±æ•—æ™‚ã®å¯¾å‡¦
        sqrt_matrix = np.zeros((L, L))
        for i in range(L):
            sqrt_matrix[i, i] = np.sqrt(max((L + lambda_param) * covariance[i, i], 1e-8))
    
    for i in range(L):
        sigma_points[i + 1] = mean + sqrt_matrix[:, i]
        sigma_points[i + 1 + L] = mean - sqrt_matrix[:, i]
    
    return sigma_points, Wm, Wc


@njit(fastmath=True, cache=True)
def state_transition_academic(state: np.ndarray, dt: float = 1.0) -> np.ndarray:
    """å­¦è¡“ç‰ˆçŠ¶æ…‹é·ç§»ï¼ˆé‡‘èå‘ã‘ï¼‰"""
    price, velocity, acceleration = state[0], state[1], state[2]
    
    # é‡‘èæ™‚ç³»åˆ—ã«é©ã—ãŸçŠ¶æ…‹é·ç§»
    new_price = price + velocity * dt + 0.5 * acceleration * dt * dt
    new_velocity = velocity * 0.98 + acceleration * dt  # é€Ÿåº¦æ¸›è¡°
    new_acceleration = acceleration * 0.9  # åŠ é€Ÿåº¦æ¸›è¡°
    
    return np.array([new_price, new_velocity, new_acceleration])


@njit(fastmath=True, cache=True)
def observation_function_academic(state: np.ndarray) -> float:
    """å­¦è¡“ç‰ˆè¦³æ¸¬é–¢æ•°"""
    return state[0]  # ä¾¡æ ¼ã‚’ç›´æ¥è¦³æ¸¬


@njit(fastmath=True, cache=True)
def estimate_process_noise_academic(
    innovations: np.ndarray,
    residuals: np.ndarray,
    window_size: int = 25
) -> np.ndarray:
    """
    è«–æ–‡ã®æ‰‹æ³•ï¼šå·®åˆ†ç³»åˆ—å…±åˆ†æ•£ã‹ã‚‰ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºQæ¨å®š
    
    E[(Î·â‚– - Îµâ‚–)(Î·â‚– - Îµâ‚–)áµ€] = H_{k|k-1} Î“_{k-1} Q_{k-1} Î“_{k-1}^T H_{k|k-1}^T
    """
    n = len(innovations)
    Q_estimates = np.zeros(n)
    
    for t in range(window_size, n):
        # å·®åˆ†ç³»åˆ—ã®è¨ˆç®—
        start_idx = t - window_size + 1
        window_innovations = innovations[start_idx:t+1]
        window_residuals = residuals[start_idx:t+1]
        
        # å·®åˆ†ç³»åˆ—
        differences = window_residuals - window_innovations
        
        # å·®åˆ†ç³»åˆ—ã®å…±åˆ†æ•£æ¨å®š
        if len(differences) > 1:
            diff_covariance = np.var(differences)
            
            # è«–æ–‡ã®ç·šå½¢æ–¹ç¨‹å¼ã‚’ç°¡ç•¥åŒ–ã—ã¦è§£ã
            # ç°¡ç•¥åŒ–ç‰ˆï¼šQ âˆ å·®åˆ†ç³»åˆ—å…±åˆ†æ•£
            Q_estimate = max(diff_covariance * 0.1, 1e-6)
            Q_estimates[t] = Q_estimate
        else:
            Q_estimates[t] = Q_estimates[t-1] if t > 0 else 0.001
    
    # åˆæœŸå€¤ã®è¨­å®š
    if window_size > 0 and window_size < n:
        initial_q = Q_estimates[window_size]
        for i in range(window_size):
            Q_estimates[i] = initial_q
    
    return Q_estimates


@njit(fastmath=True, cache=True)
def estimate_measurement_noise_rmnce(
    measurements: np.ndarray,
    redundant_measurements: np.ndarray,
    fading_factor: float = 0.98
) -> np.ndarray:
    """
    è«–æ–‡ã®RMNCEæ‰‹æ³•ï¼šå†—é•·è¨ˆæ¸¬ãƒã‚¤ã‚ºå…±åˆ†æ•£æ¨å®š
    
    Râ‚ = [âˆ‡Z(k)âˆ‡Z(k)áµ€ + Î”Zâ‚(k)Î”Zâ‚(k)áµ€ - Î”Zâ‚‚(k)Î”Zâ‚‚(k)áµ€] / 4
    """
    n = len(measurements)
    R_estimates = np.zeros(n)
    
    for k in range(1, n):
        # 1æ¬¡å·®åˆ†ç³»åˆ—
        delta_z1 = measurements[k] - measurements[k-1]
        delta_z2 = redundant_measurements[k] - redundant_measurements[k-1]
        
        # 2æ¬¡ç›¸äº’å·®åˆ†ç³»åˆ—
        nabla_z = delta_z1 - delta_z2
        
        # RMNCEæ¨å®šå¼
        term1 = nabla_z * nabla_z
        term2 = delta_z1 * delta_z1
        term3 = delta_z2 * delta_z2
        
        R_current = (term1 + term2 - term3) / 4.0
        R_current = max(R_current, 1e-6)
        
        # å†å¸°çš„æ¨å®šï¼ˆã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
        if k == 1:
            R_estimates[k] = R_current
        else:
            d_k = 1.0 - fading_factor**(k)
            R_estimates[k] = (1 - d_k) * R_estimates[k-1] + d_k * R_current
    
    # åˆæœŸå€¤è¨­å®š
    if n > 1:
        R_estimates[0] = R_estimates[1]
    
    return R_estimates


@njit(fastmath=True, cache=True)
def calculate_academic_adaptive_ukf(
    prices: np.ndarray,
    redundant_prices: Optional[np.ndarray],
    window_size: int = 25,
    fading_factor: float = 0.98
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, 
           np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, 
           np.ndarray, np.ndarray, np.ndarray]:
    """å­¦è¡“ç‰ˆé©å¿œUKFãƒ¡ã‚¤ãƒ³è¨ˆç®—"""
    n = len(prices)
    L = 3  # çŠ¶æ…‹æ¬¡å…ƒ
    
    if n < 5:
        zeros = np.zeros(n)
        ones = np.ones(n)
        return (prices.copy(), zeros, zeros, ones, ones * 0.5, zeros, zeros, 
                ones, ones * 0.001, ones * 0.01, zeros, zeros, zeros)
    
    # çµæœé…åˆ—åˆæœŸåŒ–
    filtered_prices = np.zeros(n)
    velocity_estimates = np.zeros(n)
    acceleration_estimates = np.zeros(n)
    uncertainty = np.zeros(n)
    kalman_gains = np.zeros(n)
    innovations = np.zeros(n)
    residuals = np.zeros(n)
    confidence_scores = np.zeros(n)
    adaptive_process_noise = np.zeros(n)
    adaptive_observation_noise = np.zeros(n)
    cross_correlation = np.zeros(n)
    innovation_covariance = np.zeros(n)
    residual_covariance = np.zeros(n)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.array([prices[0], 0.0, 0.0])
    P = np.array([[1.0, 0.0, 0.0],
                  [0.0, 0.1, 0.0],
                  [0.0, 0.0, 0.01]])
    
    # åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    alpha, beta, kappa = 0.001, 2.0, 0.0
    current_Q = 0.001
    current_R = 0.01
    
    # åˆæœŸå€¤è¨­å®š
    filtered_prices[0] = prices[0]
    adaptive_process_noise[0] = current_Q
    adaptive_observation_noise[0] = current_R
    confidence_scores[0] = 1.0
    
    # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
    for t in range(1, n):
        # === UKFäºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ— ===
        
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
        sigma_points, Wm, Wc = generate_sigma_points_academic(x, P, alpha, beta, kappa)
        
        # çŠ¶æ…‹äºˆæ¸¬
        n_sigma = len(sigma_points)
        sigma_points_pred = np.zeros((n_sigma, L))
        for i in range(n_sigma):
            sigma_points_pred[i] = state_transition_academic(sigma_points[i], 1.0)
        
        # äºˆæ¸¬çŠ¶æ…‹å¹³å‡
        x_pred = np.zeros(L)
        for i in range(n_sigma):
            x_pred += Wm[i] * sigma_points_pred[i]
        
        # äºˆæ¸¬å…±åˆ†æ•£
        Q_matrix = np.array([[current_Q, 0.0, 0.0],
                            [0.0, current_Q * 0.1, 0.0],
                            [0.0, 0.0, current_Q * 0.01]])
        
        P_pred = Q_matrix.copy()
        for i in range(n_sigma):
            diff = sigma_points_pred[i] - x_pred
            P_pred += Wc[i] * np.outer(diff, diff)
        
        # === UKFæ›´æ–°ã‚¹ãƒ†ãƒƒãƒ— ===
        
        # è¦³æ¸¬äºˆæ¸¬
        z_sigma = np.zeros(n_sigma)
        for i in range(n_sigma):
            z_sigma[i] = observation_function_academic(sigma_points_pred[i])
        
        z_pred = 0.0
        for i in range(n_sigma):
            z_pred += Wm[i] * z_sigma[i]
        
        # è¦³æ¸¬å…±åˆ†æ•£
        S = current_R
        Pxz = np.zeros(L)
        
        for i in range(n_sigma):
            z_diff = z_sigma[i] - z_pred
            x_diff = sigma_points_pred[i] - x_pred
            S += Wc[i] * z_diff * z_diff
            Pxz += Wc[i] * x_diff * z_diff
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        if S > 1e-12:
            K = Pxz / S
        else:
            K = np.array([0.5, 0.0, 0.0])
        
        # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç®—
        innovation = prices[t] - z_pred
        innovations[t] = innovation
        
        # çŠ¶æ…‹æ›´æ–°
        x = x_pred + K * innovation
        P = P_pred - np.outer(K, K) * S
        
        # æ®‹å·®è¨ˆç®—ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œï¼‰
        residual = prices[t] - observation_function_academic(x)
        residuals[t] = residual
        
        # è«–æ–‡ã®ç›¸äº’ç›¸é–¢è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        cross_correlation[t] = innovation * residual
        
        # æ•°å€¤å®‰å®šæ€§ç¢ºä¿
        for i in range(L):
            if P[i, i] < 1e-8:
                P[i, i] = 1e-8
        
        # çµæœä¿å­˜
        filtered_prices[t] = x[0]
        velocity_estimates[t] = x[1]
        acceleration_estimates[t] = x[2]
        uncertainty[t] = np.sqrt(max(P[0, 0], 0))
        kalman_gains[t] = K[0]
        confidence_scores[t] = 1.0 / (1.0 + uncertainty[t] * 10.0)
        
        # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»æ®‹å·®å…±åˆ†æ•£
        innovation_covariance[t] = innovation * innovation
        residual_covariance[t] = residual * residual
    
    # === è«–æ–‡ã®é©å¿œçš„æ¨å®š ===
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºQæ¨å®šï¼ˆè«–æ–‡æ‰‹æ³•ï¼‰
    adaptive_process_noise = estimate_process_noise_academic(
        innovations, residuals, window_size
    )
    
    # è¦³æ¸¬ãƒã‚¤ã‚ºRæ¨å®šï¼ˆRMNCEï¼‰
    if redundant_prices is not None:
        adaptive_observation_noise = estimate_measurement_noise_rmnce(
            prices, redundant_prices, fading_factor
        )
    else:
        # å†—é•·è¨ˆæ¸¬ãŒãªã„å ´åˆã¯ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ•£ãƒ™ãƒ¼ã‚¹
        for t in range(window_size, n):
            start_idx = t - window_size + 1
            window_innovations = innovations[start_idx:t+1]
            adaptive_observation_noise[t] = max(np.var(window_innovations), 1e-6)
        
        # åˆæœŸå€¤è¨­å®š
        if window_size < n:
            initial_r = adaptive_observation_noise[window_size]
            for i in range(window_size):
                adaptive_observation_noise[i] = initial_r
    
    return (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
            kalman_gains, innovations, residuals, confidence_scores,
            adaptive_process_noise, adaptive_observation_noise, cross_correlation,
            innovation_covariance, residual_covariance)


class AcademicAdaptiveUnscentedKalmanFilter(Indicator):
    """
    å­¦è¡“ç‰ˆé©å¿œçš„ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆGe et al. 2019ï¼‰
    
    ğŸ¯ **è«–æ–‡ã®å³å¯†å®Ÿè£…:**
    - Baoshuang Ge et al., "Adaptive Unscented Kalman Filter for Target Tracking 
      with Unknown Time-Varying Noise Covariance", Sensors 2019
    
    ğŸŒŸ **é©æ–°çš„ç‰¹å¾´:**
    1. **ç›¸äº’ç›¸é–¢ç†è«–**: æ•°å­¦çš„å³å¯†ãªã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³-æ®‹å·®ç›¸äº’ç›¸é–¢
    2. **ç·šå½¢è¡Œåˆ—æ–¹ç¨‹å¼**: å·®åˆ†ç³»åˆ—å…±åˆ†æ•£ã‹ã‚‰Qæ¨å®š
    3. **RMNCE**: å†—é•·è¨ˆæ¸¬ãƒã‚¤ã‚ºå…±åˆ†æ•£æ¨å®š
    4. **ç†è«–çš„å³å¯†æ€§**: è¨¼æ˜ã•ã‚ŒãŸæ•°å­¦çš„æ‰‹æ³•
    
    ğŸ“ˆ **é©ç”¨åˆ†é‡:**
    - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¿½è·¡ï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ï¼‰
    - é‡‘èæ™‚ç³»åˆ—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    - ã‚»ãƒ³ã‚µãƒ¼èåˆ
    """
    
    def __init__(
        self,
        src_type: str = 'close',
        window_size: int = 25,
        fading_factor: float = 0.98,
        use_redundant_measurement: bool = False
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
            window_size: æ¨å®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
            fading_factor: ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°
            use_redundant_measurement: å†—é•·è¨ˆæ¸¬ä½¿ç”¨ãƒ•ãƒ©ã‚°
        """
        super().__init__(f"AcademicAUKF(src={src_type})")
        
        self.src_type = src_type.lower()
        self.window_size = window_size
        self.fading_factor = fading_factor
        self.use_redundant_measurement = use_redundant_measurement
        
        self._latest_result = None
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> AcademicAUKFResult:
        """å­¦è¡“ç‰ˆé©å¿œUKFè¨ˆç®—"""
        try:
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            src_prices = PriceSource.calculate_source(data, self.src_type)
            
            if len(src_prices) < 5:
                return self._create_empty_result(len(src_prices), src_prices)
            
            # å†—é•·è¨ˆæ¸¬ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã®å¿œç”¨ã§ã¯åˆ¥ã®ã‚»ãƒ³ã‚µãƒ¼ã‹ã‚‰å–å¾—ï¼‰
            redundant_prices = None
            if self.use_redundant_measurement:
                # æ¨¡æ“¬å†—é•·è¨ˆæ¸¬ï¼ˆå°‘ã—ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸç‰ˆï¼‰
                np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
                noise = np.random.normal(0, 0.1, len(src_prices))
                redundant_prices = src_prices + noise
            
            # å­¦è¡“ç‰ˆAUKFè¨ˆç®—
            (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
             kalman_gains, innovations, residuals, confidence_scores,
             adaptive_process_noise, adaptive_observation_noise, cross_correlation,
             innovation_covariance, residual_covariance) = calculate_academic_adaptive_ukf(
                src_prices, redundant_prices, self.window_size, self.fading_factor
            )
            
            # å·®åˆ†ç³»åˆ—å…±åˆ†æ•£è¨ˆç®—
            differences = residuals - innovations
            difference_covariance = np.zeros(len(differences))
            for i in range(self.window_size, len(differences)):
                start_idx = i - self.window_size + 1
                window_diffs = differences[start_idx:i+1]
                difference_covariance[i] = np.var(window_diffs) if len(window_diffs) > 1 else 0.0
            
            # åˆæœŸå€¤è¨­å®š
            if self.window_size < len(difference_covariance):
                initial_diff_cov = difference_covariance[self.window_size]
                for i in range(self.window_size):
                    difference_covariance[i] = initial_diff_cov
            
            # çµæœä½œæˆ
            result = AcademicAUKFResult(
                filtered_values=filtered_prices.copy(),
                velocity_estimates=velocity_estimates.copy(),
                acceleration_estimates=acceleration_estimates.copy(),
                uncertainty=uncertainty.copy(),
                kalman_gains=kalman_gains.copy(),
                innovations=innovations.copy(),
                residuals=residuals.copy(),
                confidence_scores=confidence_scores.copy(),
                raw_values=src_prices.copy(),
                adaptive_process_noise=adaptive_process_noise.copy(),
                adaptive_observation_noise=adaptive_observation_noise.copy(),
                cross_correlation=cross_correlation.copy(),
                innovation_covariance=innovation_covariance.copy(),
                residual_covariance=residual_covariance.copy(),
                difference_covariance=difference_covariance.copy()
            )
            
            self._latest_result = result
            self._values = filtered_prices
            return result
            
        except Exception as e:
            self.logger.error(f"Academic AUKFè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            if isinstance(data, (pd.DataFrame, np.ndarray)) and len(data) > 0:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                return self._create_empty_result(len(src_prices), src_prices)
            else:
                return self._create_empty_result(0, np.array([]))
    
    def _create_empty_result(self, length: int, raw_prices: np.ndarray) -> AcademicAUKFResult:
        """ç©ºã®çµæœä½œæˆ"""
        nan_array = np.full(length, np.nan)
        return AcademicAUKFResult(
            filtered_values=nan_array.copy(),
            velocity_estimates=nan_array.copy(),
            acceleration_estimates=nan_array.copy(),
            uncertainty=nan_array.copy(),
            kalman_gains=nan_array.copy(),
            innovations=nan_array.copy(),
            residuals=nan_array.copy(),
            confidence_scores=nan_array.copy(),
            raw_values=raw_prices,
            adaptive_process_noise=nan_array.copy(),
            adaptive_observation_noise=nan_array.copy(),
            cross_correlation=nan_array.copy(),
            innovation_covariance=nan_array.copy(),
            residual_covariance=nan_array.copy(),
            difference_covariance=nan_array.copy()
        )
    
    def get_academic_summary(self) -> Dict:
        """å­¦è¡“ç‰ˆè¦ç´„"""
        if not self._latest_result:
            return {}
        
        result = self._latest_result
        return {
            'filter_type': 'Academic Adaptive UKF (Ge et al. 2019)',
            'theoretical_basis': 'Innovation-Residual Cross-Correlation Theory',
            'q_estimation_method': 'Linear Matrix Equation from Difference Sequence Covariance',
            'r_estimation_method': 'RMNCE (Redundant Measurement Noise Covariance Estimation)',
            'avg_cross_correlation': np.nanmean(result.cross_correlation),
            'avg_process_noise': np.nanmean(result.adaptive_process_noise),
            'avg_observation_noise': np.nanmean(result.adaptive_observation_noise),
            'innovation_residual_correlation': np.corrcoef(
                result.innovations[~np.isnan(result.innovations)],
                result.residuals[~np.isnan(result.residuals)]
            )[0, 1] if len(result.innovations[~np.isnan(result.innovations)]) > 1 else 0.0,
            'academic_features': [
                'æ•°å­¦çš„å³å¯†ãªç›¸äº’ç›¸é–¢ç†è«–',
                'ç·šå½¢è¡Œåˆ—æ–¹ç¨‹å¼ã«ã‚ˆã‚‹Qæ¨å®š',
                'RMNCEå†—é•·è¨ˆæ¸¬Ræ¨å®š',
                'ç†è«–çš„è¨¼æ˜ã«åŸºã¥ãæ‰‹æ³•'
            ]
        } 