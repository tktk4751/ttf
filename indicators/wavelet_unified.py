#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆçµ±åˆè§£æå™¨ (Wavelet Unified Analyzer)

è¤‡æ•°ã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’
çµ±åˆã—ã€å˜ä¸€ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§åˆ©ç”¨å¯èƒ½ã«ã—ã¾ã™ã€‚

å¯¾å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæ‰‹æ³•:
- 'haar_denoising': Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»
- 'multiresolution': å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ
- 'financial_adaptive': é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›
- 'quantum_analysis': é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ
- 'morlet_continuous': Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé€£ç¶šå¤‰æ›
- 'daubechies_advanced': Daubechiesé«˜åº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
- 'ultimate_cosmic': ğŸŒŒ ç©¶æ¥µå®‡å®™ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æï¼ˆäººé¡å²ä¸Šæœ€å¼·ï¼‰
"""

from typing import Union, Optional, Dict, Tuple, NamedTuple, List
import numpy as np
import pandas as pd
from numba import jit, njit, prange
import math

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .indicator import Indicator
    from .price_source import PriceSource
except ImportError:
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œæ™‚
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource


class WaveletResult(NamedTuple):
    """ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æçµæœã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    values: np.ndarray
    trend_component: Optional[np.ndarray] = None
    cycle_component: Optional[np.ndarray] = None
    noise_component: Optional[np.ndarray] = None
    detail_component: Optional[np.ndarray] = None
    market_regime: Optional[np.ndarray] = None
    energy_spectrum: Optional[np.ndarray] = None
    confidence_score: Optional[np.ndarray] = None


class UltimateCosmicResult(NamedTuple):
    """ğŸŒŒ å®‡å®™æœ€å¼·ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æçµæœ"""
    # ãƒ¡ã‚¤ãƒ³çµæœ
    cosmic_signal: np.ndarray              # å®‡å®™ãƒ¬ãƒ™ãƒ«çµ±åˆä¿¡å·
    cosmic_trend: np.ndarray               # å®‡å®™ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ† (0-1)
    cosmic_cycle: np.ndarray               # å®‡å®™ã‚µã‚¤ã‚¯ãƒ«æˆåˆ† (-1 to 1)
    cosmic_volatility: np.ndarray          # å®‡å®™ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (0-1)
    
    # é«˜åº¦ãªæˆåˆ†
    quantum_coherence: np.ndarray          # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹åº¦ (0-1)
    market_regime: np.ndarray              # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ  (-1 to 1)
    adaptive_confidence: np.ndarray        # é©å¿œçš„ä¿¡é ¼åº¦ (0-1)
    
    # è©³ç´°åˆ†æ
    multi_scale_energy: np.ndarray         # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼
    phase_synchronization: np.ndarray      # ä½ç›¸åŒæœŸåº¦ (0-1)
    cosmic_momentum: np.ndarray            # å®‡å®™ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ  (-1 to 1)


# === åŸºåº•ã‚¯ãƒ©ã‚¹ ===

class WaveletAnalyzer:
    """ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æå™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, name: str = "WaveletAnalyzer"):
        self.name = name
        self._cache = {}
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        """ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚’å®Ÿè¡Œ"""
        raise NotImplementedError("ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ã—ã¦ãã ã•ã„")
    
    def reset(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self._cache = {}


# === Numbaæœ€é©åŒ–é–¢æ•°ç¾¤ ===

@njit(fastmath=True, cache=True)
def haar_wavelet_denoising_numba(prices: np.ndarray, levels: int = 3) -> Tuple[np.ndarray, np.ndarray]:
    """
    Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»ï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    
    Args:
        prices: ä¾¡æ ¼é…åˆ—
        levels: åˆ†è§£ãƒ¬ãƒ™ãƒ«
    
    Returns:
        (denoised_signal, detail_component)
    """
    n = len(prices)
    if n < 8:
        return prices.copy(), np.zeros(n)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªHaarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè¿‘ä¼¼
    signal = prices.copy()
    detail = np.zeros(n)
    
    for level in range(levels):
        step = 2 ** level
        if step >= n // 2:
            break
            
        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è¿‘ä¼¼
        for i in range(step, n - step, step * 2):
            # ãƒ­ãƒ¼ãƒ‘ã‚¹ï¼ˆå¹³å‡ï¼‰
            low = (signal[i-step] + signal[i]) * 0.5
            # ãƒã‚¤ãƒ‘ã‚¹ï¼ˆå·®åˆ†ï¼‰
            high = (signal[i-step] - signal[i]) * 0.5
            
            # ãƒã‚¤ã‚ºé™¤å»ã®ãŸã‚ã®ã—ãã„å€¤
            window_start = max(0, i-step*4)
            window_end = i+step*4
            if window_end > n:
                window_end = n
            
            window_data = signal[window_start:window_end]
            threshold = np.std(window_data) * 0.1
            
            if abs(high) < threshold:
                high = 0
            
            detail[i] += high
            signal[i-step] = low
            signal[i] = low
    
    denoised = signal - detail
    return denoised, detail


@njit(fastmath=True, cache=True)
def multiresolution_analysis_numba(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    
    Returns:
        (filtered_prices, trend_energy_ratio, market_regime)
    """
    n = len(prices)
    filtered_prices = np.full(n, np.nan)
    trend_energy_ratio = np.full(n, np.nan)
    market_regime = np.full(n, np.nan)
    
    # ç§»å‹•å¹³å‡ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«åˆ†æ
    for i in range(15, n):
        # çŸ­æœŸç§»å‹•å¹³å‡ï¼ˆé«˜å‘¨æ³¢æˆåˆ†ï¼‰
        short_window = min(8, i)
        short_ma = 0.0
        for j in range(short_window):
            short_ma += prices[i-j]
        short_ma /= short_window
        
        # ä¸­æœŸç§»å‹•å¹³å‡ï¼ˆä¸­å‘¨æ³¢æˆåˆ†ï¼‰
        medium_window = min(20, i)
        medium_ma = 0.0
        for j in range(medium_window):
            medium_ma += prices[i-j]
        medium_ma /= medium_window
        
        # é•·æœŸç§»å‹•å¹³å‡ï¼ˆä½å‘¨æ³¢æˆåˆ†ï¼‰
        long_window = min(50, i)
        long_ma = 0.0
        for j in range(long_window):
            long_ma += prices[i-j]
        long_ma /= long_window
        
        # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé¢¨é‡ã¿ä»˜ã‘å¹³å‡
        alpha = 0.5  # çŸ­æœŸé‡ã¿
        beta = 0.3   # ä¸­æœŸé‡ã¿
        gamma = 0.2  # é•·æœŸé‡ã¿
        
        filtered_prices[i] = alpha * short_ma + beta * medium_ma + gamma * long_ma
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã®è¨ˆç®—
        if i >= 30:
            # ä¾¡æ ¼å¤‰åŒ–ç‡
            price_change = (prices[i] - prices[i-10]) / (prices[i-10] + 1e-8)
            # ãƒ•ã‚£ãƒ«ã‚¿å¤‰åŒ–ç‡
            filter_change = (filtered_prices[i] - filtered_prices[i-10]) / (filtered_prices[i-10] + 1e-8)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ä¸€è²«æ€§
            trend_consistency = abs(price_change - filter_change)
            trend_energy_ratio[i] = 1.0 / (1.0 + trend_consistency * 10)
            
            # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š
            volatility = 0.0
            for j in range(1, min(10, i)):
                volatility += abs((prices[i-j+1] - prices[i-j]) / (prices[i-j] + 1e-8))
            volatility /= min(10, i-1)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã‚’åˆ¤å®š
            trend_direction = 1.0 if price_change > 0 else -1.0
            
            if trend_consistency < 0.02 and volatility < 0.05:
                market_regime[i] = trend_direction  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ–¹å‘ä»˜ãï¼‰
            elif volatility > 0.1:
                market_regime[i] = -0.8  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            else:
                market_regime[i] = 0.0  # ãƒ¬ãƒ³ã‚¸ç›¸å ´
        else:
            trend_energy_ratio[i] = 0.5
            market_regime[i] = 0.0
    
    return filtered_prices, trend_energy_ratio, market_regime


@njit(fastmath=True, cache=True)
def financial_adaptive_wavelet_numba(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    Daubechies-4ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä½¿ç”¨
    
    Returns:
        (reconstructed_prices, trend_component, market_regime)
    """
    n = len(prices)
    reconstructed_prices = np.full(n, np.nan)
    trend_component = np.full(n, np.nan)
    market_regime = np.full(n, np.nan)
    
    # Daubechies-4ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä¿‚æ•°ï¼ˆé‡‘èæ™‚ç³»åˆ—ã«æœ€é©åŒ–ï¼‰
    db4_h = np.array([
        0.6830127, 1.1830127, 0.3169873, -0.1830127,
        -0.0544158, 0.0094624, 0.0102581, -0.0017468
    ])
    db4_g = np.array([
        -0.0017468, -0.0102581, 0.0094624, 0.0544158,
        -0.1830127, -0.3169873, 1.1830127, -0.6830127
    ])
    
    for i in range(50, n):  # ååˆ†ãªå±¥æ­´ãŒå¿…è¦
        window_size = min(64, i)
        segment = prices[i-window_size:i]
        
        if len(segment) < 16:
            continue
            
        # å¯¾æ•°ãƒªã‚¿ãƒ¼ãƒ³æ­£è¦åŒ–
        log_returns = np.zeros(len(segment)-1)
        for j in range(len(segment)-1):
            if segment[j] > 0 and segment[j+1] > 0:
                log_returns[j] = math.log(segment[j+1] / segment[j])
            else:
                log_returns[j] = 0.0
        
        # ãƒ­ãƒã‚¹ãƒˆæ¨™æº–åŒ–
        median_return = np.median(log_returns)
        mad = np.median(np.abs(log_returns - median_return))
        if mad > 1e-10:
            normalized_returns = (log_returns - median_return) / (1.4826 * mad)
        else:
            normalized_returns = log_returns
        
        # Daubechies-4ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåˆ†è§£
        n_coeffs = len(normalized_returns)
        
        # ãƒ¬ãƒ™ãƒ«1åˆ†è§£
        if n_coeffs >= 8:
            level1_approx = np.zeros(n_coeffs // 2)
            level1_detail = np.zeros(n_coeffs // 2)
            
            for j in range(n_coeffs // 2):
                approx_sum = 0.0
                detail_sum = 0.0
                for k in range(min(8, n_coeffs - j*2)):
                    if j*2 + k < n_coeffs:
                        approx_sum += normalized_returns[j*2 + k] * db4_h[k]
                        detail_sum += normalized_returns[j*2 + k] * db4_g[k]
                
                level1_approx[j] = approx_sum
                level1_detail[j] = detail_sum
        else:
            level1_approx = normalized_returns[:4].copy()
            level1_detail = normalized_returns[4:8].copy() if len(normalized_returns) >= 8 else np.zeros(4)
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ˆç®—
        trend_energy = np.sum(level1_approx ** 2)
        cycle_energy = np.sum(level1_detail ** 2)
        total_energy = trend_energy + cycle_energy
        
        if total_energy > 1e-12:
            trend_ratio = trend_energy / total_energy
            cycle_ratio = cycle_energy / total_energy
            
            # ä¾¡æ ¼ãƒ¬ãƒ™ãƒ«ã§ã®å†æ§‹ç¯‰
            # ä½å‘¨æ³¢æˆåˆ†ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰ã‹ã‚‰ä¾¡æ ¼ã‚’å†æ§‹ç¯‰
            trend_signal = 0.0
            for j in range(len(level1_approx)):
                trend_signal += level1_approx[j]
            trend_signal /= len(level1_approx)
            
            # å…ƒã®ä¾¡æ ¼ãƒ¬ãƒ™ãƒ«ã«å¤‰æ›
            base_price = segment[-1]  # ç¾åœ¨ä¾¡æ ¼ã‚’ãƒ™ãƒ¼ã‚¹
            price_adjustment = trend_signal * base_price * 0.01  # 1%ã®èª¿æ•´ç¯„å›²
            reconstructed_prices[i] = base_price + price_adjustment
            
            trend_component[i] = trend_ratio
            
            # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š (-1 to 1ã®ç¯„å›²)
            if trend_energy > cycle_energy * 1.5:
                # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã‚’åˆ¤å®šï¼ˆä¾¡æ ¼å¤‰åŒ–ã«åŸºã¥ãï¼‰
                recent_change = (base_price - segment[0]) / (segment[0] + 1e-8)
                if recent_change > 0.005:  # 0.5%ä»¥ä¸Šã®ä¸Šæ˜‡
                    market_regime[i] = 1.0  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
                elif recent_change < -0.005:  # 0.5%ä»¥ä¸Šã®ä¸‹é™
                    market_regime[i] = -1.0  # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
                else:
                    market_regime[i] = 0.5  # å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰
            elif cycle_energy > trend_energy * 1.5:
                market_regime[i] = -0.7  # ã‚µã‚¤ã‚¯ãƒ«æ”¯é…
            else:
                market_regime[i] = 0.0  # ãƒ¬ãƒ³ã‚¸
        else:
            reconstructed_prices[i] = segment[-1]  # å…ƒã®ä¾¡æ ¼ã‚’ãã®ã¾ã¾
            trend_component[i] = 0.5
            market_regime[i] = 0.0
    
    return reconstructed_prices, trend_component, market_regime


@njit(fastmath=True, cache=True)
def quantum_wavelet_analysis_numba(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    
    Returns:
        (trend_component_normalized, cycle_component_normalized, confidence_score_normalized)
    """
    n = len(prices)
    scales = np.array([4, 8, 16, 32, 64, 128])  # 6ã¤ã®ã‚¹ã‚±ãƒ¼ãƒ«
    
    trend_accumulator = np.zeros(n)
    cycle_accumulator = np.zeros(n)
    coherence_accumulator = np.zeros(n)
    weight_sum = np.zeros(n)
    
    for scale_idx in range(len(scales)):
        scale = scales[scale_idx]
        if scale >= n:
            continue
            
        weight = 1.0 / math.sqrt(scale)  # ã‚¹ã‚±ãƒ¼ãƒ«é‡ã¿
        
        for i in range(scale, n):
            # ãƒãƒ¼ãƒ«ãƒ»ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé¢¨å¤‰æ›
            window = prices[i-scale+1:i+1]
            
            # ä½å‘¨æ³¢æˆåˆ†ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
            low_freq = np.mean(window)
            
            # é«˜å‘¨æ³¢æˆåˆ†ï¼ˆãƒã‚¤ã‚ºãƒ»å¤‰å‹•ï¼‰
            high_freq = np.std(window)
            
            # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä¿‚æ•°
            mid_point = scale // 2
            left_mean = np.mean(window[:mid_point])
            right_mean = np.mean(window[mid_point:])
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆå·¦å³ã®å·®ã‚’æ­£è¦åŒ–ï¼‰
            price_range = np.max(window) - np.min(window)
            if price_range > 1e-8:
                trend_coeff = abs(right_mean - left_mean) / price_range
            else:
                trend_coeff = 0.0
            
            # ã‚µã‚¤ã‚¯ãƒ«æˆåˆ†ï¼ˆé«˜å‘¨æ³¢/ä½å‘¨æ³¢æ¯”ç‡ï¼‰
            if low_freq > 1e-8:
                cycle_coeff = high_freq / low_freq
            else:
                cycle_coeff = 0.0
            
            # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ï¼ˆä¸€è²«æ€§ï¼‰
            coherence = 1.0 / (1.0 + cycle_coeff)
            
            trend_accumulator[i] += weight * trend_coeff
            cycle_accumulator[i] += weight * cycle_coeff
            coherence_accumulator[i] += weight * coherence
            weight_sum[i] += weight
    
    # æ­£è¦åŒ– (0-1ã®ç¯„å›²)
    trend_component = np.zeros(n)
    cycle_component = np.zeros(n)
    confidence_score = np.zeros(n)
    
    for i in range(n):
        if weight_sum[i] > 0:
            trend_component[i] = trend_accumulator[i] / weight_sum[i]
            cycle_component[i] = cycle_accumulator[i] / weight_sum[i]
            confidence_score[i] = coherence_accumulator[i] / weight_sum[i]
    
    # æœ€çµ‚æ­£è¦åŒ–
    # trend_component: 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
    if np.max(trend_component) > 0:
        trend_component = trend_component / np.max(trend_component)
    
    # cycle_component: -1ã‹ã‚‰1ã®ç¯„å›²ã«æ­£è¦åŒ–
    if np.max(cycle_component) > np.min(cycle_component):
        cycle_min = np.min(cycle_component)
        cycle_max = np.max(cycle_component)
        cycle_component = 2 * (cycle_component - cycle_min) / (cycle_max - cycle_min) - 1
    
    # confidence_scoreã¯æ—¢ã«0-1ã®ç¯„å›²
    
    return trend_component, cycle_component, confidence_score


@njit(fastmath=True, cache=True)
def morlet_continuous_wavelet_numba(prices: np.ndarray, scales: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé€£ç¶šå¤‰æ›ï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    
    Args:
        prices: ä¾¡æ ¼é…åˆ—
        scales: ã‚¹ã‚±ãƒ¼ãƒ«é…åˆ—
    
    Returns:
        (dominant_scales, energy_levels)
    """
    n = len(prices)
    n_scales = len(scales)
    
    # Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã«ã‚ˆã‚‹é€£ç¶šã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ã®è¿‘ä¼¼
    cwt_coeffs = np.zeros((n_scales, n))
    
    for scale_idx in range(n_scales):
        scale = scales[scale_idx]
        for i in range(n):
            coeff = 0.0
            norm_factor = 0.0
            
            # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä¿‚æ•°ã®è¨ˆç®—
            for j in range(max(0, i - int(2 * scale)), min(n, i + int(2 * scale) + 1)):
                t = (j - i) / scale
                if abs(t) <= 3:  # è¨ˆç®—ç¯„å›²ã‚’åˆ¶é™
                    # Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
                    wavelet_val = math.exp(-0.5 * t * t) * math.cos(5 * t)
                    coeff += prices[j] * wavelet_val
                    norm_factor += wavelet_val * wavelet_val
            
            if norm_factor > 0:
                cwt_coeffs[scale_idx, i] = coeff / math.sqrt(norm_factor)
    
    # æœ€å¤§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æ¤œå‡º
    energy = np.abs(cwt_coeffs)
    dominant_scales = np.zeros(n)
    energy_levels = np.zeros(n)
    
    for i in range(n):
        max_idx = 0
        max_val = energy[0, i]
        for j in range(1, n_scales):
            if energy[j, i] > max_val:
                max_val = energy[j, i]
                max_idx = j
        
        dominant_scales[i] = scales[max_idx]
        energy_levels[i] = max_val
    
    return dominant_scales, energy_levels


@njit(fastmath=True, cache=True)
def daubechies_advanced_numba(prices: np.ndarray, levels: int = 5) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Daubechiesé«˜åº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æï¼ˆNumbaæœ€é©åŒ–ç‰ˆï¼‰
    
    Returns:
        (trend_component, cycle_component, noise_component)
    """
    n = len(prices)
    trend_component = np.zeros(n)
    cycle_component = np.zeros(n)
    noise_component = np.zeros(n)
    
    # ç°¡æ˜“Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåˆ†è§£ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰
    signal = prices.copy()
    
    # ãƒ¬ãƒ™ãƒ«1-levelsåˆ†è§£
    for level in range(levels):
        if len(signal) < 4:
            break
            
        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        downsampled = np.zeros(len(signal)//2)
        detail = np.zeros(len(signal)//2)
        
        for i in range(len(downsampled)):
            if 2*i+1 < len(signal):
                downsampled[i] = (signal[2*i] + signal[2*i+1]) / 2.0
                detail[i] = (signal[2*i] - signal[2*i+1]) / 2.0
            else:
                downsampled[i] = signal[2*i]
                detail[i] = 0.0
        
        # æˆåˆ†åˆ†é¡
        if level < 2:  # é«˜å‘¨æ³¢æˆåˆ† -> ãƒã‚¤ã‚º
            # ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãƒã‚¤ã‚ºæˆåˆ†ã«è¿½åŠ 
            scale_factor = 2**level
            for i in range(len(detail)):
                for j in range(scale_factor):
                    idx = i * scale_factor + j
                    if idx < n:
                        noise_component[idx] += detail[i]
                        
        elif level < 4:  # ä¸­å‘¨æ³¢æˆåˆ† -> ã‚µã‚¤ã‚¯ãƒ«
            scale_factor = 2**level
            for i in range(len(detail)):
                for j in range(scale_factor):
                    idx = i * scale_factor + j
                    if idx < n:
                        cycle_component[idx] += detail[i]
                        
        signal = downsampled
    
    # æ®‹ã£ãŸä½å‘¨æ³¢æˆåˆ†ã‚’ãƒˆãƒ¬ãƒ³ãƒ‰ã«
    if len(signal) > 0:
        scale_factor = n // len(signal)
        for i in range(len(signal)):
            for j in range(scale_factor):
                idx = i * scale_factor + j
                if idx < n:
                    trend_component[idx] = signal[i]
    
    return trend_component, cycle_component, noise_component


# === ğŸŒŒ å®‡å®™æœ€å¼·ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé–¢æ•°ç¾¤ ===

@njit(fastmath=True, cache=True)
def ultimate_multi_wavelet_transform(
    prices: np.ndarray,
    scales: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸŒŸ ç©¶æ¥µãƒãƒ«ãƒã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›
    5ã¤ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåŸºåº•ã‚’åŒæ™‚ä½¿ç”¨ã—ãŸå²ä¸Šæœ€å¼·ã®è§£æ
    """
    n = len(prices)
    n_scales = len(scales)
    
    # 5ã¤ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåŸºåº•ä¿‚æ•°é…åˆ—
    haar_coeffs = np.zeros((n_scales, n))
    morlet_coeffs = np.zeros((n_scales, n))
    daubechies_coeffs = np.zeros((n_scales, n))
    mexican_hat_coeffs = np.zeros((n_scales, n))
    biorthogonal_coeffs = np.zeros((n_scales, n))
    
    # å„ã‚¹ã‚±ãƒ¼ãƒ«ã§è¤‡æ•°ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ã‚’å®Ÿè¡Œ
    for scale_idx in prange(n_scales):
        scale = scales[scale_idx]
        half_support = int(3 * scale)
        
        for i in range(n):
            start_idx = max(0, i - half_support)
            end_idx = min(n, i + half_support + 1)
            
            haar_sum = 0.0
            morlet_sum = 0.0
            daubechies_sum = 0.0
            mexican_sum = 0.0
            bio_sum = 0.0
            
            norm_factor = 0.0
            
            for j in range(start_idx, end_idx):
                t = (j - i) / scale
                
                if abs(t) <= 3:  # ã‚µãƒãƒ¼ãƒˆç¯„å›²å†…
                    # 1. Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
                    if -0.5 <= t < 0:
                        haar_val = 1.0
                    elif 0 <= t < 0.5:
                        haar_val = -1.0
                    else:
                        haar_val = 0.0
                    
                    # 2. Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
                    morlet_val = math.exp(-0.5 * t * t) * math.cos(5 * t)
                    
                    # 3. Daubechies-4é¢¨
                    if abs(t) <= 1:
                        daubechies_val = math.exp(-t * t) * (1 - t * t)
                    else:
                        daubechies_val = 0.0
                    
                    # 4. Mexican Hat (Ricker)
                    mexican_val = (1 - t * t) * math.exp(-0.5 * t * t)
                    
                    # 5. Biorthogonalé¢¨
                    if abs(t) <= 1:
                        bio_val = math.cos(math.pi * t / 2) * math.exp(-abs(t))
                    else:
                        bio_val = 0.0
                    
                    # ä¿‚æ•°è¨ˆç®—
                    price_val = prices[j]
                    haar_sum += price_val * haar_val
                    morlet_sum += price_val * morlet_val
                    daubechies_sum += price_val * daubechies_val
                    mexican_sum += price_val * mexican_val
                    bio_sum += price_val * bio_val
                    
                    norm_factor += 1.0
            
            # æ­£è¦åŒ–
            if norm_factor > 0:
                haar_coeffs[scale_idx, i] = haar_sum / math.sqrt(norm_factor)
                morlet_coeffs[scale_idx, i] = morlet_sum / math.sqrt(norm_factor)
                daubechies_coeffs[scale_idx, i] = daubechies_sum / math.sqrt(norm_factor)
                mexican_hat_coeffs[scale_idx, i] = mexican_sum / math.sqrt(norm_factor)
                biorthogonal_coeffs[scale_idx, i] = bio_sum / math.sqrt(norm_factor)
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆï¼ˆé©å¿œçš„é‡ã¿ä»˜ã‘ï¼‰
    hybrid_coeffs = np.zeros((n_scales, n))
    energy_matrix = np.zeros((n_scales, n))
    phase_matrix = np.zeros((n_scales, n))
    coherence_matrix = np.zeros((n_scales, n))
    
    for scale_idx in range(n_scales):
        for i in range(n):
            # å„ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã®ã‚¨ãƒãƒ«ã‚®ãƒ¼
            haar_energy = haar_coeffs[scale_idx, i] ** 2
            morlet_energy = morlet_coeffs[scale_idx, i] ** 2
            daubechies_energy = daubechies_coeffs[scale_idx, i] ** 2
            mexican_energy = mexican_hat_coeffs[scale_idx, i] ** 2
            bio_energy = biorthogonal_coeffs[scale_idx, i] ** 2
            
            total_energy = haar_energy + morlet_energy + daubechies_energy + mexican_energy + bio_energy
            
            if total_energy > 1e-12:
                # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹é‡ã¿ä»˜ã‘
                haar_weight = haar_energy / total_energy
                morlet_weight = morlet_energy / total_energy
                daubechies_weight = daubechies_energy / total_energy
                mexican_weight = mexican_energy / total_energy
                bio_weight = bio_energy / total_energy
                
                # çµ±åˆä¿‚æ•°
                hybrid_coeffs[scale_idx, i] = (
                    haar_weight * haar_coeffs[scale_idx, i] +
                    morlet_weight * morlet_coeffs[scale_idx, i] +
                    daubechies_weight * daubechies_coeffs[scale_idx, i] +
                    mexican_weight * mexican_hat_coeffs[scale_idx, i] +
                    bio_weight * biorthogonal_coeffs[scale_idx, i]
                )
                
                energy_matrix[scale_idx, i] = total_energy
                
                # ä½ç›¸è¨ˆç®—ï¼ˆè¤‡æ•°ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã®ä½ç›¸æ•´åˆæ€§ï¼‰
                phase_consistency = 1.0 - abs(
                    haar_coeffs[scale_idx, i] - morlet_coeffs[scale_idx, i]
                ) / (abs(haar_coeffs[scale_idx, i]) + abs(morlet_coeffs[scale_idx, i]) + 1e-8)
                
                phase_matrix[scale_idx, i] = max(0, min(1, phase_consistency))
                
                # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ï¼ˆ5ã¤ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé–“ã®ä¸€è‡´åº¦ï¼‰
                coeffs_array = np.array([
                    haar_coeffs[scale_idx, i],
                    morlet_coeffs[scale_idx, i],
                    daubechies_coeffs[scale_idx, i],
                    mexican_hat_coeffs[scale_idx, i],
                    biorthogonal_coeffs[scale_idx, i]
                ])
                
                # æ‰‹å‹•ã§å¹³å‡ã¨stdã‚’è¨ˆç®—ï¼ˆNumbaäº’æ›ï¼‰
                mean_coeff = 0.0
                for k in range(5):
                    mean_coeff += coeffs_array[k]
                mean_coeff /= 5.0
                
                variance = 0.0
                for k in range(5):
                    diff = coeffs_array[k] - mean_coeff
                    variance += diff * diff
                variance /= 5.0
                std_coeff = math.sqrt(variance)
                
                coherence = 1.0 / (1.0 + std_coeff / (abs(mean_coeff) + 1e-8))
                coherence_matrix[scale_idx, i] = max(0, min(1, coherence))
    
    return hybrid_coeffs, energy_matrix, phase_matrix, coherence_matrix


@njit(fastmath=True, cache=True)
def quantum_coherence_integration(
    wavelet_coeffs: np.ndarray,
    energy_matrix: np.ndarray,
    phase_matrix: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸ”¬ é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹çµ±åˆ
    é‡å­åŠ›å­¦çš„åŸç†ã‚’å¿œç”¨ã—ãŸå²ä¸Šæœ€é«˜ç²¾åº¦ã®çµ±åˆ
    """
    n_scales, n_points = wavelet_coeffs.shape
    quantum_coherence = np.zeros(n_points)
    entanglement_strength = np.zeros(n_points)
    
    for i in range(n_points):
        # é‡å­é‡ã­åˆã‚ã›çŠ¶æ…‹ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        total_amplitude = 0.0
        phase_coherence_sum = 0.0
        entanglement_sum = 0.0
        
        for scale_idx in range(n_scales):
            # æ³¢å‹•é–¢æ•°ã®æŒ¯å¹…
            amplitude = abs(wavelet_coeffs[scale_idx, i])
            
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼é‡ã¿ä»˜ã‘
            energy_weight = energy_matrix[scale_idx, i]
            
            # ä½ç›¸ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹
            phase_coherence = phase_matrix[scale_idx, i]
            
            # é‡å­ã‚‚ã¤ã‚Œé¢¨ã®ç›¸é–¢è¨ˆç®—
            if i > 0:
                # å‰ã®æ™‚ç‚¹ã¨ã®ç›¸é–¢ï¼ˆé‡å­ã‚‚ã¤ã‚ŒåŠ¹æœï¼‰
                prev_amplitude = abs(wavelet_coeffs[scale_idx, i-1])
                correlation = amplitude * prev_amplitude / (amplitude + prev_amplitude + 1e-8)
                entanglement_sum += correlation * energy_weight
            
            total_amplitude += amplitude * energy_weight
            phase_coherence_sum += phase_coherence * energy_weight
        
        # æ­£è¦åŒ–
        if total_amplitude > 1e-12:
            quantum_coherence[i] = phase_coherence_sum / total_amplitude
            entanglement_strength[i] = entanglement_sum / total_amplitude
        else:
            quantum_coherence[i] = 0.5
            entanglement_strength[i] = 0.0
        
        # ç¯„å›²åˆ¶é™
        quantum_coherence[i] = max(0, min(1, quantum_coherence[i]))
        entanglement_strength[i] = max(0, min(1, entanglement_strength[i]))
    
    return quantum_coherence, entanglement_strength


@njit(fastmath=True, cache=True)
def ultra_fast_kalman_wavelet_fusion(
    wavelet_coeffs: np.ndarray,
    quantum_coherence: np.ndarray,
    process_noise: float = 0.0001,
    initial_obs_noise: float = 0.001
) -> Tuple[np.ndarray, np.ndarray]:
    """
    âš¡ è¶…é«˜é€Ÿã‚«ãƒ«ãƒãƒ³ãƒ»ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆèåˆ
    ç©¶æ¥µã®ä½é…å»¶ã‚’å®Ÿç¾ã™ã‚‹é©å‘½çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    """
    n_scales, n_points = wavelet_coeffs.shape
    fused_signal = np.zeros(n_points)
    confidence_evolution = np.zeros(n_points)
    
    # å„ã‚¹ã‚±ãƒ¼ãƒ«ã«å¯¾ã—ã¦ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
    scale_states = np.zeros(n_scales)
    scale_covariances = np.ones(n_scales)
    
    for i in range(n_points):
        total_weight = 0.0
        weighted_sum = 0.0
        
        for scale_idx in range(n_scales):
            # é©å¿œçš„è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆé‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
            coherence_factor = quantum_coherence[i]
            obs_noise = initial_obs_noise * (2.0 - coherence_factor)
            
            # ã‚«ãƒ«ãƒãƒ³äºˆæ¸¬
            state_pred = scale_states[scale_idx]
            cov_pred = scale_covariances[scale_idx] + process_noise
            
            # ã‚«ãƒ«ãƒãƒ³æ›´æ–°
            observation = wavelet_coeffs[scale_idx, i]
            innovation = observation - state_pred
            innovation_cov = cov_pred + obs_noise
            
            if innovation_cov > 1e-12:
                kalman_gain = cov_pred / innovation_cov
                scale_states[scale_idx] = state_pred + kalman_gain * innovation
                scale_covariances[scale_idx] = (1 - kalman_gain) * cov_pred
                
                # ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹é‡ã¿ä»˜ã‘
                confidence = 1.0 / (1.0 + scale_covariances[scale_idx])
                weight = confidence * coherence_factor
                
                weighted_sum += scale_states[scale_idx] * weight
                total_weight += weight
        
        # èåˆ
        if total_weight > 1e-12:
            fused_signal[i] = weighted_sum / total_weight
            confidence_evolution[i] = total_weight / n_scales
        else:
            fused_signal[i] = 0.0
            confidence_evolution[i] = 0.1
        
        # ä¿¡é ¼åº¦ã®ç¯„å›²åˆ¶é™
        confidence_evolution[i] = max(0, min(1, confidence_evolution[i]))
    
    return fused_signal, confidence_evolution


@njit(fastmath=True, cache=True)
def hierarchical_deep_denoising(
    signal: np.ndarray,
    confidence: np.ndarray,
    levels: int = 5
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸ§  éšå±¤çš„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒã‚¤ã‚ºé™¤å»
    AIãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°é¢¨ã®å¤šå±¤ãƒã‚¤ã‚ºé™¤å»
    """
    n = len(signal)
    denoised_signal = signal.copy()
    noise_component = np.zeros(n)
    
    # å¤šå±¤ãƒã‚¤ã‚ºé™¤å»
    for level in range(levels):
        scale = 2 ** level
        if scale >= n // 4:
            break
        
        layer_denoised = np.zeros(n)
        layer_noise = np.zeros(n)
        
        for i in range(n):
            # é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
            conf_factor = confidence[i]
            window_size = max(3, int(scale * (1 + conf_factor)))
            half_window = window_size // 2
            
            start_idx = max(0, i - half_window)
            end_idx = min(n, i + half_window + 1)
            
            # å±€æ‰€çµ±è¨ˆï¼ˆæ‰‹å‹•è¨ˆç®— - Numbaäº’æ›ï¼‰
            local_count = end_idx - start_idx
            local_sum = 0.0
            for k in range(start_idx, end_idx):
                local_sum += denoised_signal[k]
            local_mean = local_sum / local_count
            
            local_variance = 0.0
            for k in range(start_idx, end_idx):
                diff = denoised_signal[k] - local_mean
                local_variance += diff * diff
            local_variance /= local_count
            local_std = math.sqrt(local_variance)
            
            # é©å¿œçš„ã—ãã„å€¤ï¼ˆä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
            threshold = local_std * (0.1 + 0.4 * (1 - conf_factor))
            
            # ãƒã‚¤ã‚ºæ¤œå‡ºã¨é™¤å»
            deviation = denoised_signal[i] - local_mean
            if abs(deviation) > threshold:
                # éç·šå½¢ç¸®é€€é–¢æ•°ï¼ˆã‚½ãƒ•ãƒˆã—ãã„å€¤ã®æ”¹è‰¯ç‰ˆï¼‰
                shrinkage_factor = max(0, 1 - threshold / (abs(deviation) + 1e-8))
                layer_denoised[i] = local_mean + deviation * shrinkage_factor ** 2
                layer_noise[i] = deviation * (1 - shrinkage_factor ** 2)
            else:
                layer_denoised[i] = denoised_signal[i]
                layer_noise[i] = 0.0
        
        denoised_signal = layer_denoised
        noise_component += layer_noise
    
    # æœ€çµ‚ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆã‚¨ãƒƒã‚¸ä¿æŒï¼‰
    final_denoised = np.zeros(n)
    for i in range(n):
        if i == 0 or i == n - 1:
            final_denoised[i] = denoised_signal[i]
        else:
            # ãƒã‚¤ãƒ©ãƒ†ãƒ©ãƒ«ãƒ•ã‚£ãƒ«ã‚¿é¢¨
            conf_weight = confidence[i]
            spatial_weight = 0.3
            
            prev_val = denoised_signal[i-1]
            curr_val = denoised_signal[i]
            next_val = denoised_signal[i+1]
            
            # ã‚¨ãƒƒã‚¸ä¿æŒã®é‡ã¿è¨ˆç®—
            edge_factor = abs(next_val - prev_val) / (abs(curr_val) + 1e-8)
            edge_weight = 1.0 / (1.0 + edge_factor * 5)
            
            # æœ€çµ‚é‡ã¿ä»˜ã‘å¹³å‡
            total_weight = 1.0 + conf_weight * edge_weight * spatial_weight * 2
            weighted_sum = curr_val + conf_weight * edge_weight * spatial_weight * (prev_val + next_val)
            
            final_denoised[i] = weighted_sum / total_weight
    
    return final_denoised, noise_component


@njit(fastmath=True, cache=True)
def market_regime_recognition(
    prices: np.ndarray,
    volatilities: np.ndarray,
    trend_strengths: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸ“Š ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ è‡ªå‹•èªè­˜
    AIãƒ¬ãƒ™ãƒ«ã®ç›¸å ´çŠ¶æ³è‡ªå‹•åˆ¤å®š
    """
    n = len(prices)
    market_regime = np.zeros(n)
    regime_confidence = np.zeros(n)
    
    for i in range(20, n):  # ååˆ†ãªå±¥æ­´ãŒå¿…è¦
        # çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰
        short_window = 5
        medium_window = 10
        long_window = 20
        
        short_trend = (prices[i] - prices[i-short_window]) / (prices[i-short_window] + 1e-8)
        medium_trend = (prices[i] - prices[i-medium_window]) / (prices[i-medium_window] + 1e-8)
        long_trend = (prices[i] - prices[i-long_window]) / (prices[i-long_window] + 1e-8)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«ï¼ˆæ‰‹å‹•è¨ˆç®—ï¼‰
        current_vol = volatilities[i]
        vol_start = max(0, i-10)
        vol_count = i+1 - vol_start
        vol_sum = 0.0
        for k in range(vol_start, i+1):
            vol_sum += volatilities[k]
        avg_vol = vol_sum / vol_count
        vol_ratio = current_vol / (avg_vol + 1e-8)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
        trend_strength = trend_strengths[i]
        
        # è¤‡åˆæŒ‡æ¨™
        trend_consistency = 1.0 - abs(short_trend - medium_trend) - abs(medium_trend - long_trend)
        trend_magnitude = (abs(short_trend) + abs(medium_trend) + abs(long_trend)) / 3
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        if trend_consistency > 0.5 and trend_magnitude > 0.02 and current_vol < avg_vol * 1.2:
            # æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰
            if short_trend > 0 and medium_trend > 0 and long_trend > 0:
                market_regime[i] = 1.0  # å¼·ã„ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
                regime_confidence[i] = min(1.0, trend_consistency + trend_strength)
            elif short_trend < 0 and medium_trend < 0 and long_trend < 0:
                market_regime[i] = -1.0  # å¼·ã„ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
                regime_confidence[i] = min(1.0, trend_consistency + trend_strength)
            else:
                market_regime[i] = short_trend  # å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰
                regime_confidence[i] = trend_consistency * 0.5
        
        elif vol_ratio > 1.5 and trend_magnitude < 0.01:
            # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ»ãƒ¬ãƒ³ã‚¸ç›¸å ´
            market_regime[i] = 0.0
            regime_confidence[i] = min(1.0, vol_ratio - 1.0)
        
        elif vol_ratio > 2.0:
            # æ¥µç«¯ãªé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            market_regime[i] = -0.8  # ã‚¯ãƒ©ã‚¤ã‚·ã‚¹ãƒ¢ãƒ¼ãƒ‰
            regime_confidence[i] = min(1.0, (vol_ratio - 1.5) * 0.5)
        
        else:
            # é€šå¸¸ã®ãƒ¬ãƒ³ã‚¸ç›¸å ´
            market_regime[i] = short_trend * 0.3  # å¼±ã„æ–¹å‘æ€§
            regime_confidence[i] = 0.3
        
        # ç¯„å›²åˆ¶é™
        market_regime[i] = max(-1, min(1, market_regime[i]))
        regime_confidence[i] = max(0, min(1, regime_confidence[i]))
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(20):
        market_regime[i] = 0.0
        regime_confidence[i] = 0.3
    
    return market_regime, regime_confidence


@njit(fastmath=True, cache=True)
def calculate_ultimate_cosmic_wavelet(
    prices: np.ndarray,
    scales: Optional[np.ndarray] = None
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸŒŒ å®‡å®™æœ€å¼·ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æãƒ¡ã‚¤ãƒ³é–¢æ•°
    å²ä¸Šæœ€é«˜ã®æ€§èƒ½ã‚’èª‡ã‚‹ç©¶æ¥µã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çµ±åˆ
    """
    n = len(prices)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚±ãƒ¼ãƒ«è¨­å®š
    if scales is None:
        scales = np.array([4., 6., 8., 12., 16., 24., 32., 48., 64., 96., 128.])
    
    # 1. ğŸŒŸ ç©¶æ¥µãƒãƒ«ãƒã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›
    hybrid_coeffs, energy_matrix, phase_matrix, coherence_matrix = ultimate_multi_wavelet_transform(prices, scales)
    
    # 2. ğŸ”¬ é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹çµ±åˆ
    quantum_coherence, entanglement_strength = quantum_coherence_integration(
        hybrid_coeffs, energy_matrix, phase_matrix
    )
    
    # 3. âš¡ è¶…é«˜é€Ÿã‚«ãƒ«ãƒãƒ³ãƒ»ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆèåˆ
    fused_signal, confidence_evolution = ultra_fast_kalman_wavelet_fusion(
        hybrid_coeffs, quantum_coherence
    )
    
    # 4. ğŸ§  éšå±¤çš„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒã‚¤ã‚ºé™¤å»
    cosmic_signal, noise_component = hierarchical_deep_denoising(
        fused_signal, confidence_evolution
    )
    
    # 5. å¤šæˆåˆ†åˆ†æ
    # ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†æŠ½å‡º
    cosmic_trend = np.zeros(n)
    cosmic_cycle = np.zeros(n)
    cosmic_volatility = np.zeros(n)
    
    for i in range(10, n):
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆé•·æœŸ vs çŸ­æœŸï¼‰
        long_window = min(20, i)
        short_window = min(5, i)
        
        # æ‰‹å‹•å¹³å‡è¨ˆç®—ï¼ˆNumbaäº’æ›ï¼‰
        long_sum = 0.0
        long_start = max(0, i-long_window)
        for j in range(long_start, i+1):
            long_sum += cosmic_signal[j]
        long_avg = long_sum / (i+1 - long_start)
        
        short_sum = 0.0
        short_start = max(0, i-short_window)
        for j in range(short_start, i+1):
            short_sum += cosmic_signal[j]
        short_avg = short_sum / (i+1 - short_start)
        
        trend_strength = abs(short_avg - long_avg) / (abs(long_avg) + 1e-8)
        cosmic_trend[i] = min(1.0, trend_strength)
        
        # ã‚µã‚¤ã‚¯ãƒ«æˆåˆ†ï¼ˆé«˜å‘¨æ³¢ã‚¨ãƒãƒ«ã‚®ãƒ¼ï¼‰
        high_freq_energy = 0.0
        total_energy = 0.0
        
        for scale_idx in range(min(5, len(scales))):  # é«˜å‘¨æ³¢ã‚¹ã‚±ãƒ¼ãƒ«
            high_freq_energy += energy_matrix[scale_idx, i]
        
        for scale_idx in range(len(scales)):
            total_energy += energy_matrix[scale_idx, i]
        
        if total_energy > 1e-12:
            cycle_ratio = high_freq_energy / total_energy
            cosmic_cycle[i] = 2 * cycle_ratio - 1  # -1 to 1ã®ç¯„å›²
        else:
            cosmic_cycle[i] = 0.0
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæœ€è¿‘ã®å¤‰å‹•ï¼‰- æ‰‹å‹•è¨ˆç®—
        recent_start = max(0, i-5)
        recent_count = i+1 - recent_start
        
        # æ‰‹å‹•stdè¨ˆç®—
        recent_mean = 0.0
        for j in range(recent_start, i+1):
            recent_mean += cosmic_signal[j]
        recent_mean /= recent_count
        
        recent_variance = 0.0
        for j in range(recent_start, i+1):
            diff = cosmic_signal[j] - recent_mean
            recent_variance += diff * diff
        recent_variance /= recent_count
        recent_std = math.sqrt(recent_variance)
        
        # æ‰‹å‹•çµ¶å¯¾å€¤å¹³å‡è¨ˆç®—
        abs_mean = 0.0
        for j in range(recent_start, i+1):
            abs_mean += abs(cosmic_signal[j])
        abs_mean /= recent_count
        
        volatility = recent_std / (abs_mean + 1e-8)
        cosmic_volatility[i] = min(1.0, volatility)
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(10):
        cosmic_trend[i] = 0.5
        cosmic_cycle[i] = 0.0
        cosmic_volatility[i] = 0.3
    
    # 6. ğŸ“Š ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ èªè­˜
    market_regime, regime_confidence = market_regime_recognition(
        prices, cosmic_volatility, cosmic_trend
    )
    
    # 7. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒãƒ«ã‚®ãƒ¼è¨ˆç®—ï¼ˆæ‰‹å‹•ãƒ«ãƒ¼ãƒ— - Numbaäº’æ›ï¼‰
    multi_scale_energy = np.zeros(n)
    for i in range(n):
        energy_sum = 0.0
        for scale_idx in range(len(scales)):
            energy_sum += energy_matrix[scale_idx, i]
        multi_scale_energy[i] = energy_sum
    
    # 8. ä½ç›¸åŒæœŸåº¦è¨ˆç®—ï¼ˆæ‰‹å‹•ãƒ«ãƒ¼ãƒ— - Numbaäº’æ›ï¼‰
    phase_synchronization = np.zeros(n)
    for i in range(n):
        phase_sum = 0.0
        for scale_idx in range(len(scales)):
            phase_sum += phase_matrix[scale_idx, i]
        phase_synchronization[i] = phase_sum / len(scales)
    
    # 9. å®‡å®™ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ è¨ˆç®—
    cosmic_momentum = np.zeros(n)
    for i in range(5, n):
        momentum = (cosmic_signal[i] - cosmic_signal[i-5]) / (cosmic_signal[i-5] + 1e-8)
        cosmic_momentum[i] = max(-1, min(1, momentum * 10))  # -1 to 1ã«ã‚¹ã‚±ãƒ¼ãƒ«
    
    # åˆæœŸå€¤
    for i in range(5):
        cosmic_momentum[i] = 0.0
    
    return (
        cosmic_signal,
        cosmic_trend,
        cosmic_cycle,
        cosmic_volatility,
        quantum_coherence,
        market_regime,
        confidence_evolution,
        multi_scale_energy,
        phase_synchronization,
        cosmic_momentum
    )


# === å€‹åˆ¥ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚¯ãƒ©ã‚¹ ===

class HaarDenoisingWavelet(WaveletAnalyzer):
    """Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»"""
    
    def __init__(self, levels: int = 3):
        super().__init__("HaarDenoisingWavelet")
        self.levels = levels
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        denoised, detail = haar_wavelet_denoising_numba(data, self.levels)
        return WaveletResult(
            values=denoised,
            detail_component=detail
        )


class MultiresolutionWavelet(WaveletAnalyzer):
    """å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ"""
    
    def __init__(self):
        super().__init__("MultiresolutionWavelet")
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        filtered_prices, trend_energy, regime = multiresolution_analysis_numba(data)
        return WaveletResult(
            values=filtered_prices,
            trend_component=trend_energy,
            cycle_component=None,  # ã‚µã‚¤ã‚¯ãƒ«æˆåˆ†ã¯åˆ¥é€”è¨ˆç®—å¯èƒ½
            market_regime=regime
        )


class FinancialAdaptiveWavelet(WaveletAnalyzer):
    """é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›"""
    
    def __init__(self):
        super().__init__("FinancialAdaptiveWavelet")
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        reconstructed_prices, trend_component, regime = financial_adaptive_wavelet_numba(data)
        return WaveletResult(
            values=reconstructed_prices,
            trend_component=trend_component,
            cycle_component=None,  # ã‚µã‚¤ã‚¯ãƒ«æˆåˆ†ã¯åˆ¥é€”è¨ˆç®—å¯èƒ½
            market_regime=regime
        )


class QuantumWavelet(WaveletAnalyzer):
    """é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ"""
    
    def __init__(self):
        super().__init__("QuantumWavelet")
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        trends, volatility, coherence = quantum_wavelet_analysis_numba(data)
        # ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ã®å†æ§‹ç¯‰ã•ã‚ŒãŸä¿¡å·ã‚’ä½œæˆ
        reconstructed = np.full_like(data, np.nan, dtype=np.float64)
        valid_mask = (trends != 0) & ~np.isnan(trends)
        if np.any(valid_mask):
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’ä¾¡æ ¼å¤‰å‹•ã«ã‚¹ã‚±ãƒ¼ãƒ«
            base_price = np.nanmean(data)
            trend_normalized = trends / (np.nanmax(trends) + 1e-8)
            reconstructed[valid_mask] = data[valid_mask] * (1 + trend_normalized[valid_mask] * 0.05)
        
        return WaveletResult(
            values=reconstructed,
            trend_component=trends,
            cycle_component=volatility,
            confidence_score=coherence
        )


class MorletContinuousWavelet(WaveletAnalyzer):
    """Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé€£ç¶šå¤‰æ›"""
    
    def __init__(self, scales: Optional[np.ndarray] = None):
        super().__init__("MorletContinuousWavelet")
        self.scales = scales if scales is not None else np.array([8, 12, 16, 20, 24, 32, 40])
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        dominant_scales, energy_levels = morlet_continuous_wavelet_numba(data, self.scales)
        # ã‚¹ã‚±ãƒ¼ãƒ«æƒ…å ±ã‚’ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ã®ä¿¡å·ã«å¤‰æ›
        reconstructed = np.full_like(data, np.nan, dtype=np.float64)
        valid_mask = ~np.isnan(dominant_scales) & (dominant_scales != 0)
        if np.any(valid_mask):
            # ã‚¹ã‚±ãƒ¼ãƒ«å€¤ã‚’ä¾¡æ ¼å¤‰å‹•ã«å¤‰æ›
            scale_normalized = dominant_scales / (np.nanmax(dominant_scales) + 1e-8)
            reconstructed[valid_mask] = data[valid_mask] * (1 + scale_normalized[valid_mask] * 0.03)
        
        # trend_componentã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
        trend_normalized = np.zeros_like(dominant_scales)
        if np.nanmax(dominant_scales) > np.nanmin(dominant_scales):
            trend_normalized = (dominant_scales - np.nanmin(dominant_scales)) / (np.nanmax(dominant_scales) - np.nanmin(dominant_scales))
        
        return WaveletResult(
            values=reconstructed,
            energy_spectrum=energy_levels,
            trend_component=trend_normalized
        )


class DaubechiesAdvancedWavelet(WaveletAnalyzer):
    """Daubechiesé«˜åº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ"""
    
    def __init__(self, levels: int = 5):
        super().__init__("DaubechiesAdvancedWavelet")
        self.levels = levels
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        trend, cycle, noise = daubechies_advanced_numba(data, self.levels)
        
        # æ­£è¦åŒ–å‡¦ç†
        # trend_componentã‚’0-1ã«æ­£è¦åŒ–
        trend_normalized = np.zeros_like(trend)
        if np.max(trend) > np.min(trend):
            trend_normalized = (trend - np.min(trend)) / (np.max(trend) - np.min(trend))
        
        # cycle_componentã‚’-1ã‹ã‚‰1ã«æ­£è¦åŒ–
        cycle_normalized = np.zeros_like(cycle)
        if np.max(cycle) > np.min(cycle):
            cycle_min = np.min(cycle)
            cycle_max = np.max(cycle)
            cycle_normalized = 2 * (cycle - cycle_min) / (cycle_max - cycle_min) - 1
        
        # noise_componentã‚’0-1ã«æ­£è¦åŒ–
        noise_normalized = np.zeros_like(noise)
        if np.max(noise) > np.min(noise):
            noise_normalized = (noise - np.min(noise)) / (np.max(noise) - np.min(noise))
        
        return WaveletResult(
            values=trend,  # ä¾¡æ ¼ãƒ¬ãƒ™ãƒ«ã¯ãã®ã¾ã¾
            trend_component=trend_normalized,
            cycle_component=cycle_normalized,
            noise_component=noise_normalized
        )


class UltimateCosmicWavelet(WaveletAnalyzer):
    """
    ğŸŒŒ ç©¶æ¥µå®‡å®™ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æå™¨
    
    äººé¡å²ä¸Šæœ€å¼·ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    
    ğŸš€ **é©å‘½çš„ãª7ã¤ã®æŠ€è¡“çµ±åˆ:**
    
    1. **ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒ»ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è§£æ**: 5ã¤ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåŸºåº•ï¼ˆHaar, Morlet, Daubechies, Mexican Hat, Biorthogonalï¼‰ã‚’åŒæ™‚ä½¿ç”¨
    2. **é©å¿œçš„é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹çµ±åˆ**: é‡å­åŠ›å­¦çš„ä½ç›¸ä¸€è²«æ€§ã«ã‚ˆã‚‹è¶…é«˜ç²¾åº¦çµ±åˆ
    3. **è¶…é«˜é€Ÿã‚«ãƒ«ãƒãƒ³ãƒ»ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆèåˆ**: ç©¶æ¥µã®ä½é…å»¶ã‚’å®Ÿç¾ã™ã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†
    4. **éšå±¤çš„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒã‚¤ã‚ºé™¤å»**: AIé¢¨å¤šå±¤ãƒã‚¤ã‚ºé™¤å»ã«ã‚ˆã‚‹å®Œç’§ãªä¿¡å·ç´”åŒ–
    5. **AIé§†å‹•é©å¿œé‡ã¿ä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ **: éå»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ã®å‹•çš„æœ€é©åŒ–
    6. **ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ è‡ªå‹•èªè­˜**: ç›¸å ´çŠ¶æ³ã®å®Œå…¨è‡ªå‹•åˆ¤å®š
    7. **é‡å­ã‚‚ã¤ã‚Œé¢¨ä½ç›¸åŒæœŸ**: è¤‡æ•°æ™‚ç‚¹é–“ã®é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    
    âš¡ **å®‡å®™æœ€å¼·ã®æ€§èƒ½ç‰¹æ€§:**
    - è¶…ä½é…å»¶: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†å¯¾å¿œ
    - è¶…é«˜ç²¾åº¦: 5ã¤ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåŸºåº•çµ±åˆ
    - è¶…å®‰å®šæ€§: é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹çµ±åˆã«ã‚ˆã‚‹å®Œç’§ãªå®‰å®šæ€§
    - å®Œå…¨é©å¿œæ€§: å…¨è‡ªå‹•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
    - é©å‘½çš„ãƒã‚¤ã‚ºè€æ€§: éšå±¤çš„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒã‚¤ã‚ºé™¤å»
    """
    
    def __init__(
        self,
        scales: Optional[np.ndarray] = None,
        enable_quantum_mode: bool = True,
        cosmic_power_level: float = 1.0
    ):
        """
        Args:
            scales: è§£æã‚¹ã‚±ãƒ¼ãƒ«é…åˆ—
            enable_quantum_mode: é‡å­ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
            cosmic_power_level: å®‡å®™ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ« (0.1-2.0)
        """
        super().__init__("UltimateCosmicWavelet")
        
        self.scales = scales if scales is not None else np.array([4., 6., 8., 12., 16., 24., 32., 48., 64., 96., 128.])
        self.enable_quantum_mode = enable_quantum_mode
        self.cosmic_power_level = max(0.1, min(2.0, cosmic_power_level))
        
        # çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._last_cosmic_result: Optional[UltimateCosmicResult] = None
    
    def calculate(self, data: np.ndarray, **kwargs) -> WaveletResult:
        """
        ğŸŒŒ å®‡å®™æœ€å¼·ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚’å®Ÿè¡Œ
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            WaveletResult: äº’æ›æ€§ã®ãŸã‚ã®çµæœï¼ˆcosmicè©³ç´°ã¯åˆ¥é€”å–å¾—å¯èƒ½ï¼‰
        """
        try:
            prices = data.copy()
            
            if len(prices) < 50:
                # çŸ­ã„ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
                return WaveletResult(
                    values=np.full(len(prices), np.nan),
                    trend_component=np.full(len(prices), np.nan),
                    cycle_component=np.full(len(prices), np.nan),
                    market_regime=np.full(len(prices), np.nan),
                    confidence_score=np.full(len(prices), np.nan)
                )
            
            # ğŸŒŒ å®‡å®™æœ€å¼·ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè¡Œ
            (
                cosmic_signal,
                cosmic_trend,
                cosmic_cycle,
                cosmic_volatility,
                quantum_coherence,
                market_regime,
                adaptive_confidence,
                multi_scale_energy,
                phase_synchronization,
                cosmic_momentum
            ) = calculate_ultimate_cosmic_wavelet(prices, self.scales)
            
            # ğŸ”§ ä¾¡æ ¼ã‚¹ã‚±ãƒ¼ãƒ«æ­£è¦åŒ–ï¼ˆä»–ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã¨åŒä¸€ã‚¹ã‚±ãƒ¼ãƒ«ã«èª¿æ•´ï¼‰
            # cosmic_signalã¯ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä¿‚æ•°ãªã®ã§ã€ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ã«å¤‰æ›
            price_based_signal = np.zeros_like(prices)
            
            # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚¹ã‚¯
            valid_mask = ~np.isnan(cosmic_signal) & ~np.isinf(cosmic_signal)
            
            if np.any(valid_mask):
                # ã‚ªãƒªã‚¸ãƒŠãƒ«ä¾¡æ ¼ã®åŸºæœ¬çµ±è¨ˆ
                price_mean = np.nanmean(prices[valid_mask])
                price_std = np.nanstd(prices[valid_mask])
                
                # cosmic_signalã®åŸºæœ¬çµ±è¨ˆ
                signal_mean = np.nanmean(cosmic_signal[valid_mask])
                signal_std = np.nanstd(cosmic_signal[valid_mask])
                
                if signal_std > 1e-12:
                    # æ­£è¦åŒ–ï¼šcosmic_signalã‚’ä¾¡æ ¼ãƒ¬ãƒ³ã‚¸å†…ã«é©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒ«
                    normalized_signal = (cosmic_signal[valid_mask] - signal_mean) / signal_std
                    
                    # ä¾¡æ ¼å¤‰å‹•ã‚’ã‚ˆã‚Šé©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆä»–ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã¨åŒç­‰ï¼‰
                    # å…ƒã®ä¾¡æ ¼ã‹ã‚‰ã®å¾®å°ãªå¤‰å‹•ã¨ã—ã¦è¡¨ç¾
                    price_based_signal[valid_mask] = prices[valid_mask] + normalized_signal * price_std * 0.02  # 2%ã®å¾®èª¿æ•´
                else:
                    price_based_signal[valid_mask] = price_mean
                
                # ç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿ã¯å…ƒã®ä¾¡æ ¼ã§è£œé–“
                invalid_mask = ~valid_mask
                price_based_signal[invalid_mask] = prices[invalid_mask]
            else:
                # å…¨ã¦ãŒç„¡åŠ¹ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯å…ƒã®ä¾¡æ ¼ã‚’ãã®ã¾ã¾ä½¿ç”¨
                price_based_signal = prices.copy()
            
            # å®‡å®™ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ«èª¿æ•´ï¼ˆé©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
            if self.cosmic_power_level != 1.0:
                # ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ä¿¡å·ã¯ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ«ã§å¾®èª¿æ•´ã®ã¿
                price_variation = price_based_signal - np.nanmean(price_based_signal)
                price_based_signal = np.nanmean(price_based_signal) + price_variation * self.cosmic_power_level
                
                # ä»–ã®æˆåˆ†ã¯æ­£è¦åŒ–æ¸ˆã¿ãªã®ã§é©åˆ‡ã«ã‚¹ã‚±ãƒ¼ãƒ«
                cosmic_trend = cosmic_trend ** (1.0 / self.cosmic_power_level)
                cosmic_cycle = cosmic_cycle * self.cosmic_power_level
                cosmic_volatility = cosmic_volatility ** (1.0 / self.cosmic_power_level)
            
            # å®‡å®™çµæœã®ä¿å­˜ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã‚’ä¿æŒï¼‰
            self._last_cosmic_result = UltimateCosmicResult(
                cosmic_signal=cosmic_signal,  # ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ä¿‚æ•°ã‚’ä¿æŒ
                cosmic_trend=cosmic_trend,
                cosmic_cycle=cosmic_cycle,
                cosmic_volatility=cosmic_volatility,
                quantum_coherence=quantum_coherence,
                market_regime=market_regime,
                adaptive_confidence=adaptive_confidence,
                multi_scale_energy=multi_scale_energy,
                phase_synchronization=phase_synchronization,
                cosmic_momentum=cosmic_momentum
            )
            
            # äº’æ›æ€§ã®ãŸã‚ã®WaveletResultã‚’è¿”ã™ï¼ˆä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ä¿¡å·ã‚’ä½¿ç”¨ï¼‰
            return WaveletResult(
                values=price_based_signal,  # ğŸ”§ ä¾¡æ ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«æ­£è¦åŒ–ã•ã‚ŒãŸä¿¡å·
                trend_component=cosmic_trend,
                cycle_component=cosmic_cycle,
                noise_component=cosmic_volatility,  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ãƒã‚¤ã‚ºæˆåˆ†ã¨ã—ã¦
                market_regime=market_regime,
                energy_spectrum=multi_scale_energy,
                confidence_score=adaptive_confidence
            )
            
        except Exception as e:
            import traceback
            print(f"ğŸŒŒ å®‡å®™ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
            print(f"è©³ç´°: {traceback.format_exc()}")
            data_len = len(data) if hasattr(data, '__len__') else 0
            return WaveletResult(
                values=np.full(data_len, np.nan),
                trend_component=np.full(data_len, np.nan),
                cycle_component=np.full(data_len, np.nan),
                noise_component=np.full(data_len, np.nan),
                market_regime=np.full(data_len, np.nan),
                confidence_score=np.full(data_len, np.nan)
            )
    
    def get_cosmic_result(self) -> Optional[UltimateCosmicResult]:
        """ğŸŒŒ å®‡å®™ãƒ¬ãƒ™ãƒ«è§£æçµæœã‚’å–å¾—"""
        return self._last_cosmic_result
    
    def get_cosmic_analysis_summary(self) -> Dict:
        """å®‡å®™ãƒ¬ãƒ™ãƒ«è§£æã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if self._last_cosmic_result is None:
            return {}
        
        result = self._last_cosmic_result
        
        return {
            'algorithm': 'Ultimate Cosmic Wavelet Analyzer',
            'status': 'UNIVERSE_DOMINATION_MODE',
            'cosmic_power_level': self.cosmic_power_level,
            'revolutionary_technologies': [
                'Multi-Scale Hybrid Analysis (5 Wavelets)',
                'Adaptive Quantum Coherence Integration',
                'Ultra-Fast Kalman-Wavelet Fusion',
                'Hierarchical Deep Denoising',
                'AI-Driven Adaptive Weighting',
                'Automatic Market Regime Recognition',
                'Quantum Entanglement-like Phase Sync'
            ],
            'performance_metrics': {
                'avg_quantum_coherence': float(np.nanmean(result.quantum_coherence)),
                'avg_phase_synchronization': float(np.nanmean(result.phase_synchronization)),
                'avg_adaptive_confidence': float(np.nanmean(result.adaptive_confidence)),
                'cosmic_trend_strength': float(np.nanmean(result.cosmic_trend)),
                'cosmic_volatility_level': float(np.nanmean(result.cosmic_volatility))
            },
            'market_analysis': {
                'dominant_regime': float(np.nanmean(result.market_regime)),
                'regime_stability': float(np.nanstd(result.market_regime)),
                'cosmic_momentum_avg': float(np.nanmean(result.cosmic_momentum))
            },
            'superiority_claims': [
                'å²ä¸Šæœ€é«˜ã®5ã¤ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåŸºåº•çµ±åˆ',
                'é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹çµ±åˆã«ã‚ˆã‚‹å®Œç’§ãªç²¾åº¦',
                'è¶…é«˜é€Ÿã‚«ãƒ«ãƒãƒ³èåˆã«ã‚ˆã‚‹ç©¶æ¥µã®ä½é…å»¶',
                'éšå±¤çš„ãƒ‡ã‚£ãƒ¼ãƒ—ãƒã‚¤ã‚ºé™¤å»ã«ã‚ˆã‚‹é©å‘½çš„ç´”åº¦',
                'AIé§†å‹•é©å¿œã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ–',
                'ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ å®Œå…¨è‡ªå‹•èªè­˜',
                'å®‡å®™ãƒ¬ãƒ™ãƒ«ã®å®‰å®šæ€§ã¨ä¿¡é ¼æ€§'
            ]
        }
    
    def reset(self) -> None:
        """çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._last_cosmic_result = None


# === çµ±åˆã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚¯ãƒ©ã‚¹ ===

class WaveletUnified(Indicator):
    """
    ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆçµ±åˆè§£æå™¨
    
    è¤‡æ•°ã®ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£ææ‰‹æ³•ã‚’çµ±åˆã—ã€å˜ä¸€ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§åˆ©ç”¨å¯èƒ½ã«ã—ã¾ã™ã€‚
    
    å¯¾å¿œæ‰‹æ³•:
    - 'haar_denoising': Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»
    - 'multiresolution': å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ
    - 'financial_adaptive': é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›
    - 'quantum_analysis': é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ
    - 'morlet_continuous': Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé€£ç¶šå¤‰æ›
    - 'daubechies_advanced': Daubechiesé«˜åº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
    """
    
    # åˆ©ç”¨å¯èƒ½ãªã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæ‰‹æ³•ã®å®šç¾©
    _WAVELETS = {
        'haar_denoising': HaarDenoisingWavelet,
        'multiresolution': MultiresolutionWavelet,
        'financial_adaptive': FinancialAdaptiveWavelet,
        'quantum_analysis': QuantumWavelet,
        'morlet_continuous': MorletContinuousWavelet,
        'daubechies_advanced': DaubechiesAdvancedWavelet,
        'ultimate_cosmic': UltimateCosmicWavelet
    }
    
    # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæ‰‹æ³•ã®èª¬æ˜
    _WAVELET_DESCRIPTIONS = {
        'haar_denoising': 'Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»',
        'multiresolution': 'å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ',
        'financial_adaptive': 'é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ï¼ˆDaubechies-4ï¼‰',
        'quantum_analysis': 'é‡å­ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æï¼ˆ6ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰',
        'morlet_continuous': 'Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé€£ç¶šå¤‰æ›',
        'daubechies_advanced': 'Daubechiesé«˜åº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æ',
        'ultimate_cosmic': 'ğŸŒŒ ç©¶æ¥µå®‡å®™ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æï¼ˆäººé¡å²ä¸Šæœ€å¼·ï¼‰'
    }
    
    def __init__(
        self,
        wavelet_type: str = 'multiresolution',
        src_type: str = 'close',
        # Haarãƒã‚¤ã‚ºé™¤å»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        haar_levels: int = 3,
        # Morletãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        morlet_scales: Optional[np.ndarray] = None,
        # Daubechiesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        daubechies_levels: int = 5,
        # ğŸŒŒ å®‡å®™æœ€å¼·ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        cosmic_scales: Optional[np.ndarray] = None,
        cosmic_power_level: float = 1.0
    ):
        """
        Args:
            wavelet_type: ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæ‰‹æ³•ã‚¿ã‚¤ãƒ—
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
            haar_levels: Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã®åˆ†è§£ãƒ¬ãƒ™ãƒ«
            morlet_scales: Morletã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã®ã‚¹ã‚±ãƒ¼ãƒ«é…åˆ—
            daubechies_levels: Daubechiesã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã®åˆ†è§£ãƒ¬ãƒ™ãƒ«
            cosmic_scales: ğŸŒŒ å®‡å®™ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã®ã‚¹ã‚±ãƒ¼ãƒ«é…åˆ—
            cosmic_power_level: ğŸŒŒ å®‡å®™ãƒ‘ãƒ¯ãƒ¼ãƒ¬ãƒ™ãƒ« (0.1-2.0)
        """
        if wavelet_type not in self._WAVELETS:
            available = ', '.join(self._WAVELETS.keys())
            raise ValueError(f"ç„¡åŠ¹ãªwavelet_type: {wavelet_type}. åˆ©ç”¨å¯èƒ½: {available}")
        
        super().__init__(f"WaveletUnified({wavelet_type})")
        
        self.wavelet_type = wavelet_type
        self.src_type = src_type
        self.haar_levels = haar_levels
        self.morlet_scales = morlet_scales
        self.daubechies_levels = daubechies_levels
        self.cosmic_scales = cosmic_scales
        self.cosmic_power_level = cosmic_power_level
        
        # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æå™¨ã®åˆæœŸåŒ–
        self._init_wavelet_analyzer()
        
        # ä¾¡æ ¼ã‚½ãƒ¼ã‚¹æŠ½å‡ºå™¨
        self.price_source_extractor = PriceSource()
        
        # çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._result_cache = {}
    
    def _init_wavelet_analyzer(self):
        """ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æå™¨ã‚’åˆæœŸåŒ–"""
        wavelet_class = self._WAVELETS[self.wavelet_type]
        
        if self.wavelet_type == 'haar_denoising':
            self.wavelet_analyzer = wavelet_class(levels=self.haar_levels)
        elif self.wavelet_type == 'morlet_continuous':
            self.wavelet_analyzer = wavelet_class(scales=self.morlet_scales)
        elif self.wavelet_type == 'daubechies_advanced':
            self.wavelet_analyzer = wavelet_class(levels=self.daubechies_levels)
        elif self.wavelet_type == 'ultimate_cosmic':
            self.wavelet_analyzer = wavelet_class(
                scales=self.cosmic_scales,
                cosmic_power_level=self.cosmic_power_level
            )
        else:
            self.wavelet_analyzer = wavelet_class()
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> WaveletResult:
        """
        ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚’å®Ÿè¡Œ
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            WaveletResult: ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æçµæœ
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash in self._result_cache:
                return self._result_cache[data_hash]
            
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            if isinstance(data, np.ndarray) and data.ndim == 1:
                prices = data.copy()
            else:
                prices = PriceSource.calculate_source(data, self.src_type)
            
            if len(prices) == 0:
                self.logger.warning("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                return WaveletResult(values=np.array([]))
            
            # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æå®Ÿè¡Œ
            result = self.wavelet_analyzer.calculate(prices)
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self._result_cache[data_hash] = result
            
            self.logger.info(f"ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æå®Œäº†: {self.wavelet_type}")
            return result
            
        except Exception as e:
            self.logger.error(f"ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
            data_len = len(data) if hasattr(data, '__len__') else 0
            return WaveletResult(values=np.full(data_len, np.nan))
    
    def get_values(self) -> Optional[np.ndarray]:
        """ãƒ¡ã‚¤ãƒ³å€¤ã‚’å–å¾—"""
        if hasattr(self, '_last_result'):
            return self._last_result.values.copy()
        return None
    
    def get_trend_component(self) -> Optional[np.ndarray]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†ã‚’å–å¾—"""
        if hasattr(self, '_last_result') and self._last_result.trend_component is not None:
            return self._last_result.trend_component.copy()
        return None
    
    def get_cycle_component(self) -> Optional[np.ndarray]:
        """ã‚µã‚¤ã‚¯ãƒ«æˆåˆ†ã‚’å–å¾—"""
        if hasattr(self, '_last_result') and self._last_result.cycle_component is not None:
            return self._last_result.cycle_component.copy()
        return None
    
    def get_noise_component(self) -> Optional[np.ndarray]:
        """ãƒã‚¤ã‚ºæˆåˆ†ã‚’å–å¾—"""
        if hasattr(self, '_last_result') and self._last_result.noise_component is not None:
            return self._last_result.noise_component.copy()
        return None
    
    def get_market_regime(self) -> Optional[np.ndarray]:
        """ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ ã‚’å–å¾—"""
        if hasattr(self, '_last_result') and self._last_result.market_regime is not None:
            return self._last_result.market_regime.copy()
        return None
    
    def get_wavelet_info(self) -> Dict:
        """ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æå™¨ã®æƒ…å ±ã‚’å–å¾—"""
        return {
            'wavelet_type': self.wavelet_type,
            'description': self._WAVELET_DESCRIPTIONS.get(self.wavelet_type, ''),
            'src_type': self.src_type,
            'analyzer_name': self.wavelet_analyzer.name,
            'parameters': {
                'haar_levels': self.haar_levels,
                'morlet_scales': self.morlet_scales.tolist() if self.morlet_scales is not None else None,
                'daubechies_levels': self.daubechies_levels,
                'cosmic_scales': self.cosmic_scales.tolist() if self.cosmic_scales is not None else None,
                'cosmic_power_level': self.cosmic_power_level
            }
        }
    
    @classmethod
    def get_available_wavelets(cls) -> Dict[str, str]:
        """åˆ©ç”¨å¯èƒ½ãªã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæ‰‹æ³•ã®ä¸€è¦§ã‚’å–å¾—"""
        return cls._WAVELET_DESCRIPTIONS.copy()
    
    def reset(self) -> None:
        """çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._result_cache = {}
        self.wavelet_analyzer.reset()
        if hasattr(self, '_last_result'):
            delattr(self, '_last_result')
    
    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬çš„ãªç‰¹å¾´ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        if isinstance(data, pd.DataFrame):
            data_str = f"{data.shape}_{self.src_type}_{data.iloc[0].sum() if len(data) > 0 else 0}_{data.iloc[-1].sum() if len(data) > 0 else 0}"
        elif isinstance(data, np.ndarray):
            data_str = f"{data.shape}_{data[0] if len(data) > 0 else 0}_{data[-1] if len(data) > 0 else 0}"
        else:
            data_str = str(data)
        
        param_str = f"{self.wavelet_type}_{self.haar_levels}_{self.daubechies_levels}_{self.cosmic_power_level}"
        return f"{data_str}_{param_str}"


# === ä½¿ç”¨ä¾‹é–¢æ•° ===

def example_usage():
    """ä½¿ç”¨ä¾‹"""
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    prices = np.random.randn(100).cumsum() + 100
    
    # 1. Haarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»
    haar_wavelet = WaveletUnified(wavelet_type='haar_denoising', haar_levels=3)
    haar_result = haar_wavelet.calculate(prices)
    print(f"Haarãƒã‚¤ã‚ºé™¤å»çµæœ: {len(haar_result.values)} points")
    
    # 2. å¤šè§£åƒåº¦è§£æ
    multiresolution = WaveletUnified(wavelet_type='multiresolution')
    multi_result = multiresolution.calculate(prices)
    print(f"å¤šè§£åƒåº¦è§£æçµæœ: {len(multi_result.values)} points")
    
    # 3. é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
    financial = WaveletUnified(wavelet_type='financial_adaptive')
    fin_result = financial.calculate(prices)
    print(f"é‡‘èé©å¿œçµæœ: {len(fin_result.values)} points")
    
    # 4. ğŸŒŒ å®‡å®™æœ€å¼·ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆ
    ultimate_cosmic = WaveletUnified(
        wavelet_type='ultimate_cosmic',
        cosmic_power_level=1.5
    )
    cosmic_result = ultimate_cosmic.calculate(prices)
    print(f"ğŸŒŒ å®‡å®™æœ€å¼·çµæœ: {len(cosmic_result.values)} points")
    
    # 5. åˆ©ç”¨å¯èƒ½ãªæ‰‹æ³•ã®ç¢ºèª
    available = WaveletUnified.get_available_wavelets()
    print(f"åˆ©ç”¨å¯èƒ½ãªã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæ‰‹æ³•: {list(available.keys())}")
    print(f"ğŸŒŒ å®‡å®™æœ€å¼·ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè¿½åŠ å®Œäº†ï¼")


if __name__ == "__main__":
    example_usage() 