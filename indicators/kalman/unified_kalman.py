#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸš€ **Unified Kalman Filter - çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼** ğŸš€

kalmanãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’çµ±åˆã—ã€
å˜ä¸€ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§åˆ©ç”¨å¯èƒ½ã«ã—ã¾ã™ã€‚

ğŸŒŸ **å¯¾å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:**
- 'adaptive': é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (Adaptive Kalman Filter)
- 'multivariate': å¤šå¤‰é‡ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (Multivariate Kalman Filter)
- 'quantum_adaptive': é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (Quantum Adaptive Kalman Filter)
- 'simple': ã‚·ãƒ³ãƒ—ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (Simple Kalman Filter - ãƒ‘ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆäº’æ›)
- 'unscented': ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (Unscented Kalman Filter)
- 'unscented_v2': ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼V2 (Unscented Kalman Filter V2)

ğŸ“Š **ç‰¹å¾´:**
- è¤‡æ•°ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é¸æŠå¯èƒ½
- çµ±ä¸€ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†
- ãƒ—ãƒ©ã‚¤ã‚¹ã‚½ãƒ¼ã‚¹å¯¾å¿œ
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
"""

from dataclasses import dataclass
from typing import Union, Optional, Dict, Any, Type
import numpy as np
import pandas as pd
import traceback

try:
    from ..indicator import Indicator
    from ..price_source import PriceSource
    # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from .adaptive_kalman import AdaptiveKalman, AdaptiveKalmanResult
    from .multivariate_kalman import MultivariateKalman, MultivariateKalmanResult
    from .quantum_adaptive_kalman import QuantumAdaptiveKalman, QuantumAdaptiveKalmanResult
    from .simple_kalman import SimpleKalman, SimpleKalmanResult
    from .unscented_kalman_filter import UnscentedKalmanFilter, UnscentedKalmanResult
    from .unscented_kalman_filter_v2 import UnscentedKalmanFilterV2Wrapper, UKFResult
except ImportError:
    # Fallback for potential execution context issues
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from indicator import Indicator
    from price_source import PriceSource
    # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (çµ¶å¯¾ãƒ‘ã‚¹)
    from indicators.kalman.adaptive_kalman import AdaptiveKalman, AdaptiveKalmanResult
    from indicators.kalman.multivariate_kalman import MultivariateKalman, MultivariateKalmanResult
    from indicators.kalman.quantum_adaptive_kalman import QuantumAdaptiveKalman, QuantumAdaptiveKalmanResult
    from indicators.kalman.simple_kalman import SimpleKalman, SimpleKalmanResult
    from indicators.kalman.unscented_kalman_filter import UnscentedKalmanFilter, UnscentedKalmanResult
    from indicators.kalman.unscented_kalman_filter_v2 import UnscentedKalmanFilterV2Wrapper, UKFResult


@dataclass
class UnifiedKalmanResult:
    """çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—çµæœ"""
    values: np.ndarray                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸå€¤ï¼ˆãƒ¡ã‚¤ãƒ³çµæœï¼‰
    filter_type: str                      # ä½¿ç”¨ã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
    parameters: Dict[str, Any]            # ä½¿ç”¨ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    additional_data: Dict[str, np.ndarray]  # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿
    raw_values: np.ndarray                # å…ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿


class UnifiedKalman(Indicator):
    """
    çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    
    kalmanãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§åˆ©ç”¨ï¼š
    - è¤‡æ•°ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é¸æŠå¯èƒ½
    - ä¸€è²«ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    - ãƒ—ãƒ©ã‚¤ã‚¹ã‚½ãƒ¼ã‚¹å¯¾å¿œ
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
    """
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å®šç¾©
    _FILTERS = {
        'adaptive': AdaptiveKalman,
        'multivariate': MultivariateKalman,
        'quantum_adaptive': QuantumAdaptiveKalman,
        'simple': SimpleKalman,
        'unscented': UnscentedKalmanFilter,
        'unscented_v2': UnscentedKalmanFilterV2Wrapper,
    }
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®èª¬æ˜
    _FILTER_DESCRIPTIONS = {
        'adaptive': 'é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆAdaptive Kalman Filterï¼‰',
        'multivariate': 'å¤šå¤‰é‡ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆMultivariate Kalman Filterï¼‰',
        'quantum_adaptive': 'é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆQuantum Adaptive Kalman Filterï¼‰',
        'simple': 'ã‚·ãƒ³ãƒ—ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆSimple Kalman Filter - ãƒ‘ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆäº’æ›ï¼‰',
        'unscented': 'ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUnscented Kalman Filterï¼‰',
        'unscented_v2': 'ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼V2ï¼ˆUnscented Kalman Filter V2ï¼‰',
    }
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    _DEFAULT_PARAMS = {
        'adaptive': {
            'process_noise': 1e-5,
            'min_observation_noise': 1e-6,
            'adaptation_window': 5
        },
        'multivariate': {
            'process_noise': 1e-5,
            'observation_noise': 1e-3,
            'volatility_noise': 1e-4
        },
        'quantum_adaptive': {
            'base_process_noise': 0.001,
            'amplitude_window': 14,
            'coherence_lookback': 5
        },
        'simple': {
            'R': 0.1,
            'Q': 0.01,
            'initial_covariance': 1.0,
            'enable_trend_detection': True
        },
        'unscented': {
            'alpha': 0.1,
            'beta': 2.0,
            'kappa': 0.0,
            'process_noise_scale': 0.01,
            'volatility_window': 10,
            'adaptive_noise': True
        },
        'unscented_v2': {
            'kappa': 0.0,
            'process_noise_scale': 0.01,
            'observation_noise_scale': 0.001,
            'max_steps': 1000
        }
    }
    
    def __init__(
        self,
        filter_type: str = 'adaptive',
        src_type: str = 'close',
        **kwargs
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            filter_type: ä½¿ç”¨ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
                - 'adaptive': é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                - 'multivariate': å¤šå¤‰é‡ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                - 'quantum_adaptive': é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                - 'simple': ã‚·ãƒ³ãƒ—ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆãƒ‘ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆäº’æ›ï¼‰
                - 'unscented': ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                - 'unscented_v2': ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼V2
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ ('close', 'hlc3', 'hl2', 'ohlc4', etc.)
            **kwargs: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—ã®æ­£è¦åŒ–
        filter_type = filter_type.lower()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—ã®æ¤œè¨¼
        if filter_type not in self._FILTERS:
            raise ValueError(
                f"ç„¡åŠ¹ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—: {filter_type}ã€‚"
                f"æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {', '.join(self._FILTERS.keys())}"
            )
        
        # ã‚¤ãƒ³ãƒ‡ã‚£ã‚±ãƒ¼ã‚¿ãƒ¼åã®è¨­å®š
        indicator_name = f"UnifiedKalman({filter_type}, src={src_type})"
        super().__init__(indicator_name)
        
        self.filter_type = filter_type
        self.src_type = src_type.lower()
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®æ¤œè¨¼
        valid_sources = ['close', 'hlc3', 'hl2', 'ohlc4', 'high', 'low', 'open', 'oc2']
        if self.src_type not in valid_sources:
            raise ValueError(f"ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—: {src_type}")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        self.parameters = self._DEFAULT_PARAMS[filter_type].copy()
        self.parameters.update(kwargs)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
        self.filter = self._create_filter_instance()
        
        # çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._result_cache = {}
        self._max_cache_size = 10
        self._cache_keys = []
    
    def _create_filter_instance(self):
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        filter_class = self._FILTERS[self.filter_type]
        
        try:
            # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
            if self.filter_type == 'adaptive':
                return filter_class(
                    process_noise=self.parameters.get('process_noise', 1e-5),
                    src_type=self.src_type,
                    min_observation_noise=self.parameters.get('min_observation_noise', 1e-6),
                    adaptation_window=self.parameters.get('adaptation_window', 5)
                )
            
            elif self.filter_type == 'multivariate':
                return filter_class(
                    process_noise=self.parameters.get('process_noise', 1e-5),
                    observation_noise=self.parameters.get('observation_noise', 1e-3),
                    volatility_noise=self.parameters.get('volatility_noise', 1e-4)
                )
            
            elif self.filter_type == 'quantum_adaptive':
                return filter_class(
                    src_type=self.src_type,
                    base_process_noise=self.parameters.get('base_process_noise', 0.001),
                    amplitude_window=self.parameters.get('amplitude_window', 14),
                    coherence_lookback=self.parameters.get('coherence_lookback', 5)
                )
            
            elif self.filter_type == 'simple':
                return filter_class(
                    R=self.parameters.get('R', 0.1),
                    Q=self.parameters.get('Q', 0.01),
                    src_type=self.src_type,
                    initial_covariance=self.parameters.get('initial_covariance', 1.0),
                    enable_trend_detection=self.parameters.get('enable_trend_detection', True)
                )
            
            elif self.filter_type == 'unscented':
                return filter_class(
                    src_type=self.src_type,
                    alpha=self.parameters.get('alpha', 0.1),
                    beta=self.parameters.get('beta', 2.0),
                    kappa=self.parameters.get('kappa', 0.0),
                    process_noise_scale=self.parameters.get('process_noise_scale', 0.01),
                    volatility_window=self.parameters.get('volatility_window', 10),
                    adaptive_noise=self.parameters.get('adaptive_noise', True)
                )
            
            elif self.filter_type == 'unscented_v2':
                return filter_class(
                    src_type=self.src_type,
                    kappa=self.parameters.get('kappa', 0.0),
                    process_noise_scale=self.parameters.get('process_noise_scale', 0.01),
                    observation_noise_scale=self.parameters.get('observation_noise_scale', 0.001),
                    max_steps=self.parameters.get('max_steps', 1000)
                )
            
            else:
                # æ±ç”¨ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
                return filter_class(src_type=self.src_type, **self.parameters)
                
        except Exception as e:
            self.logger.error(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼ ({self.filter_type}): {e}")
            raise
    
    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        try:
            if isinstance(data, pd.DataFrame):
                length = len(data)
                if length > 0:
                    first_val = float(data.iloc[0].get('close', data.iloc[0, -1]))
                    last_val = float(data.iloc[-1].get('close', data.iloc[-1, -1]))
                else:
                    first_val = last_val = 0.0
            else:
                length = len(data)
                if length > 0:
                    if data.ndim > 1:
                        first_val = float(data[0, -1])
                        last_val = float(data[-1, -1])
                    else:
                        first_val = float(data[0])
                        last_val = float(data[-1])
                else:
                    first_val = last_val = 0.0
            
            params_sig = f"{self.filter_type}_{self.src_type}_{hash(str(sorted(self.parameters.items())))}"
            data_sig = (length, first_val, last_val)
            return f"{hash(data_sig)}_{hash(params_sig)}"
            
        except Exception:
            return f"{id(data)}_{self.filter_type}_{self.src_type}"
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UnifiedKalmanResult:
        """
        çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¨ˆç®—
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            UnifiedKalmanResult: è¨ˆç®—çµæœ
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            
            if data_hash in self._result_cache:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
                if data_hash in self._cache_keys:
                    self._cache_keys.remove(data_hash)
                self._cache_keys.append(data_hash)
                cached_result = self._result_cache[data_hash]
                return UnifiedKalmanResult(
                    values=cached_result.values.copy(),
                    filter_type=cached_result.filter_type,
                    parameters=cached_result.parameters.copy(),
                    additional_data={k: v.copy() if isinstance(v, np.ndarray) else v 
                                   for k, v in cached_result.additional_data.items()},
                    raw_values=cached_result.raw_values.copy()
                )
            
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            src_prices = PriceSource.calculate_source(data, self.src_type)
            data_length = len(src_prices)
            
            if data_length < 2:
                return self._create_empty_result(data_length, src_prices)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—å®Ÿè¡Œ
            filter_result = self.filter.calculate(data)
            
            # çµæœã®æ¨™æº–åŒ–ï¼ˆå„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®å‡¦ç†ï¼‰
            filtered_values, additional_data = self._standardize_result(filter_result)
            
            # NumPyé…åˆ—ã¸ã®å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if not isinstance(filtered_values, np.ndarray):
                filtered_values = np.array(filtered_values)
            
            # çµæœã®ä½œæˆ
            result = UnifiedKalmanResult(
                values=filtered_values.copy(),
                filter_type=self.filter_type,
                parameters=self.parameters.copy(),
                additional_data=additional_data,
                raw_values=src_prices.copy()
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
            if len(self._result_cache) >= self._max_cache_size and self._cache_keys:
                oldest_key = self._cache_keys.pop(0)
                if oldest_key in self._result_cache:
                    del self._result_cache[oldest_key]
            
            self._result_cache[data_hash] = result
            self._cache_keys.append(data_hash)
            
            # åŸºåº•ã‚¯ãƒ©ã‚¹ç”¨ã®å€¤è¨­å®š
            self._values = filtered_values
            return result
            
        except Exception as e:
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¿”ã™
            if isinstance(data, (pd.DataFrame, np.ndarray)) and len(data) > 0:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                return self._create_empty_result(len(src_prices), src_prices)
            else:
                return self._create_empty_result(0, np.array([]))
    
    def _standardize_result(self, filter_result) -> tuple:
        """
        ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã‚’æ¨™æº–åŒ–ã—ã¦ã€çµ±ä¸€ã•ã‚ŒãŸå½¢å¼ã§è¿”ã™
        
        Args:
            filter_result: å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—çµæœ
            
        Returns:
            tuple: (filtered_values, additional_data)
        """
        additional_data = {}
        
        try:
            # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®çµæœå‡¦ç†
            if self.filter_type == 'simple':
                # SimpleKalmanResult
                filtered_values = filter_result.values
                additional_data['kalman_gains'] = filter_result.kalman_gains.copy()
                additional_data['error_covariances'] = filter_result.error_covariances.copy()
                additional_data['predictions'] = filter_result.predictions.copy()
                additional_data['trend_signals'] = filter_result.trend_signals.copy()
                
            elif self.filter_type == 'unscented':
                # UKFResult
                filtered_values = filter_result.filtered_values
                additional_data['velocity_estimates'] = filter_result.velocity_estimates.copy()
                additional_data['acceleration_estimates'] = filter_result.acceleration_estimates.copy()
                additional_data['uncertainty'] = filter_result.uncertainty.copy()
                additional_data['kalman_gains'] = filter_result.kalman_gains.copy()
                additional_data['innovations'] = filter_result.innovations.copy()
                additional_data['confidence_scores'] = filter_result.confidence_scores.copy()
                
            elif self.filter_type == 'unscented_v2':
                # UKFResult from V2
                if hasattr(filter_result, 'filtered_values'):
                    filtered_values = filter_result.filtered_values
                elif hasattr(filter_result, 'values'):
                    filtered_values = filter_result.values
                else:
                    filtered_values = filter_result
                self._extract_ukf_additional_data(filter_result, additional_data)
                
            elif self.filter_type == 'quantum_adaptive':
                # QuantumAdaptiveKalmanResult
                filtered_values = filter_result.values
                additional_data['quantum_coherence'] = filter_result.quantum_coherence.copy()
                additional_data['kalman_gains'] = filter_result.kalman_gains.copy()
                additional_data['innovations'] = filter_result.innovations.copy()
                additional_data['confidence_scores'] = filter_result.confidence_scores.copy()
                
            elif self.filter_type == 'adaptive':
                # AdaptiveKalmanResult
                filtered_values = filter_result.filtered_signal
                if hasattr(filter_result, 'adaptive_gain'):
                    additional_data['adaptive_gain'] = filter_result.adaptive_gain.copy()
                if hasattr(filter_result, 'error_covariance'):
                    additional_data['error_covariance'] = filter_result.error_covariance.copy()
                if hasattr(filter_result, 'innovations'):
                    additional_data['innovations'] = filter_result.innovations.copy()
                if hasattr(filter_result, 'confidence_score'):
                    additional_data['confidence_score'] = filter_result.confidence_score.copy()
                
            elif self.filter_type == 'multivariate':
                # MultivariateKalmanResult
                filtered_values = filter_result.filtered_prices
                if hasattr(filter_result, 'price_range_estimates'):
                    additional_data['price_range_estimates'] = filter_result.price_range_estimates.copy()
                if hasattr(filter_result, 'volatility_estimates'):
                    additional_data['volatility_estimates'] = filter_result.volatility_estimates.copy()
                if hasattr(filter_result, 'state_estimates'):
                    additional_data['state_estimates'] = filter_result.state_estimates.copy()
                if hasattr(filter_result, 'velocity_estimates'):
                    additional_data['velocity_estimates'] = filter_result.velocity_estimates.copy()
                if hasattr(filter_result, 'filtered_high'):
                    additional_data['filtered_high'] = filter_result.filtered_high.copy()
                if hasattr(filter_result, 'filtered_low'):
                    additional_data['filtered_low'] = filter_result.filtered_low.copy()
                if hasattr(filter_result, 'filtered_close'):
                    additional_data['filtered_close'] = filter_result.filtered_close.copy()
                if hasattr(filter_result, 'kalman_gains'):
                    additional_data['kalman_gains'] = filter_result.kalman_gains.copy()
                if hasattr(filter_result, 'innovations'):
                    additional_data['innovations'] = filter_result.innovations.copy()
                if hasattr(filter_result, 'confidence_scores'):
                    additional_data['confidence_scores'] = filter_result.confidence_scores.copy()
                
            else:
                # æ±ç”¨å‡¦ç†
                if hasattr(filter_result, 'values'):
                    filtered_values = filter_result.values
                elif hasattr(filter_result, 'filtered_values'):
                    filtered_values = filter_result.filtered_values
                elif hasattr(filter_result, 'filtered_signal'):
                    filtered_values = filter_result.filtered_signal
                elif hasattr(filter_result, 'filtered_prices'):
                    filtered_values = filter_result.filtered_prices
                else:
                    filtered_values = filter_result
                    
                # å…±é€šã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                self._extract_common_additional_data(filter_result, additional_data)
                    
        except Exception as e:
            self.logger.error(f"çµæœæ¨™æº–åŒ–ã‚¨ãƒ©ãƒ¼ ({self.filter_type}): {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã‚’ãã®ã¾ã¾ä½¿ç”¨
            if hasattr(filter_result, 'values'):
                filtered_values = filter_result.values
            elif hasattr(filter_result, 'filtered_values'):
                filtered_values = filter_result.filtered_values
            elif hasattr(filter_result, 'filtered_signal'):
                filtered_values = filter_result.filtered_signal
            elif hasattr(filter_result, 'filtered_prices'):
                filtered_values = filter_result.filtered_prices
            else:
                filtered_values = filter_result
                
        return filtered_values, additional_data
    
    def _extract_ukf_additional_data(self, filter_result, additional_data: Dict[str, np.ndarray]):
        """UKFå›ºæœ‰ã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            if hasattr(filter_result, 'velocity_estimates'):
                additional_data['velocity_estimates'] = filter_result.velocity_estimates.copy()
            if hasattr(filter_result, 'acceleration_estimates'):
                additional_data['acceleration_estimates'] = filter_result.acceleration_estimates.copy()
            if hasattr(filter_result, 'uncertainty'):
                additional_data['uncertainty'] = filter_result.uncertainty.copy()
            if hasattr(filter_result, 'kalman_gains'):
                additional_data['kalman_gains'] = filter_result.kalman_gains.copy()
            if hasattr(filter_result, 'innovations'):
                additional_data['innovations'] = filter_result.innovations.copy()
            if hasattr(filter_result, 'confidence_scores'):
                additional_data['confidence_scores'] = filter_result.confidence_scores.copy()
        except Exception as e:
            self.logger.warning(f"UKFè¿½åŠ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            
    def _extract_common_additional_data(self, filter_result, additional_data: Dict[str, np.ndarray]):
        """å…±é€šã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            common_attributes = [
                'kalman_gains', 'innovations', 'error_covariance', 'confidence_score',
                'confidence_scores', 'uncertainty', 'predictions', 'trend_signals'
            ]
            
            for attr in common_attributes:
                if hasattr(filter_result, attr):
                    value = getattr(filter_result, attr)
                    if isinstance(value, np.ndarray):
                        additional_data[attr] = value.copy()
                    else:
                        additional_data[attr] = np.array(value) if value is not None else np.array([])
        except Exception as e:
            self.logger.warning(f"å…±é€šè¿½åŠ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            
    def _extract_additional_data(self, filter_result) -> Dict[str, np.ndarray]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰"""
        additional_data = {}
        
        try:
            # é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ãƒ‡ãƒ¼ã‚¿
            if hasattr(filter_result, 'adaptive_gain'):
                additional_data['adaptive_gain'] = filter_result.adaptive_gain.copy()
            if hasattr(filter_result, 'innovations'):
                additional_data['innovations'] = filter_result.innovations.copy()
            if hasattr(filter_result, 'error_covariance'):
                additional_data['error_covariance'] = filter_result.error_covariance.copy()
            if hasattr(filter_result, 'confidence_score'):
                additional_data['confidence_score'] = filter_result.confidence_score.copy()
            
            # å¤šå¤‰é‡ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ãƒ‡ãƒ¼ã‚¿
            if hasattr(filter_result, 'price_range_estimates'):
                additional_data['price_range_estimates'] = filter_result.price_range_estimates.copy()
            if hasattr(filter_result, 'volatility_estimates'):
                additional_data['volatility_estimates'] = filter_result.volatility_estimates.copy()
            if hasattr(filter_result, 'state_estimates'):
                additional_data['state_estimates'] = filter_result.state_estimates.copy()
            if hasattr(filter_result, 'uncertainty_estimates'):
                additional_data['uncertainty_estimates'] = filter_result.uncertainty_estimates.copy()
            
            # é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ãƒ‡ãƒ¼ã‚¿
            if hasattr(filter_result, 'quantum_coherence'):
                additional_data['quantum_coherence'] = filter_result.quantum_coherence.copy()
            if hasattr(filter_result, 'kalman_gains'):
                additional_data['kalman_gains'] = filter_result.kalman_gains.copy()
            if hasattr(filter_result, 'confidence_scores'):
                additional_data['confidence_scores'] = filter_result.confidence_scores.copy()
            
            # ã‚·ãƒ³ãƒ—ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ãƒ‡ãƒ¼ã‚¿
            if hasattr(filter_result, 'kalman_gains'):
                additional_data['kalman_gains'] = filter_result.kalman_gains.copy()
            if hasattr(filter_result, 'error_covariances'):
                additional_data['error_covariances'] = filter_result.error_covariances.copy()
            if hasattr(filter_result, 'predictions'):
                additional_data['predictions'] = filter_result.predictions.copy()
            if hasattr(filter_result, 'trend_signals'):
                additional_data['trend_signals'] = filter_result.trend_signals.copy()
            
            # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ãƒ‡ãƒ¼ã‚¿
            if hasattr(filter_result, 'velocity_estimates'):
                additional_data['velocity_estimates'] = filter_result.velocity_estimates.copy()
            if hasattr(filter_result, 'acceleration_estimates'):
                additional_data['acceleration_estimates'] = filter_result.acceleration_estimates.copy()
            if hasattr(filter_result, 'uncertainty'):
                additional_data['uncertainty'] = filter_result.uncertainty.copy()
            if hasattr(filter_result, 'sigma_points'):
                additional_data['sigma_points'] = filter_result.sigma_points.copy()
            
        except Exception as e:
            self.logger.warning(f"è¿½åŠ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        return additional_data
    
    def _create_empty_result(self, length: int, raw_prices: np.ndarray) -> UnifiedKalmanResult:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        return UnifiedKalmanResult(
            values=np.full(length, np.nan),
            filter_type=self.filter_type,
            parameters=self.parameters.copy(),
            additional_data={},
            raw_values=raw_prices
        )
    
    def get_values(self) -> Optional[np.ndarray]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸå€¤ã‚’å–å¾—"""
        if not self._result_cache:
            return None
        
        result = self._get_latest_result()
        return result.values.copy() if result else None
    
    def get_raw_values(self) -> Optional[np.ndarray]:
        """å…ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        result = self._get_latest_result()
        return result.raw_values.copy() if result else None
    
    def get_additional_data(self, key: str) -> Optional[np.ndarray]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        result = self._get_latest_result()
        if result and key in result.additional_data:
            return result.additional_data[key].copy()
        return None
    
    def get_filter_info(self) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±ã‚’å–å¾—"""
        return {
            'type': self.filter_type,
            'description': self._FILTER_DESCRIPTIONS.get(self.filter_type, 'Unknown'),
            'src_type': self.src_type,
            'parameters': self.parameters.copy()
        }
    
    def _get_latest_result(self) -> Optional[UnifiedKalmanResult]:
        """æœ€æ–°ã®çµæœã‚’å–å¾—"""
        if not self._result_cache:
            return None
        
        if self._cache_keys:
            return self._result_cache[self._cache_keys[-1]]
        else:
            return next(iter(self._result_cache.values()))
    
    def reset(self) -> None:
        """çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        if hasattr(self.filter, 'reset'):
            self.filter.reset()
        self._result_cache = {}
        self._cache_keys = []
    
    @classmethod
    def get_available_filters(cls) -> Dict[str, str]:
        """
        åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨ãã®èª¬æ˜ã‚’è¿”ã™
        
        Returns:
            Dict[str, str]: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åã¨ãã®èª¬æ˜ã®è¾æ›¸
        """
        return cls._FILTER_DESCRIPTIONS.copy()
    
    @classmethod
    def get_default_parameters(cls, filter_type: str) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        
        Args:
            filter_type: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
            
        Returns:
            Dict[str, Any]: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        filter_type = filter_type.lower()
        return cls._DEFAULT_PARAMS.get(filter_type, {}).copy()


# ä¾¿åˆ©é–¢æ•°
def filter_data(
    data: Union[pd.DataFrame, np.ndarray], 
    filter_type: str = 'adaptive',
    src_type: str = 'close',
    **kwargs
) -> np.ndarray:
    """
    çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰
    
    Args:
        data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        filter_type: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
        src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
        **kwargs: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸå€¤
    """
    kalman_filter = UnifiedKalman(filter_type=filter_type, src_type=src_type, **kwargs)
    result = kalman_filter.calculate(data)
    return result.values


def compare_filters(
    data: Union[pd.DataFrame, np.ndarray],
    filter_types: list = ['adaptive', 'quantum_adaptive', 'unscented'],
    src_type: str = 'close',
    **kwargs
) -> Dict[str, np.ndarray]:
    """
    è¤‡æ•°ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’æ¯”è¼ƒ
    
    Args:
        data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        filter_types: æ¯”è¼ƒã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
        src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
        **kwargs: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        Dict[str, np.ndarray]: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®çµæœ
    """
    results = {}
    
    for filter_type in filter_types:
        try:
            kalman_filter = UnifiedKalman(filter_type=filter_type, src_type=src_type, **kwargs)
            result = kalman_filter.calculate(data)
            results[filter_type] = result.values
        except Exception as e:
            print(f"Error with {filter_type}: {e}")
            results[filter_type] = np.full(len(data), np.nan)
    
    return results


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œéƒ¨åˆ†
if __name__ == "__main__":
    import matplotlib.pyplot as plt
    from datetime import datetime, timedelta
    
    print("=== çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ†ã‚¹ãƒˆ ===")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    length = 200
    base_price = 100.0
    trend = 0.001
    volatility = 0.02
    
    prices = [base_price]
    for i in range(1, length):
        change = trend + np.random.normal(0, volatility)
        new_price = prices[-1] * (1 + change)
        prices.append(new_price)
    
    # OHLC ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    data = []
    for i, close in enumerate(prices):
        daily_range = abs(np.random.normal(0, volatility * close * 0.5))
        
        high = close + daily_range * np.random.uniform(0.3, 1.0)
        low = close - daily_range * np.random.uniform(0.3, 1.0)
        
        if i == 0:
            open_price = close
        else:
            gap = np.random.normal(0, volatility * close * 0.2)
            open_price = prices[i-1] + gap
        
        # è«–ç†çš„æ•´åˆæ€§ã®ç¢ºä¿
        high = max(high, open_price, close)
        low = min(low, open_price, close)
        
        data.append({
            'open': open_price,
            'high': high,
            'low': low,
            'close': close,
            'volume': np.random.uniform(1000, 10000)
        })
    
    df = pd.DataFrame(data)
    start_date = datetime.now() - timedelta(days=length)
    df.index = [start_date + timedelta(hours=i) for i in range(length)]
    
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(df)}ãƒã‚¤ãƒ³ãƒˆ")
    print(f"ä¾¡æ ¼ç¯„å›²: {df['close'].min():.2f} - {df['close'].max():.2f}")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¡¨ç¤º
    filters = UnifiedKalman.get_available_filters()
    print(f"\nåˆ©ç”¨å¯èƒ½ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼: {len(filters)}ç¨®é¡")
    for name, desc in filters.items():
        print(f"  {name}: {desc}")
    
    # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
    test_filters = ['adaptive', 'quantum_adaptive', 'unscented']
    print(f"\nãƒ†ã‚¹ãƒˆå¯¾è±¡: {test_filters}")
    
    results = {}
    for filter_type in test_filters:
        try:
            print(f"\n{filter_type} ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
            kalman_filter = UnifiedKalman(filter_type=filter_type, src_type='close')
            result = kalman_filter.calculate(df)
            
            mean_filtered = np.nanmean(result.values)
            mean_raw = np.nanmean(result.raw_values)
            valid_count = np.sum(~np.isnan(result.values))
            
            results[filter_type] = result
            
            print(f"  å¹³å‡å€¤: {mean_filtered:.4f} (å…ƒ: {mean_raw:.4f})")
            print(f"  æœ‰åŠ¹å€¤æ•°: {valid_count}/{len(df)}")
            print(f"  è¿½åŠ ãƒ‡ãƒ¼ã‚¿: {list(result.additional_data.keys()) if result.additional_data else 'ãªã—'}")
            
        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\n=== ãƒ†ã‚¹ãƒˆå®Œäº† ===")