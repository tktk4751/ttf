#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ **Unscented Kalman Filter (UKF) - ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼** ğŸ¯

æ­£ç¢ºãªç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å®Ÿè£…ï¼š
- ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆæ³•ã«ã‚ˆã‚‹éç·šå½¢ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹æ¨å®š
- ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ä¸è¦ã§é«˜ç²¾åº¦ãªéç·šå½¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®šæ©Ÿèƒ½
- è¤‡æ•°ã®ä¾¡æ ¼ã‚½ãƒ¼ã‚¹å¯¾å¿œ
- å‹•çš„æœŸé–“èª¿æ•´æ©Ÿèƒ½

ğŸŒŸ **UKFã®ç‰¹å¾´:**
1. **ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ**: ç¢ºç‡åˆ†å¸ƒã‚’ä»£è¡¨ã™ã‚‹ç‚¹ç¾¤ã‚’ç”Ÿæˆ
2. **éç·šå½¢ä¼æ’­**: å„ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã‚’éç·šå½¢é–¢æ•°ã§å¤‰æ›
3. **çµ±è¨ˆãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆå†æ§‹ç¯‰**: é‡ã¿ä»˜ãå¹³å‡ã¨å…±åˆ†æ•£ã‚’è¨ˆç®—
4. **é«˜ç²¾åº¦æ¨å®š**: 3æ¬¡ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã¾ã§æ­£ç¢ºã«æ¨å®š
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Optional, List, NamedTuple
import numpy as np
import pandas as pd
from numba import jit, njit, types
import traceback

try:
    from ..indicator import Indicator
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator

# PriceSourceã¯é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å›é¿
PriceSource = None


@dataclass
class UKFResult:
    """ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—çµæœ"""
    filtered_values: np.ndarray      # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    velocity_estimates: np.ndarray   # é€Ÿåº¦æ¨å®šå€¤
    acceleration_estimates: np.ndarray  # åŠ é€Ÿåº¦æ¨å®šå€¤
    uncertainty: np.ndarray          # æ¨å®šä¸ç¢ºå®Ÿæ€§ï¼ˆæ¨™æº–åå·®ï¼‰
    kalman_gains: np.ndarray         # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
    innovations: np.ndarray          # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäºˆæ¸¬èª¤å·®ï¼‰
    sigma_points: np.ndarray         # æœ€çµ‚ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    confidence_scores: np.ndarray    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    raw_values: np.ndarray          # å…ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿

# Alias for compatibility
UnscentedKalmanResult = UKFResult


@njit(fastmath=True, cache=True)
def generate_sigma_points(
    mean: np.ndarray, 
    covariance: np.ndarray, 
    alpha: float, 
    beta: float, 
    kappa: float
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    UKFç”¨ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰
    
    Args:
        mean: çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
        covariance: å…±åˆ†æ•£è¡Œåˆ—
        alpha: ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®åˆ†æ•£åº¦åˆã„
        beta: é«˜æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæƒ…å ±ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å ´åˆã¯2.0ï¼‰
        kappa: äºŒæ¬¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    Returns:
        sigma_points: ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆè¡Œåˆ—
        Wm: å¹³å‡è¨ˆç®—ç”¨é‡ã¿
        Wc: å…±åˆ†æ•£è¨ˆç®—ç”¨é‡ã¿
    """
    L = len(mean)  # çŠ¶æ…‹æ¬¡å…ƒ
    lambda_param = alpha * alpha * (L + kappa) - L
    
    # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆæ•°: 2L + 1
    n_sigma = 2 * L + 1
    sigma_points = np.zeros((n_sigma, L))
    
    # é‡ã¿è¨ˆç®—ï¼ˆä¿®æ­£ç‰ˆï¼‰
    Wm = np.zeros(n_sigma)
    Wc = np.zeros(n_sigma)
    
    # ä¸­å¿ƒç‚¹ã®é‡ã¿
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    # å‘¨è¾ºç‚¹ã®é‡ã¿
    weight_peripheral = 0.5 / (L + lambda_param)
    for i in range(1, n_sigma):
        Wm[i] = weight_peripheral
        Wc[i] = weight_peripheral
    
    # ä¸­å¿ƒã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆ
    sigma_points[0] = mean
    
    # å…±åˆ†æ•£è¡Œåˆ—ã®æ•°å€¤å®‰å®šåŒ–
    # å¯¾è§’è¦ç´ ã«å°ã•ãªå€¤ã‚’è¿½åŠ ã—ã¦æ­£å®šå€¤æ€§ã‚’ä¿è¨¼
    stabilized_cov = covariance.copy()
    for i in range(L):
        stabilized_cov[i, i] += 1e-9
    
    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
    scale = L + lambda_param
    scaled_cov = scale * stabilized_cov
    
    # å…±åˆ†æ•£è¡Œåˆ—ã®å¹³æ–¹æ ¹è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    try:
        sqrt_matrix = np.linalg.cholesky(scaled_cov)
    except:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºæœ‰å€¤åˆ†è§£
        try:
            eigenvals, eigenvecs = np.linalg.eigh(scaled_cov)
            # è² ã®å›ºæœ‰å€¤ã‚’ä¿®æ­£
            eigenvals = np.maximum(eigenvals, 1e-8)
            sqrt_matrix = eigenvecs @ np.diag(np.sqrt(eigenvals))
        except:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¯¾è§’è¡Œåˆ—
            sqrt_matrix = np.zeros((L, L))
            for i in range(L):
                sqrt_matrix[i, i] = np.sqrt(max(scaled_cov[i, i], 1e-8))
    
    # æ­£è² ã®ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
    for i in range(L):
        sigma_points[i + 1] = mean + sqrt_matrix[:, i]
        sigma_points[i + 1 + L] = mean - sqrt_matrix[:, i]
    
    return sigma_points, Wm, Wc


@njit(fastmath=True, cache=True)
def state_transition_function(state: np.ndarray, dt: float = 1.0) -> np.ndarray:
    """
    çŠ¶æ…‹é·ç§»é–¢æ•°ï¼ˆéç·šå½¢ï¼‰
    
    çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«: [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦]
    
    Args:
        state: ç¾åœ¨ã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
        dt: æ™‚é–“å·®åˆ†
    
    Returns:
        æ¬¡ã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
    """
    price, velocity, acceleration = state[0], state[1], state[2]
    
    # éç·šå½¢å‹•åŠ›å­¦ãƒ¢ãƒ‡ãƒ«
    # ä¾¡æ ¼ = ä¾¡æ ¼ + é€Ÿåº¦*dt + 0.5*åŠ é€Ÿåº¦*dt^2 + éç·šå½¢é …
    new_price = price + velocity * dt + 0.5 * acceleration * dt * dt
    
    # é€Ÿåº¦ã®æ¸›è¡°ã¨åŠ é€Ÿåº¦ã®å½±éŸ¿
    damping_factor = 0.95  # æ¸›è¡°ä¿‚æ•°
    new_velocity = velocity * damping_factor + acceleration * dt
    
    # åŠ é€Ÿåº¦ã®æ¸›è¡°ï¼ˆå¹³å‡å›å¸°ï¼‰
    new_acceleration = acceleration * 0.9
    
    return np.array([new_price, new_velocity, new_acceleration])


@njit(fastmath=True, cache=True)
def observation_function(state: np.ndarray) -> float:
    """
    è¦³æ¸¬é–¢æ•°ï¼ˆä¾¡æ ¼ã®ã¿è¦³æ¸¬ï¼‰
    
    Args:
        state: çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
    
    Returns:
        è¦³æ¸¬å€¤ï¼ˆä¾¡æ ¼ï¼‰
    """
    return state[0]  # ä¾¡æ ¼ã®ã¿ã‚’è¦³æ¸¬


@njit(fastmath=True, cache=True)
def calculate_unscented_kalman_filter(
    prices: np.ndarray,
    volatility: np.ndarray,
    alpha: float = 0.1,
    beta: float = 2.0,
    kappa: float = 0.0,
    process_noise_scale: float = 0.01,
    adaptive_noise: bool = True
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    æ­£ç¢ºãªç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å®Ÿè£…
    
    Args:
        prices: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        volatility: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šå€¤
        alpha: UKFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ¨å¥¨å€¤: 0.001ï¼‰
        beta: UKFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒã§ã¯2.0ï¼‰
        kappa: UKFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸0.0ï¼‰
        process_noise_scale: ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºã®ã‚¹ã‚±ãƒ¼ãƒ«
        adaptive_noise: é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®šã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        filtered_prices: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
        velocity_estimates: é€Ÿåº¦æ¨å®š
        acceleration_estimates: åŠ é€Ÿåº¦æ¨å®š
        uncertainty: ä¸ç¢ºå®Ÿæ€§
        kalman_gains: ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        innovations: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
        confidence_scores: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        final_sigma_points: æœ€çµ‚ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆ
    """
    n = len(prices)
    L = 3  # çŠ¶æ…‹æ¬¡å…ƒ [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦]
    
    if n < 5:
        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å ´åˆ - ç”Ÿã®ä¾¡æ ¼ã‚’ãã®ã¾ã¾è¿”ã™
        uncertainty_vals = np.full(n, 0.1)
        confidence_vals = np.full(n, 0.5)
        kalman_gains_vals = np.full(n, 0.1)
        return (prices.copy(), np.zeros(n), np.zeros(n), uncertainty_vals, 
                kalman_gains_vals, np.zeros(n), confidence_vals, np.zeros((2 * L + 1, L)))
    
    # çµæœé…åˆ—ã®åˆæœŸåŒ–
    filtered_prices = np.zeros(n)
    velocity_estimates = np.zeros(n)
    acceleration_estimates = np.zeros(n)
    uncertainty = np.zeros(n)
    kalman_gains = np.zeros(n)
    innovations = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.array([prices[0], 0.0, 0.0])  # [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦]
    P = np.array([[1.0, 0.0, 0.0],       # åˆæœŸå…±åˆ†æ•£è¡Œåˆ—
                  [0.0, 0.1, 0.0],
                  [0.0, 0.0, 0.01]])
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºè¡Œåˆ—
    Q = np.array([[process_noise_scale, 0.0, 0.0],
                  [0.0, process_noise_scale * 0.1, 0.0],
                  [0.0, 0.0, process_noise_scale * 0.01]])
    
    # åˆæœŸå€¤è¨­å®š
    filtered_prices[0] = prices[0]
    velocity_estimates[0] = 0.0
    acceleration_estimates[0] = 0.0
    uncertainty[0] = np.sqrt(P[0, 0])
    kalman_gains[0] = 0.5
    innovations[0] = 0.0
    confidence_scores[0] = 1.0
    
    # æœ€çµ‚ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    final_sigma_points = np.zeros((2 * L + 1, L))
    
    # UKFãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    for t in range(1, n):
        # === äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ— ===
        
        # 1. ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
        sigma_points, Wm, Wc = generate_sigma_points(x, P, alpha, beta, kappa)
        
        # 2. å„ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã‚’çŠ¶æ…‹é·ç§»é–¢æ•°ã«é€šã™
        n_sigma = len(sigma_points)
        sigma_points_pred = np.zeros((n_sigma, L))
        
        for i in range(n_sigma):
            sigma_points_pred[i] = state_transition_function(sigma_points[i], 1.0)
        
        # 3. äºˆæ¸¬çŠ¶æ…‹ã®å¹³å‡è¨ˆç®—
        x_pred = np.zeros(L)
        for i in range(n_sigma):
            x_pred += Wm[i] * sigma_points_pred[i]
        
        # 4. äºˆæ¸¬å…±åˆ†æ•£è¨ˆç®—
        P_pred = Q.copy()  # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºã‚’åŠ ç®—
        for i in range(n_sigma):
            diff = sigma_points_pred[i] - x_pred
            P_pred += Wc[i] * np.outer(diff, diff)
        
        # === æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ— ===
        
        # 5. è¦³æ¸¬ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆè¨ˆç®—
        z_sigma = np.zeros(n_sigma)
        for i in range(n_sigma):
            z_sigma[i] = observation_function(sigma_points_pred[i])
        
        # 6. äºˆæ¸¬è¦³æ¸¬å€¤ã®å¹³å‡
        z_pred = 0.0
        for i in range(n_sigma):
            z_pred += Wm[i] * z_sigma[i]
        
        # 7. è¦³æ¸¬ãƒã‚¤ã‚ºã®é©å¿œçš„èª¿æ•´
        if adaptive_noise and t >= 5:
            # æœ€è¿‘ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨
            recent_vol = np.mean(volatility[max(0, t-5):t])
            R = max(recent_vol * recent_vol, 0.0001)
        else:
            R = max(volatility[t] * volatility[t], 0.0001)
        
        # 8. ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å…±åˆ†æ•£ã¨ç›¸äº’å…±åˆ†æ•£è¨ˆç®—
        S = R  # è¦³æ¸¬ãƒã‚¤ã‚º
        Pxz = np.zeros(L)  # çŠ¶æ…‹-è¦³æ¸¬é–“ã®ç›¸äº’å…±åˆ†æ•£
        
        for i in range(n_sigma):
            z_diff = z_sigma[i] - z_pred
            x_diff = sigma_points_pred[i] - x_pred
            
            S += Wc[i] * z_diff * z_diff
            Pxz += Wc[i] * x_diff * z_diff
        
        # 9. ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³è¨ˆç®—
        if S > 1e-12:
            K = Pxz / S
        else:
            K = np.array([0.5, 0.0, 0.0])
        
        # 10. çŠ¶æ…‹æ›´æ–°
        innovation = prices[t] - z_pred
        x = x_pred + K * innovation
        P = P_pred - np.outer(K, K) * S
        
        # 11. æ•°å€¤å®‰å®šæ€§ã®ç¢ºä¿
        # å…±åˆ†æ•£è¡Œåˆ—ã®å¯¾è§’è¦ç´ ãŒè² ã«ãªã‚‰ãªã„ã‚ˆã†èª¿æ•´
        for i in range(L):
            if P[i, i] < 1e-8:
                P[i, i] = 1e-8
        
        # çŠ¶æ…‹å€¤ã®å¢ƒç•Œåˆ¶é™
        max_velocity = abs(prices[t]) * 0.5
        if abs(x[1]) > max_velocity:
            x[1] = np.sign(x[1]) * max_velocity
            
        max_acceleration = abs(prices[t]) * 0.1
        if abs(x[2]) > max_acceleration:
            x[2] = np.sign(x[2]) * max_acceleration
        
        # 12. çµæœã®ä¿å­˜
        filtered_prices[t] = x[0]
        velocity_estimates[t] = x[1]
        acceleration_estimates[t] = x[2]
        uncertainty[t] = np.sqrt(max(P[0, 0], 0))
        kalman_gains[t] = K[0]
        innovations[t] = innovation
        
        # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆä¸ç¢ºå®Ÿæ€§ã®é€†æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        confidence_scores[t] = 1.0 / (1.0 + uncertainty[t] * 10.0)
        
        # æœ€çµ‚ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ï¼ˆæœ€æ–°ã®ã‚‚ã®ï¼‰
        if t == n - 1:
            final_sigma_points = sigma_points_pred.copy()
    
    return (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
            kalman_gains, innovations, confidence_scores, final_sigma_points)


@njit(fastmath=True, cache=True)
def estimate_volatility(prices: np.ndarray, window: int = 10) -> np.ndarray:
    """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®š"""
    n = len(prices)
    volatility = np.full(n, 0.01)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    for i in range(window, n):
        window_prices = prices[i-window:i]
        vol = np.std(window_prices)
        volatility[i] = max(vol, 0.001)  # æœ€å°å€¤è¨­å®š
    
    # åˆæœŸå€¤ã®è£œå®Œ
    if n >= window:
        initial_vol = volatility[window]
        for i in range(window):
            volatility[i] = initial_vol
    
    return volatility


class UnscentedKalmanFilter(Indicator):
    """
    ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUKFï¼‰ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼
    
    æ­£ç¢ºãªUKFå®Ÿè£…ã«ã‚ˆã‚‹ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆæ³•ãƒ™ãƒ¼ã‚¹ã®éç·šå½¢çŠ¶æ…‹æ¨å®šï¼š
    - ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆã¨ä¼æ’­
    - é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š
    - è¤‡æ•°ã®ä¾¡æ ¼ã‚½ãƒ¼ã‚¹å¯¾å¿œ
    - é«˜ç²¾åº¦ãªéç·šå½¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    ç‰¹å¾´:
    - ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¨ˆç®—ä¸è¦
    - 3æ¬¡ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆã¾ã§æ­£ç¢º
    - å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
    """
    
    def __init__(
        self,
        src_type: str = 'close',           # ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
        alpha: float = 0.1,               # UKFã‚¢ãƒ«ãƒ•ã‚¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¿®æ­£: ã‚ˆã‚Šå¤§ããªå€¤ï¼‰
        beta: float = 2.0,                # UKFãƒ™ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        kappa: float = 0.0,               # UKFã‚«ãƒƒãƒ‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        process_noise_scale: float = 0.01,  # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆä¿®æ­£: ã‚ˆã‚Šå¤§ããªå€¤ï¼‰
        volatility_window: int = 10,      # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—çª“
        adaptive_noise: bool = True       # é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ ('close', 'hlc3', 'hl2', 'ohlc4')
            alpha: UKFã‚¢ãƒ«ãƒ•ã‚¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ¨å¥¨: 0.001ï¼‰
            beta: UKFãƒ™ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒ: 2.0ï¼‰
            kappa: UKFã‚«ãƒƒãƒ‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé€šå¸¸: 0.0ï¼‰
            process_noise_scale: ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºã®ã‚¹ã‚±ãƒ¼ãƒ«
            volatility_window: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            adaptive_noise: é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®šã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        # æŒ‡æ¨™åã®ä½œæˆ
        indicator_name = f"UKF(src={src_type}, Î±={alpha}, Î²={beta}, Îº={kappa})"
        super().__init__(indicator_name)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
        self.src_type = src_type.lower()
        self.alpha = alpha
        self.beta = beta
        self.kappa = kappa
        self.process_noise_scale = process_noise_scale
        self.volatility_window = volatility_window
        self.adaptive_noise = adaptive_noise
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®æ¤œè¨¼
        valid_sources = ['close', 'hlc3', 'hl2', 'ohlc4', 'high', 'low', 'open', 'oc2']
        if self.src_type not in valid_sources:
            raise ValueError(f"ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—: {src_type}ã€‚æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {', '.join(valid_sources)}")
        
        # çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._result_cache = {}
        self._max_cache_size = 10
        self._cache_keys = []
    
    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        try:
            if isinstance(data, pd.DataFrame):
                length = len(data)
                if length > 0:
                    first_val = float(data.iloc[0].get('close', data.iloc[0, -1]))
                    last_val = float(data.iloc[-1].get('close', data.iloc[-1, -1]))
                else:
                    first_val = last_val = 0.0
            else:
                length = len(data)
                if length > 0:
                    if data.ndim > 1:
                        first_val = float(data[0, -1])
                        last_val = float(data[-1, -1])
                    else:
                        first_val = float(data[0])
                        last_val = float(data[-1])
                else:
                    first_val = last_val = 0.0
            
            params_sig = f"{self.alpha}_{self.beta}_{self.kappa}_{self.src_type}_{self.process_noise_scale}"
            data_sig = (length, first_val, last_val)
            return f"{hash(data_sig)}_{hash(params_sig)}"
            
        except Exception:
            return f"{id(data)}_{self.alpha}_{self.beta}"
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UKFResult:
        """
        ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨ˆç®—
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯é…åˆ—ï¼‰
        
        Returns:
            UKFResult: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            
            if data_hash in self._result_cache:
                if data_hash in self._cache_keys:
                    self._cache_keys.remove(data_hash)
                self._cache_keys.append(data_hash)
                cached_result = self._result_cache[data_hash]
                return UKFResult(
                    filtered_values=cached_result.filtered_values.copy(),
                    velocity_estimates=cached_result.velocity_estimates.copy(),
                    acceleration_estimates=cached_result.acceleration_estimates.copy(),
                    uncertainty=cached_result.uncertainty.copy(),
                    kalman_gains=cached_result.kalman_gains.copy(),
                    innovations=cached_result.innovations.copy(),
                    sigma_points=cached_result.sigma_points.copy(),
                    confidence_scores=cached_result.confidence_scores.copy(),
                    raw_values=cached_result.raw_values.copy()
                )
            
            # PriceSourceã‚’é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            global PriceSource
            if PriceSource is None:
                try:
                    from ..price_source import PriceSource
                except ImportError:
                    try:
                        from indicators.price_source import PriceSource
                    except ImportError:
                        # åŸºæœ¬çš„ãªä¾¡æ ¼æŠ½å‡ºã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        if isinstance(data, pd.DataFrame):
                            if self.src_type == 'close':
                                src_prices = data['close'].values
                            elif self.src_type == 'hlc3':
                                src_prices = ((data['high'] + data['low'] + data['close']) / 3.0).values
                            else:
                                src_prices = data['close'].values
                        else:
                            src_prices = data[:, 3] if data.ndim > 1 else data
                        
                        if len(src_prices) < 5:
                            return self._create_empty_result(len(src_prices), src_prices)
                        
                        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šã«ç›´æ¥é€²ã‚€
                        volatility = estimate_volatility(src_prices, self.volatility_window)
                        
                        # UKFè¨ˆç®—ã«ç›´æ¥é€²ã‚€
                        (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
                         kalman_gains, innovations, confidence_scores, final_sigma_points) = calculate_unscented_kalman_filter(
                            src_prices, volatility, self.alpha, self.beta, self.kappa,
                            self.process_noise_scale, self.adaptive_noise
                        )
                        
                        # çµæœä½œæˆ
                        result = UKFResult(
                            filtered_values=filtered_prices.copy(),
                            velocity_estimates=velocity_estimates.copy(),
                            acceleration_estimates=acceleration_estimates.copy(),
                            uncertainty=uncertainty.copy(),
                            kalman_gains=kalman_gains.copy(),
                            innovations=innovations.copy(),
                            sigma_points=final_sigma_points.copy(),
                            confidence_scores=confidence_scores.copy(),
                            raw_values=src_prices.copy()
                        )
                        
                        self._values = filtered_prices  # åŸºåº•ã‚¯ãƒ©ã‚¹ç”¨
                        return result
            
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            src_prices = PriceSource.calculate_source(data, self.src_type)
            
            if len(src_prices) < 5:
                return self._create_empty_result(len(src_prices), src_prices)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®š
            volatility = estimate_volatility(src_prices, self.volatility_window)
            
            # UKFè¨ˆç®—
            (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
             kalman_gains, innovations, confidence_scores, final_sigma_points) = calculate_unscented_kalman_filter(
                src_prices, volatility, self.alpha, self.beta, self.kappa,
                self.process_noise_scale, self.adaptive_noise
            )
            
            # çµæœä½œæˆ
            result = UKFResult(
                filtered_values=filtered_prices.copy(),
                velocity_estimates=velocity_estimates.copy(),
                acceleration_estimates=acceleration_estimates.copy(),
                uncertainty=uncertainty.copy(),
                kalman_gains=kalman_gains.copy(),
                innovations=innovations.copy(),
                sigma_points=final_sigma_points.copy(),
                confidence_scores=confidence_scores.copy(),
                raw_values=src_prices.copy()
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
            if len(self._result_cache) >= self._max_cache_size and self._cache_keys:
                oldest_key = self._cache_keys.pop(0)
                if oldest_key in self._result_cache:
                    del self._result_cache[oldest_key]
            
            self._result_cache[data_hash] = result
            self._cache_keys.append(data_hash)
            
            self._values = filtered_prices  # åŸºåº•ã‚¯ãƒ©ã‚¹ç”¨
            return result
            
        except Exception as e:
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"UKFè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¿”ã™
            if isinstance(data, (pd.DataFrame, np.ndarray)) and len(data) > 0:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                return self._create_empty_result(len(src_prices), src_prices)
            else:
                return self._create_empty_result(0, np.array([]))
    
    def _create_empty_result(self, length: int, raw_prices: np.ndarray) -> UKFResult:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        return UKFResult(
            filtered_values=np.full(length, np.nan),
            velocity_estimates=np.full(length, np.nan),
            acceleration_estimates=np.full(length, np.nan),
            uncertainty=np.full(length, np.nan),
            kalman_gains=np.full(length, np.nan),
            innovations=np.full(length, np.nan),
            sigma_points=np.full((7, 3), np.nan),  # 2*3+1=7ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆ, 3æ¬¡å…ƒçŠ¶æ…‹
            confidence_scores=np.full(length, np.nan),
            raw_values=raw_prices
        )
    
    def get_values(self) -> Optional[np.ndarray]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼ã‚’å–å¾—"""
        if not self._result_cache:
            return None
        
        result = self._get_latest_result()
        return result.filtered_values.copy() if result else None
    
    def get_velocity_estimates(self) -> Optional[np.ndarray]:
        """é€Ÿåº¦æ¨å®šå€¤ã‚’å–å¾—"""
        result = self._get_latest_result()
        return result.velocity_estimates.copy() if result else None
    
    def get_acceleration_estimates(self) -> Optional[np.ndarray]:
        """åŠ é€Ÿåº¦æ¨å®šå€¤ã‚’å–å¾—"""
        result = self._get_latest_result()
        return result.acceleration_estimates.copy() if result else None
    
    def get_uncertainty(self) -> Optional[np.ndarray]:
        """ä¸ç¢ºå®Ÿæ€§ã‚’å–å¾—"""
        result = self._get_latest_result()
        return result.uncertainty.copy() if result else None
    
    def get_confidence_scores(self) -> Optional[np.ndarray]:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        result = self._get_latest_result()
        return result.confidence_scores.copy() if result else None
    
    def get_innovations(self) -> Optional[np.ndarray]:
        """ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäºˆæ¸¬èª¤å·®ï¼‰ã‚’å–å¾—"""
        result = self._get_latest_result()
        return result.innovations.copy() if result else None
    
    def get_sigma_points(self) -> Optional[np.ndarray]:
        """æœ€çµ‚ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—"""
        result = self._get_latest_result()
        return result.sigma_points.copy() if result else None
    
    def _get_latest_result(self) -> Optional[UKFResult]:
        """æœ€æ–°ã®çµæœã‚’å–å¾—"""
        if not self._result_cache:
            return None
        
        if self._cache_keys:
            return self._result_cache[self._cache_keys[-1]]
        else:
            return next(iter(self._result_cache.values()))
    
    def get_filter_metadata(self) -> Dict:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        result = self._get_latest_result()
        if not result:
            return {}
        
        return {
            'filter_type': 'Unscented Kalman Filter',
            'src_type': self.src_type,
            'alpha': self.alpha,
            'beta': self.beta,
            'kappa': self.kappa,
            'process_noise_scale': self.process_noise_scale,
            'data_points': len(result.filtered_values),
            'avg_uncertainty': np.nanmean(result.uncertainty),
            'avg_confidence': np.nanmean(result.confidence_scores),
            'avg_velocity': np.nanmean(np.abs(result.velocity_estimates)),
            'avg_acceleration': np.nanmean(np.abs(result.acceleration_estimates)),
            'sigma_point_count': len(result.sigma_points)
        }
    
    def reset(self) -> None:
        """çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._result_cache = {}
        self._cache_keys = [] 