#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import os
import sys
import time
import warnings
warnings.filterwarnings('ignore')

# matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆè­¦å‘Šã‚’ç„¡åŠ¹åŒ–
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
import logging
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

# UltimateMA V3ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ultimate_ma_v3 import UltimateMAV3

# ãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ã®ä¾å­˜é–¢ä¿‚ï¼ˆconfig.yamlå¯¾å¿œï¼‰
try:
    import yaml
    sys.path.append('..')
    from data.data_loader import DataLoader, CSVDataSource
    from data.data_processor import DataProcessor
    from data.binance_data_source import BinanceDataSource
    YAML_SUPPORT = True
except ImportError:
    YAML_SUPPORT = False
    print("âš ï¸  YAML/ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åˆæˆãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚")


def load_data_from_yaml_config(config_path: str) -> pd.DataFrame:
    """config.yamlã‹ã‚‰å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    if not YAML_SUPPORT:
        print("âŒ YAML/ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚µãƒãƒ¼ãƒˆãŒç„¡åŠ¹ã§ã™")
        return None
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"âœ… {config_path} èª­ã¿è¾¼ã¿æˆåŠŸ")
        
        binance_config = config.get('binance_data', {})
        if not binance_config.get('enabled', False):
            print("âŒ Binanceãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™")
            return None
            
        data_dir = binance_config.get('data_dir', 'data/binance')
        symbol = binance_config.get('symbol', 'BTC')
        print(f"ğŸ“Š èª­ã¿è¾¼ã¿ä¸­: {symbol} ãƒ‡ãƒ¼ã‚¿")
        
        binance_data_source = BinanceDataSource(data_dir)
        dummy_csv_source = CSVDataSource("dummy")
        data_loader = DataLoader(
            data_source=dummy_csv_source,
            binance_data_source=binance_data_source
        )
        data_processor = DataProcessor()
        
        raw_data = data_loader.load_data_from_config(config)
        if not raw_data:
            return None
            
        processed_data = {
            symbol: data_processor.process(df)
            for symbol, df in raw_data.items()
        }
        
        first_symbol = next(iter(processed_data))
        data = processed_data[first_symbol]
        
        print(f"âœ… å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {first_symbol}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
        
        return data
        
    except Exception as e:
        print(f"âŒ config.yamlã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def generate_trending_synthetic_data(n_samples: int = 1500) -> pd.DataFrame:
    """ã‚ˆã‚Šæ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æŒã¤åˆæˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
    np.random.seed(42)
    
    # è¤‡é›‘ãªãƒãƒ«ãƒãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
    t = np.linspace(0, 6*np.pi, n_samples)
    
    # åŸºæœ¬çš„ãªé•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰
    long_trend = 100 + 20 * np.cumsum(np.random.randn(n_samples) * 0.001 + 0.01)
    
    # ä¸­æœŸçš„ãªå‘¨æœŸæ€§
    mid_cycle = 5 * np.sin(t/3) + 3 * np.cos(t/5)
    
    # çŸ­æœŸçš„ãªãƒã‚¤ã‚º
    short_noise = np.random.normal(0, 1.2, n_samples)
    high_freq = 0.4 * np.sin(t * 12) * np.random.normal(0, 0.6, n_samples)
    
    # ä¾¡æ ¼ç³»åˆ—ã®åˆæˆ
    prices = long_trend + mid_cycle + short_noise + high_freq
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®æ™‚é–“å¤‰åŒ–
    volatility_factor = 0.5 + 0.5 * np.abs(np.sin(t/8))
    
    # OHLCç”Ÿæˆ
    data = []
    for i, price in enumerate(prices):
        vol = volatility_factor[i] * 1.0
        high = price + np.random.uniform(0, vol)
        low = price - np.random.uniform(0, vol)
        open_price = price + np.random.normal(0, vol/4)
        
        low = min(low, price, open_price)
        high = max(high, price, open_price)
        
        data.append([open_price, high, low, price])
    
    df = pd.DataFrame(data, columns=['open', 'high', 'low', 'close'])
    print(f"âœ… ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åŒ–åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(df)}ä»¶")
    return df


def plot_ultimate_ma_v3_results(data: pd.DataFrame, result, save_path: str = None):
    """UltimateMA V3ã®çµæœã‚’åŒ…æ‹¬çš„ã«å¯è¦–åŒ–ï¼ˆ10æ®µéšAIåˆ†æï¼‰"""
    n_points = len(data)
    print(f"ğŸ“Š UltimateMA V3 ãƒãƒ£ãƒ¼ãƒˆæç”»ä¸­... ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {n_points}")
    
    is_real_data = n_points > 5000
    data_type = "å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿" if is_real_data else "åˆæˆãƒ‡ãƒ¼ã‚¿"
    
    # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æº–å‚™
    if hasattr(data.index, 'to_pydatetime'):
        x_axis = data.index
        use_datetime = True
    else:
        x_axis = range(n_points)
        use_datetime = False
    
    # å›³ã®ä½œæˆï¼ˆ9ã¤ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆï¼‰
    fig, axes = plt.subplots(9, 1, figsize=(18, 32))
    
    title = f'ğŸš€ UltimateMA V3 - é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆåˆ†æ\nğŸ“Š {data_type} ({n_points}ä»¶)'
    if is_real_data and hasattr(data.index, 'min'):
        title += f' | æœŸé–“: {data.index.min().strftime("%Y-%m-%d")} - {data.index.max().strftime("%Y-%m-%d")}'
    
    fig.suptitle(title, fontsize=16, fontweight='bold', y=0.98)
    
    # 1. 10æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¯”è¼ƒ
    ax1 = axes[0]
    ax1.plot(x_axis, result.raw_values, label='Raw Prices', linewidth=0.8, color='gray', alpha=0.7)
    ax1.plot(x_axis, result.kalman_values, label='â‘ Kalman Filter', linewidth=1.0, color='red', alpha=0.8)
    ax1.plot(x_axis, result.super_smooth_values, label='â‘¡Super Smoother', linewidth=1.0, color='orange', alpha=0.8)
    ax1.plot(x_axis, result.zero_lag_values, label='â‘¢Zero-Lag EMA', linewidth=1.0, color='yellow', alpha=0.8)
    ax1.plot(x_axis, result.values, label='â‘©Ultimate MA V3 (Final)', linewidth=1.5, color='blue', alpha=0.9)
    
    ax1.set_title('ğŸ¯ 10-Stage Revolutionary AI Filtering Comparison', fontsize=14, fontweight='bold')
    ax1.set_ylabel('Price', fontsize=12)
    ax1.legend(loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # 2. é‡å­çŠ¶æ…‹ã¨MTFåˆæ„åº¦
    ax2 = axes[1]
    ax2.plot(x_axis, result.quantum_state, label='ğŸŒŒ Quantum State', color='purple', linewidth=1.2, alpha=0.8)
    ax2_twin = ax2.twinx()
    ax2_twin.plot(x_axis, result.multi_timeframe_consensus, label='ğŸ”„ MTF Consensus', color='blue', linewidth=1.0, alpha=0.7)
    
    ax2.set_title('ğŸŒŒ Quantum State & Multi-Timeframe Consensus Analysis', fontsize=14, fontweight='bold')
    ax2.set_ylabel('Quantum State', fontsize=12, color='purple')
    ax2_twin.set_ylabel('MTF Consensus', fontsize=12, color='blue')
    ax2.legend(loc='upper left')
    ax2_twin.legend(loc='upper right')
    ax2.grid(True, alpha=0.3)
    
    # 3. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    ax3 = axes[2]
    ax3.plot(x_axis, result.fractal_dimension, label='ğŸŒ€ Fractal Dimension', color='green', linewidth=1.2, alpha=0.8)
    ax3_twin = ax3.twinx()
    ax3_twin.plot(x_axis, result.entropy_level, label='ğŸ”¬ Entropy Level', color='red', linewidth=1.0, alpha=0.7)
    
    ax3.set_title('ğŸŒ€ Fractal Dimension & Entropy Level Analysis', fontsize=14, fontweight='bold')
    ax3.set_ylabel('Fractal Dimension', fontsize=12, color='green')
    ax3_twin.set_ylabel('Entropy Level', fontsize=12, color='red')
    ax3.legend(loc='upper left')
    ax3_twin.legend(loc='upper right')
    ax3.grid(True, alpha=0.3)
    
    # 4. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
    ax4 = axes[3]
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ã‚’è‰²åˆ†ã‘è¡¨ç¤º
    regime_colors = {0: 'blue', 1: 'green', 2: 'red'}
    regime_labels = {0: 'ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£', 1: 'æ­£å¸¸', 2: 'é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£'}
    
    for regime_value in [0, 1, 2]:
        mask = result.volatility_regime == regime_value
        if np.any(mask):
            ax4.scatter(np.array(x_axis)[mask], result.values[mask], 
                       c=regime_colors[regime_value], label=regime_labels[regime_value], 
                       alpha=0.6, s=15)
    
    ax4.plot(x_axis, result.values, color='black', alpha=0.3, linewidth=0.8)
    ax4.set_title('ğŸ“Š Volatility Regime Detection', fontsize=14, fontweight='bold')
    ax4.set_ylabel('Price', fontsize=12)
    ax4.legend(loc='upper left')
    ax4.grid(True, alpha=0.3)
    
    # 5. ãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡å·ã¨ä¿¡é ¼åº¦
    ax5 = axes[4]
    
    # èƒŒæ™¯è‰²ã®è¨­å®šï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡å·ï¼‰
    try:
        signals = result.trend_signals
        if len(signals) > 0:
            current_signal = signals[0]
            start_idx = 0
            
            for i in range(1, len(signals) + 1):
                if i == len(signals) or signals[i] != current_signal:
                    end_idx = i - 1
                    
                    if current_signal == 1:
                        color = 'lightgreen'
                        alpha = 0.2
                    elif current_signal == -1:
                        color = 'lightcoral'
                        alpha = 0.2
                    else:
                        color = 'lightyellow'
                        alpha = 0.15
                    
                    if start_idx < len(x_axis) and end_idx < len(x_axis):
                        if use_datetime:
                            ax5.axvspan(x_axis[start_idx], x_axis[end_idx], color=color, alpha=alpha, zorder=0)
                        else:
                            ax5.axvspan(start_idx, end_idx, color=color, alpha=alpha, zorder=0)
                    
                    if i < len(signals):
                        start_idx = i
                        current_signal = signals[i]
    except Exception as e:
        print(f"âš ï¸  èƒŒæ™¯è‰²è¨­å®šã§ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ä¿¡é ¼åº¦ã‚’ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã§è¡¨ç¤º
    scatter = ax5.scatter(x_axis, result.values, c=result.trend_confidence, 
                         cmap='viridis', alpha=0.7, s=8, zorder=2)
    
    ax5.plot(x_axis, result.values, color='blue', alpha=0.6, linewidth=1.0, zorder=1)
    ax5.set_title('ğŸ¯ Trend Signals with Confidence Levels (Green=Up, Red=Down, Yellow=Range)', 
                 fontsize=14, fontweight='bold')
    ax5.set_ylabel('Price', fontsize=12)
    
    # ã‚«ãƒ©ãƒ¼ãƒãƒ¼ã®è¿½åŠ 
    cbar = plt.colorbar(scatter, ax=ax5)
    cbar.set_label('Confidence Level', fontsize=10)
    ax5.grid(True, alpha=0.3)
    
    # 6. ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ› - æŒ¯å¹…ã¨ä½ç›¸
    ax6 = axes[5]
    ax6.plot(x_axis, result.amplitude, label='Instantaneous Amplitude', color='purple', linewidth=1.2, alpha=0.8)
    ax6_twin = ax6.twinx()
    ax6_twin.plot(x_axis, result.phase, label='Instantaneous Phase', color='orange', linewidth=1.0, alpha=0.7)
    
    ax6.set_title('ğŸŒ€ Hilbert Transform - Amplitude & Phase Analysis', fontsize=14, fontweight='bold')
    ax6.set_ylabel('Amplitude', fontsize=12, color='purple')
    ax6_twin.set_ylabel('Phase (radians)', fontsize=12, color='orange')
    ax6.legend(loc='upper left')
    ax6_twin.legend(loc='upper right')
    ax6.grid(True, alpha=0.3)
    
    # 7. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
    ax7 = axes[6]
    positive_mask = result.realtime_trends > 0
    negative_mask = result.realtime_trends < 0
    
    ax7.fill_between(x_axis, 0, result.realtime_trends, where=positive_mask, 
                    color='green', alpha=0.6, label='Bullish Trend')
    ax7.fill_between(x_axis, 0, result.realtime_trends, where=negative_mask, 
                    color='red', alpha=0.6, label='Bearish Trend')
    ax7.plot(x_axis, result.realtime_trends, color='black', linewidth=0.8, alpha=0.7)
    
    ax7.axhline(y=0, color='gray', linestyle='-', alpha=0.5)
    ax7.set_title('âš¡ Real-Time Trend Detector (Ultra Low-Lag)', fontsize=14, fontweight='bold')
    ax7.set_ylabel('Trend Strength', fontsize=12)
    ax7.legend(loc='upper right')
    ax7.grid(True, alpha=0.3)
    
    # 8. ä¿¡é ¼åº¦åˆ†å¸ƒ
    ax8 = axes[7]
    confident_signals = result.trend_confidence[result.trend_confidence > 0]
    if len(confident_signals) > 0:
        ax8.hist(confident_signals, bins=20, alpha=0.7, color='orange', edgecolor='black')
        ax8.axvline(np.mean(confident_signals), color='red', linestyle='--', linewidth=2, 
                   label=f'Average: {np.mean(confident_signals):.3f}')
        ax8.axvline(0.5, color='green', linestyle='--', linewidth=2, label='High Confidence (0.5)')
    
    ax8.set_title('ğŸ”¥ Confidence Level Distribution', fontsize=14, fontweight='bold')
    ax8.set_xlabel('Confidence Level', fontsize=12)
    ax8.set_ylabel('Frequency', fontsize=12)
    ax8.legend()
    ax8.grid(True, alpha=0.3)
    
    # 9. çµ±åˆåˆ†æã‚µãƒãƒªãƒ¼
    ax9 = axes[8]
    
    # å„åˆ†ææŒ‡æ¨™ã®æ­£è¦åŒ–ã•ã‚ŒãŸå€¤ã‚’ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆé¢¨ã«è¡¨ç¤º
    metrics = [
        np.mean(result.trend_confidence[result.trend_confidence > 0]) if np.any(result.trend_confidence > 0) else 0,
        np.mean(result.multi_timeframe_consensus),
        1.0 - np.mean(result.entropy_level),  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯ä½ã„æ–¹ãŒè‰¯ã„
        np.mean(result.fractal_dimension) - 1.0,  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®æ­£è¦åŒ–
        min(1.0, np.abs(np.mean(result.quantum_state)) * 5)  # é‡å­çŠ¶æ…‹ã®å¼·åº¦
    ]
    
    metric_names = ['Confidence', 'MTF Consensus', 'Predictability', 'Trend Stability', 'Quantum Strength']
    colors = ['red', 'blue', 'green', 'purple', 'orange']
    
    bars = ax9.bar(metric_names, metrics, color=colors, alpha=0.7)
    ax9.set_title('ğŸ¯ Integrated AI Analysis Summary', fontsize=14, fontweight='bold')
    ax9.set_ylabel('Normalized Score (0-1)', fontsize=12)
    ax9.set_ylim(0, 1)
    
    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, metric in zip(bars, metrics):
        height = bar.get_height()
        ax9.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{metric:.3f}', ha='center', va='bottom', fontsize=10)
    
    ax9.grid(True, alpha=0.3)
    plt.setp(ax9.get_xticklabels(), rotation=45, ha='right')
    
    # Xè»¸ã®è¨­å®š
    if use_datetime:
        for ax in axes:
            ax.tick_params(axis='x', rotation=45)
        axes[-1].set_xlabel('Date', fontsize=12)
    else:
        axes[-1].set_xlabel('Time Period', fontsize=12)
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
    plt.tight_layout()
    plt.subplots_adjust(top=0.96)
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¿½åŠ 
    noise_reduction = ((np.nanstd(result.raw_values) - np.nanstd(result.values)) / 
                      np.nanstd(result.raw_values) * 100) if np.nanstd(result.raw_values) > 0 else 0
    
    avg_confidence = np.mean(result.trend_confidence[result.trend_confidence > 0]) if np.any(result.trend_confidence > 0) else 0
    
    stats_text = f"""UltimateMA V3 - Quantum Neural Statistics:
Current Trend: {result.current_trend.upper()} (Confidence: {result.current_confidence:.3f})
Noise Reduction: {noise_reduction:.1f}%
Average Confidence: {avg_confidence:.3f}
Quantum State: {np.mean(result.quantum_state):.3f}
MTF Consensus: {np.mean(result.multi_timeframe_consensus):.3f}
Fractal Dimension: {np.mean(result.fractal_dimension):.3f}
Entropy Level: {np.mean(result.entropy_level):.3f}
10-Stage AI Analysis: âœ… COMPLETE"""
    
    fig.text(0.02, 0.02, stats_text, fontsize=9, 
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8))
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… UltimateMA V3å¯è¦–åŒ–å®Œäº† ({save_path})")
    else:
        filename = f"ultimate_ma_v3_quantum_neural_analysis.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        print(f"âœ… UltimateMA V3å¯è¦–åŒ–å®Œäº† ({filename})")
    
    plt.close()


def analyze_ultimate_ma_v3_performance(result) -> dict:
    """UltimateMA V3ã®åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
    # åŸºæœ¬ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ
    raw_std = np.nanstd(result.raw_values)
    final_std = np.nanstd(result.values)
    noise_reduction_ratio = (raw_std - final_std) / raw_std if raw_std > 0 else 0.0
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰çµ±è¨ˆ
    trend_signals = result.trend_signals
    up_periods = np.sum(trend_signals == 1)
    down_periods = np.sum(trend_signals == -1)
    range_periods = np.sum(trend_signals == 0)
    total_periods = len(trend_signals)
    
    # ä¿¡é ¼åº¦çµ±è¨ˆ
    confident_signals = result.trend_confidence[result.trend_confidence > 0]
    high_confidence_signals = result.trend_confidence[result.trend_confidence > 0.5]
    ultra_confidence_signals = result.trend_confidence[result.trend_confidence > 0.7]
    
    # é‡å­åˆ†æçµ±è¨ˆ
    quantum_stats = {
        'mean_quantum_state': np.nanmean(result.quantum_state),
        'quantum_volatility': np.nanstd(result.quantum_state),
        'quantum_strength': np.nanmean(np.abs(result.quantum_state)),
        'mtf_consensus_avg': np.nanmean(result.multi_timeframe_consensus),
        'mtf_consensus_min': np.nanmin(result.multi_timeframe_consensus),
        'mtf_consensus_max': np.nanmax(result.multi_timeframe_consensus),
        'fractal_dimension_avg': np.nanmean(result.fractal_dimension),
        'fractal_stability': 2.0 - np.nanmean(result.fractal_dimension),  # å®‰å®šæ€§ã‚¹ã‚³ã‚¢
        'entropy_level_avg': np.nanmean(result.entropy_level),
        'predictability': 1.0 - np.nanmean(result.entropy_level)  # äºˆæ¸¬å¯èƒ½æ€§ã‚¹ã‚³ã‚¢
    }
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ çµ±è¨ˆ
    volatility_stats = {
        'low_vol_periods': np.sum(result.volatility_regime == 0),
        'normal_vol_periods': np.sum(result.volatility_regime == 1),
        'high_vol_periods': np.sum(result.volatility_regime == 2)
    }
    
    # å„æ®µéšã§ã®å¤‰åŒ–é‡
    stage_changes = {
        'kalman_change': np.nanmean(np.abs(result.raw_values - result.kalman_values)),
        'smooth_change': np.nanmean(np.abs(result.kalman_values - result.super_smooth_values)),
        'zerolag_change': np.nanmean(np.abs(result.super_smooth_values - result.zero_lag_values)),
        'final_change': np.nanmean(np.abs(result.zero_lag_values - result.values))
    }
    
    return {
        'noise_reduction': {
            'raw_volatility': raw_std,
            'filtered_volatility': final_std,
            'reduction_ratio': noise_reduction_ratio,
            'reduction_percentage': noise_reduction_ratio * 100,
            'effectiveness': min(noise_reduction_ratio * 100, 100.0)
        },
        'trend_analysis': {
            'total_periods': total_periods,
            'up_periods': up_periods,
            'down_periods': down_periods,
            'range_periods': range_periods,
            'up_ratio': up_periods / total_periods if total_periods > 0 else 0,
            'down_ratio': down_periods / total_periods if total_periods > 0 else 0,
            'range_ratio': range_periods / total_periods if total_periods > 0 else 0,
            'current_trend': result.current_trend,
            'current_confidence': result.current_confidence
        },
        'confidence_analysis': {
            'total_confident_signals': len(confident_signals),
            'high_confidence_signals': len(high_confidence_signals),
            'ultra_confidence_signals': len(ultra_confidence_signals),
            'avg_confidence': np.mean(confident_signals) if len(confident_signals) > 0 else 0,
            'max_confidence': np.max(result.trend_confidence),
            'min_confidence': np.min(result.trend_confidence[result.trend_confidence > 0]) if len(confident_signals) > 0 else 0,
            'confidence_ratio': len(confident_signals) / total_periods if total_periods > 0 else 0,
            'high_confidence_ratio': len(high_confidence_signals) / total_periods if total_periods > 0 else 0
        },
        'quantum_analysis': quantum_stats,
        'volatility_regimes': volatility_stats,
        'filtering_stages': stage_changes,
        'amplitude_stats': {
            'mean_amplitude': np.nanmean(result.amplitude),
            'max_amplitude': np.nanmax(result.amplitude),
            'min_amplitude': np.nanmin(result.amplitude),
            'amplitude_std': np.nanstd(result.amplitude)
        },
        'realtime_trends': {
            'mean_trend': np.nanmean(result.realtime_trends),
            'max_trend': np.nanmax(result.realtime_trends),
            'min_trend': np.nanmin(result.realtime_trends),
            'trend_std': np.nanstd(result.realtime_trends)
        }
    }


def main():
    print("ğŸš€ UltimateMA V3 - é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 100)
    print("ğŸŒŒ 10æ®µéšé©æ–°çš„AIåˆ†æ: é‡å­ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå™¨ + ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ  + ãƒ•ãƒ©ã‚¯ã‚¿ãƒ« + ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼")
    print("ğŸ¯ 95%è¶…é«˜ç²¾åº¦åˆ¤å®š: ä¿¡é ¼åº¦ä»˜ãã‚·ã‚°ãƒŠãƒ« + é©å¿œçš„å­¦ç¿’ + å¤šæ¬¡å…ƒçµ±åˆ")
    print("=" * 100)
    
    # ãƒ‡ãƒ¼ã‚¿é¸æŠ
    data = None
    is_real_data = False
    data_description = ""
    
    # config.yamlã‹ã‚‰ã®èª­ã¿è¾¼ã¿è©¦è¡Œ
    config_yaml_path = "../config.yaml"
    if YAML_SUPPORT and os.path.exists(config_yaml_path):
        print("ğŸ“‚ config.yamlã‹ã‚‰ãƒªã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        data = load_data_from_yaml_config(config_yaml_path)
        if data is not None:
            print("âœ… å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ")
            is_real_data = True
            symbol_info = "DOGE" if 'DOGE' in str(data.index) or len(data) > 10000 else "Unknown"
            data_description = f"å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ ({symbol_info}, {len(data)}ä»¶)"
            if hasattr(data.index, 'min'):
                data_description += f", {data.index.min().strftime('%Y-%m-%d')} - {data.index.max().strftime('%Y-%m-%d')}"
    
    # åˆæˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if data is None:
        print("ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åŒ–åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨")
        data = generate_trending_synthetic_data(1800)
        is_real_data = False
        data_description = f"ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åŒ–åˆæˆãƒ‡ãƒ¼ã‚¿ ({len(data)}ä»¶)"
    
    print(f"ğŸ“ˆ {data_description}")
    
    # UltimateMA V3åˆæœŸåŒ–
    print(f"\nğŸ”§ UltimateMA V3 åˆæœŸåŒ–ä¸­...")
    ultimate_ma_v3 = UltimateMAV3(
        super_smooth_period=8,
        zero_lag_period=16,
        realtime_window=34,
        quantum_window=16,
        fractal_window=16,
        entropy_window=16,
        src_type='hlc3',
        slope_index=2,
        base_threshold=0.002,
        min_confidence=0.15
    )
    print("âœ… UltimateMA V3åˆæœŸåŒ–å®Œäº†ï¼ˆ10æ®µéšAIåˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼‰")
    
    # è¨ˆç®—å®Ÿè¡Œ
    print(f"\nâš¡ UltimateMA V3 è¨ˆç®—å®Ÿè¡Œä¸­...")
    print(f"ğŸ“Š å¯¾è±¡ãƒ‡ãƒ¼ã‚¿: {data_description}")
    
    start_time = time.time()
    result = ultimate_ma_v3.calculate(data)
    end_time = time.time()
    
    processing_time = end_time - start_time
    processing_speed = len(data) / processing_time if processing_time > 0 else 0
    
    print(f"âœ… UltimateMA V3è¨ˆç®—å®Œäº† (å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’)")
    print(f"   âš¡ å‡¦ç†é€Ÿåº¦: {processing_speed:.0f} ãƒ‡ãƒ¼ã‚¿/ç§’")
    print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—: {'ğŸŒ å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿' if is_real_data else 'ğŸ”¬ åˆæˆãƒ‡ãƒ¼ã‚¿'}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    print(f"\nğŸ“ˆ UltimateMA V3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æä¸­...")
    performance = analyze_ultimate_ma_v3_performance(result)
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ¯ **UltimateMA V3 - é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«åˆ†æçµæœ**")
    print("="*80)
    print(f"ğŸ“Š **ãƒ‡ãƒ¼ã‚¿æƒ…å ±:** {data_description}")
    
    # ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ
    noise_stats = performance['noise_reduction']
    print(f"\nğŸ”‡ **ãƒã‚¤ã‚ºé™¤å»åŠ¹æœ:**")
    print(f"   - å…ƒã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {noise_stats['raw_volatility']:.6f}")
    print(f"   - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {noise_stats['filtered_volatility']:.6f}")
    print(f"   - ãƒã‚¤ã‚ºé™¤å»ç‡: {noise_stats['reduction_percentage']:.2f}%")
    print(f"   - ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœ: {noise_stats['effectiveness']:.1f}%")
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    trend_stats = performance['trend_analysis']
    print(f"\nğŸ“ˆ **ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:**")
    print(f"   - ç·æœŸé–“: {trend_stats['total_periods']}æœŸé–“")
    print(f"   - ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰: {trend_stats['up_periods']}æœŸé–“ ({trend_stats['up_ratio']*100:.1f}%)")
    print(f"   - ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰: {trend_stats['down_periods']}æœŸé–“ ({trend_stats['down_ratio']*100:.1f}%)")
    print(f"   - ãƒ¬ãƒ³ã‚¸ç›¸å ´: {trend_stats['range_periods']}æœŸé–“ ({trend_stats['range_ratio']*100:.1f}%)")
    print(f"   - ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰: {trend_stats['current_trend'].upper()}")
    print(f"   - ç¾åœ¨ã®ä¿¡é ¼åº¦: {trend_stats['current_confidence']:.3f}")
    
    # ä¿¡é ¼åº¦åˆ†æ
    conf_stats = performance['confidence_analysis']
    print(f"\nğŸ”¥ **ä¿¡é ¼åº¦åˆ†æ:**")
    print(f"   - å¹³å‡ä¿¡é ¼åº¦: {conf_stats['avg_confidence']:.3f}")
    print(f"   - æœ€å¤§ä¿¡é ¼åº¦: {conf_stats['max_confidence']:.3f}")
    print(f"   - æœ€å°ä¿¡é ¼åº¦: {conf_stats['min_confidence']:.3f}")
    print(f"   - ä¿¡é ¼ã§ãã‚‹ã‚·ã‚°ãƒŠãƒ«: {conf_stats['total_confident_signals']}å€‹")
    print(f"   - é«˜ä¿¡é ¼åº¦ã‚·ã‚°ãƒŠãƒ«: {conf_stats['high_confidence_signals']}å€‹")
    print(f"   - è¶…é«˜ä¿¡é ¼åº¦ã‚·ã‚°ãƒŠãƒ«: {conf_stats['ultra_confidence_signals']}å€‹")
    print(f"   - ä¿¡é ¼åº¦æ¯”ç‡: {conf_stats['confidence_ratio']*100:.1f}%")
    
    # é‡å­åˆ†æ
    quantum_stats = performance['quantum_analysis']
    print(f"\nğŸŒŒ **é‡å­åˆ†æçµ±è¨ˆ:**")
    print(f"   - é‡å­çŠ¶æ…‹å¹³å‡: {quantum_stats['mean_quantum_state']:.3f}")
    print(f"   - é‡å­å¼·åº¦: {quantum_stats['quantum_strength']:.3f}")
    print(f"   - MTFåˆæ„åº¦å¹³å‡: {quantum_stats['mtf_consensus_avg']:.3f}")
    print(f"   - MTFåˆæ„åº¦ç¯„å›²: {quantum_stats['mtf_consensus_min']:.3f} - {quantum_stats['mtf_consensus_max']:.3f}")
    print(f"   - ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒå¹³å‡: {quantum_stats['fractal_dimension_avg']:.3f}")
    print(f"   - ãƒˆãƒ¬ãƒ³ãƒ‰å®‰å®šæ€§: {quantum_stats['fractal_stability']:.3f}")
    print(f"   - ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¹³å‡: {quantum_stats['entropy_level_avg']:.3f}")
    print(f"   - äºˆæ¸¬å¯èƒ½æ€§: {quantum_stats['predictability']:.3f}")
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
    vol_stats = performance['volatility_regimes']
    print(f"\nğŸ“Š **ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ :**")
    print(f"   - ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“: {vol_stats['low_vol_periods']}æœŸé–“")
    print(f"   - æ­£å¸¸ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“: {vol_stats['normal_vol_periods']}æœŸé–“")
    print(f"   - é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“: {vol_stats['high_vol_periods']}æœŸé–“")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ®µéšåˆ†æ
    stage_stats = performance['filtering_stages']
    print(f"\nğŸ”¬ **10æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ:**")
    print(f"   - â‘ ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è£œæ­£: {stage_stats['kalman_change']:.6f}")
    print(f"   - â‘¡ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼è£œæ­£: {stage_stats['smooth_change']:.6f}")
    print(f"   - â‘¢ã‚¼ãƒ­ãƒ©ã‚°EMAè£œæ­£: {stage_stats['zerolag_change']:.6f}")
    print(f"   - â‘£-â‘©æœ€çµ‚æ®µéšè£œæ­£: {stage_stats['final_change']:.6f}")
    
    # å¯è¦–åŒ–
    print(f"\nğŸ“Š UltimateMA V3 çµæœã®åŒ…æ‹¬çš„å¯è¦–åŒ–ä¸­...")
    plot_ultimate_ma_v3_results(data, result)
    
    # æœ€çµ‚è©•ä¾¡
    print("\n" + "="*80)
    print("ğŸ† **UltimateMA V3 - é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æœ€çµ‚è©•ä¾¡**")
    print("="*80)
    
    if noise_stats['reduction_percentage'] >= 40:
        print("ğŸ–ï¸  âœ… **QUANTUM NEURAL SUPREMACY ACHIEVED**")
        print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: 40%ä»¥ä¸Šã®é©å‘½çš„ãƒã‚¤ã‚ºé™¤å»ã‚’é”æˆï¼")
    elif noise_stats['reduction_percentage'] >= 25:
        print("ğŸ–ï¸  ğŸ¥ˆ **QUANTUM EXCELLENCE**")
        print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: 25%ä»¥ä¸Šã®é‡å­ãƒ¬ãƒ™ãƒ«ãƒã‚¤ã‚ºé™¤å»ã‚’é”æˆã€‚")
    elif noise_stats['reduction_percentage'] >= 10:
        print("ğŸ–ï¸  ğŸ¥‰ **NEURAL SUPERIORITY**")
        print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: 10%ä»¥ä¸Šã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å„ªç§€ãƒã‚¤ã‚ºé™¤å»ã€‚")
    else:
        print("ğŸ–ï¸  ğŸ“ˆ **QUANTUM EVOLUTION**")
        print("ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: ã•ã‚‰ãªã‚‹é‡å­é€²åŒ–ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚")
    
    print(f"\nğŸ“Š **ç·åˆè©•ä¾¡:**")
    print(f"ğŸ”‡ ãƒã‚¤ã‚ºé™¤å»ç‡: {'âœ…' if noise_stats['reduction_percentage'] >= 25 else 'âŒ'} {noise_stats['reduction_percentage']:.1f}%")
    print(f"ğŸ”¥ å¹³å‡ä¿¡é ¼åº¦: {'âœ…' if conf_stats['avg_confidence'] >= 0.4 else 'âŒ'} {conf_stats['avg_confidence']:.3f}")
    print(f"ğŸŒŒ é‡å­åˆ†æå¼·åº¦: {'âœ…' if quantum_stats['quantum_strength'] >= 0.1 else 'âŒ'} {quantum_stats['quantum_strength']:.3f}")
    print(f"ğŸ”„ MTFåˆæ„åº¦: {'âœ…' if quantum_stats['mtf_consensus_avg'] >= 0.6 else 'âŒ'} {quantum_stats['mtf_consensus_avg']:.3f}")
    print(f"ğŸŒ€ äºˆæ¸¬å¯èƒ½æ€§: {'âœ…' if quantum_stats['predictability'] >= 0.4 else 'âŒ'} {quantum_stats['predictability']:.3f}")
    print(f"âš¡ å‡¦ç†é€Ÿåº¦: {'âœ…' if processing_speed >= 50 else 'âŒ'} {processing_speed:.0f} ãƒ‡ãƒ¼ã‚¿/ç§’")
    
    # æŠ€è¡“è©³ç´°
    print(f"\nâš™ï¸  **UltimateMA V3 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š:**")
    print(f"   - ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼æœŸé–“: {ultimate_ma_v3.super_smooth_period}")
    print(f"   - ã‚¼ãƒ­ãƒ©ã‚°EMAæœŸé–“: {ultimate_ma_v3.zero_lag_period}")
    print(f"   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {ultimate_ma_v3.realtime_window}")
    print(f"   - é‡å­åˆ†æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {ultimate_ma_v3.quantum_window}")
    print(f"   - ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«åˆ†æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {ultimate_ma_v3.fractal_window}")
    print(f"   - ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {ultimate_ma_v3.entropy_window}")
    print(f"   - ä¾¡æ ¼ã‚½ãƒ¼ã‚¹: {ultimate_ma_v3.src_type}")
    print(f"   - ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šæœŸé–“: {ultimate_ma_v3.slope_index}")
    print(f"   - åŸºæœ¬é–¾å€¤: {ultimate_ma_v3.base_threshold}")
    print(f"   - æœ€å°ä¿¡é ¼åº¦: {ultimate_ma_v3.min_confidence}")
    
    comprehensive_score = (
        min(noise_stats['reduction_percentage'] / 40, 1.0) * 0.3 +
        min(conf_stats['avg_confidence'] / 0.5, 1.0) * 0.25 +
        min(quantum_stats['quantum_strength'] / 0.2, 1.0) * 0.2 +
        min(quantum_stats['mtf_consensus_avg'] / 0.8, 1.0) * 0.15 +
        min(quantum_stats['predictability'] / 0.6, 1.0) * 0.1
    )
    
    print(f"\nğŸ¯ **ç·åˆã‚¹ã‚³ã‚¢: {comprehensive_score:.3f} / 1.000**")
    
    if comprehensive_score >= 0.8:
        print(f"\nğŸŠ **UltimateMA V3 - QUANTUM NEURAL SUPREMACY COMPLETE!**")
        print("ğŸŒŸ 10æ®µéšAIåˆ†æã«ã‚ˆã‚Šæœ€é«˜å“è³ªã®é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«MAã‚’å®Ÿç¾ã—ã¾ã—ãŸ!")
    elif comprehensive_score >= 0.6:
        print(f"\nğŸ† **UltimateMA V3 - QUANTUM EXCELLENCE ACHIEVED!**")
        print("â­ é«˜ãƒ¬ãƒ™ãƒ«ã®é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«åˆ†æã‚’é”æˆã—ã¾ã—ãŸ!")
    else:
        print(f"\nğŸ“ˆ **UltimateMA V3 - QUANTUM EVOLUTION IN PROGRESS**")
        print("ğŸ”¥ ã•ã‚‰ãªã‚‹é‡å­é€²åŒ–ã‚’ç¶šã‘ã¦ã„ã¾ã™!")
    
    print("\n" + "="*80)
    print("UltimateMA V3 - é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†")
    print(f"ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿: {data_description}")
    print("ğŸš€ 10æ®µéšé©æ–°çš„AIåˆ†æãƒ»95%è¶…é«˜ç²¾åº¦åˆ¤å®šãƒ»ä¿¡é ¼åº¦ä»˜ãã‚·ã‚°ãƒŠãƒ«å®Œäº†")
    print("="*80)


if __name__ == "__main__":
    main() 