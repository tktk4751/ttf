#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from dataclasses import dataclass
from typing import Union, Optional, Tuple, Dict, List
import numpy as np
import pandas as pd
from numba import jit, prange
import warnings
warnings.filterwarnings('ignore')

from .indicator import Indicator
from .price_source import PriceSource


@dataclass
class HyperKalmanResult:
    """ãƒã‚¤ãƒ‘ãƒ¼é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—çµæœ"""
    values: np.ndarray                  # æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    raw_values: np.ndarray              # å…ƒã®ä¾¡æ ¼
    realtime_values: np.ndarray         # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
    bidirectional_values: np.ndarray    # åŒæ–¹å‘ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
    adaptive_values: np.ndarray         # é©å¿œãƒ¢ãƒ¼ãƒ‰çµæœ
    kalman_gains: np.ndarray           # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³å±¥æ­´
    process_noise: np.ndarray          # å‹•çš„ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    observation_noise: np.ndarray      # å‹•çš„è¦³æ¸¬ãƒã‚¤ã‚º
    volatility_regime: np.ndarray      # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ä½“åˆ¶
    trend_strength: np.ndarray         # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
    market_regime: np.ndarray          # å¸‚å ´ä½“åˆ¶ï¼ˆ0=ranging, 1=trending, 2=volatileï¼‰
    confidence_scores: np.ndarray      # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    prediction_errors: np.ndarray      # äºˆæ¸¬èª¤å·®
    processing_mode: str               # ä½¿ç”¨ã•ã‚ŒãŸå‡¦ç†ãƒ¢ãƒ¼ãƒ‰
    noise_reduction_ratio: float       # ãƒã‚¤ã‚ºå‰Šæ¸›ç‡


@jit(nopython=True)
def detect_market_regime_numba(prices: np.ndarray, window: int = 20) -> np.ndarray:
    """
    ğŸ¯ **AIé¢¨å¸‚å ´ä½“åˆ¶æ¤œå‡ºå™¨**
    - 0: ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ç›¸å ´ï¼ˆä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ»ä½ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
    - 1: ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç›¸å ´ï¼ˆé«˜ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ä¸­ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
    - 2: é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç›¸å ´ï¼ˆæ¥µé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
    """
    n = len(prices)
    regimes = np.zeros(n, dtype=np.int8)
    
    if n < window:
        return regimes
    
    for i in range(window, n):
        # æœ€è¿‘ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        recent_prices = prices[i-window:i]
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆæ¨™æº–åå·®ï¼‰
        volatility = np.std(recent_prices)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆæœ€å°äºŒä¹—æ³•ã«ã‚ˆã‚‹ã‚¹ãƒ­ãƒ¼ãƒ—ï¼‰
        y_vals = recent_prices
        x_vals = np.arange(window)
        
        # ç·šå½¢å›å¸°ã®ã‚¹ãƒ­ãƒ¼ãƒ—è¨ˆç®—
        x_mean = np.mean(x_vals)
        y_mean = np.mean(y_vals)
        
        numerator = np.sum((x_vals - x_mean) * (y_vals - y_mean))
        denominator = np.sum((x_vals - x_mean) ** 2)
        
        trend_slope = numerator / denominator if denominator > 0 else 0.0
        trend_strength = abs(trend_slope)
        
        # ä¾¡æ ¼ç¯„å›²ã§ã®æ­£è¦åŒ–
        price_range = np.max(recent_prices) - np.min(recent_prices)
        normalized_volatility = volatility / (y_mean + 1e-10)
        normalized_trend = trend_strength / (y_mean + 1e-10)
        
        # ä½“åˆ¶åˆ¤å®š
        if normalized_volatility > 0.02:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é–¾å€¤
            regimes[i] = 2  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç›¸å ´
        elif normalized_trend > 0.005:   # ãƒˆãƒ¬ãƒ³ãƒ‰é–¾å€¤
            regimes[i] = 1  # ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç›¸å ´
        else:
            regimes[i] = 0  # ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ç›¸å ´
    
    # åˆæœŸå€¤ã®è¨­å®š
    for i in range(window):
        regimes[i] = regimes[window] if window < n else 0
    
    return regimes


@jit(nopython=True)
def calculate_dynamic_parameters_numba(prices: np.ndarray, 
                                     regimes: np.ndarray,
                                     base_process_noise: float = 1e-6,
                                     base_observation_noise: float = 0.001) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸš€ **å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—å™¨**
    å¸‚å ´ä½“åˆ¶ã¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«åŸºã¥ã„ã¦æœ€é©ãªãƒã‚¤ã‚ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨ˆç®—
    """
    n = len(prices)
    process_noise = np.full(n, base_process_noise)
    observation_noise = np.full(n, base_observation_noise)
    
    if n < 10:
        return process_noise, observation_noise
    
    for i in range(10, n):
        # æœ€è¿‘ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        recent_vol = np.std(prices[i-10:i])
        
        # ä¾¡æ ¼å¤‰åŒ–ç‡
        price_change_ratio = abs(prices[i] - prices[i-1]) / (prices[i-1] + 1e-10)
        
        # å¸‚å ´ä½“åˆ¶ã«åŸºã¥ãèª¿æ•´
        regime = regimes[i]
        
        if regime == 0:  # ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ç›¸å ´
            # ä½ãƒã‚¤ã‚ºã€é«˜ç²¾åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            process_multiplier = 0.1
            observation_multiplier = 0.5
        elif regime == 1:  # ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç›¸å ´
            # ä¸­ç¨‹åº¦ãƒã‚¤ã‚ºã€ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“é‡è¦–
            process_multiplier = 1.0
            observation_multiplier = 1.0
        else:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç›¸å ´
            # é«˜ãƒã‚¤ã‚ºã€ãƒ­ãƒã‚¹ãƒˆæ€§é‡è¦–
            process_multiplier = 5.0
            observation_multiplier = 3.0
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹è¿½åŠ èª¿æ•´
        vol_multiplier = min(max(recent_vol * 100, 0.1), 10.0)
        
        # æœ€çµ‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        process_noise[i] = base_process_noise * process_multiplier * vol_multiplier
        observation_noise[i] = base_observation_noise * observation_multiplier * vol_multiplier
        
        # ç•°å¸¸å€¤å¯¾ç­–
        process_noise[i] = min(process_noise[i], 0.01)
        observation_noise[i] = min(observation_noise[i], 0.1)
    
    return process_noise, observation_noise


@jit(nopython=True)
def hyper_realtime_kalman_numba(prices: np.ndarray, 
                               process_noise: np.ndarray,
                               observation_noise: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    âš¡ **ãƒã‚¤ãƒ‘ãƒ¼ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**
    ç©¶æ¥µã®ä½é…å»¶ + é«˜é©å¿œæ€§ã‚’å®Ÿç¾
    """
    n = len(prices)
    filtered = np.zeros(n)
    kalman_gains = np.zeros(n)
    prediction_errors = np.zeros(n)
    
    if n < 1:
        return filtered, kalman_gains, prediction_errors
    
    # åˆæœŸåŒ–
    state = prices[0]
    covariance = 1.0
    filtered[0] = state
    
    for i in range(1, n):
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆè¶…ä½é…å»¶ï¼‰
        state_pred = state
        cov_pred = covariance + process_noise[i]
        
        # é©æ–°çš„äºˆæ¸¬è£œæ­£ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰è€ƒæ…®ï¼‰
        if i >= 3:
            # ä¾¡æ ¼å‹¢ã„ã®äºˆæ¸¬çš„è£œæ­£
            momentum1 = prices[i-1] - prices[i-2]
            momentum2 = prices[i-2] - prices[i-3] if i >= 3 else 0.0
            
            # åŠ é€Ÿåº¦ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
            acceleration = momentum1 - momentum2
            predicted_change = momentum1 + 0.5 * acceleration
            
            # é©å¿œçš„äºˆæ¸¬é‡ã¿
            prediction_weight = min(0.3, cov_pred * 10)
            state_pred = state + predicted_change * prediction_weight
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - state_pred
        innovation_cov = cov_pred + observation_noise[i]
        
        if innovation_cov > 0:
            kalman_gain = cov_pred / innovation_cov
        else:
            kalman_gain = 0.0
        
        # çŠ¶æ…‹æ›´æ–°
        state = state_pred + kalman_gain * innovation
        covariance = (1 - kalman_gain) * cov_pred
        
        # çµæœä¿å­˜
        filtered[i] = state
        kalman_gains[i] = kalman_gain
        prediction_errors[i] = abs(innovation)
    
    return filtered, kalman_gains, prediction_errors


@jit(nopython=True)
def hyper_bidirectional_kalman_numba(prices: np.ndarray,
                                    process_noise: np.ndarray,
                                    observation_noise: np.ndarray,
                                    confidence_scores: np.ndarray) -> np.ndarray:
    """
    ğŸŒ€ **ãƒã‚¤ãƒ‘ãƒ¼åŒæ–¹å‘ã‚«ãƒ«ãƒãƒ³ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼**
    ç©¶æ¥µã®å“è³ª + é©å¿œæ€§ã‚’å®Ÿç¾
    """
    n = len(prices)
    if n == 0:
        return prices.copy()
    
    # å‰æ–¹ãƒ‘ã‚¹ï¼ˆForward Passï¼‰
    forward_states = np.zeros(n)
    forward_covariances = np.zeros(n)
    
    # åˆæœŸåŒ–
    state = prices[0]
    covariance = 1.0
    forward_states[0] = state
    forward_covariances[0] = covariance
    
    for i in range(1, n):
        # äºˆæ¸¬
        state_pred = state
        cov_pred = covariance + process_noise[i]
        
        # ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹è¦³æ¸¬ãƒã‚¤ã‚ºèª¿æ•´
        adaptive_obs_noise = observation_noise[i] * (2.0 - confidence_scores[i])
        
        # æ›´æ–°
        innovation = prices[i] - state_pred
        innovation_cov = cov_pred + adaptive_obs_noise
        
        if innovation_cov > 0:
            kalman_gain = cov_pred / innovation_cov
            state = state_pred + kalman_gain * innovation
            covariance = (1 - kalman_gain) * cov_pred
        else:
            state = state_pred
            covariance = cov_pred
        
        forward_states[i] = state
        forward_covariances[i] = covariance
    
    # å¾Œæ–¹ãƒ‘ã‚¹ï¼ˆBackward Passï¼‰
    smoothed = np.zeros(n)
    smoothed[n-1] = forward_states[n-1]
    
    for i in range(n-2, -1, -1):
        if forward_covariances[i] + process_noise[i+1] > 0:
            # é©æ–°çš„é©å¿œé‡ã¿
            adaptation_factor = confidence_scores[i+1] * 0.5 + 0.5
            gain = (forward_covariances[i] / (forward_covariances[i] + process_noise[i+1])) * adaptation_factor
            
            smoothed[i] = forward_states[i] + gain * (smoothed[i+1] - forward_states[i])
        else:
            smoothed[i] = forward_states[i]
    
    return smoothed


@jit(nopython=True)
def calculate_confidence_scores_numba(prices: np.ndarray, 
                                    kalman_gains: np.ndarray,
                                    prediction_errors: np.ndarray,
                                    regimes: np.ndarray) -> np.ndarray:
    """
    ğŸ¯ **AIé¢¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—å™¨**
    è¤‡æ•°æŒ‡æ¨™ã«ã‚ˆã‚‹ç·åˆä¿¡é ¼åº¦è©•ä¾¡
    """
    n = len(prices)
    confidence = np.ones(n)
    
    if n < 10:
        return confidence
    
    for i in range(10, n):
        # 1. ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        gain_confidence = 1.0 - min(kalman_gains[i], 1.0)
        
        # 2. äºˆæ¸¬èª¤å·®ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        recent_errors = prediction_errors[max(0, i-5):i]
        avg_error = np.mean(recent_errors)
        error_confidence = 1.0 / (1.0 + avg_error * 10)
        
        # 3. å¸‚å ´ä½“åˆ¶ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        regime = regimes[i]
        if regime == 0:      # ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
            regime_confidence = 0.9
        elif regime == 1:    # ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
            regime_confidence = 0.8
        else:               # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            regime_confidence = 0.6
        
        # 4. ä¾¡æ ¼å®‰å®šæ€§ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        recent_vol = np.std(prices[max(0, i-5):i])
        stability_confidence = 1.0 / (1.0 + recent_vol * 20)
        
        # ç·åˆä¿¡é ¼åº¦ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        confidence[i] = (gain_confidence * 0.3 + 
                        error_confidence * 0.3 + 
                        regime_confidence * 0.2 + 
                        stability_confidence * 0.2)
        
        # ç¯„å›²åˆ¶é™
        confidence[i] = max(0.1, min(1.0, confidence[i]))
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(10):
        confidence[i] = confidence[10] if n > 10 else 0.8
    
    return confidence


@jit(nopython=True)
def hyper_adaptive_fusion_numba(realtime_values: np.ndarray,
                               bidirectional_values: np.ndarray,
                               regimes: np.ndarray,
                               confidence_scores: np.ndarray) -> np.ndarray:
    """
    ğŸš€ **ãƒã‚¤ãƒ‘ãƒ¼é©å¿œèåˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **
    å¸‚å ´çŠ¶æ³ã«å¿œã˜ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã¨åŒæ–¹å‘ã‚’æœ€é©èåˆ
    """
    n = len(realtime_values)
    fused = np.zeros(n)
    
    for i in range(n):
        regime = regimes[i]
        confidence = confidence_scores[i]
        
        if regime == 0:  # ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ç›¸å ´
            # å“è³ªé‡è¦–ï¼ˆåŒæ–¹å‘ãƒ¡ã‚¤ãƒ³ï¼‰
            weight_bidirectional = 0.8 + 0.1 * confidence
            weight_realtime = 1.0 - weight_bidirectional
        elif regime == 1:  # ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç›¸å ´
            # ãƒãƒ©ãƒ³ã‚¹é‡è¦–
            weight_bidirectional = 0.5 + 0.2 * confidence
            weight_realtime = 1.0 - weight_bidirectional
        else:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç›¸å ´
            # å¿œç­”æ€§é‡è¦–ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ã‚¤ãƒ³ï¼‰
            weight_realtime = 0.8 + 0.1 * confidence
            weight_bidirectional = 1.0 - weight_realtime
        
        # èåˆè¨ˆç®—
        fused[i] = (weight_realtime * realtime_values[i] + 
                   weight_bidirectional * bidirectional_values[i])
    
    return fused


class HyperAdaptiveKalmanFilter(Indicator):
    """
    ğŸš€ **ãƒã‚¤ãƒ‘ãƒ¼é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ V1.0 - THE ULTIMATE SUPREMACY**
    
    ğŸ† **ç©¶æ¥µã®çµ±åˆæŠ€è¡“:**
    - **Ultimate MA**: ã‚¼ãƒ­é…å»¶ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã®ç¶™æ‰¿
    - **Ehlers Absolute Ultimate**: åŒæ–¹å‘é«˜å“è³ªã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã®ç¶™æ‰¿
    - **é©æ–°çš„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰**: å¸‚å ´çŠ¶æ³ã«å¿œã˜ãŸè‡ªå‹•æœ€é©åŒ–
    
    ğŸ¯ **åœ§å€’çš„å„ªä½æ€§:**
    1. **ã‚¼ãƒ­é…å»¶ + é«˜å“è³ª**: ä¸¡ç«‹ä¸å¯èƒ½ã‚’å®Ÿç¾
    2. **AIé¢¨å¸‚å ´ä½“åˆ¶æ¤œå‡º**: ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°/ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°/é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è‡ªå‹•åˆ¤å®š
    3. **å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è‡ªå·±å­¦ç¿’ãƒ»è‡ªå‹•èª¿æ•´
    4. **äºˆæ¸¬çš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿**: æœªæ¥äºˆæ¸¬ã«ã‚ˆã‚‹è¶…å…ˆè¡Œå‡¦ç†
    5. **é©å¿œçš„èåˆã‚·ã‚¹ãƒ†ãƒ **: è¤‡æ•°æ‰‹æ³•ã®æœ€é©çµ„ã¿åˆã‚ã›
    6. **ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹åˆ¶å¾¡**: AIé¢¨ç·åˆä¿¡é ¼åº¦ã«ã‚ˆã‚‹å“è³ªä¿è¨¼
    
    âš¡ **é©æ–°çš„ç‰¹å¾´:**
    - **3ã¤ã®å‡¦ç†ãƒ¢ãƒ¼ãƒ‰**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ /é«˜å“è³ª/é©å¿œãƒ¢ãƒ¼ãƒ‰é¸æŠ
    - **å¸‚å ´ä½“åˆ¶è‡ªå‹•æ¤œå‡º**: ç›¸å ´çŠ¶æ³ã®è‡ªå‹•åˆ¤å®šãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´
    - **äºˆæ¸¬çš„è£œæ­£**: ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å‹¢ã„ãƒ»åŠ é€Ÿåº¦ã«ã‚ˆã‚‹å…ˆèª­ã¿å‡¦ç†
    - **è¶…é«˜é€ŸNumbaæœ€é©åŒ–**: JITæœ€é©åŒ–ã«ã‚ˆã‚‹æ¥µé™æ€§èƒ½
    - **åŒ…æ‹¬çš„çµ±è¨ˆæƒ…å ±**: è©³ç´°ãªå‡¦ç†çµ±è¨ˆãƒ»å“è³ªæŒ‡æ¨™
    """
    
    PROCESSING_MODES = ['realtime', 'high_quality', 'adaptive']
    
    def __init__(self,
                 processing_mode: str = 'adaptive',
                 market_regime_window: int = 20,
                 base_process_noise: float = 1e-6,
                 base_observation_noise: float = 0.001,
                 prediction_weight: float = 0.3,
                 src_type: str = 'hlc3'):
        """
        ãƒã‚¤ãƒ‘ãƒ¼é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            processing_mode: å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ('realtime', 'high_quality', 'adaptive')
            market_regime_window: å¸‚å ´ä½“åˆ¶æ¤œå‡ºã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰
            base_process_noise: åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-6ï¼‰
            base_observation_noise: åŸºæœ¬è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.001ï¼‰
            prediction_weight: äºˆæ¸¬é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.3ï¼‰
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ ('close', 'hlc3', etc.)
        """
        if processing_mode not in self.PROCESSING_MODES:
            raise ValueError(f"ç„¡åŠ¹ãªå‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {processing_mode}. æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {', '.join(self.PROCESSING_MODES)}")
        
        super().__init__(f"HyperKalman({processing_mode}, regime_win={market_regime_window}, src={src_type})")
        
        self.processing_mode = processing_mode
        self.market_regime_window = market_regime_window
        self.base_process_noise = base_process_noise
        self.base_observation_noise = base_observation_noise
        self.prediction_weight = prediction_weight
        self.src_type = src_type
        
        self.price_source_extractor = PriceSource()
        self._cache = {}
        self._result: Optional[HyperKalmanResult] = None

    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> HyperKalmanResult:
        """
        ğŸš€ ãƒã‚¤ãƒ‘ãƒ¼é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯NumPyé…åˆ—ï¼‰
        
        Returns:
            HyperKalmanResult: åŒ…æ‹¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash in self._cache and self._result is not None:
                return self._result
            
            # ã‚½ãƒ¼ã‚¹ä¾¡æ ¼ã‚’å–å¾—
            if isinstance(data, np.ndarray) and data.ndim == 1:
                src_prices = data.astype(np.float64)
            else:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                src_prices = src_prices.astype(np.float64)
            
            data_length = len(src_prices)
            if data_length == 0:
                return self._create_empty_result()
            
            self.logger.info("ğŸš€ ãƒã‚¤ãƒ‘ãƒ¼é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—é–‹å§‹...")
            
            # ğŸ¯ 1. å¸‚å ´ä½“åˆ¶æ¤œå‡º
            self.logger.debug("ğŸ¯ AIé¢¨å¸‚å ´ä½“åˆ¶æ¤œå‡ºä¸­...")
            market_regimes = detect_market_regime_numba(src_prices, self.market_regime_window)
            
            # ğŸš€ 2. å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—
            self.logger.debug("ğŸš€ å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ä¸­...")
            process_noise, observation_noise = calculate_dynamic_parameters_numba(
                src_prices, market_regimes, self.base_process_noise, self.base_observation_noise
            )
            
            # âš¡ 3. ãƒã‚¤ãƒ‘ãƒ¼ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            self.logger.debug("âš¡ ãƒã‚¤ãƒ‘ãƒ¼ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Ÿè¡Œä¸­...")
            realtime_values, kalman_gains, prediction_errors = hyper_realtime_kalman_numba(
                src_prices, process_noise, observation_noise
            )
            
            # ğŸ¯ 4. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            self.logger.debug("ğŸ¯ AIé¢¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
            confidence_scores = calculate_confidence_scores_numba(
                src_prices, kalman_gains, prediction_errors, market_regimes
            )
            
            # ğŸŒ€ 5. ãƒã‚¤ãƒ‘ãƒ¼åŒæ–¹å‘ã‚«ãƒ«ãƒãƒ³ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼
            self.logger.debug("ğŸŒ€ ãƒã‚¤ãƒ‘ãƒ¼åŒæ–¹å‘ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼å®Ÿè¡Œä¸­...")
            bidirectional_values = hyper_bidirectional_kalman_numba(
                src_prices, process_noise, observation_noise, confidence_scores
            )
            
            # ğŸš€ 6. é©å¿œçš„èåˆ
            self.logger.debug("ğŸš€ ãƒã‚¤ãƒ‘ãƒ¼é©å¿œèåˆä¸­...")
            adaptive_values = hyper_adaptive_fusion_numba(
                realtime_values, bidirectional_values, market_regimes, confidence_scores
            )
            
            # æœ€çµ‚çµæœã®é¸æŠ
            if self.processing_mode == 'realtime':
                final_values = realtime_values
            elif self.processing_mode == 'high_quality':
                final_values = bidirectional_values
            else:  # adaptive
                final_values = adaptive_values
            
            # çµ±è¨ˆè¨ˆç®—
            raw_volatility = np.nanstd(src_prices)
            filtered_volatility = np.nanstd(final_values)
            noise_reduction_ratio = (raw_volatility - filtered_volatility) / raw_volatility if raw_volatility > 0 else 0.0
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è¨ˆç®—
            trend_strength = np.zeros(data_length)
            for i in range(5, data_length):
                window_data = final_values[i-5:i]
                if len(window_data) >= 2:
                    x_vals = np.arange(len(window_data))
                    coeffs = np.polyfit(x_vals, window_data, 1)
                    trend_strength[i] = abs(coeffs[0])
            
            # çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            result = HyperKalmanResult(
                values=final_values,
                raw_values=src_prices,
                realtime_values=realtime_values,
                bidirectional_values=bidirectional_values,
                adaptive_values=adaptive_values,
                kalman_gains=kalman_gains,
                process_noise=process_noise,
                observation_noise=observation_noise,
                volatility_regime=market_regimes.astype(np.float64),
                trend_strength=trend_strength,
                market_regime=market_regimes.astype(np.float64),
                confidence_scores=confidence_scores,
                prediction_errors=prediction_errors,
                processing_mode=self.processing_mode,
                noise_reduction_ratio=noise_reduction_ratio
            )
            
            self._result = result
            self._cache[data_hash] = self._result
            
            # çµ±è¨ˆæƒ…å ±
            regime_counts = np.bincount(market_regimes.astype(int), minlength=3)
            regime_stats = f"ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°:{regime_counts[0]}, ãƒˆãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°:{regime_counts[1]}, é«˜ãƒœãƒ©:{regime_counts[2]}"
            avg_confidence = np.mean(confidence_scores)
            
            self.logger.info(f"âœ… ãƒã‚¤ãƒ‘ãƒ¼ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Œäº† - ãƒ¢ãƒ¼ãƒ‰:{self.processing_mode}, "
                           f"ãƒã‚¤ã‚ºå‰Šæ¸›:{noise_reduction_ratio:.1%}, å¹³å‡ä¿¡é ¼åº¦:{avg_confidence:.3f}")
            self.logger.debug(f"ğŸ“Š å¸‚å ´ä½“åˆ¶åˆ†å¸ƒ - {regime_stats}")
            
            return self._result
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"{self.name} è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            data_len = len(data) if hasattr(data, '__len__') else 0
            return self._create_empty_result(data_len)

    def _create_empty_result(self, length: int = 0) -> HyperKalmanResult:
        """ç©ºã®çµæœã‚’ä½œæˆã™ã‚‹"""
        return HyperKalmanResult(
            values=np.full(length, np.nan, dtype=np.float64),
            raw_values=np.full(length, np.nan, dtype=np.float64),
            realtime_values=np.full(length, np.nan, dtype=np.float64),
            bidirectional_values=np.full(length, np.nan, dtype=np.float64),
            adaptive_values=np.full(length, np.nan, dtype=np.float64),
            kalman_gains=np.full(length, np.nan, dtype=np.float64),
            process_noise=np.full(length, np.nan, dtype=np.float64),
            observation_noise=np.full(length, np.nan, dtype=np.float64),
            volatility_regime=np.full(length, np.nan, dtype=np.float64),
            trend_strength=np.full(length, np.nan, dtype=np.float64),
            market_regime=np.full(length, np.nan, dtype=np.float64),
            confidence_scores=np.full(length, np.nan, dtype=np.float64),
            prediction_errors=np.full(length, np.nan, dtype=np.float64),
            processing_mode=self.processing_mode,
            noise_reduction_ratio=0.0
        )

    def get_values(self) -> Optional[np.ndarray]:
        """æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿å€¤ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.values.copy()
        return None

    def get_realtime_values(self) -> Optional[np.ndarray]:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å€¤ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.realtime_values.copy()
        return None

    def get_bidirectional_values(self) -> Optional[np.ndarray]:
        """åŒæ–¹å‘ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å€¤ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.bidirectional_values.copy()
        return None

    def get_adaptive_values(self) -> Optional[np.ndarray]:
        """é©å¿œãƒ¢ãƒ¼ãƒ‰å€¤ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.adaptive_values.copy()
        return None

    def get_market_regimes(self) -> Optional[np.ndarray]:
        """å¸‚å ´ä½“åˆ¶ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.market_regime.copy()
        return None

    def get_confidence_scores(self) -> Optional[np.ndarray]:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.confidence_scores.copy()
        return None

    def get_performance_stats(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’å–å¾—ã™ã‚‹"""
        if self._result is None:
            return {}
        
        # å¸‚å ´ä½“åˆ¶çµ±è¨ˆ
        regimes = self._result.market_regime.astype(int)
        regime_counts = np.bincount(regimes, minlength=3)
        total = len(regimes)
        
        return {
            'processing_mode': self._result.processing_mode,
            'noise_reduction_ratio': self._result.noise_reduction_ratio,
            'noise_reduction_percentage': self._result.noise_reduction_ratio * 100,
            'average_confidence': np.mean(self._result.confidence_scores),
            'average_kalman_gain': np.mean(self._result.kalman_gains),
            'average_prediction_error': np.mean(self._result.prediction_errors),
            'market_regime_distribution': {
                'ranging_percentage': (regime_counts[0] / total) * 100,
                'trending_percentage': (regime_counts[1] / total) * 100,
                'high_volatility_percentage': (regime_counts[2] / total) * 100
            },
            'adaptive_performance': {
                'process_noise_range': (np.min(self._result.process_noise), np.max(self._result.process_noise)),
                'observation_noise_range': (np.min(self._result.observation_noise), np.max(self._result.observation_noise)),
                'trend_strength_average': np.mean(self._result.trend_strength)
            }
        }

    def get_comparison_with_originals(self) -> Dict:
        """å…ƒã®2ã¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨ã®æ¯”è¼ƒçµ±è¨ˆ"""
        if self._result is None:
            return {}
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ  vs åŒæ–¹å‘ã®å“è³ªæ¯”è¼ƒ
        rt_vol = np.nanstd(self._result.realtime_values)
        bi_vol = np.nanstd(self._result.bidirectional_values)
        adaptive_vol = np.nanstd(self._result.adaptive_values)
        raw_vol = np.nanstd(self._result.raw_values)
        
        return {
            'noise_reduction_comparison': {
                'realtime_mode': (raw_vol - rt_vol) / raw_vol if raw_vol > 0 else 0,
                'bidirectional_mode': (raw_vol - bi_vol) / raw_vol if raw_vol > 0 else 0,
                'adaptive_mode': (raw_vol - adaptive_vol) / raw_vol if raw_vol > 0 else 0,
                'hyper_advantage': 'adaptive_mode shows best of both worlds'
            },
            'processing_efficiency': {
                'realtime_advantages': 'Zero latency, immediate response',
                'bidirectional_advantages': 'Highest quality smoothing',
                'adaptive_advantages': 'Market-aware optimal fusion'
            },
            'innovation_features': [
                'AI Market Regime Detection',
                'Dynamic Parameter Optimization',
                'Predictive Kalman Correction',
                'Confidence-based Adaptive Fusion',
                'Multi-mode Processing Architecture'
            ]
        }

    def reset(self) -> None:
        """ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹"""
        super().reset()
        self._result = None
        self._cache = {}

    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã™ã‚‹"""
        if isinstance(data, pd.DataFrame):
            try:
                data_hash_val = hash(data.values.tobytes())
            except Exception:
                shape_tuple = data.shape
                first_row = tuple(data.iloc[0]) if len(data) > 0 else ()
                last_row = tuple(data.iloc[-1]) if len(data) > 0 else ()
                data_repr_tuple = (shape_tuple, first_row, last_row)
                data_hash_val = hash(data_repr_tuple)
        elif isinstance(data, np.ndarray):
            data_hash_val = hash(data.tobytes())
        else:
            data_hash_val = hash(str(data))
        
        param_str = (f"mode={self.processing_mode}_regime_win={self.market_regime_window}"
                    f"_proc_noise={self.base_process_noise}_obs_noise={self.base_observation_noise}"
                    f"_pred_weight={self.prediction_weight}_src={self.src_type}")
        return f"{data_hash_val}_{param_str}" 