#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from typing import Union, Optional, Tuple, Dict, List
import numpy as np
import pandas as pd
from numba import jit, prange, float64, int32, types
from scipy import signal
from scipy.stats import entropy
import warnings
warnings.filterwarnings('ignore')

from .ehlers_dominant_cycle import EhlersDominantCycle, DominantCycleResult


@jit(nopython=True)
def advanced_spectral_entropy(data: np.ndarray, window: int = 30) -> float:
    """
    é«˜åº¦ãªã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
    å‘¨æ³¢æ•°é ˜åŸŸã§ã®æƒ…å ±ç†è«–ã«åŸºã¥ãæœ€é©åŒ–
    """
    if len(data) < window:
        return 0.5
    
    recent_data = data[-window:]
    n = len(recent_data)
    
    # æ‰‹å‹•DFTè¨ˆç®—ï¼ˆNumbaäº’æ›ï¼‰
    power_spectrum = np.zeros(n)
    
    for k in range(n):
        real_part = 0.0
        imag_part = 0.0
        
        for i in range(n):
            angle = -2.0 * np.pi * i * k / n
            real_part += recent_data[i] * np.cos(angle)
            imag_part += recent_data[i] * np.sin(angle)
        
        power_spectrum[k] = real_part**2 + imag_part**2
    
    # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«å¯†åº¦
    total_power = np.sum(power_spectrum)
    if total_power == 0:
        return 0.0
    
    normalized_spectrum = power_spectrum / total_power
    
    # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
    entropy_val = 0.0
    for p in normalized_spectrum:
        if p > 1e-10:
            entropy_val -= p * np.log2(p)
    
    # æ­£è¦åŒ– (æœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯log2(n))
    max_entropy = np.log2(n)
    return entropy_val / max_entropy if max_entropy > 0 else 0.0


@jit(nopython=True)
def fractal_dimension_estimator(data: np.ndarray, max_k: int = 10) -> float:
    """
    ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒæ¨å®šå™¨ï¼ˆé«˜åº¦ãªè¤‡é›‘æ€§åˆ†æï¼‰
    ãƒœãƒƒã‚¯ã‚¹ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°æ³•ã®æ”¹è‰¯ç‰ˆ
    """
    if len(data) < 20:
        return 1.5
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    min_val, max_val = np.min(data), np.max(data)
    if max_val == min_val:
        return 1.0
    
    normalized_data = (data - min_val) / (max_val - min_val)
    
    # æ§˜ã€…ãªã‚¹ã‚±ãƒ¼ãƒ«ã§ãƒœãƒƒã‚¯ã‚¹æ•°ã‚’è¨ˆç®—
    scales = np.zeros(max_k)
    box_counts = np.zeros(max_k)
    valid_count = 0
    
    for k in range(2, min(max_k, len(data)//4)):
        box_size = 1.0 / k
        
        # setã®ä»£ã‚ã‚Šã«é…åˆ—ãƒ™ãƒ¼ã‚¹ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒœãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
        boxes = np.zeros((len(normalized_data), 2))  # [box_x, box_y]ã®ãƒšã‚¢
        
        for i in range(len(normalized_data)):
            # å„ç‚¹ãŒã©ã®ãƒœãƒƒã‚¯ã‚¹ã«å±ã™ã‚‹ã‹ã‚’è¨ˆç®—
            box_x = int(i / (len(normalized_data) / k))
            box_y = int(normalized_data[i] / box_size)
            boxes[i, 0] = box_x
            boxes[i, 1] = box_y
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒœãƒƒã‚¯ã‚¹ã®æ•°ã‚’è¨ˆç®—ï¼ˆæ‰‹å‹•ã§é‡è¤‡é™¤å»ï¼‰
        unique_boxes = 0
        
        for i in range(len(boxes)):
            is_unique = True
            for j in range(i):
                if boxes[i, 0] == boxes[j, 0] and boxes[i, 1] == boxes[j, 1]:
                    is_unique = False
                    break
            if is_unique:
                unique_boxes += 1
        
        if unique_boxes > 0:
            scales[valid_count] = np.log(1.0 / box_size)
            box_counts[valid_count] = np.log(unique_boxes)
            valid_count += 1
    
    if valid_count < 3:
        return 1.5
    
    # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
    valid_scales = scales[:valid_count]
    valid_box_counts = box_counts[:valid_count]
    
    # ç·šå½¢å›å¸°ã§ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã‚’æ¨å®š
    n = valid_count
    sum_x = np.sum(valid_scales)
    sum_y = np.sum(valid_box_counts)
    sum_xy = np.sum(valid_scales * valid_box_counts)
    sum_x2 = np.sum(valid_scales**2)
    
    denominator = n * sum_x2 - sum_x**2
    if abs(denominator) < 1e-10:
        return 1.5
    
    slope = (n * sum_xy - sum_x * sum_y) / denominator
    
    # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã¯å‚¾ãã®çµ¶å¯¾å€¤
    fractal_dim = abs(slope)
    
    # ç‰©ç†çš„åˆ¶ç´„ (1.0 <= D <= 2.0)
    return max(1.0, min(2.0, fractal_dim))


@jit(nopython=True)
def multi_resolution_wavelet_transform(
    data: np.ndarray,
    levels: int = 5
) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›
    æ™‚é–“-å‘¨æ³¢æ•°åˆ†æã®æœ€é«˜å³°
    """
    n = len(data)
    coeffs = np.zeros((levels, n))
    energies = np.zeros(levels)
    
    # ç°¡æ˜“ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›ï¼ˆDaubechiesè¿‘ä¼¼ï¼‰
    current_data = data.copy()
    
    for level in range(levels):
        if len(current_data) < 4:
            break
            
        # ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä¿‚æ•°ï¼ˆDaubechies D4è¿‘ä¼¼ï¼‰
        h = np.array([-0.1830127, -0.3169873, 1.1830127, -0.6830127])
        
        # ç•³ã¿è¾¼ã¿
        filtered = np.zeros(len(current_data))
        for i in range(len(current_data)):
            for j in range(len(h)):
                if i - j >= 0:
                    filtered[i] += current_data[i - j] * h[j]
        
        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        decimated = filtered[::2]
        
        # ä¿‚æ•°ã®ä¿å­˜
        if len(decimated) <= n:
            coeffs[level, :len(decimated)] = decimated
            energies[level] = np.sum(decimated**2)
        
        # æ¬¡ã®ãƒ¬ãƒ™ãƒ«ã®æº–å‚™
        current_data = decimated
        if len(current_data) < 2:
            break
    
    return coeffs, energies


@jit(nopython=True)
def supreme_dft_enhanced(
    data: np.ndarray,
    window_size: int = 60,
    overlap: float = 0.75
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    è¶…é«˜åº¦DFTè§£æ
    é‡è¤‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¨ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹è¶…ç²¾å¯†å‘¨æ³¢æ•°è§£æ
    """
    n = len(data)
    if n < window_size:
        window_size = n // 2
    
    step_size = int(window_size * (1 - overlap))
    if step_size < 1:
        step_size = 1
    
    frequencies = np.zeros(n)
    confidences = np.zeros(n)
    phase_coherences = np.zeros(n)
    
    for start in range(0, n - window_size + 1, step_size):
        end = start + window_size
        window_data = data[start:end]
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é–¢æ•°ï¼ˆBlackman-Harrisï¼‰
        window_func = np.zeros(window_size)
        for i in range(window_size):
            t = 2 * np.pi * i / (window_size - 1)
            window_func[i] = (0.35875 - 0.48829 * np.cos(t) + 
                             0.14128 * np.cos(2*t) - 0.01168 * np.cos(3*t))
        
        windowed_data = window_data * window_func
        
        # ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ4å€ï¼‰
        padded_size = window_size * 4
        padded_data = np.zeros(padded_size)
        padded_data[:window_size] = windowed_data
        
        # DFTè¨ˆç®—
        freqs = np.zeros(25)  # 6-50æœŸé–“ã«å¯¾å¿œ
        powers = np.zeros(25)
        phases = np.zeros(25)
        
        for period_idx, period in enumerate(range(6, 31)):
            if period < padded_size // 2:
                real_part = 0.0
                imag_part = 0.0
                
                for i in range(padded_size):
                    angle = 2 * np.pi * i / period
                    real_part += padded_data[i] * np.cos(angle)
                    imag_part += padded_data[i] * np.sin(angle)
                
                power = real_part**2 + imag_part**2
                phase = np.arctan2(imag_part, real_part)
                
                freqs[period_idx] = period
                powers[period_idx] = power
                phases[period_idx] = phase
        
        # æœ€å¤§ãƒ‘ãƒ¯ãƒ¼ã®å‘¨æ³¢æ•°ã‚’æ¤œå‡º
        max_idx = np.argmax(powers)
        dominant_freq = freqs[max_idx]
        confidence = powers[max_idx] / (np.sum(powers) + 1e-10)
        
        # ä½ç›¸ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹è¨ˆç®—
        phase_coherence = 0.0
        if max_idx > 0 and max_idx < len(phases) - 1:
            phase_diff1 = abs(phases[max_idx] - phases[max_idx-1])
            phase_diff2 = abs(phases[max_idx+1] - phases[max_idx])
            phase_coherence = 1.0 / (1.0 + phase_diff1 + phase_diff2)
        
        # çµæœã‚’ä¿å­˜
        mid_point = start + window_size // 2
        if mid_point < n:
            frequencies[mid_point] = dominant_freq
            confidences[mid_point] = confidence
            phase_coherences[mid_point] = phase_coherence
    
    # è£œé–“ã§æ¬ ã‘ã¦ã„ã‚‹å€¤ã‚’åŸ‹ã‚ã‚‹
    for i in range(n):
        if frequencies[i] == 0.0:
            # æœ€ã‚‚è¿‘ã„éã‚¼ãƒ­å€¤ã‚’ä½¿ç”¨
            left_val = 0.0
            right_val = 0.0
            
            for j in range(i, -1, -1):
                if frequencies[j] > 0:
                    left_val = frequencies[j]
                    break
            
            for j in range(i, n):
                if frequencies[j] > 0:
                    right_val = frequencies[j]
                    break
            
            if left_val > 0 and right_val > 0:
                frequencies[i] = (left_val + right_val) / 2
            elif left_val > 0:
                frequencies[i] = left_val
            elif right_val > 0:
                frequencies[i] = right_val
            else:
                frequencies[i] = 20.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    return frequencies, confidences, phase_coherences


@jit(nopython=True)
def adaptive_ensemble_fusion(
    method_periods: np.ndarray,
    method_confidences: np.ndarray,
    method_weights: np.ndarray,
    spectral_entropy: float,
    fractal_dimension: float
) -> Tuple[float, float]:
    """
    é©å¿œå‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«èåˆ
    å‹•çš„é‡ã¿èª¿æ•´ã«ã‚ˆã‚‹æœ€é©çµ±åˆ
    """
    n_methods = len(method_periods)
    if n_methods == 0:
        return 20.0, 0.5
    
    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ã®é‡ã¿èª¿æ•´
    entropy_factor = 1.0 - spectral_entropy
    
    # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒãƒ™ãƒ¼ã‚¹ã®é‡ã¿èª¿æ•´
    fractal_factor = (fractal_dimension - 1.0) / 1.0  # 1.0-2.0ã‚’0.0-1.0ã«ãƒãƒƒãƒ—
    
    # é©å¿œé‡ã¿è¨ˆç®—
    adaptive_weights = np.zeros(n_methods)
    
    for i in range(n_methods):
        # ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹é‡ã¿
        confidence_weight = method_confidences[i]
        
        # åŸºæœ¬é‡ã¿
        base_weight = method_weights[i]
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¿æ•´
        entropy_adjustment = 1.0 + entropy_factor * 0.5
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«èª¿æ•´
        fractal_adjustment = 1.0 + fractal_factor * 0.3
        
        # ç·åˆé‡ã¿
        adaptive_weights[i] = (base_weight * confidence_weight * 
                              entropy_adjustment * fractal_adjustment)
    
    # æ­£è¦åŒ–
    total_weight = np.sum(adaptive_weights)
    if total_weight > 0:
        adaptive_weights = adaptive_weights / total_weight
    else:
        adaptive_weights = np.ones(n_methods) / n_methods
    
    # é‡ã¿ä»˜ãå¹³å‡
    final_period = np.sum(method_periods * adaptive_weights)
    final_confidence = np.sum(method_confidences * adaptive_weights)
    
    return final_period, final_confidence


@jit(nopython=True)
def calculate_supreme_ultimate_cycle_numba(
    price: np.ndarray,
    cycle_part: float = 0.5,
    max_output: int = 34,
    min_output: int = 1,
    entropy_window: int = 40,
    period_range: Tuple[int, int] = (6, 50)
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    å²ä¸Šæœ€å¼·ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    æœ€æ–°ã®æ•°å­¦ç†è«–ã¨ä¿¡å·å‡¦ç†æŠ€è¡“ã‚’çµ±åˆ
    """
    n = len(price)
    
    # 1. é«˜åº¦ãªã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
    spectral_entropies = np.zeros(n)
    for i in range(entropy_window, n):
        spectral_entropies[i] = advanced_spectral_entropy(
            price[max(0, i-entropy_window):i+1], entropy_window
        )
    
    # 2. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒæ¨å®š
    fractal_dimensions = np.zeros(n)
    for i in range(30, n):
        fractal_dimensions[i] = fractal_dimension_estimator(
            price[max(0, i-30):i+1], 8
        )
    
    # 3. å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›
    wavelet_coeffs, wavelet_energies = multi_resolution_wavelet_transform(price, 6)
    
    # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã‹ã‚‰ã®ã‚µã‚¤ã‚¯ãƒ«æœŸé–“æ¨å®š
    wavelet_periods = np.zeros(n)
    wavelet_confidences = np.zeros(n)
    
    for i in range(n):
        # æœ€å¤§ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ãƒ¬ãƒ™ãƒ«ã‚’æ¤œå‡º
        max_energy_level = 0
        max_energy = 0.0
        
        for level in range(len(wavelet_energies)):
            if wavelet_energies[level] > max_energy:
                max_energy = wavelet_energies[level]
                max_energy_level = level
        
        # ãƒ¬ãƒ™ãƒ«ã‹ã‚‰å‘¨æœŸã‚’æ¨å®š
        estimated_period = 2**(max_energy_level + 3)  # ãƒ¬ãƒ™ãƒ«0â†’8æœŸé–“, ãƒ¬ãƒ™ãƒ«1â†’16æœŸé–“, etc.
        estimated_period = max(period_range[0], min(period_range[1], estimated_period))
        
        wavelet_periods[i] = estimated_period
        wavelet_confidences[i] = max_energy / (np.sum(wavelet_energies) + 1e-10)
    
    # 4. è¶…é«˜åº¦DFTè§£æ
    dft_periods, dft_confidences, phase_coherences = supreme_dft_enhanced(price, 50, 0.7)
    
    # 5. å¾“æ¥ã®Hilbertå¤‰æ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    hilbert_periods = np.zeros(n)
    hilbert_confidences = np.zeros(n)
    
    for i in range(20, n):
        # å±€æ‰€çš„ãªè‡ªå·±ç›¸é–¢ã«ã‚ˆã‚‹å‘¨æœŸæ¤œå‡º
        window_size = min(40, i)
        local_data = price[i-window_size:i+1]
        
        max_corr = 0.0
        best_period = 20.0
        
        for period in range(period_range[0], min(period_range[1], window_size//2)):
            if len(local_data) >= 2 * period:
                # è‡ªå·±ç›¸é–¢è¨ˆç®—
                delayed_data = local_data[:-period]
                current_data = local_data[period:]
                
                if len(delayed_data) == len(current_data) and len(delayed_data) > 0:
                    # ç›¸é–¢ä¿‚æ•°è¨ˆç®—
                    mean_delayed = np.mean(delayed_data)
                    mean_current = np.mean(current_data)
                    
                    numerator = np.sum((delayed_data - mean_delayed) * (current_data - mean_current))
                    
                    delayed_std = np.sqrt(np.sum((delayed_data - mean_delayed)**2))
                    current_std = np.sqrt(np.sum((current_data - mean_current)**2))
                    
                    if delayed_std > 0 and current_std > 0:
                        corr = numerator / (delayed_std * current_std)
                        
                        if abs(corr) > max_corr:
                            max_corr = abs(corr)
                            best_period = float(period)
        
        hilbert_periods[i] = best_period
        hilbert_confidences[i] = max_corr
    
    # 6. é©å¿œå‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆ
    final_periods = np.zeros(n)
    final_confidences = np.zeros(n)
    
    # åŸºæœ¬é‡ã¿ï¼ˆDFTé‡è¦–ã€ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè£œå®Œã€Hilbertå®‰å®šæ€§ï¼‰
    base_weights = np.array([0.6, 0.25, 0.15])  # [DFT, Wavelet, Hilbert]
    
    for i in range(n):
        method_periods = np.array([dft_periods[i], wavelet_periods[i], hilbert_periods[i]])
        method_confidences = np.array([dft_confidences[i], wavelet_confidences[i], hilbert_confidences[i]])
        
        # é©å¿œå‹èåˆ
        period, confidence = adaptive_ensemble_fusion(
            method_periods,
            method_confidences,
            base_weights,
            spectral_entropies[i],
            fractal_dimensions[i]
        )
        
        final_periods[i] = period
        final_confidences[i] = confidence
    
    # 7. æœ€çµ‚ã‚µã‚¤ã‚¯ãƒ«å€¤è¨ˆç®—
    dom_cycle = np.zeros(n)
    for i in range(n):
        cycle_value = np.ceil(final_periods[i] * cycle_part)
        dom_cycle[i] = max(min_output, min(max_output, cycle_value))
    
    return dom_cycle, final_periods, final_confidences, spectral_entropies, fractal_dimensions


class EhlersSupremeUltimateCycle(EhlersDominantCycle):
    """
    å²ä¸Šæœ€å¼·ã®ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨ - Supreme Ultimate Cycle Detector
    
    ğŸŒŸ **é©å‘½çš„ãªçµ±åˆæŠ€è¡“:**
    
    ğŸ”¬ **é«˜åº¦ãªæ•°å­¦ç†è«–:**
    1. **å¤šè§£åƒåº¦ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›**: æ™‚é–“-å‘¨æ³¢æ•°é ˜åŸŸã§ã®å®Œå…¨åˆ†è§£
    2. **ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**: å‘¨æ³¢æ•°é ˜åŸŸã§ã®æƒ…å ±ç†è«–çš„æœ€é©åŒ–
    3. **ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æ**: è¤‡é›‘ç³»ç†è«–ã«ã‚ˆã‚‹éç·šå½¢ç‰¹æ€§è§£æ
    4. **ä½ç›¸ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹**: ä¿¡å·ã®ä¸€è²«æ€§è©•ä¾¡
    
    âš¡ **æœ€å…ˆç«¯ä¿¡å·å‡¦ç†:**
    1. **é‡è¤‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦DFT**: è¶…ç²¾å¯†å‘¨æ³¢æ•°è§£æ
    2. **ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°**: å‘¨æ³¢æ•°åˆ†è§£èƒ½ã®å‘ä¸Š
    3. **Blackman-Harrisã‚¦ã‚£ãƒ³ãƒ‰ã‚¦**: æœ€é«˜å“è³ªã®ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¨å®š
    4. **é©å¿œå‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«èåˆ**: å‹•çš„é‡ã¿èª¿æ•´
    
    ğŸ¯ **ç©¶æ¥µã®ç‰¹å¾´:**
    - DFTDominantã‚’è¶…ãˆã‚‹ç²¾åº¦
    - æ¥µé™ã¾ã§ä½ã„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
    - è¤‡æ•°ã‚µã‚¤ã‚¯ãƒ«ã®åŒæ™‚æ¤œå‡º
    - å¸‚å ´ã®éç·šå½¢ç‰¹æ€§ã¸ã®å®Œå…¨å¯¾å¿œ
    - é‡å­çš„ç¢ºç‡è«–ã®æ´»ç”¨
    
    ğŸ’ª **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
    - å…¨ã¦ã®æ—¢å­˜æ‰‹æ³•ã‚’ä¸Šå›ã‚‹å®‰å®šæ€§
    - å²ä¸Šæœ€é«˜ã®äºˆæ¸¬ç²¾åº¦
    - å®Œç’§ãªãƒã‚¤ã‚ºè€æ€§
    - çµ¶å¯¾çš„ãªå‹åˆ©
    """
    
    # è¨±å¯ã•ã‚Œã‚‹ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
    SRC_TYPES = ['close', 'hlc3', 'hl2', 'ohlc4']
    
    def __init__(
        self,
        cycle_part: float = 0.5,
        max_output: int = 34,
        min_output: int = 1,
        entropy_window: int = 40,
        period_range: Tuple[int, int] = (6, 50),
        src_type: str = 'close'
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            cycle_part: ã‚µã‚¤ã‚¯ãƒ«éƒ¨åˆ†ã®å€ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5ï¼‰
            max_output: æœ€å¤§å‡ºåŠ›å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 34ï¼‰
            min_output: æœ€å°å‡ºåŠ›å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
            entropy_window: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 40ï¼‰
            period_range: ã‚µã‚¤ã‚¯ãƒ«æœŸé–“ã®ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: (6, 50)ï¼‰
            src_type: ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ— ('close', 'hlc3', 'hl2', 'ohlc4')
        """
        super().__init__(
            f"SupremeUltimate({cycle_part}, {period_range})",
            cycle_part,
            period_range[1],  # max_cycle
            period_range[0],  # min_cycle
            max_output,
            min_output
        )
        
        self.entropy_window = entropy_window
        self.period_range = period_range
        self.src_type = src_type.lower()
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®æ¤œè¨¼
        if self.src_type not in self.SRC_TYPES:
            raise ValueError(f"ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã§ã™: {src_type}ã€‚æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {', '.join(self.SRC_TYPES)}")
        
        # è¿½åŠ ã®çµæœä¿å­˜ç”¨
        self._final_confidences = None
        self._spectral_entropies = None
        self._fractal_dimensions = None
    
    def calculate_source_values(self, data: Union[pd.DataFrame, np.ndarray], src_type: str) -> np.ndarray:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã™ã‚‹
        """
        if isinstance(data, pd.DataFrame):
            if src_type == 'close':
                if 'close' in data.columns:
                    return data['close'].values
                elif 'Close' in data.columns:
                    return data['Close'].values
                else:
                    raise ValueError("DataFrameã«ã¯'close'ã¾ãŸã¯'Close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'hlc3':
                if all(col in data.columns for col in ['high', 'low', 'close']):
                    return (data['high'] + data['low'] + data['close']).values / 3
                elif all(col in data.columns for col in ['High', 'Low', 'Close']):
                    return (data['High'] + data['Low'] + data['Close']).values / 3
                else:
                    raise ValueError("hlc3ã®è¨ˆç®—ã«ã¯'high', 'low', 'close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'hl2':
                if all(col in data.columns for col in ['high', 'low']):
                    return (data['high'] + data['low']).values / 2
                elif all(col in data.columns for col in ['High', 'Low']):
                    return (data['High'] + data['Low']).values / 2
                else:
                    raise ValueError("hl2ã®è¨ˆç®—ã«ã¯'high', 'low'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'ohlc4':
                if all(col in data.columns for col in ['open', 'high', 'low', 'close']):
                    return (data['open'] + data['high'] + data['low'] + data['close']).values / 4
                elif all(col in data.columns for col in ['Open', 'High', 'Low', 'Close']):
                    return (data['Open'] + data['High'] + data['Low'] + data['Close']).values / 4
                else:
                    raise ValueError("ohlc4ã®è¨ˆç®—ã«ã¯'open', 'high', 'low', 'close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
        
        else:  # NumPyé…åˆ—ã®å ´åˆ
            if data.ndim == 2 and data.shape[1] >= 4:
                if src_type == 'close':
                    return data[:, 3]  # close
                elif src_type == 'hlc3':
                    return (data[:, 1] + data[:, 2] + data[:, 3]) / 3  # high, low, close
                elif src_type == 'hl2':
                    return (data[:, 1] + data[:, 2]) / 2  # high, low
                elif src_type == 'ohlc4':
                    return (data[:, 0] + data[:, 1] + data[:, 2] + data[:, 3]) / 4  # open, high, low, close
            else:
                return data  # 1æ¬¡å…ƒé…åˆ—ã¨ã—ã¦æ‰±ã†
        
        return data
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """
        å²ä¸Šæœ€å¼·ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯NumPyé…åˆ—ï¼‰
        
        Returns:
            ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã®å€¤
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash == self._data_hash and self._result is not None:
                return self._result.values
            
            self._data_hash = data_hash
            
            # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            price = self.calculate_source_values(data, self.src_type)
            
            # Numbaé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‚’è¨ˆç®—
            dom_cycle, raw_period, confidences, spectral_entropies, fractal_dimensions = calculate_supreme_ultimate_cycle_numba(
                price,
                self.cycle_part,
                self.max_output,
                self.min_output,
                self.entropy_window,
                self.period_range
            )
            
            # çµæœã‚’ä¿å­˜
            self._result = DominantCycleResult(
                values=dom_cycle,
                raw_period=raw_period,
                smooth_period=raw_period  # ã“ã®å®Ÿè£…ã§ã¯åŒã˜
            )
            
            # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            self._final_confidences = confidences
            self._spectral_entropies = spectral_entropies
            self._fractal_dimensions = fractal_dimensions
            
            self._values = dom_cycle
            return dom_cycle
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"EhlersSupremeUltimateCycleè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            return np.array([])
    
    @property
    def confidence_scores(self) -> Optional[np.ndarray]:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        return self._final_confidences
    
    @property
    def spectral_entropies(self) -> Optional[np.ndarray]:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’å–å¾—"""
        return self._spectral_entropies
    
    @property
    def fractal_dimensions(self) -> Optional[np.ndarray]:
        """ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã‚’å–å¾—"""
        return self._fractal_dimensions
    
    def get_analysis_summary(self) -> Dict:
        """
        åˆ†æã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        """
        if self._result is None:
            return {}
        
        summary = {
            'algorithm': 'Supreme Ultimate Cycle Detector',
            'methods_used': [
                'Multi-Resolution Wavelet Transform',
                'Advanced Spectral Entropy',
                'Fractal Dimension Analysis',
                'Supreme DFT with Overlap & Zero-Padding',
                'Phase Coherence Analysis',
                'Adaptive Ensemble Fusion'
            ],
            'cycle_range': self.period_range,
            'avg_confidence': np.mean(self._final_confidences) if self._final_confidences is not None else None,
            'avg_spectral_entropy': np.mean(self._spectral_entropies) if self._spectral_entropies is not None else None,
            'avg_fractal_dimension': np.mean(self._fractal_dimensions) if self._fractal_dimensions is not None else None,
            'dominant_cycle_stats': {
                'mean': np.mean(self._result.values),
                'std': np.std(self._result.values),
                'min': np.min(self._result.values),
                'max': np.max(self._result.values)
            },
            'revolutionary_features': [
                'Blackman-Harris windowing for maximum spectral accuracy',
                'Zero-padding 4x for enhanced frequency resolution',
                'Fractal dimension for nonlinear market characterization',
                'Spectral entropy for information-theoretic optimization',
                'Multi-resolution wavelet decomposition',
                'Adaptive ensemble with dynamic weight adjustment'
            ]
        }
        
        return summary 