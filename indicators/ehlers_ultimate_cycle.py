#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from typing import Union, Optional, Tuple, Dict, List
import numpy as np
import pandas as pd
from numba import jit, prange, float64, int32
from scipy import signal
import warnings
warnings.filterwarnings('ignore')

from .ehlers_dominant_cycle import EhlersDominantCycle, DominantCycleResult


@jit(nopython=True)
def robust_entropy(data: np.ndarray, window: int = 20) -> float:
    """
    ãƒ­ãƒã‚¹ãƒˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆå¤–ã‚Œå€¤ã«å¯¾ã—ã¦å®‰å®šï¼‰
    """
    if len(data) < window:
        return 0.5
    
    recent_data = data[-window:]
    
    # å¤–ã‚Œå€¤é™¤å»ï¼ˆIQRæ–¹å¼ï¼‰
    q1 = np.percentile(recent_data, 25)
    q3 = np.percentile(recent_data, 75)
    iqr = q3 - q1
    
    if iqr > 0:
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        filtered_data = recent_data[(recent_data >= lower_bound) & (recent_data <= upper_bound)]
    else:
        filtered_data = recent_data
    
    if len(filtered_data) < 3:
        return 0.5
    
    # æ­£è¦åŒ–
    data_range = np.max(filtered_data) - np.min(filtered_data)
    if data_range == 0:
        return 0.0
    
    normalized = (filtered_data - np.min(filtered_data)) / data_range
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¨ˆç®—
    hist, _ = np.histogram(normalized, bins=8, range=(0, 1))
    hist = hist.astype(np.float64)
    total = np.sum(hist)
    
    if total == 0:
        return 0.0
    
    prob_dist = hist / total
    
    # ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    entropy = 0.0
    for p in prob_dist:
        if p > 0:
            entropy -= p * np.log2(p)
    
    return entropy / np.log2(8)


@jit(nopython=True)
def enhanced_kalman_smoother(
    observations: np.ndarray,
    process_noise: float = 0.001,
    observation_noise: float = 0.01
) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¼·åŒ–ç‰ˆKalmanã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ï¼ˆå‰æ–¹-å¾Œæ–¹ãƒ‘ã‚¹ï¼‰
    """
    n = len(observations)
    
    # å‰æ–¹ãƒ‘ã‚¹
    states_forward = np.zeros(n)
    covariances_forward = np.zeros(n)
    
    # åˆæœŸçŠ¶æ…‹
    state = observations[0] if n > 0 else 20.0
    covariance = 1.0
    
    for i in range(n):
        # äºˆæ¸¬
        state_pred = state
        cov_pred = covariance + process_noise
        
        # é©å¿œçš„ãªè¦³æ¸¬ãƒã‚¤ã‚º
        innovation = observations[i] - state_pred
        adaptive_obs_noise = observation_noise * (1 + 0.5 * abs(innovation) / (abs(state_pred) + 1e-10))
        
        # æ›´æ–°
        innovation_cov = cov_pred + adaptive_obs_noise
        kalman_gain = cov_pred / innovation_cov if innovation_cov > 0 else 0
        
        state = state_pred + kalman_gain * innovation
        covariance = (1 - kalman_gain) * cov_pred
        
        states_forward[i] = state
        covariances_forward[i] = covariance
    
    # å¾Œæ–¹ãƒ‘ã‚¹ï¼ˆã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
    smoothed_states = np.zeros(n)
    smoothed_states[-1] = states_forward[-1]
    
    for i in range(n-2, -1, -1):
        if i < n-1:
            A = covariances_forward[i] / (covariances_forward[i] + process_noise)
            smoothed_states[i] = states_forward[i] + A * (smoothed_states[i+1] - states_forward[i])
        else:
            smoothed_states[i] = states_forward[i]
    
    # åˆ¶ç´„é©ç”¨
    for i in range(n):
        smoothed_states[i] = max(6.0, min(50.0, smoothed_states[i]))
    
    return smoothed_states, covariances_forward


@jit(nopython=True)
def stable_hilbert_cycle(
    price: np.ndarray,
    period_range: Tuple[int, int] = (6, 50)
) -> np.ndarray:
    """
    å®‰å®šåŒ–Hilbertå¤‰æ›ã«ã‚ˆã‚‹ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º
    """
    n = len(price)
    pi = 2 * np.arcsin(1.0)
    
    # åˆæœŸåŒ–
    smooth = np.zeros(n)
    detrender = np.zeros(n)
    i1 = np.zeros(n)
    q1 = np.zeros(n)
    period = np.zeros(n)
    
    # å®‰å®šæ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    alpha = 0.07  # ã‚ˆã‚Šä¿å®ˆçš„ãªå€¤
    
    for i in range(n):
        if i < 6:
            smooth[i] = price[i]
            period[i] = 20.0
            continue
        
        # å®‰å®šåŒ–ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        smooth[i] = (4 * price[i] + 3 * price[i-1] + 2 * price[i-2] + price[i-3]) / 10
        
        # å®‰å®šåŒ–ãƒ‡ãƒˆãƒ¬ãƒ³ãƒ‰
        detrender[i] = (0.0962 * smooth[i] + 0.5769 * smooth[i-2] - 
                       0.5769 * smooth[i-4] - 0.0962 * smooth[i-6]) * (0.075 * period[i-1] + 0.54)
        
        # Hilbertå¤‰æ›è¿‘ä¼¼
        q1[i] = (0.0962 * detrender[i] + 0.5769 * detrender[i-2] - 
                0.5769 * detrender[i-4] - 0.0962 * detrender[i-6]) * (0.075 * period[i-1] + 0.54)
        i1[i] = detrender[i-3] if i >= 3 else 0
        
        # å®‰å®šåŒ–ãƒ•ã‚£ãƒ«ã‚¿
        if i >= 1:
            i1[i] = alpha * i1[i] + (1 - alpha) * i1[i-1]
            q1[i] = alpha * q1[i] + (1 - alpha) * q1[i-1]
        
        # å®‰å®šåŒ–æœŸé–“è¨ˆç®—
        if i >= 1 and (i1[i] != 0 or q1[i] != 0):
            re = i1[i] * i1[i-1] + q1[i] * q1[i-1]
            im = i1[i] * q1[i-1] - q1[i] * i1[i-1]
            
            if abs(im) > 1e-10 and abs(re) > 1e-10:
                raw_period = 2 * pi / abs(np.arctan(im / re))
                
                # ã‚ˆã‚Šå³ã—ã„åˆ¶é™
                max_change = 0.2  # 20%ã®å¤‰åŒ–ã¾ã§è¨±å¯
                if raw_period > period[i-1] * (1 + max_change):
                    raw_period = period[i-1] * (1 + max_change)
                elif raw_period < period[i-1] * (1 - max_change):
                    raw_period = period[i-1] * (1 - max_change)
                
                raw_period = max(period_range[0], min(period_range[1], raw_period))
                
                # ã‚ˆã‚Šå¼·ã„å¹³æ»‘åŒ–
                period[i] = 0.1 * raw_period + 0.9 * period[i-1]
            else:
                period[i] = period[i-1]
        else:
            period[i] = period[i-1] if i >= 1 else 20.0
    
    return period


@jit(nopython=True)
def adaptive_autocorrelation_cycle(
    price: np.ndarray,
    max_period: int = 50,
    min_period: int = 6
) -> Tuple[np.ndarray, np.ndarray]:
    """
    é©å¿œçš„è‡ªå·±ç›¸é–¢ã«ã‚ˆã‚‹ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º
    """
    n = len(price)
    periods = np.zeros(n)
    confidences = np.zeros(n)
    
    for i in range(max_period, n):
        window_size = min(100, i)  # é•·æœŸçª“ã§å®‰å®šæ€§å‘ä¸Š
        recent_data = price[i-window_size:i+1]
        
        max_corr = 0.0
        best_period = 20.0
        
        # è‡ªå·±ç›¸é–¢è¨ˆç®—
        for period in range(min_period, min(max_period, window_size//3)):
            if len(recent_data) >= 2 * period:
                # ã‚ˆã‚Šé•·ã„åŒºé–“ã§ã®ç›¸é–¢è¨ˆç®—
                data1 = recent_data[:-period]
                data2 = recent_data[period:]
                
                if len(data1) > 10 and len(data2) > 10:
                    # ç›¸é–¢ä¿‚æ•°è¨ˆç®—
                    mean1 = np.mean(data1)
                    mean2 = np.mean(data2)
                    
                    num = np.sum((data1 - mean1) * (data2 - mean2))
                    den1 = np.sqrt(np.sum((data1 - mean1)**2))
                    den2 = np.sqrt(np.sum((data2 - mean2)**2))
                    
                    if den1 > 0 and den2 > 0:
                        corr = num / (den1 * den2)
                        
                        if abs(corr) > max_corr:
                            max_corr = abs(corr)
                            best_period = float(period)
        
        periods[i] = best_period
        confidences[i] = max_corr
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(max_period):
        periods[i] = 20.0
        confidences[i] = 0.5
    
    return periods, confidences


@jit(nopython=True)
def multi_method_consensus(
    hilbert_periods: np.ndarray,
    autocorr_periods: np.ndarray,
    autocorr_confidences: np.ndarray,
    noise_levels: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    è¤‡æ•°æ‰‹æ³•ã®åˆæ„å½¢æˆã«ã‚ˆã‚‹æœ€çµ‚æœŸé–“æ±ºå®š
    """
    n = len(hilbert_periods)
    final_periods = np.zeros(n)
    consensus_scores = np.zeros(n)
    
    for i in range(n):
        # æ‰‹æ³•é–“ã®ä¸€è‡´åº¦è¨ˆç®—
        period_diff = abs(hilbert_periods[i] - autocorr_periods[i])
        max_period = max(hilbert_periods[i], autocorr_periods[i])
        
        if max_period > 0:
            agreement = 1.0 - (period_diff / max_period)
        else:
            agreement = 0.0
        
        # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹èª¿æ•´
        noise_factor = 1.0 - noise_levels[i]
        
        # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹é‡ã¿è¨ˆç®—
        hilbert_weight = 0.7 * noise_factor  # Hilbertã¯ä¸€èˆ¬çš„ã«å®‰å®š
        autocorr_weight = autocorr_confidences[i] * (1 - noise_factor)
        
        total_weight = hilbert_weight + autocorr_weight
        if total_weight > 0:
            hilbert_weight /= total_weight
            autocorr_weight /= total_weight
        else:
            hilbert_weight = 0.7
            autocorr_weight = 0.3
        
        # é‡ã¿ä»˜ãå¹³å‡
        final_periods[i] = (hilbert_weight * hilbert_periods[i] + 
                           autocorr_weight * autocorr_periods[i])
        
        # åˆæ„ã‚¹ã‚³ã‚¢
        consensus_scores[i] = agreement * min(1.0, hilbert_weight + autocorr_weight)
    
    return final_periods, consensus_scores


@jit(nopython=True)
def calculate_ultimate_cycle_numba(
    price: np.ndarray,
    cycle_part: float = 0.5,
    max_output: int = 34,
    min_output: int = 1,
    entropy_window: int = 30,
    period_range: Tuple[int, int] = (6, 50)
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ç©¶æ¥µã®ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    """
    n = len(price)
    
    # 1. ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«è©•ä¾¡
    noise_levels = np.zeros(n)
    for i in range(entropy_window, n):
        noise_levels[i] = robust_entropy(price[max(0, i-entropy_window):i+1], entropy_window)
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(entropy_window):
        noise_levels[i] = 0.5
    
    # 2. å®‰å®šåŒ–Hilbertå¤‰æ›
    hilbert_periods = stable_hilbert_cycle(price, period_range)
    
    # 3. é©å¿œçš„è‡ªå·±ç›¸é–¢
    autocorr_periods, autocorr_confidences = adaptive_autocorrelation_cycle(
        price, period_range[1], period_range[0]
    )
    
    # 4. è¤‡æ•°æ‰‹æ³•ã®åˆæ„å½¢æˆ
    consensus_periods, consensus_scores = multi_method_consensus(
        hilbert_periods, autocorr_periods, autocorr_confidences, noise_levels
    )
    
    # 5. å¼·åŒ–ç‰ˆKalmanã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    final_periods, uncertainties = enhanced_kalman_smoother(
        consensus_periods, process_noise=0.001, observation_noise=0.01
    )
    
    # 6. æœ€çµ‚ã‚µã‚¤ã‚¯ãƒ«å€¤è¨ˆç®—
    dom_cycle = np.zeros(n)
    for i in range(n):
        cycle_value = np.ceil(final_periods[i] * cycle_part)
        dom_cycle[i] = max(min_output, min(max_output, cycle_value))
    
    return dom_cycle, final_periods, consensus_scores, noise_levels


class EhlersUltimateCycle(EhlersDominantCycle):
    """
    ç©¶æ¥µã®ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨ - æœ€é«˜ã®å®‰å®šæ€§ã‚’ç›®æŒ‡ã—ãŸçµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
    
    ğŸ¯ **è¨­è¨ˆæ€æƒ³:**
    å¾“æ¥æ‰‹æ³•ã®å®‰å®šæ€§ã‚’è¶…ãˆã‚‹ã“ã¨ã‚’æœ€å„ªå…ˆã«ã€è¤‡æ•°ã®é©æ–°çš„æŠ€è¡“ã‚’æ…é‡ã«çµ±åˆ
    
    ğŸ† **ä¸»è¦ãªç‰¹å¾´:**
    1. **ãƒ­ãƒã‚¹ãƒˆã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼**: å¤–ã‚Œå€¤ã«å¼·ã„ãƒã‚¤ã‚ºè©•ä¾¡
    2. **å®‰å®šåŒ–Hilbertå¤‰æ›**: æ€¥æ¿€ãªå¤‰åŒ–ã‚’æŠ‘åˆ¶ã—ãŸæœŸé–“æ¤œå‡º
    3. **é©å¿œçš„è‡ªå·±ç›¸é–¢**: é•·æœŸçª“ã«ã‚ˆã‚‹ä¿¡é ¼æ€§ã®é«˜ã„å‘¨æœŸæ¤œå‡º
    4. **å¤šæ‰‹æ³•åˆæ„å½¢æˆ**: è¤‡æ•°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ…é‡ãªçµ±åˆ
    5. **å¼·åŒ–ç‰ˆKalmanã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°**: å‰æ–¹-å¾Œæ–¹ãƒ‘ã‚¹ã«ã‚ˆã‚‹æœ€é©å¹³æ»‘åŒ–
    
    ğŸ¨ **é©æ–°ãƒã‚¤ãƒ³ãƒˆ:**
    - å®‰å®šæ€§ã‚’æœ€å„ªå…ˆã¨ã—ãŸä¿å®ˆçš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    - å¤–ã‚Œå€¤ã«å¯¾ã™ã‚‹ãƒ­ãƒã‚¹ãƒˆå‡¦ç†
    - æ®µéšçš„ãªä¿¡é ¼åº¦è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
    - é•·æœŸçš„ãªä¸€è²«æ€§ã‚’é‡è¦–ã—ãŸè¨­è¨ˆ
    
    ğŸ’ª **æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ:**
    - å¾“æ¥æ‰‹æ³•ã‚’è¶…ãˆã‚‹å®‰å®šæ€§
    - å¸‚å ´ã‚·ãƒ§ãƒƒã‚¯ã«å¯¾ã™ã‚‹é«˜ã„è€æ€§
    - ä¸€è²«ã—ãŸã‚µã‚¤ã‚¯ãƒ«æ¤œå‡º
    - æ¥µã‚ã¦ä½ã„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
    """
    
    # è¨±å¯ã•ã‚Œã‚‹ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆ
    SRC_TYPES = ['close', 'hlc3', 'hl2', 'ohlc4']
    
    def __init__(
        self,
        cycle_part: float = 0.5,
        max_output: int = 34,
        min_output: int = 1,
        entropy_window: int = 30,
        period_range: Tuple[int, int] = (6, 50),
        src_type: str = 'close'
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            cycle_part: ã‚µã‚¤ã‚¯ãƒ«éƒ¨åˆ†ã®å€ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5ï¼‰
            max_output: æœ€å¤§å‡ºåŠ›å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 34ï¼‰
            min_output: æœ€å°å‡ºåŠ›å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
            entropy_window: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30ï¼‰
            period_range: ã‚µã‚¤ã‚¯ãƒ«æœŸé–“ã®ç¯„å›²ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: (6, 50)ï¼‰
            src_type: ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ— ('close', 'hlc3', 'hl2', 'ohlc4')
        """
        super().__init__(
            f"EhlersUltimate({cycle_part}, {period_range})",
            cycle_part,
            period_range[1],  # max_cycle
            period_range[0],  # min_cycle
            max_output,
            min_output
        )
        
        self.entropy_window = entropy_window
        self.period_range = period_range
        self.src_type = src_type.lower()
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®æ¤œè¨¼
        if self.src_type not in self.SRC_TYPES:
            raise ValueError(f"ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã§ã™: {src_type}ã€‚æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {', '.join(self.SRC_TYPES)}")
        
        # è¿½åŠ ã®çµæœä¿å­˜ç”¨
        self._consensus_scores = None
        self._noise_levels = None
    
    def calculate_source_values(self, data: Union[pd.DataFrame, np.ndarray], src_type: str) -> np.ndarray:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã™ã‚‹
        """
        if isinstance(data, pd.DataFrame):
            if src_type == 'close':
                if 'close' in data.columns:
                    return data['close'].values
                elif 'Close' in data.columns:
                    return data['Close'].values
                else:
                    raise ValueError("DataFrameã«ã¯'close'ã¾ãŸã¯'Close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'hlc3':
                if all(col in data.columns for col in ['high', 'low', 'close']):
                    return (data['high'] + data['low'] + data['close']).values / 3
                elif all(col in data.columns for col in ['High', 'Low', 'Close']):
                    return (data['High'] + data['Low'] + data['Close']).values / 3
                else:
                    raise ValueError("hlc3ã®è¨ˆç®—ã«ã¯'high', 'low', 'close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'hl2':
                if all(col in data.columns for col in ['high', 'low']):
                    return (data['high'] + data['low']).values / 2
                elif all(col in data.columns for col in ['High', 'Low']):
                    return (data['High'] + data['Low']).values / 2
                else:
                    raise ValueError("hl2ã®è¨ˆç®—ã«ã¯'high', 'low'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
            
            elif src_type == 'ohlc4':
                if all(col in data.columns for col in ['open', 'high', 'low', 'close']):
                    return (data['open'] + data['high'] + data['low'] + data['close']).values / 4
                elif all(col in data.columns for col in ['Open', 'High', 'Low', 'Close']):
                    return (data['Open'] + data['High'] + data['Low'] + data['Close']).values / 4
                else:
                    raise ValueError("ohlc4ã®è¨ˆç®—ã«ã¯'open', 'high', 'low', 'close'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
        
        else:  # NumPyé…åˆ—ã®å ´åˆ
            if data.ndim == 2 and data.shape[1] >= 4:
                if src_type == 'close':
                    return data[:, 3]  # close
                elif src_type == 'hlc3':
                    return (data[:, 1] + data[:, 2] + data[:, 3]) / 3  # high, low, close
                elif src_type == 'hl2':
                    return (data[:, 1] + data[:, 2]) / 2  # high, low
                elif src_type == 'ohlc4':
                    return (data[:, 0] + data[:, 1] + data[:, 2] + data[:, 3]) / 4  # open, high, low, close
            else:
                return data  # 1æ¬¡å…ƒé…åˆ—ã¨ã—ã¦æ‰±ã†
        
        return data
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:
        """
        ç©¶æ¥µã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯NumPyé…åˆ—ï¼‰
        
        Returns:
            ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã®å€¤
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash == self._data_hash and self._result is not None:
                return self._result.values
            
            self._data_hash = data_hash
            
            # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            price = self.calculate_source_values(data, self.src_type)
            
            # Numbaé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ‰ãƒŸãƒŠãƒ³ãƒˆã‚µã‚¤ã‚¯ãƒ«ã‚’è¨ˆç®—
            dom_cycle, raw_period, consensus_scores, noise_levels = calculate_ultimate_cycle_numba(
                price,
                self.cycle_part,
                self.max_output,
                self.min_output,
                self.entropy_window,
                self.period_range
            )
            
            # çµæœã‚’ä¿å­˜
            self._result = DominantCycleResult(
                values=dom_cycle,
                raw_period=raw_period,
                smooth_period=raw_period  # ã“ã®å®Ÿè£…ã§ã¯åŒã˜
            )
            
            # è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            self._consensus_scores = consensus_scores
            self._noise_levels = noise_levels
            
            self._values = dom_cycle
            return dom_cycle
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"EhlersUltimateCycleè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            return np.array([])
    
    @property
    def consensus_scores(self) -> Optional[np.ndarray]:
        """åˆæ„ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        return self._consensus_scores
    
    @property
    def noise_levels(self) -> Optional[np.ndarray]:
        """ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’å–å¾—"""
        return self._noise_levels
    
    def get_analysis_summary(self) -> Dict:
        """
        åˆ†æã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        """
        if self._result is None:
            return {}
        
        summary = {
            'algorithm': 'Ultimate Cycle Detector',
            'methods_used': [
                'Robust Entropy Assessment',
                'Stabilized Hilbert Transform',
                'Adaptive Autocorrelation',
                'Multi-Method Consensus',
                'Enhanced Kalman Smoother'
            ],
            'cycle_range': self.period_range,
            'avg_consensus': np.mean(self._consensus_scores) if self._consensus_scores is not None else None,
            'avg_noise_level': np.mean(self._noise_levels) if self._noise_levels is not None else None,
            'dominant_cycle_stats': {
                'mean': np.mean(self._result.values),
                'std': np.std(self._result.values),
                'min': np.min(self._result.values),
                'max': np.max(self._result.values)
            },
            'stability_features': [
                'Conservative parameter settings for maximum stability',
                'Robust outlier handling in entropy calculation',
                'Gradual change constraints in Hilbert transform',
                'Long-term window autocorrelation analysis',
                'Forward-backward Kalman smoothing'
            ]
        }
        
        return summary 