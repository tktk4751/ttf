#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultimate Trend Range Filter - å®‡å®™æœ€å¼·ã®ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤åˆ¥ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼

Ultimate Chop Trendã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ–¹å‘å•ã‚ãšï¼‰ã¨ãƒ¬ãƒ³ã‚¸ã®2ã¤ã«ç‰¹åŒ–ã—ãŸç©¶æ¥µã®åˆ¤åˆ¥ã‚·ã‚¹ãƒ†ãƒ ï¼š
- æœ€æ–°ã®æ•°å­¦ãƒ»çµ±è¨ˆå­¦ãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»ä¿¡å·å‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆ
- ãƒˆãƒ¬ãƒ³ãƒ‰ã®å­˜åœ¨ãã®ã‚‚ã®ã‚’æ¤œå‡ºï¼ˆæ–¹å‘æ€§ã¯å•ã‚ãªã„ï¼‰
- 0=å®Œå…¨ãªãƒ¬ãƒ³ã‚¸ã€1=å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ˜ç¢ºãªåˆ¤å®š
- è¶…ä½é…å»¶ãƒ»è¶…é«˜ç²¾åº¦ãƒ»è¶…é«˜ä¿¡é ¼æ€§

ã€çµ±åˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‘
ğŸ”¬ Advanced Trend Detection - é«˜åº¦ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
ğŸ“Š Range Consolidation Analysis - ãƒ¬ãƒ³ã‚¸çµ±åˆè§£æ
ğŸ§  Multi-Scale Trend Filtering - ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
ğŸ¯ Adaptive Noise Suppression - é©å¿œãƒã‚¤ã‚ºæŠ‘åˆ¶
ğŸ“¡ Harmonic Pattern Recognition - ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
ğŸŒŠ Volatility Regime Classification - ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡
ğŸš€ Machine Learning Feature Extraction - æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´æŠ½å‡º
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Any, Optional, NamedTuple, List
import numpy as np
import pandas as pd
from numba import jit, njit, prange
import traceback
import math
import warnings
warnings.filterwarnings("ignore")

# Base classes
try:
    from .indicator import Indicator
    from .atr import ATR
    from .cycle.ehlers_unified_dc import EhlersUnifiedDC
except ImportError:
    print("Warning: Could not import from relative path. Assuming base classes are available.")
    class Indicator:
        def __init__(self, name): self.name = name; self.logger = self._get_logger()
        def reset(self): pass
        def _get_logger(self): import logging; return logging.getLogger(self.__class__.__name__)
    class ATR:
        def __init__(self, **kwargs): pass
        def calculate(self, data): return None
        def get_values(self): return np.array([])
        def reset(self): pass
    class EhlersUnifiedDC:
        def __init__(self, **kwargs): pass
        def calculate(self, data): return np.full(len(data), 10.0)
        def reset(self): pass


class UltimateTrendRangeResult(NamedTuple):
    """Ultimate Trend Range Filterçµæœ"""
    # ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™
    trend_strength: np.ndarray          # 0=ãƒ¬ãƒ³ã‚¸, 1=å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰
    trend_classification: np.ndarray    # 0=ãƒ¬ãƒ³ã‚¸, 1=ãƒˆãƒ¬ãƒ³ãƒ‰
    range_probability: np.ndarray       # ãƒ¬ãƒ³ã‚¸ç¢ºç‡ï¼ˆ0-1ï¼‰
    trend_probability: np.ndarray       # ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºç‡ï¼ˆ0-1ï¼‰
    
    # ä¿¡é ¼åº¦ãƒ»å“è³ªæŒ‡æ¨™
    confidence_score: np.ndarray        # åˆ¤å®šä¿¡é ¼åº¦ï¼ˆ0-1ï¼‰
    signal_quality: np.ndarray          # ã‚·ã‚°ãƒŠãƒ«å“è³ªï¼ˆ0-1ï¼‰
    noise_level: np.ndarray             # ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ï¼ˆ0-1ï¼‰
    
    # è©³ç´°æˆåˆ†
    directional_movement: np.ndarray    # æ–¹å‘æ€§ç§»å‹•é‡
    consolidation_index: np.ndarray     # æ¨ªã°ã„æŒ‡æ•°
    volatility_regime: np.ndarray       # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
    
    # é«˜åº¦è§£ææˆåˆ†
    harmonic_strength: np.ndarray       # ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯å¼·åº¦
    fractal_dimension: np.ndarray       # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
    persistence_factor: np.ndarray      # æŒç¶šæ€§è¦å› 
    market_microstructure: np.ndarray   # å¸‚å ´ãƒã‚¤ã‚¯ãƒ­æ§‹é€ 
    
    # äºˆæ¸¬æˆåˆ†
    trend_continuation: np.ndarray      # ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šäºˆæ¸¬
    regime_change_probability: np.ndarray # ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç¢ºç‡
    
    # ç¾åœ¨çŠ¶æ…‹
    current_state: str                  # "trend" or "range"
    current_strength: float             # ç¾åœ¨ã®å¼·åº¦
    current_confidence: float           # ç¾åœ¨ã®ä¿¡é ¼åº¦


@njit(fastmath=True, cache=True)
def advanced_trend_detection(
    prices: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    period: int = 20
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    é«˜åº¦ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (trend_strength, directional_movement, trend_consistency)
    """
    n = len(prices)
    if n < period * 2:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    trend_strength = np.full(n, np.nan)
    directional_movement = np.full(n, np.nan)
    trend_consistency = np.full(n, np.nan)
    
    for i in range(period, n):
        price_window = prices[i-period:i+1]
        high_window = high[i-period:i+1]
        low_window = low[i-period:i+1]
        
        # 1. ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
        x_vals = np.arange(len(price_window))
        n_points = len(price_window)
        sum_x = np.sum(x_vals)
        sum_y = np.sum(price_window)
        sum_xy = np.sum(x_vals * price_window)
        sum_x2 = np.sum(x_vals * x_vals)
        
        denom = n_points * sum_x2 - sum_x * sum_x
        if abs(denom) > 1e-15:
            slope = (n_points * sum_xy - sum_x * sum_y) / denom
            intercept = (sum_y - slope * sum_x) / n_points
            
            # å›å¸°ç›´ç·šã‹ã‚‰ã®å¹³å‡çµ¶å¯¾åå·®
            predicted = slope * x_vals + intercept
            mae = np.mean(np.abs(price_window - predicted))
            
            # ä¾¡æ ¼ãƒ¬ãƒ³ã‚¸ã«å¯¾ã™ã‚‹ç›¸å¯¾çš„ãªå‚¾ã
            price_range = np.max(price_window) - np.min(price_window)
            if price_range > 0:
                relative_slope = abs(slope * period) / price_range
                trend_strength[i] = min(relative_slope, 1.0)
            else:
                trend_strength[i] = 0.0
        else:
            trend_strength[i] = 0.0
            slope = 0.0
        
        # 2. æ–¹å‘æ€§ç§»å‹•é‡ï¼ˆADXãƒ©ã‚¤ã‚¯ãªè¨ˆç®—ï¼‰
        dm_plus = 0.0
        dm_minus = 0.0
        true_range_sum = 0.0
        
        for j in range(1, len(high_window)):
            high_diff = high_window[j] - high_window[j-1]
            low_diff = low_window[j-1] - low_window[j]
            
            if high_diff > low_diff and high_diff > 0:
                dm_plus += high_diff
            elif low_diff > high_diff and low_diff > 0:
                dm_minus += low_diff
                
            # True Range
            tr = max(
                high_window[j] - low_window[j],
                abs(high_window[j] - price_window[j-1]),
                abs(low_window[j] - price_window[j-1])
            )
            true_range_sum += tr
        
        if true_range_sum > 0:
            di_plus = dm_plus / true_range_sum
            di_minus = dm_minus / true_range_sum
            directional_movement[i] = abs(di_plus - di_minus)
        else:
            directional_movement[i] = 0.0
        
        # 3. ãƒˆãƒ¬ãƒ³ãƒ‰ä¸€è²«æ€§ï¼ˆä¾¡æ ¼ã®å˜èª¿æ€§ï¼‰
        price_changes = np.diff(price_window)
        if len(price_changes) > 0:
            positive_changes = np.sum(price_changes > 0)
            negative_changes = np.sum(price_changes < 0)
            total_changes = len(price_changes)
            
            if total_changes > 0:
                consistency = abs(positive_changes - negative_changes) / total_changes
                trend_consistency[i] = consistency
            else:
                trend_consistency[i] = 0.0
        else:
            trend_consistency[i] = 0.0
    
    return trend_strength, directional_movement, trend_consistency


@njit(fastmath=True, cache=True)
def range_consolidation_analysis(
    prices: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    period: int = 20
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ãƒ¬ãƒ³ã‚¸çµ±åˆè§£æã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (consolidation_index, range_tightness, sideways_strength)
    """
    n = len(prices)
    if n < period * 2:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    consolidation_index = np.full(n, np.nan)
    range_tightness = np.full(n, np.nan)
    sideways_strength = np.full(n, np.nan)
    
    for i in range(period, n):
        price_window = prices[i-period:i+1]
        high_window = high[i-period:i+1]
        low_window = low[i-period:i+1]
        
        # 1. çµ±åˆæŒ‡æ•°ï¼ˆä¾¡æ ¼ã®é›†ä¸­åº¦ï¼‰
        price_mean = np.mean(price_window)
        price_std = np.std(price_window)
        
        if price_std > 0:
            # æ­£è¦åŒ–ã•ã‚ŒãŸåˆ†æ•£ã®é€†æ•°
            consolidation_index[i] = 1.0 / (1.0 + price_std / price_mean)
        else:
            consolidation_index[i] = 1.0
        
        # 2. ãƒ¬ãƒ³ã‚¸ã®å¯†é›†åº¦
        price_range = np.max(price_window) - np.min(price_window)
        high_low_range = np.max(high_window) - np.min(low_window)
        
        if high_low_range > 0:
            # ãƒ¬ãƒ³ã‚¸å†…ã§ã®ä¾¡æ ¼åˆ†å¸ƒã®å‡ç­‰æ€§
            range_tightness[i] = 1.0 - (price_range / high_low_range)
        else:
            range_tightness[i] = 1.0
        
        # 3. æ¨ªã°ã„å¼·åº¦ï¼ˆä¾¡æ ¼ã®æˆ»ã‚Šå‚¾å‘ï¼‰
        # ä¸­å¤®å€¤ã‹ã‚‰ã®å¹³å‡è·é›¢
        price_median = np.median(price_window)
        avg_deviation = np.mean(np.abs(price_window - price_median))
        
        if price_range > 0:
            # æ­£è¦åŒ–ã•ã‚ŒãŸå¹³å‡åå·®
            sideways_strength[i] = 1.0 - (2.0 * avg_deviation / price_range)
            sideways_strength[i] = max(sideways_strength[i], 0.0)
        else:
            sideways_strength[i] = 1.0
    
    return consolidation_index, range_tightness, sideways_strength


@njit(fastmath=True, cache=True)
def multi_scale_trend_filtering(
    prices: np.ndarray,
    scales: np.ndarray = np.array([5, 10, 20, 40])
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    
    Returns:
        (multi_scale_trend, scale_consistency)
    """
    n = len(prices)
    if n < np.max(scales) * 2:
        return np.full(n, np.nan), np.full(n, np.nan)
    
    multi_scale_trend = np.full(n, np.nan)
    scale_consistency = np.full(n, np.nan)
    
    for i in range(np.max(scales), n):
        scale_trends = []
        
        for scale in scales:
            if i >= scale:
                # ã‚¹ã‚±ãƒ¼ãƒ«å›ºæœ‰ã®ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—
                price_segment = prices[i-scale:i+1]
                
                # ç·šå½¢å›å¸°ã®å‚¾ã
                x_vals = np.arange(len(price_segment))
                n_points = len(price_segment)
                sum_x = np.sum(x_vals)
                sum_y = np.sum(price_segment)
                sum_xy = np.sum(x_vals * price_segment)
                sum_x2 = np.sum(x_vals * x_vals)
                
                denom = n_points * sum_x2 - sum_x * sum_x
                if abs(denom) > 1e-15:
                    slope = (n_points * sum_xy - sum_x * sum_y) / denom
                    
                    # æ­£è¦åŒ–ã•ã‚ŒãŸå‚¾ã
                    price_range = np.max(price_segment) - np.min(price_segment)
                    if price_range > 0:
                        normalized_slope = (slope * scale) / price_range
                        scale_trends.append(normalized_slope)
                    else:
                        scale_trends.append(0.0)
                else:
                    scale_trends.append(0.0)
        
        if len(scale_trends) > 0:
            # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒˆãƒ¬ãƒ³ãƒ‰ã®çµ±åˆ
            trend_magnitudes = np.array([abs(t) for t in scale_trends])
            multi_scale_trend[i] = np.mean(trend_magnitudes)
            
            # ã‚¹ã‚±ãƒ¼ãƒ«é–“ã®ä¸€è²«æ€§
            if len(trend_magnitudes) > 1:
                trend_std = np.std(trend_magnitudes)
                trend_mean = np.mean(trend_magnitudes)
                if trend_mean > 0:
                    scale_consistency[i] = 1.0 - (trend_std / trend_mean)
                    scale_consistency[i] = max(scale_consistency[i], 0.0)
                else:
                    scale_consistency[i] = 1.0
            else:
                scale_consistency[i] = 1.0
        else:
            multi_scale_trend[i] = 0.0
            scale_consistency[i] = 0.0
    
    return multi_scale_trend, scale_consistency


@njit(fastmath=True, cache=True)
def adaptive_noise_suppression(
    signal: np.ndarray,
    volatility: np.ndarray,
    adaptation_factor: float = 0.1
) -> Tuple[np.ndarray, np.ndarray]:
    """
    é©å¿œãƒã‚¤ã‚ºæŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (denoised_signal, noise_level)
    """
    n = len(signal)
    if n < 10:
        return signal.copy(), np.zeros(n)
    
    denoised_signal = signal.copy()
    noise_level = np.zeros(n)
    
    # é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    for i in range(5, n):
        # å±€æ‰€çš„ãªä¿¡å·çµ±è¨ˆ
        window = signal[i-5:i+1]
        local_mean = np.mean(window)
        local_std = np.std(window)
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®é©å¿œ
        vol_factor = min(volatility[i] / np.mean(volatility[max(0, i-20):i+1]), 3.0)
        
        # é©å¿œã—ãã„å€¤
        threshold = local_std * vol_factor * adaptation_factor
        
        # ãƒã‚¤ã‚ºæŠ‘åˆ¶
        signal_deviation = abs(signal[i] - local_mean)
        if signal_deviation > threshold:
            # å¼·ã„ã‚·ã‚°ãƒŠãƒ«ã¯ä¿æŒ
            denoised_signal[i] = signal[i]
            noise_level[i] = 0.0
        else:
            # å¼±ã„ã‚·ã‚°ãƒŠãƒ«ã¯ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
            denoised_signal[i] = 0.7 * signal[i] + 0.3 * local_mean
            noise_level[i] = signal_deviation / threshold if threshold > 0 else 0.0
    
    return denoised_signal, noise_level


@njit(fastmath=True, cache=True)
def harmonic_pattern_recognition(
    prices: np.ndarray,
    period: int = 30
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (harmonic_strength, pattern_type)
    """
    n = len(prices)
    if n < period * 2:
        return np.full(n, np.nan), np.full(n, 0.0)
    
    harmonic_strength = np.full(n, np.nan)
    pattern_type = np.full(n, 0.0)  # 0=ãªã—, 1=æ”¯æŒæŠµæŠ—, 2=ä¸‰è§’å½¢, 3=ãƒ•ãƒ©ãƒƒã‚°
    
    for i in range(period, n):
        price_window = prices[i-period:i+1]
        
        # 1. æ”¯æŒãƒ»æŠµæŠ—ãƒ¬ãƒ™ãƒ«ã®æ¤œå‡º
        price_max = np.max(price_window)
        price_min = np.min(price_window)
        price_range = price_max - price_min
        
        if price_range > 0:
            # ä¾¡æ ¼ãƒ¬ãƒ™ãƒ«ã®åˆ†å¸ƒè§£æ
            resistance_touches = 0
            support_touches = 0
            
            for price in price_window:
                if abs(price - price_max) / price_range < 0.02:  # ä¸Šé™ã®2%ä»¥å†…
                    resistance_touches += 1
                elif abs(price - price_min) / price_range < 0.02:  # ä¸‹é™ã®2%ä»¥å†…
                    support_touches += 1
            
            # æ”¯æŒæŠµæŠ—ã®å¼·åº¦
            total_points = len(price_window)
            support_resistance_strength = (resistance_touches + support_touches) / total_points
            
            # 2. ä¸‰è§’å½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆåæŸï¼‰
            first_half = price_window[:period//2]
            second_half = price_window[period//2:]
            
            first_range = np.max(first_half) - np.min(first_half)
            second_range = np.max(second_half) - np.min(second_half)
            
            if first_range > 0:
                convergence_ratio = 1.0 - (second_range / first_range)
                convergence_ratio = max(convergence_ratio, 0.0)
            else:
                convergence_ratio = 0.0
            
            # 3. ãƒ•ãƒ©ãƒƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¹³è¡Œãƒãƒ£ãƒãƒ«ï¼‰
            # ä¸Šéƒ¨ã¨ä¸‹éƒ¨ã®ç·šå½¢å›å¸°
            x_vals = np.arange(len(price_window))
            
            # ä¸Šéƒ¨ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—
            upper_envelope = []
            lower_envelope = []
            
            for j in range(len(price_window)):
                local_window = price_window[max(0, j-5):j+6]
                upper_envelope.append(np.max(local_window))
                lower_envelope.append(np.min(local_window))
            
            upper_envelope = np.array(upper_envelope)
            lower_envelope = np.array(lower_envelope)
            
            # ä¸Šä¸‹ã‚¨ãƒ³ãƒ™ãƒ­ãƒ¼ãƒ—ã®å¹³è¡Œæ€§
            upper_slope = 0.0
            lower_slope = 0.0
            
            if len(x_vals) > 1:
                # ç°¡æ˜“å›å¸°
                n_points = len(x_vals)
                sum_x = np.sum(x_vals)
                
                # ä¸Šéƒ¨ã®å‚¾ã
                sum_y_upper = np.sum(upper_envelope)
                sum_xy_upper = np.sum(x_vals * upper_envelope)
                sum_x2 = np.sum(x_vals * x_vals)
                
                denom = n_points * sum_x2 - sum_x * sum_x
                if abs(denom) > 1e-15:
                    upper_slope = (n_points * sum_xy_upper - sum_x * sum_y_upper) / denom
                
                # ä¸‹éƒ¨ã®å‚¾ã
                sum_y_lower = np.sum(lower_envelope)
                sum_xy_lower = np.sum(x_vals * lower_envelope)
                
                if abs(denom) > 1e-15:
                    lower_slope = (n_points * sum_xy_lower - sum_x * sum_y_lower) / denom
            
            # å¹³è¡Œæ€§ã®æ¸¬å®š
            slope_difference = abs(upper_slope - lower_slope)
            if price_range > 0:
                slope_similarity = 1.0 / (1.0 + slope_difference * period / price_range)
            else:
                slope_similarity = 0.0
            
            # æœ€çµ‚çš„ãªãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯å¼·åº¦
            pattern_scores = [
                support_resistance_strength,
                convergence_ratio,
                slope_similarity
            ]
            
            max_score = max(pattern_scores)
            harmonic_strength[i] = max_score
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
            if max_score == support_resistance_strength and max_score > 0.3:
                pattern_type[i] = 1.0  # æ”¯æŒæŠµæŠ—
            elif max_score == convergence_ratio and max_score > 0.4:
                pattern_type[i] = 2.0  # ä¸‰è§’å½¢
            elif max_score == slope_similarity and max_score > 0.6:
                pattern_type[i] = 3.0  # ãƒ•ãƒ©ãƒƒã‚°
            else:
                pattern_type[i] = 0.0  # ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã—
        else:
            harmonic_strength[i] = 0.0
            pattern_type[i] = 0.0
    
    return harmonic_strength, pattern_type 


@njit(fastmath=True, cache=True)
def volatility_regime_classification(
    prices: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    period: int = 20
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (volatility_regime, regime_stability, regime_transition_prob)
    """
    n = len(prices)
    if n < period * 3:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    volatility_regime = np.full(n, np.nan)  # 0=ä½ãƒœãƒ©, 1=ä¸­ãƒœãƒ©, 2=é«˜ãƒœãƒ©
    regime_stability = np.full(n, np.nan)
    regime_transition_prob = np.full(n, np.nan)
    
    for i in range(period * 2, n):
        # çœŸã®å€¤å¹…ï¼ˆTrue Rangeï¼‰ã®è¨ˆç®—
        tr_values = []
        for j in range(i-period, i):
            if j > 0:
                tr = max(
                    high[j] - low[j],
                    abs(high[j] - prices[j-1]),
                    abs(low[j] - prices[j-1])
                )
                tr_values.append(tr)
        
        if len(tr_values) == 0:
            continue
            
        tr_array = np.array(tr_values)
        current_volatility = np.mean(tr_array)
        volatility_std = np.std(tr_array)
        
        # é•·æœŸãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨ã®æ¯”è¼ƒ
        long_term_period = min(period * 2, i)
        max_tr_count = max(1, long_term_period)
        long_term_tr = np.zeros(max_tr_count)
        tr_count = 0
        
        for j in range(i-long_term_period, i):
            if j > 0 and tr_count < max_tr_count:
                tr = max(
                    high[j] - low[j],
                    abs(high[j] - prices[j-1]),
                    abs(low[j] - prices[j-1])
                )
                long_term_tr[tr_count] = tr
                tr_count += 1
        
        if tr_count > 0:
            long_term_tr_valid = long_term_tr[:tr_count]
            long_term_vol = np.mean(long_term_tr_valid)
            long_term_std = np.std(long_term_tr_valid)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ã®åˆ†é¡
            if long_term_std > 0:
                vol_z_score = (current_volatility - long_term_vol) / long_term_std
                
                if vol_z_score < -0.5:
                    volatility_regime[i] = 0.0  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                elif vol_z_score > 0.5:
                    volatility_regime[i] = 2.0  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                else:
                    volatility_regime[i] = 1.0  # ä¸­ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                
                # ãƒ¬ã‚¸ãƒ¼ãƒ ã®å®‰å®šæ€§
                if volatility_std > 0:
                    stability = 1.0 / (1.0 + volatility_std / current_volatility)
                else:
                    stability = 1.0
                regime_stability[i] = stability
                
                # ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç¢ºç‡
                vol_change_rate = abs(vol_z_score)
                regime_transition_prob[i] = min(vol_change_rate / 2.0, 1.0)
            else:
                volatility_regime[i] = 1.0
                regime_stability[i] = 1.0
                regime_transition_prob[i] = 0.0
        else:
            volatility_regime[i] = 1.0
            regime_stability[i] = 0.5
            regime_transition_prob[i] = 0.5
    
    return volatility_regime, regime_stability, regime_transition_prob


@njit(fastmath=True, cache=True)
def ml_feature_extraction(
    prices: np.ndarray,
    volume: np.ndarray,
    period: int = 20
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (momentum_features, volatility_features, volume_features, composite_features)
    """
    n = len(prices)
    if n < period * 2:
        return (np.full(n, np.nan), np.full(n, np.nan), 
                np.full(n, np.nan), np.full(n, np.nan))
    
    momentum_features = np.full(n, np.nan)
    volatility_features = np.full(n, np.nan)
    volume_features = np.full(n, np.nan)
    composite_features = np.full(n, np.nan)
    
    for i in range(period, n):
        price_window = prices[i-period:i+1]
        
        # ãƒœãƒªãƒ¥ãƒ¼ãƒ å‡¦ç†ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if len(volume) > i:
            volume_window = volume[i-period:i+1]
        else:
            volume_window = np.ones(len(price_window))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # 1. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡
        returns = np.diff(price_window)
        if len(returns) > 0:
            # è¤‡æ•°æœŸé–“ã®ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
            momentum_1 = returns[-1] if len(returns) >= 1 else 0
            momentum_3 = np.mean(returns[-3:]) if len(returns) >= 3 else 0
            momentum_5 = np.mean(returns[-5:]) if len(returns) >= 5 else 0
            
            # æ­£è¦åŒ–
            price_std = np.std(price_window)
            if price_std > 0:
                momentum_features[i] = (momentum_1 + momentum_3 + momentum_5) / (3 * price_std)
            else:
                momentum_features[i] = 0.0
        else:
            momentum_features[i] = 0.0
        
        # 2. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡
        if len(returns) > 1:
            # å®Ÿç¾ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            realized_vol = np.std(returns)
            
            # GARCHæ§˜ã®ç‰¹å¾´é‡
            squared_returns = returns ** 2
            vol_persistence = np.corrcoef(squared_returns[:-1], squared_returns[1:])[0, 1] if len(squared_returns) > 1 else 0
            vol_persistence = vol_persistence if not np.isnan(vol_persistence) else 0
            
            # æ­£è¦åŒ–
            price_level = np.mean(price_window)
            if price_level > 0:
                volatility_features[i] = realized_vol / price_level + abs(vol_persistence) * 0.1
            else:
                volatility_features[i] = realized_vol
        else:
            volatility_features[i] = 0.0
        
        # 3. ãƒœãƒªãƒ¥ãƒ¼ãƒ ç‰¹å¾´é‡
        if len(volume_window) > 1:
            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ»ä¾¡æ ¼é–¢ä¿‚
            price_returns = returns
            volume_changes = np.diff(volume_window)
            
            if len(price_returns) == len(volume_changes) and len(price_returns) > 1:
                # ä¾¡æ ¼ãƒ»ãƒœãƒªãƒ¥ãƒ¼ãƒ ç›¸é–¢
                pv_correlation = np.corrcoef(price_returns, volume_changes)[0, 1]
                pv_correlation = pv_correlation if not np.isnan(pv_correlation) else 0
                
                # ãƒœãƒªãƒ¥ãƒ¼ãƒ ç§»å‹•å¹³å‡ã¨ã®ä¹–é›¢
                volume_ma = np.mean(volume_window)
                current_volume = volume_window[-1]
                if volume_ma > 0:
                    volume_deviation = (current_volume - volume_ma) / volume_ma
                else:
                    volume_deviation = 0
                
                volume_features[i] = abs(pv_correlation) * 0.5 + abs(volume_deviation) * 0.5
            else:
                volume_features[i] = 0.0
        else:
            volume_features[i] = 0.0
        
        # 4. è¤‡åˆç‰¹å¾´é‡
        # å„ç‰¹å¾´é‡ã®çµ±åˆ
        feat1 = momentum_features[i] if not np.isnan(momentum_features[i]) else 0
        feat2 = volatility_features[i] if not np.isnan(volatility_features[i]) else 0
        feat3 = volume_features[i] if not np.isnan(volume_features[i]) else 0
        
        # éç·šå½¢çµåˆ
        composite_features[i] = 0.0
        weight1 = 1.0
        weight2 = 0.5
        weight3 = 0.33333
        
        # ã‚¿ãƒ³ã‚¸ã‚§ãƒ³ãƒˆé–¢æ•°ã§éç·šå½¢å¤‰æ›
        transformed1 = math.tanh(feat1 * 2.0)
        transformed2 = math.tanh(feat2 * 2.0)
        transformed3 = math.tanh(feat3 * 2.0)
        
        composite_features[i] = weight1 * transformed1 + weight2 * transformed2 + weight3 * transformed3
        
        # æ­£è¦åŒ–
        total_weight = weight1 + weight2 + weight3
        composite_features[i] = composite_features[i] / total_weight
    
    return momentum_features, volatility_features, volume_features, composite_features


@njit(fastmath=True, cache=True)
def ultimate_trend_range_ensemble(
    trend_strength: np.ndarray,
    directional_movement: np.ndarray,
    consolidation_index: np.ndarray,
    multi_scale_trend: np.ndarray,
    harmonic_strength: np.ndarray,
    volatility_regime: np.ndarray,
    ml_features: np.ndarray,
    weights: np.ndarray = np.array([0.25, 0.20, 0.20, 0.15, 0.10, 0.05, 0.05])
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Ultimate Trend Range ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (final_trend_strength, trend_probability, confidence_score)
    """
    n = len(trend_strength)
    final_trend_strength = np.full(n, np.nan)
    trend_probability = np.full(n, np.nan)
    confidence_score = np.full(n, np.nan)
    
    for i in range(n):
        # å„æŒ‡æ¨™ã®å®‰å…¨ãªå–å¾—
        ind1 = trend_strength[i] if not np.isnan(trend_strength[i]) else 0.5
        ind2 = directional_movement[i] if not np.isnan(directional_movement[i]) else 0.5
        ind3 = 1.0 - (consolidation_index[i] if not np.isnan(consolidation_index[i]) else 0.5)  # ãƒ¬ãƒ³ã‚¸æŒ‡æ¨™ã®é€†è»¢
        ind4 = multi_scale_trend[i] if not np.isnan(multi_scale_trend[i]) else 0.5
        ind5 = harmonic_strength[i] if not np.isnan(harmonic_strength[i]) else 0.5
        ind6 = (volatility_regime[i] / 2.0 if not np.isnan(volatility_regime[i]) and volatility_regime[i] != 0 else 0.5)  # 0-1ã«æ­£è¦åŒ–
        ind7 = abs(ml_features[i]) if not np.isnan(ml_features[i]) else 0.5
        
        # é‡ã¿ä»˜ãå¹³å‡ï¼ˆå®‰å…¨ãªé…åˆ—ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
        if len(weights) >= 7:
            w1, w2, w3, w4, w5, w6, w7 = weights[0], weights[1], weights[2], weights[3], weights[4], weights[5], weights[6]
        else:
            w1 = w2 = w3 = w4 = w5 = w6 = w7 = 1.0 / 7.0
        
        # å€¤ã‚’0-1ç¯„å›²ã«åˆ¶é™
        norm_ind1 = min(max(ind1, 0.0), 1.0)
        norm_ind2 = min(max(ind2, 0.0), 1.0)
        norm_ind3 = min(max(ind3, 0.0), 1.0)
        norm_ind4 = min(max(ind4, 0.0), 1.0)
        norm_ind5 = min(max(ind5, 0.0), 1.0)
        norm_ind6 = min(max(ind6, 0.0), 1.0)
        norm_ind7 = min(max(ind7, 0.0), 1.0)
        
        weighted_sum = (norm_ind1 * w1 + norm_ind2 * w2 + norm_ind3 * w3 + 
                       norm_ind4 * w4 + norm_ind5 * w5 + norm_ind6 * w6 + norm_ind7 * w7)
        weight_sum = w1 + w2 + w3 + w4 + w5 + w6 + w7
        
        if weight_sum > 1e-15:  # ã‚ˆã‚Šå³å¯†ãªã‚¼ãƒ­é™¤ç®—å¯¾ç­–
            final_trend_strength[i] = weighted_sum / weight_sum
        else:
            final_trend_strength[i] = 0.5
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºç‡ï¼ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¤‰æ›ï¼‰
        # 0.5ã‚’ä¸­å¿ƒã¨ã—ãŸå¤‰æ›
        trend_score = (final_trend_strength[i] - 0.5) * 4  # -2ã‹ã‚‰2ã®ç¯„å›²ã«æ‹¡å¼µ
        trend_probability[i] = 1.0 / (1.0 + math.exp(-trend_score))
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆæŒ‡æ¨™é–“ã®ä¸€è‡´åº¦ï¼‰
        mean_indicator = (ind1 + ind2 + ind3 + ind4 + ind5 + ind6 + ind7) / 7.0
        var1 = (ind1 - mean_indicator) ** 2
        var2 = (ind2 - mean_indicator) ** 2
        var3 = (ind3 - mean_indicator) ** 2
        var4 = (ind4 - mean_indicator) ** 2
        var5 = (ind5 - mean_indicator) ** 2
        var6 = (ind6 - mean_indicator) ** 2
        var7 = (ind7 - mean_indicator) ** 2
        variance = (var1 + var2 + var3 + var4 + var5 + var6 + var7) / 7.0
        
        # ã‚¼ãƒ­é™¤ç®—å¯¾ç­–ã‚’å¼·åŒ–ã—ãŸä¿¡é ¼åº¦è¨ˆç®—
        variance_safe = max(variance, 1e-15)  # æœ€å°åˆ†æ•£ã‚’ä¿è¨¼
        confidence_score[i] = 1.0 / (1.0 + variance_safe * 4.0)  # åˆ†æ•£ã®é€†æ•°ãƒ™ãƒ¼ã‚¹
    
    return final_trend_strength, trend_probability, confidence_score


class UltimateTrendRangeFilter(Indicator):
    """
    Ultimate Trend Range Filter - å®‡å®™æœ€å¼·ã®ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤åˆ¥ã‚·ã‚¹ãƒ†ãƒ  ğŸš€
    
    Ultimate Chop Trendã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãƒ¬ãƒ³ã‚¸ã®2ã¤ã«ç‰¹åŒ–ã—ãŸç©¶æ¥µã®åˆ¤åˆ¥ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ï¼š
    
    ã€çµ±åˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‘
    ğŸ”¬ Advanced Trend Detection - å¤šæ¬¡å…ƒãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è§£æ
    ğŸ“Š Range Consolidation Analysis - é«˜åº¦ãƒ¬ãƒ³ã‚¸çµ±åˆè§£æ  
    ğŸ§  Multi-Scale Trend Filtering - ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    ğŸ¯ Adaptive Noise Suppression - é©å¿œçš„ãƒã‚¤ã‚ºæŠ‘åˆ¶ã‚·ã‚¹ãƒ†ãƒ 
    ğŸ“¡ Harmonic Pattern Recognition - ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•èªè­˜
    ğŸŒŠ Volatility Regime Classification - ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡
    ğŸš€ ML Feature Extraction - æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´æŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³
    ğŸ† Ultimate Ensemble System - æœ€å¼·ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
    
    ã€å‡ºåŠ›ã€‘
    - 0.0 = å®Œå…¨ãªãƒ¬ãƒ³ã‚¸ç›¸å ´
    - 1.0 = å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ç›¸å ´
    - æ–¹å‘æ€§ã¯å•ã‚ãšã€ãƒˆãƒ¬ãƒ³ãƒ‰ã®å­˜åœ¨ãã®ã‚‚ã®ã‚’æ¤œå‡º
    """
    
    def __init__(
        self,
        # ã‚³ã‚¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        analysis_period: int = 20,
        ensemble_window: int = 50,
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹åŒ–
        enable_advanced_trend: bool = True,
        enable_range_analysis: bool = True,
        enable_multi_scale: bool = True,
        enable_noise_suppression: bool = True,
        enable_harmonic_patterns: bool = True,
        enable_volatility_regime: bool = True,
        enable_ml_features: bool = True,
        
        # åˆ¤å®šã—ãã„å€¤ï¼ˆå®Ÿè·µçš„ãªå€¤ã«èª¿æ•´ï¼‰
        trend_threshold: float = 0.4,        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã—ãã„å€¤ï¼ˆ0.6â†’0.4ã«ä¸‹ã’ã¦å®Ÿç”¨çš„ã«ï¼‰
        strong_trend_threshold: float = 0.7, # å¼·ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã—ãã„å€¤ï¼ˆ0.8â†’0.7ã«ä¸‹ã’ã¦å®Ÿç”¨çš„ã«ï¼‰
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿
        component_weights: Optional[np.ndarray] = None,
        
        # é«˜åº¦è¨­å®š
        multi_scale_periods: Optional[np.ndarray] = None,
        noise_adaptation_factor: float = 0.1,
        harmonic_detection_period: int = 30
    ):
        """
        Ultimate Trend Range Filter - æœ€å¼·ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤åˆ¥ã‚·ã‚¹ãƒ†ãƒ 
        
        Args:
            analysis_period: åŸºæœ¬åˆ†ææœŸé–“
            ensemble_window: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            enable_*: å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°
            trend_threshold: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã—ãã„å€¤ï¼ˆ0-1ï¼‰
            strong_trend_threshold: å¼·ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã—ãã„å€¤ï¼ˆ0-1ï¼‰
            component_weights: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆåˆ†é‡ã¿
            multi_scale_periods: ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«è§£ææœŸé–“
            noise_adaptation_factor: ãƒã‚¤ã‚ºé©å¿œä¿‚æ•°
            harmonic_detection_period: ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯æ¤œå‡ºæœŸé–“
        """
        super().__init__(f"UltimateTrendRangeFilter(P={analysis_period},T={trend_threshold})")
        
        self.analysis_period = analysis_period
        self.ensemble_window = ensemble_window
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°
        self.enable_advanced_trend = enable_advanced_trend
        self.enable_range_analysis = enable_range_analysis
        self.enable_multi_scale = enable_multi_scale
        self.enable_noise_suppression = enable_noise_suppression
        self.enable_harmonic_patterns = enable_harmonic_patterns
        self.enable_volatility_regime = enable_volatility_regime
        self.enable_ml_features = enable_ml_features
        
        # ã—ãã„å€¤è¨­å®š
        self.trend_threshold = trend_threshold
        self.strong_trend_threshold = strong_trend_threshold
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºæ„Ÿåº¦ã‚’å‘ä¸Šï¼‰
        if component_weights is not None and np.sum(component_weights) > 1e-15:
            self.component_weights = component_weights
        else:
            # ã‚ˆã‚Šå®Ÿè·µçš„ãªé‡ã¿é…åˆ†ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºã¨ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«åˆ†æã‚’é‡è¦–ï¼‰
            self.component_weights = np.array([0.30, 0.25, 0.15, 0.20, 0.05, 0.03, 0.02])
        
        # é«˜åº¦è¨­å®š
        if multi_scale_periods is not None:
            self.multi_scale_periods = multi_scale_periods
        else:
            self.multi_scale_periods = np.array([5, 10, 20, 40])
        
        self.noise_adaptation_factor = noise_adaptation_factor
        self.harmonic_detection_period = harmonic_detection_period
        
        # çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._result: Optional[UltimateTrendRangeResult] = None
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateTrendRangeResult:
        """
        Ultimate Trend Range Filterã‚’è¨ˆç®—
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if len(data) == 0:
                return self._create_empty_result(0)
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            if isinstance(data, pd.DataFrame):
                prices = np.asarray(data['close'].values, dtype=np.float64)
                high = np.asarray(data['high'].values, dtype=np.float64)
                low = np.asarray(data['low'].values, dtype=np.float64)
                # ãƒœãƒªãƒ¥ãƒ¼ãƒ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if 'volume' in data.columns:
                    volume = np.asarray(data['volume'].values, dtype=np.float64)
                else:
                    volume = np.ones(len(prices))
            else:
                prices = np.asarray(data[:, 3], dtype=np.float64)  # close
                high = np.asarray(data[:, 1], dtype=np.float64)    # high
                low = np.asarray(data[:, 2], dtype=np.float64)     # low
                if data.shape[1] > 5:
                    volume = np.asarray(data[:, 5], dtype=np.float64)
                else:
                    volume = np.ones(len(prices))
            
            n = len(prices)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®è¨ˆç®—ï¼ˆATRè¿‘ä¼¼ï¼‰
            volatility = np.zeros(n)
            for i in range(1, n):
                tr = max(
                    high[i] - low[i],
                    abs(high[i] - prices[i-1]),
                    abs(low[i] - prices[i-1])
                )
                if i < 14:
                    volatility[i] = max(tr, 1e-10)
                else:
                    volatility[i] = max((volatility[i-1] * 13 + tr) / 14, 1e-10)
            volatility[0] = max(high[0] - low[0], 1e-10)
            
            # å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè¡Œ
            components = {}
            
            # 1. é«˜åº¦ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
            if self.enable_advanced_trend:
                trend_str, dir_mov, trend_cons = advanced_trend_detection(
                    prices, high, low, self.analysis_period
                )
                components['trend_strength'] = trend_str
                components['directional_movement'] = dir_mov
                components['trend_consistency'] = trend_cons
            else:
                components['trend_strength'] = np.full(n, 0.5)
                components['directional_movement'] = np.full(n, 0.5)
                components['trend_consistency'] = np.full(n, 0.5)
            
            # 2. ãƒ¬ãƒ³ã‚¸çµ±åˆè§£æ
            if self.enable_range_analysis:
                consol_idx, range_tight, sideways_str = range_consolidation_analysis(
                    prices, high, low, self.analysis_period
                )
                components['consolidation_index'] = consol_idx
                components['range_tightness'] = range_tight
                components['sideways_strength'] = sideways_str
            else:
                components['consolidation_index'] = np.full(n, 0.5)
                components['range_tightness'] = np.full(n, 0.5)
                components['sideways_strength'] = np.full(n, 0.5)
            
            # 3. ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if self.enable_multi_scale:
                multi_trend, scale_cons = multi_scale_trend_filtering(
                    prices, self.multi_scale_periods
                )
                components['multi_scale_trend'] = multi_trend
                components['scale_consistency'] = scale_cons
            else:
                components['multi_scale_trend'] = np.full(n, 0.5)
                components['scale_consistency'] = np.full(n, 0.5)
            
            # 4. é©å¿œãƒã‚¤ã‚ºæŠ‘åˆ¶
            if self.enable_noise_suppression:
                # åˆæœŸã‚·ã‚°ãƒŠãƒ«ã¨ã—ã¦ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’ä½¿ç”¨
                initial_signal = components['trend_strength']
                denoised_signal, noise_level = adaptive_noise_suppression(
                    initial_signal, volatility, self.noise_adaptation_factor
                )
                components['denoised_signal'] = denoised_signal
                components['noise_level'] = noise_level
            else:
                components['denoised_signal'] = components['trend_strength']
                components['noise_level'] = np.zeros(n)
            
            # 5. ãƒãƒ¼ãƒ¢ãƒ‹ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜
            if self.enable_harmonic_patterns:
                harmonic_str, pattern_type = harmonic_pattern_recognition(
                    prices, self.harmonic_detection_period
                )
                components['harmonic_strength'] = harmonic_str
                components['pattern_type'] = pattern_type
            else:
                components['harmonic_strength'] = np.full(n, 0.5)
                components['pattern_type'] = np.zeros(n)
            
            # 6. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡
            if self.enable_volatility_regime:
                vol_regime, regime_stab, regime_trans = volatility_regime_classification(
                    prices, high, low, self.analysis_period
                )
                components['volatility_regime'] = vol_regime
                components['regime_stability'] = regime_stab
                components['regime_transition'] = regime_trans
            else:
                components['volatility_regime'] = np.ones(n)
                components['regime_stability'] = np.ones(n)
                components['regime_transition'] = np.zeros(n)
            
            # 7. æ©Ÿæ¢°å­¦ç¿’ç‰¹å¾´æŠ½å‡º
            if self.enable_ml_features:
                momentum_feat, vol_feat, volume_feat, composite_feat = ml_feature_extraction(
                    prices, volume, self.analysis_period
                )
                components['momentum_features'] = momentum_feat
                components['volatility_features'] = vol_feat
                components['volume_features'] = volume_feat
                components['composite_features'] = composite_feat
            else:
                components['momentum_features'] = np.full(n, 0.5)
                components['volatility_features'] = np.full(n, 0.5)
                components['volume_features'] = np.full(n, 0.5)
                components['composite_features'] = np.full(n, 0.5)
            
            # 8. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµ±åˆï¼ˆå®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            # å„æˆåˆ†ã®NaNå€¤ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ç½®æ›
            safe_components = {}
            for key, values in components.items():
                safe_values = values.copy()
                # NaNå€¤ã‚’0.5ã§ç½®æ›
                nan_mask = np.isnan(safe_values)
                safe_values[nan_mask] = 0.5
                # ç„¡é™å¤§å€¤ã‚’åˆ¶é™
                inf_mask = np.isinf(safe_values)
                safe_values[inf_mask] = 0.5
                # ç¯„å›²åˆ¶é™
                safe_values = np.clip(safe_values, 0.0, 1.0)
                safe_components[key] = safe_values
            
            final_trend_strength, trend_prob, confidence = ultimate_trend_range_ensemble(
                safe_components['trend_strength'],
                safe_components['directional_movement'],
                safe_components['consolidation_index'],
                safe_components['multi_scale_trend'],
                safe_components['harmonic_strength'],
                safe_components['volatility_regime'],
                safe_components['composite_features'],
                self.component_weights
            )
            
            # æœ€çµ‚åˆ¤å®šï¼ˆåˆæœŸæœŸé–“ã‚‚å«ã‚ã¦å®Œå…¨ãªåˆ†é¡ã‚’ä¿è¨¼ï¼‰
            trend_classification = np.zeros(n)
            range_probability = np.zeros(n)
            
            for i in range(n):
                strength = final_trend_strength[i] if not np.isnan(final_trend_strength[i]) else 0.5
                
                # åˆæœŸæœŸé–“ï¼ˆanalysis_periodæœªæº€ï¼‰ã®ç‰¹åˆ¥å‡¦ç†
                if i < self.analysis_period:
                    # åˆæœŸæœŸé–“ã¯ä¾¡æ ¼å¤‰å‹•ã®ç°¡å˜ãªåˆ†æã§ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸ã‚’åˆ¤å®š
                    if i >= 5:  # æœ€ä½5æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                        recent_prices = prices[max(0, i-4):i+1]
                        price_range = np.max(recent_prices) - np.min(recent_prices)
                        price_change = abs(recent_prices[-1] - recent_prices[0])
                        
                        # ä¾¡æ ¼å¤‰å‹•ãŒç¯„å›²ã®50%ä»¥ä¸Šãªã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰ã€ãã†ã§ãªã‘ã‚Œã°ãƒ¬ãƒ³ã‚¸
                        if price_range > 0 and (price_change / price_range) > 0.5:
                            trend_classification[i] = 1.0  # ãƒˆãƒ¬ãƒ³ãƒ‰
                            strength = 0.6  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
                        else:
                            trend_classification[i] = 0.0  # ãƒ¬ãƒ³ã‚¸
                            strength = 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¬ãƒ³ã‚¸å¼·åº¦
                    else:
                        # ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ãªå ´åˆã¯ãƒ¬ãƒ³ã‚¸ã¨ã—ã¦æ‰±ã†
                        trend_classification[i] = 0.0  # ãƒ¬ãƒ³ã‚¸
                        strength = 0.3
                else:
                    # é€šå¸¸ã®åˆ¤å®š
                    if strength >= self.trend_threshold:
                        trend_classification[i] = 1.0  # ãƒˆãƒ¬ãƒ³ãƒ‰
                    else:
                        trend_classification[i] = 0.0  # ãƒ¬ãƒ³ã‚¸
                
                # ãƒ¬ãƒ³ã‚¸ç¢ºç‡
                range_probability[i] = 1.0 - strength
            
            # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆåˆæœŸæœŸé–“ã‚‚å«ã‚ã¦å‡¦ç†ï¼‰
            signal_quality = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
            persistence_factor = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
            market_microstructure = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
            trend_continuation = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
            regime_change_prob = np.full(n, 0.2)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
            
            # åˆæœŸæœŸé–“ã®å‡¦ç†
            for i in range(min(self.analysis_period, n)):
                if i >= 3:
                    # ç°¡å˜ãªå“è³ªæŒ‡æ¨™ã‚’è¨ˆç®—
                    recent_strengths = final_trend_strength[max(0, i-2):i+1]
                    recent_strengths_clean = recent_strengths[~np.isnan(recent_strengths)]
                    if len(recent_strengths_clean) > 1:
                        signal_quality[i] = max(0.3, 1.0 - np.std(recent_strengths_clean))
                    
                    # ç°¡å˜ãªæŒç¶šæ€§è¨ˆç®—
                    if len(recent_strengths_clean) > 1:
                        changes = np.abs(np.diff(recent_strengths_clean))
                        persistence_factor[i] = max(0.3, 1.0 - np.mean(changes))
            
            # é€šå¸¸æœŸé–“ã®å‡¦ç†
            for i in range(self.analysis_period, n):
                # ã‚·ã‚°ãƒŠãƒ«å“è³ªï¼ˆä¸€è²«æ€§ï¼‰
                recent_strengths = final_trend_strength[max(0, i-10):i+1]
                signal_quality[i] = 1.0 - np.std(recent_strengths) if len(recent_strengths) > 1 else 1.0
                
                # æŒç¶šæ€§è¦å› 
                strength_changes = np.diff(recent_strengths)
                if len(strength_changes) > 0:
                    persistence_factor[i] = 1.0 - np.mean(np.abs(strength_changes))
                    persistence_factor[i] = max(persistence_factor[i], 0.0)
                
                # å¸‚å ´ãƒã‚¤ã‚¯ãƒ­æ§‹é€ ï¼ˆä¾¡æ ¼åŠ¹ç‡æ€§ï¼‰
                price_changes = np.diff(prices[max(0, i-10):i+1])
                if len(price_changes) > 1:
                    autocorr = np.corrcoef(price_changes[:-1], price_changes[1:])[0, 1]
                    market_microstructure[i] = 1.0 - abs(autocorr) if not np.isnan(autocorr) else 1.0
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šäºˆæ¸¬
                if i >= 5:
                    trend_momentum = final_trend_strength[i] - final_trend_strength[i-5]
                    trend_continuation[i] = max(0, trend_momentum + 0.5)
                
                # ãƒ¬ã‚¸ãƒ¼ãƒ å¤‰åŒ–ç¢ºç‡
                if i >= 3:
                    recent_volatility = np.std(prices[i-3:i+1])
                    historical_volatility = np.std(prices[max(0, i-20):i-3])
                    if historical_volatility > 1e-15:  # ã‚ˆã‚Šå³å¯†ãªã‚¼ãƒ­é™¤ç®—å¯¾ç­–
                        vol_change = abs(recent_volatility - historical_volatility) / historical_volatility
                        regime_change_prob[i] = min(vol_change, 1.0)
                    else:
                        regime_change_prob[i] = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ç¾åœ¨çŠ¶æ…‹ã®åˆ¤å®š
            latest_strength = final_trend_strength[-1] if len(final_trend_strength) > 0 else 0.5
            latest_confidence = confidence[-1] if len(confidence) > 0 else 0.5
            
            if latest_strength >= self.strong_trend_threshold:
                current_state = "strong_trend"
            elif latest_strength >= self.trend_threshold:
                current_state = "trend"
            else:
                current_state = "range"
            
            # çµæœä½œæˆ
            result = UltimateTrendRangeResult(
                trend_strength=final_trend_strength,
                trend_classification=trend_classification,
                range_probability=range_probability,
                trend_probability=trend_prob,
                confidence_score=confidence,
                signal_quality=signal_quality,
                noise_level=components['noise_level'],
                directional_movement=components['directional_movement'],
                consolidation_index=components['consolidation_index'],
                volatility_regime=components['volatility_regime'],
                harmonic_strength=components['harmonic_strength'],
                fractal_dimension=components['scale_consistency'],  # ä»£ç”¨
                persistence_factor=persistence_factor,
                market_microstructure=market_microstructure,
                trend_continuation=trend_continuation,
                regime_change_probability=regime_change_prob,
                current_state=current_state,
                current_strength=latest_strength,
                current_confidence=latest_confidence
            )
            
            self._result = result
            self._values = final_trend_strength
            
            return result
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            self.logger.error(f"UltimateTrendRangeFilterè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\nè©³ç´°:\n{error_details}")
            return self._create_empty_result(len(data))
    
    def _create_empty_result(self, length: int) -> UltimateTrendRangeResult:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        return UltimateTrendRangeResult(
            trend_strength=np.full(length, np.nan),
            trend_classification=np.zeros(length),
            range_probability=np.full(length, 0.5),
            trend_probability=np.full(length, 0.5),
            confidence_score=np.zeros(length),
            signal_quality=np.zeros(length),
            noise_level=np.zeros(length),
            directional_movement=np.full(length, np.nan),
            consolidation_index=np.full(length, np.nan),
            volatility_regime=np.ones(length),
            harmonic_strength=np.full(length, np.nan),
            fractal_dimension=np.full(length, np.nan),
            persistence_factor=np.zeros(length),
            market_microstructure=np.zeros(length),
            trend_continuation=np.zeros(length),
            regime_change_probability=np.zeros(length),
            current_state="range",
            current_strength=0.0,
            current_confidence=0.0
        )
    
    def get_values(self) -> Optional[np.ndarray]:
        """ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™å€¤ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼‰ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.trend_strength.copy()
        return None
    
    def get_classification(self) -> Optional[np.ndarray]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†é¡ï¼ˆ0=ãƒ¬ãƒ³ã‚¸, 1=ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.trend_classification.copy()
        return None
    
    def get_result(self) -> Optional[UltimateTrendRangeResult]:
        """å®Œå…¨ãªçµæœã‚’å–å¾—"""
        return self._result
    
    def reset(self) -> None:
        """ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._result = None
    
    def is_trend(self, index: int = -1) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒˆãƒ¬ãƒ³ãƒ‰çŠ¶æ…‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        if self._result is None:
            return False
        
        classification = self._result.trend_classification
        if len(classification) == 0:
            return False
        
        return bool(classification[index] == 1.0)
    
    def is_range(self, index: int = -1) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒ¬ãƒ³ã‚¸çŠ¶æ…‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return not self.is_trend(index)
    
    def get_trend_strength(self, index: int = -1) -> float:
        """æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’å–å¾—"""
        if self._result is None:
            return 0.5
        
        strength = self._result.trend_strength
        if len(strength) == 0:
            return 0.5
        
        return float(strength[index]) if not np.isnan(strength[index]) else 0.5
    
    def get_confidence(self, index: int = -1) -> float:
        """æŒ‡å®šã•ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ã®ä¿¡é ¼åº¦ã‚’å–å¾—"""
        if self._result is None:
            return 0.0
        
        confidence = self._result.confidence_score
        if len(confidence) == 0:
            return 0.0
        
        return float(confidence[index]) if not np.isnan(confidence[index]) else 0.0 