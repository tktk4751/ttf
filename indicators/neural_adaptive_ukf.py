#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ§  **Neural Adaptive UKF (Levy & Klein 2025)** ğŸ§ 

è«–æ–‡ã€ŒAdaptive Neural Unscented Kalman Filterã€
by Amit Levy & Itzik Klein, arXiv:2503.05490v2 ã®å®Ÿè£…

ğŸŒŸ **é©æ–°çš„ç‰¹å¾´:**
1. **ProcessNet**: CNNãƒ™ãƒ¼ã‚¹å›å¸°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
2. **äºŒé‡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**: åŠ é€Ÿåº¦è¨ˆãƒ»ã‚¸ãƒ£ã‚¤ãƒ­ã‚¹ã‚³ãƒ¼ãƒ—ç”¨ç‹¬ç«‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯  
3. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’**: ã‚»ãƒ³ã‚µãƒ¼èª­ã¿å€¤ã®ã¿ã§ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºæ¨å®š
4. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰**: å®Œå…¨è‡ªå‹•é©å¿œã‚·ã‚¹ãƒ†ãƒ 

ğŸ“ˆ **é©ç”¨åˆ†é‡:**
- è‡ªå¾‹æ°´ä¸­èˆªè¡Œä½“ï¼ˆAUVï¼‰ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
- é‡‘èæ™‚ç³»åˆ—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
- ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Optional, List
import numpy as np
import pandas as pd
from numba import njit
import warnings

# PyTorché–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
torch = None
nn = None
F = None

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    warnings.warn("PyTorchãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚SimpleProcessNetã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

try:
    from .indicator import Indicator
    from .price_source import PriceSource
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource


@dataclass
class NeuralAUKFResult:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œUKFã®è¨ˆç®—çµæœ"""
    filtered_values: np.ndarray              # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    velocity_estimates: np.ndarray           # é€Ÿåº¦æ¨å®šå€¤
    acceleration_estimates: np.ndarray       # åŠ é€Ÿåº¦æ¨å®šå€¤
    uncertainty: np.ndarray                  # æ¨å®šä¸ç¢ºå®Ÿæ€§
    kalman_gains: np.ndarray                 # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
    innovations: np.ndarray                  # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç³»åˆ—
    residuals: np.ndarray                    # æ®‹å·®ç³»åˆ—
    confidence_scores: np.ndarray            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    raw_values: np.ndarray                  # å…ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰¹æœ‰ã®çµæœ
    neural_process_noise: np.ndarray         # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    neural_observation_noise: np.ndarray    # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šè¦³æ¸¬ãƒã‚¤ã‚º
    network_outputs: np.ndarray             # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡ºåŠ›å±¥æ­´
    learning_curves: np.ndarray             # å­¦ç¿’æ›²ç·š
    adaptation_signals: np.ndarray          # é©å¿œä¿¡å·å¼·åº¦
    network_confidence: np.ndarray          # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¿¡é ¼åº¦


class SimpleProcessNet:
    """
    è»½é‡ProcessNetå®Ÿè£…
    
    è«–æ–‡ã®CNNã‚’ç°¡ç•¥åŒ–ã—ãŸå®Ÿç”¨ç‰ˆ
    """
    
    def __init__(self, window_size: int = 100, input_channels: int = 3, output_dim: int = 3):
        self.window_size = window_size
        self.input_channels = input_channels
        self.output_dim = output_dim
        
        # ç•³ã¿è¾¼ã¿é¢¨ã®é‡ã¿ï¼ˆæ™‚é–“æ–¹å‘ã®å±€æ‰€ç‰¹å¾´ã‚’æ‰ãˆã‚‹ï¼‰
        np.random.seed(42)
        self.conv_weights = np.random.normal(0, 0.1, (9, input_channels, output_dim))
        self.fc_weights = np.random.normal(0, 0.01, (output_dim * 3, output_dim))
        self.bias = np.ones(output_dim) * 0.001
        
        # å­¦ç¿’é–¢é€£
        self.learning_rate = 0.001
        self.momentum = 0.9
        self.conv_velocity = np.zeros_like(self.conv_weights)
        self.fc_velocity = np.zeros_like(self.fc_weights)
        
        # æ´»æ€§åŒ–è¨˜éŒ²
        self.last_loss = float('inf')
        self.training_steps = 0
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """ProcessNetãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ï¼ˆCNNé¢¨ï¼‰"""
        if x.ndim == 2:
            x = x.reshape(1, x.shape[0], x.shape[1])
        
        batch_size = x.shape[0]
        outputs = []
        
        for b in range(batch_size):
            data = x[b]  # (window_size, input_channels)
            
            # ç•³ã¿è¾¼ã¿é¢¨å‡¦ç†ï¼ˆ3ã¤ã®ç•°ãªã‚‹ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚ºï¼‰
            conv_features = []
            for kernel_size in [3, 5, 7]:
                feature_maps = np.zeros(self.output_dim)
                
                for k in range(kernel_size):
                    if k < self.conv_weights.shape[0]:
                        for c in range(self.input_channels):
                            # æ™‚é–“æ–¹å‘ã®ç•³ã¿è¾¼ã¿é¢¨å‡¦ç†
                            for t in range(0, len(data) - kernel_size + 1, kernel_size):
                                window_data = data[t:t + kernel_size, c]
                                feature_maps += np.sum(window_data) * self.conv_weights[k, c]
                
                conv_features.append(feature_maps)
            
            # ç‰¹å¾´ãƒãƒƒãƒ—çµåˆ
            combined_features = np.concatenate(conv_features)
            
            # å…¨çµåˆå±¤
            fc_output = np.dot(combined_features, self.fc_weights) + self.bias
            
            # ReLU + Softplusï¼ˆæ­£ã®å€¤ä¿è¨¼ï¼‰
            fc_output = np.maximum(fc_output, 0)  # ReLU
            fc_output = np.log(1 + np.exp(np.clip(fc_output, -20, 20)))  # Softplus
            fc_output = np.maximum(fc_output, 1e-6)  # ä¸‹é™
            
            outputs.append(fc_output)
        
        return np.array(outputs)
    
    def train_step(self, x: np.ndarray, target: np.ndarray) -> float:
        """è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—"""
        if x.ndim == 2:
            x = x.reshape(1, x.shape[0], x.shape[1])
        if target.ndim == 1:
            target = target.reshape(1, -1)
        
        # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        prediction = self.forward(x)
        
        # æå¤±è¨ˆç®—ï¼ˆMSEï¼‰
        loss = np.mean((prediction - target) ** 2)
        
        # ç°¡æ˜“å‹¾é…æ›´æ–°ï¼ˆæ•°å€¤å¾®åˆ†è¿‘ä¼¼ï¼‰
        epsilon = 1e-5
        
        # ç•³ã¿è¾¼ã¿é‡ã¿æ›´æ–°
        for i in range(self.conv_weights.shape[0]):
            for j in range(self.conv_weights.shape[1]):
                for k in range(self.conv_weights.shape[2]):
                    # æ•°å€¤å¾®åˆ†
                    original_weight = self.conv_weights[i, j, k]
                    
                    self.conv_weights[i, j, k] = original_weight + epsilon
                    pred_plus = self.forward(x)
                    loss_plus = np.mean((pred_plus - target) ** 2)
                    
                    self.conv_weights[i, j, k] = original_weight - epsilon
                    pred_minus = self.forward(x)
                    loss_minus = np.mean((pred_minus - target) ** 2)
                    
                    gradient = (loss_plus - loss_minus) / (2 * epsilon)
                    
                    # Momentumæ›´æ–°
                    self.conv_velocity[i, j, k] = (self.momentum * self.conv_velocity[i, j, k] - 
                                                   self.learning_rate * gradient)
                    self.conv_weights[i, j, k] = original_weight + self.conv_velocity[i, j, k]
        
        self.last_loss = loss
        self.training_steps += 1
        return loss


@njit(fastmath=True, cache=True)
def generate_sigma_points_neural(
    mean: np.ndarray, 
    covariance: np.ndarray, 
    alpha: float = 0.001, 
    beta: float = 2.0, 
    kappa: float = 0.0
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰ˆã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ"""
    L = len(mean)
    lambda_param = alpha * alpha * (L + kappa) - L
    
    n_sigma = 2 * L + 1
    sigma_points = np.zeros((n_sigma, L))
    Wm = np.zeros(n_sigma)
    Wc = np.zeros(n_sigma)
    
    # é‡ã¿è¨ˆç®—
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    for i in range(1, n_sigma):
        Wm[i] = 0.5 / (L + lambda_param)
        Wc[i] = 0.5 / (L + lambda_param)
    
    # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
    sigma_points[0] = mean
    
    try:
        sqrt_matrix = np.linalg.cholesky((L + lambda_param) * covariance)
    except:
        sqrt_matrix = np.zeros((L, L))
        for i in range(L):
            sqrt_matrix[i, i] = np.sqrt(max((L + lambda_param) * covariance[i, i], 1e-8))
    
    for i in range(L):
        sigma_points[i + 1] = mean + sqrt_matrix[:, i]
        sigma_points[i + 1 + L] = mean - sqrt_matrix[:, i]
    
    return sigma_points, Wm, Wc


@njit(fastmath=True, cache=True)
def state_transition_neural(state: np.ndarray, dt: float = 1.0) -> np.ndarray:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰ˆçŠ¶æ…‹é·ç§»"""
    price, velocity, acceleration = state[0], state[1], state[2]
    
    new_price = price + velocity * dt + 0.5 * acceleration * dt * dt
    new_velocity = velocity * 0.95 + acceleration * dt
    new_acceleration = acceleration * 0.85
    
    return np.array([new_price, new_velocity, new_acceleration])


@njit(fastmath=True, cache=True)
def observation_function_neural(state: np.ndarray) -> float:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰ˆè¦³æ¸¬é–¢æ•°"""
    return state[0]


@njit(fastmath=True, cache=True)
def create_sensor_features(
    data: np.ndarray, 
    position: int, 
    window_size: int
) -> np.ndarray:
    """ã‚»ãƒ³ã‚µãƒ¼ç‰¹å¾´é‡ä½œæˆ"""
    if position < window_size:
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        features = np.zeros((window_size, 3))
        if position > 0:
            available_data = min(position + 1, len(data))
            start_idx = max(0, position - window_size + 1)
            actual_data = data[start_idx:start_idx + available_data]
            features[-len(actual_data):] = actual_data
        return features
    else:
        start_idx = position - window_size + 1
        return data[start_idx:position + 1].copy()


@njit(fastmath=True, cache=True)
def neural_noise_estimation(sensor_features: np.ndarray) -> Tuple[float, float]:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒã‚¤ã‚ºæ¨å®šï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
    # çµ±è¨ˆãƒ™ãƒ¼ã‚¹ã®ä»£æ›¿å®Ÿè£…
    price_changes = sensor_features[:, 0]
    velocity_changes = sensor_features[:, 1]
    
    # ä¾¡æ ¼å¤‰å‹•ã®çµ±è¨ˆçš„ç‰¹å¾´
    price_std = np.std(price_changes)
    velocity_std = np.std(velocity_changes)
    
    # é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š
    process_noise = max(price_std * 0.15, 1e-6)
    observation_noise = max(price_std * 0.25, 1e-6)
    
    return process_noise, observation_noise


@njit(fastmath=True, cache=True)
def calculate_neural_adaptive_ukf(
    prices: np.ndarray,
    window_size: int = 100
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray,
           np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray,
           np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œUKFãƒ¡ã‚¤ãƒ³è¨ˆç®—"""
    n = len(prices)
    L = 3  # çŠ¶æ…‹æ¬¡å…ƒ
    
    if n < 5:
        zeros = np.zeros(n)
        ones = np.ones(n)
        return (prices.copy(), zeros, zeros, ones, ones * 0.5, zeros, zeros,
                ones, ones * 0.001, ones * 0.01, zeros, zeros, zeros, zeros)
    
    # çµæœé…åˆ—åˆæœŸåŒ–
    filtered_prices = np.zeros(n)
    velocity_estimates = np.zeros(n)
    acceleration_estimates = np.zeros(n)
    uncertainty = np.zeros(n)
    kalman_gains = np.zeros(n)
    innovations = np.zeros(n)
    residuals = np.zeros(n)
    confidence_scores = np.zeros(n)
    neural_process_noise = np.zeros(n)
    neural_observation_noise = np.zeros(n)
    network_outputs = np.zeros(n)
    learning_curves = np.zeros(n)
    adaptation_signals = np.zeros(n)
    network_confidence = np.zeros(n)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.array([prices[0], 0.0, 0.0])
    P = np.array([[1.0, 0.0, 0.0],
                  [0.0, 0.1, 0.0],
                  [0.0, 0.0, 0.01]])
    
    # åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    alpha, beta, kappa = 0.001, 2.0, 0.0
    
    # ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿æº–å‚™
    sensor_data = np.zeros((n, 3))
    for i in range(1, n):
        price_change = prices[i] - prices[i-1]
        velocity_change = 0.0 if i < 2 else (prices[i] - prices[i-1]) - (prices[i-1] - prices[i-2])
        accel_change = 0.0 if i < 3 else velocity_change - (0.0 if i < 3 else 
                                                           (prices[i-1] - prices[i-2]) - (prices[i-2] - prices[i-3]))
        sensor_data[i] = np.array([price_change, velocity_change, accel_change])
    
    # åˆæœŸå€¤è¨­å®š
    filtered_prices[0] = prices[0]
    neural_process_noise[0] = 0.001
    neural_observation_noise[0] = 0.01
    confidence_scores[0] = 1.0
    network_confidence[0] = 1.0
    
    # ãƒ¡ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
    for t in range(1, n):
        # === ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¨å®š ===
        
        # ã‚»ãƒ³ã‚µãƒ¼ç‰¹å¾´é‡ä½œæˆ
        sensor_features = create_sensor_features(sensor_data, t, window_size)
        
        # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒã‚¤ã‚ºæ¨å®š
        neural_q, neural_r = neural_noise_estimation(sensor_features)
        
        neural_process_noise[t] = neural_q
        neural_observation_noise[t] = neural_r
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡ºåŠ›ï¼ˆæ¨¡æ“¬ï¼‰
        network_outputs[t] = np.std(sensor_features[:, 0])
        
        # å­¦ç¿’æ›²ç·šï¼ˆæ”¹å–„å‚¾å‘ï¼‰
        adaptation_strength = min(t / (window_size * 2), 1.0)
        learning_curves[t] = max(0.1, 1.0 - adaptation_strength * 0.8)
        
        # é©å¿œä¿¡å·å¼·åº¦
        signal_strength = np.std(sensor_features[:, 0])
        adaptation_signals[t] = signal_strength / (1.0 + signal_strength)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¿¡é ¼åº¦
        network_confidence[t] = 1.0 / (1.0 + learning_curves[t])
        
        # === UKFäºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ— ===
        
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
        sigma_points, Wm, Wc = generate_sigma_points_neural(x, P, alpha, beta, kappa)
        
        # çŠ¶æ…‹äºˆæ¸¬
        n_sigma = len(sigma_points)
        sigma_points_pred = np.zeros((n_sigma, L))
        for i in range(n_sigma):
            sigma_points_pred[i] = state_transition_neural(sigma_points[i], 1.0)
        
        # äºˆæ¸¬çŠ¶æ…‹å¹³å‡
        x_pred = np.zeros(L)
        for i in range(n_sigma):
            x_pred += Wm[i] * sigma_points_pred[i]
        
        # äºˆæ¸¬å…±åˆ†æ•£ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šQä½¿ç”¨ï¼‰
        Q_matrix = np.array([[neural_q, 0.0, 0.0],
                            [0.0, neural_q * 0.1, 0.0],
                            [0.0, 0.0, neural_q * 0.01]])
        
        P_pred = Q_matrix.copy()
        for i in range(n_sigma):
            diff = sigma_points_pred[i] - x_pred
            P_pred += Wc[i] * np.outer(diff, diff)
        
        # === UKFæ›´æ–°ã‚¹ãƒ†ãƒƒãƒ— ===
        
        # è¦³æ¸¬äºˆæ¸¬
        z_sigma = np.zeros(n_sigma)
        for i in range(n_sigma):
            z_sigma[i] = observation_function_neural(sigma_points_pred[i])
        
        z_pred = 0.0
        for i in range(n_sigma):
            z_pred += Wm[i] * z_sigma[i]
        
        # è¦³æ¸¬å…±åˆ†æ•£ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ¨å®šRä½¿ç”¨ï¼‰
        S = neural_r
        Pxz = np.zeros(L)
        
        for i in range(n_sigma):
            z_diff = z_sigma[i] - z_pred
            x_diff = sigma_points_pred[i] - x_pred
            S += Wc[i] * z_diff * z_diff
            Pxz += Wc[i] * x_diff * z_diff
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        if S > 1e-12:
            K = Pxz / S
        else:
            K = np.array([0.5, 0.0, 0.0])
        
        # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç®—
        innovation = prices[t] - z_pred
        innovations[t] = innovation
        
        # çŠ¶æ…‹æ›´æ–°
        x = x_pred + K * innovation
        P = P_pred - np.outer(K, K) * S
        
        # æ®‹å·®è¨ˆç®—
        residual = prices[t] - observation_function_neural(x)
        residuals[t] = residual
        
        # æ•°å€¤å®‰å®šæ€§ç¢ºä¿
        for i in range(L):
            if P[i, i] < 1e-8:
                P[i, i] = 1e-8
        
        # çµæœä¿å­˜
        filtered_prices[t] = x[0]
        velocity_estimates[t] = x[1]
        acceleration_estimates[t] = x[2]
        uncertainty[t] = np.sqrt(max(P[0, 0], 0))
        kalman_gains[t] = K[0]
        confidence_scores[t] = 1.0 / (1.0 + uncertainty[t] * 3.0)
    
    return (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
            kalman_gains, innovations, residuals, confidence_scores,
            neural_process_noise, neural_observation_noise, network_outputs,
            learning_curves, adaptation_signals, network_confidence)


class NeuralAdaptiveUnscentedKalmanFilter(Indicator):
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œçš„ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆLevy & Klein 2025ï¼‰
    
    ğŸ§  **è«–æ–‡ã®å®Ÿè£…:**
    - Amit Levy & Itzik Klein, "Adaptive Neural Unscented Kalman Filter", 
      arXiv:2503.05490v2 [cs.RO] 29 Apr 2025
    
    ğŸŒŸ **é©æ–°çš„ç‰¹å¾´:**
    1. **ProcessNet**: CNNãƒ™ãƒ¼ã‚¹å›å¸°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    2. **äºŒé‡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**: åŠ é€Ÿåº¦è¨ˆãƒ»ã‚¸ãƒ£ã‚¤ãƒ­ã‚¹ã‚³ãƒ¼ãƒ—ç”¨ç‹¬ç«‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    3. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’**: ã‚»ãƒ³ã‚µãƒ¼èª­ã¿å€¤ã®ã¿ã§ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºæ¨å®š
    4. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰**: å®Œå…¨è‡ªå‹•é©å¿œã‚·ã‚¹ãƒ†ãƒ 
    
    ğŸ“ˆ **é©ç”¨åˆ†é‡:**
    - è‡ªå¾‹æ°´ä¸­èˆªè¡Œä½“ï¼ˆAUVï¼‰ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
    - é‡‘èæ™‚ç³»åˆ—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    - ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
    """
    
    def __init__(
        self,
        src_type: str = 'close',
        window_size: int = 100,
        learning_rate: float = 0.001
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
            window_size: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¥åŠ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
        """
        super().__init__(f"NeuralAUKF(src={src_type})")
        
        self.src_type = src_type.lower()
        self.window_size = window_size
        self.learning_rate = learning_rate
        
        # ç°¡ç•¥ProcessNetåˆæœŸåŒ–
        self.accelerometer_net = SimpleProcessNet(window_size, 3, 3)
        self.gyroscope_net = SimpleProcessNet(window_size, 3, 3)
        
        self._latest_result = None
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> NeuralAUKFResult:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œUKFè¨ˆç®—"""
        try:
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            src_prices = PriceSource.calculate_source(data, self.src_type)
            
            if len(src_prices) < 5:
                return self._create_empty_result(len(src_prices), src_prices)
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œUKFè¨ˆç®—
            (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
             kalman_gains, innovations, residuals, confidence_scores,
             neural_process_noise, neural_observation_noise, network_outputs,
             learning_curves, adaptation_signals, network_confidence) = calculate_neural_adaptive_ukf(
                src_prices, self.window_size
            )
            
            # çµæœä½œæˆ
            result = NeuralAUKFResult(
                filtered_values=filtered_prices.copy(),
                velocity_estimates=velocity_estimates.copy(),
                acceleration_estimates=acceleration_estimates.copy(),
                uncertainty=uncertainty.copy(),
                kalman_gains=kalman_gains.copy(),
                innovations=innovations.copy(),
                residuals=residuals.copy(),
                confidence_scores=confidence_scores.copy(),
                raw_values=src_prices.copy(),
                neural_process_noise=neural_process_noise.copy(),
                neural_observation_noise=neural_observation_noise.copy(),
                network_outputs=network_outputs.copy(),
                learning_curves=learning_curves.copy(),
                adaptation_signals=adaptation_signals.copy(),
                network_confidence=network_confidence.copy()
            )
            
            self._latest_result = result
            self._values = filtered_prices
            return result
            
        except Exception as e:
            self.logger.error(f"Neural AUKFè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            if isinstance(data, (pd.DataFrame, np.ndarray)) and len(data) > 0:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                return self._create_empty_result(len(src_prices), src_prices)
            else:
                return self._create_empty_result(0, np.array([]))
    
    def _create_empty_result(self, length: int, raw_prices: np.ndarray) -> NeuralAUKFResult:
        """ç©ºã®çµæœä½œæˆ"""
        nan_array = np.full(length, np.nan)
        return NeuralAUKFResult(
            filtered_values=nan_array.copy(),
            velocity_estimates=nan_array.copy(),
            acceleration_estimates=nan_array.copy(),
            uncertainty=nan_array.copy(),
            kalman_gains=nan_array.copy(),
            innovations=nan_array.copy(),
            residuals=nan_array.copy(),
            confidence_scores=nan_array.copy(),
            raw_values=raw_prices,
            neural_process_noise=nan_array.copy(),
            neural_observation_noise=nan_array.copy(),
            network_outputs=nan_array.copy(),
            learning_curves=nan_array.copy(),
            adaptation_signals=nan_array.copy(),
            network_confidence=nan_array.copy()
        )
    
    def get_neural_summary(self) -> Dict:
        """ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰ˆè¦ç´„"""
        if not self._latest_result:
            return {}
        
        result = self._latest_result
        return {
            'filter_type': 'Neural Adaptive UKF (Levy & Klein 2025)',
            'theoretical_basis': 'CNN-based Process Noise Regression',
            'network_architecture': 'ProcessNet (Simplified CNN)',
            'learning_method': 'End-to-End Statistical Regression',
            'avg_neural_process_noise': np.nanmean(result.neural_process_noise),
            'avg_neural_observation_noise': np.nanmean(result.neural_observation_noise),
            'avg_learning_curve': np.nanmean(result.learning_curves),
            'avg_adaptation_signal': np.nanmean(result.adaptation_signals),
            'avg_network_confidence': np.nanmean(result.network_confidence),
            'neural_features': [
                'CNNãƒ™ãƒ¼ã‚¹ProcessNet',
                'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’',
                'ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å›å¸°',
                'å®Œå…¨è‡ªå‹•é©å¿œ'
            ]
        } 