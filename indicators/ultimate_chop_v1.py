#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultimate Chop Trend - å®‡å®™æœ€å¼·ã®ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼

æœ€æ–°ã®æ•°å­¦ãƒ»çµ±è¨ˆå­¦ãƒ»æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆã—ãŸè¶…ä½é…å»¶ãƒ»è¶…é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ ï¼š
- Hilbert Transform Instantaneous Analysisï¼ˆç¬æ™‚ä½ç›¸ãƒ»å‘¨æ³¢æ•°è§£æï¼‰
- Fractal Adaptive Filteringï¼ˆãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
- Wavelet Multi-Resolution Analysisï¼ˆã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æï¼‰
- Dynamic Kalman Filteringï¼ˆå‹•çš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
- Information Entropy Trend Detectionï¼ˆæƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºï¼‰
- Machine Learning Ensemble Votingï¼ˆæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ï¼‰
- Chaos Theory Indicatorsï¼ˆã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™ï¼‰
- Multi-Timeframe Regime Detectionï¼ˆå¤šé‡æ™‚é–“è»¸ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºï¼‰
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Any, Optional, NamedTuple, List
import numpy as np
import pandas as pd
from numba import jit, njit, prange
import traceback
import math
import warnings
warnings.filterwarnings("ignore")

# Assuming these base classes exist
try:
    from .indicator import Indicator
    from .atr import ATR
    from .cycle.ehlers_unified_dc import EhlersUnifiedDC
except ImportError:
    print("Warning: Could not import from relative path. Assuming base classes are available.")
    class Indicator:
        def __init__(self, name): self.name = name; self.logger = self._get_logger()
        def reset(self): pass
        def _get_logger(self): import logging; return logging.getLogger(self.__class__.__name__)
    class ATR:
        def __init__(self, **kwargs): pass
        def calculate(self, data): return None
        def get_values(self): return np.array([])
        def reset(self): pass
    class EhlersUnifiedDC:
        def __init__(self, **kwargs): pass
        def calculate(self, data): return np.full(len(data), 10.0)
        def reset(self): pass


class UltimateChopTrendResult(NamedTuple):
    """Ultimate ChopTrendè¨ˆç®—çµæœ - å®‡å®™æœ€å¼·ç‰ˆ"""
    # ã‚³ã‚¢æŒ‡æ¨™
    ultimate_trend_index: np.ndarray      # æœ€çµ‚çµ±åˆãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ•°ï¼ˆ0-1ï¼‰
    trend_signals: np.ndarray             # 1=å¼·ä¸Šæ˜‡, 0.5=å¼±ä¸Šæ˜‡, 0=ãƒ¬ãƒ³ã‚¸, -0.5=å¼±ä¸‹é™, -1=å¼·ä¸‹é™
    trend_strength: np.ndarray            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆ0-1ï¼‰
    regime_state: np.ndarray              # ãƒ¬ã‚¸ãƒ¼ãƒ çŠ¶æ…‹ï¼ˆ0=ãƒ¬ãƒ³ã‚¸ã€1=ãƒˆãƒ¬ãƒ³ãƒ‰ã€2=ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆï¼‰
    
    # æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ¤å®šï¼ˆãƒãƒ£ãƒ¼ãƒˆèƒŒæ™¯è‰²ç”¨ï¼‰
    trend_direction: np.ndarray           # 1=ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰, 0=ãƒ¬ãƒ³ã‚¸, -1=ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰
    direction_strength: np.ndarray        # æ–¹å‘æ€§ã®å¼·ã•ï¼ˆ0-1ï¼‰
    
    # ä¿¡é ¼åº¦ãƒ»ç¢ºç‡
    confidence_score: np.ndarray          # äºˆæ¸¬ä¿¡é ¼åº¦ï¼ˆ0-1ï¼‰
    trend_probability: np.ndarray         # ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºç‡ï¼ˆ0-1ï¼‰
    regime_probability: np.ndarray        # ãƒ¬ã‚¸ãƒ¼ãƒ ç¢ºç‡
    
    # å…ˆè¡ŒæŒ‡æ¨™
    predictive_signal: np.ndarray         # äºˆæ¸¬ã‚·ã‚°ãƒŠãƒ«
    momentum_forecast: np.ndarray         # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ äºˆæ¸¬
    volatility_forecast: np.ndarray       # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆåˆ†
    hilbert_component: np.ndarray         # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›æˆåˆ†
    fractal_component: np.ndarray         # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æˆåˆ†
    wavelet_component: np.ndarray         # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæˆåˆ†
    kalman_component: np.ndarray          # ã‚«ãƒ«ãƒãƒ³æˆåˆ†
    entropy_component: np.ndarray         # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æˆåˆ†
    chaos_component: np.ndarray           # ã‚«ã‚ªã‚¹æˆåˆ†
    
    # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£ææˆåˆ†
    unscented_kalman_component: np.ndarray  # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æˆåˆ†
    garch_volatility: np.ndarray            # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
    regime_switching_probs: np.ndarray      # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆç¢ºç‡ï¼ˆ3æ¬¡å…ƒ: n_samples x n_regimesï¼‰
    spectral_power: np.ndarray              # ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ‘ãƒ¯ãƒ¼
    dominant_frequency: np.ndarray          # æ”¯é…çš„å‘¨æ³¢æ•°
    multiscale_entropy: np.ndarray          # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    nonlinear_dynamics: np.ndarray          # éç·šå½¢åŠ›å­¦ç³»æŒ‡æ¨™
    predictability_score: np.ndarray       # äºˆæ¸¬å¯èƒ½æ€§ã‚¹ã‚³ã‚¢
    
    # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    market_efficiency: np.ndarray         # å¸‚å ´åŠ¹ç‡æ€§
    information_ratio: np.ndarray         # æƒ…å ±æ¯”ç‡
    adaptive_threshold: np.ndarray        # é©å¿œã—ãã„å€¤
    correlation_dimension: np.ndarray     # ç›¸é–¢æ¬¡å…ƒ
    lyapunov_exponent: np.ndarray         # ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°
    
    # ç¾åœ¨çŠ¶æ…‹
    current_trend: str
    current_strength: float
    current_confidence: float


@njit(fastmath=True, cache=True)
def hilbert_transform_analysis(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã«ã‚ˆã‚‹ç¬æ™‚è§£æ
    
    Returns:
        (instantaneous_phase, instantaneous_frequency, instantaneous_amplitude, trend_component)
    """
    n = len(prices)
    if n < 50:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã®è¿‘ä¼¼å®Ÿè£…
    phase = np.zeros(n)
    frequency = np.zeros(n)
    amplitude = np.zeros(n)
    trend_component = np.zeros(n)
    
    # ä½ç›¸å·®åˆ†ã‚’ä½¿ã£ãŸç¬æ™‚å‘¨æ³¢æ•°ã®è¿‘ä¼¼
    for i in range(7, n):
        # 4ã¤ã®ä½ç›¸æˆåˆ†ã‚’è¨ˆç®—
        real_part = (prices[i] + prices[i-2] + prices[i-4] + prices[i-6]) / 4.0
        
        # 90åº¦ä½ç›¸ã‚’ãšã‚‰ã—ãŸè™šæ•°éƒ¨ã®è¿‘ä¼¼
        imag_part = (prices[i-1] + prices[i-3] + prices[i-5] + prices[i-7]) / 4.0
        
        # ç¬æ™‚ä½ç›¸
        if real_part != 0:
            phase[i] = math.atan2(imag_part, real_part)
        
        # ç¬æ™‚æŒ¯å¹…
        amplitude[i] = math.sqrt(real_part * real_part + imag_part * imag_part)
        
        # ç¬æ™‚å‘¨æ³¢æ•°ï¼ˆä½ç›¸ã®å·®åˆ†ï¼‰
        if i > 7:
            freq_diff = phase[i] - phase[i-1]
            # ä½ç›¸ã®å·»ãæˆ»ã—ã‚’ä¿®æ­£
            if freq_diff > math.pi:
                freq_diff -= 2 * math.pi
            elif freq_diff < -math.pi:
                freq_diff += 2 * math.pi
            frequency[i] = abs(freq_diff)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†ï¼ˆä½ç›¸ã®æ–¹å‘æ€§ï¼‰
        if i > 14:
            phase_trend = 0.0
            for j in range(7):
                phase_trend += math.sin(phase[i-j])
            trend_component[i] = phase_trend / 7.0
    
    return phase, frequency, amplitude, trend_component


@njit(fastmath=True, cache=True)
def fractal_adaptive_moving_average(prices: np.ndarray, period: int = 21) -> np.ndarray:
    """
    ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡ï¼ˆFRAMAï¼‰
    
    Returns:
        FRAMAå€¤ã®é…åˆ—
    """
    n = len(prices)
    if n < period * 2:
        return np.full(n, np.nan)
    
    frama = np.full(n, np.nan)
    half_period = period // 2
    
    for i in range(period, n):
        # ä¸ŠåŠåˆ†ã¨ä¸‹åŠåˆ†ã®æœ€é«˜å€¤ãƒ»æœ€å®‰å€¤ã‚’å–å¾—
        h1 = np.max(prices[i-period:i-half_period])
        l1 = np.min(prices[i-period:i-half_period])
        h2 = np.max(prices[i-half_period:i])
        l2 = np.min(prices[i-half_period:i])
        
        # å…¨æœŸé–“ã®æœ€é«˜å€¤ãƒ»æœ€å®‰å€¤
        h_total = np.max(prices[i-period:i])
        l_total = np.min(prices[i-period:i])
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®è¨ˆç®—
        n1 = (h1 - l1) / half_period
        n2 = (h2 - l2) / half_period
        n3 = (h_total - l_total) / period
        
        if n1 > 0 and n2 > 0 and n3 > 0:
            # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
            fractal_dim = (math.log(n1 + n2) - math.log(n3)) / math.log(2.0)
            fractal_dim = min(max(fractal_dim, 1.0), 2.0)  # 1-2ã®ç¯„å›²ã«åˆ¶é™
            
            # é©å¿œä¿‚æ•°
            alpha = math.exp(-4.6 * (fractal_dim - 1.0))
            alpha = min(max(alpha, 0.01), 1.0)  # 0.01-1.0ã®ç¯„å›²ã«åˆ¶é™
        else:
            alpha = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # FRAMAã®è¨ˆç®—
        if i == period:
            frama[i] = prices[i]
        else:
            frama[i] = alpha * prices[i] + (1.0 - alpha) * frama[i-1]
    
    return frama


@njit(fastmath=True, cache=True)
def wavelet_denoising(prices: np.ndarray, levels: int = 3) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»ï¼ˆHaar wavelet approximationï¼‰
    
    Returns:
        (denoised_signal, detail_component)
    """
    n = len(prices)
    if n < 8:
        return prices.copy(), np.zeros(n)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªHaarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè¿‘ä¼¼
    signal = prices.copy()
    detail = np.zeros(n)
    
    for level in range(levels):
        step = 2 ** level
        if step >= n // 2:
            break
            
        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è¿‘ä¼¼
        for i in range(step, n - step, step * 2):
            # ãƒ­ãƒ¼ãƒ‘ã‚¹ï¼ˆå¹³å‡ï¼‰
            low = (signal[i-step] + signal[i]) * 0.5
            # ãƒã‚¤ãƒ‘ã‚¹ï¼ˆå·®åˆ†ï¼‰
            high = (signal[i-step] - signal[i]) * 0.5
            
            # ãƒã‚¤ã‚ºé™¤å»ã®ãŸã‚ã®ã—ãã„å€¤
            threshold = np.std(signal[max(0, i-step*4):i+step*4]) * 0.1
            if abs(high) < threshold:
                high = 0
            
            detail[i] += high
            signal[i-step] = low
            signal[i] = low
    
    denoised = signal - detail
    return denoised, detail


@njit(fastmath=True, cache=True)
def extended_kalman_filter(prices: np.ndarray, volatility: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®šç”¨ï¼‰
    
    Returns:
        (filtered_trend, prediction_confidence)
    """
    n = len(prices)
    if n < 10:
        return prices.copy(), np.ones(n)
    
    # çŠ¶æ…‹å¤‰æ•°ï¼š[ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦]
    state = np.array([prices[0], 0.0, 0.0])
    
    # å…±åˆ†æ•£è¡Œåˆ—
    P = np.eye(3) * 1.0
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    Q = np.array([[0.01, 0.0, 0.0],
                  [0.0, 0.01, 0.0],
                  [0.0, 0.0, 0.01]])
    
    # çŠ¶æ…‹é·ç§»è¡Œåˆ—
    F = np.array([[1.0, 1.0, 0.5],
                  [0.0, 1.0, 1.0],
                  [0.0, 0.0, 1.0]])
    
    filtered_trend = np.full(n, np.nan)
    confidence = np.full(n, np.nan)
    
    for i in range(n):
        if i == 0:
            filtered_trend[i] = prices[i]
            confidence[i] = 1.0
            continue
            
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        state_pred = np.dot(F, state)
        P_pred = np.dot(np.dot(F, P), F.T) + Q
        
        # è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ï¼‰
        R = max(volatility[i] ** 2, 0.001)
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        H = np.array([1.0, 0.0, 0.0])  # ä¾¡æ ¼ã®ã¿ã‚’è¦³æ¸¬
        K_denom = np.dot(np.dot(H, P_pred), H.T) + R
        if K_denom > 0:
            K = np.dot(P_pred, H.T) / K_denom
        else:
            K = np.zeros(3)
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - state_pred[0]
        state = state_pred + K * innovation
        P = P_pred - np.outer(K, np.dot(H, P_pred))
        
        filtered_trend[i] = state[0]
        confidence[i] = 1.0 / (1.0 + P[0, 0])  # ç¢ºä¿¡åº¦
    
    return filtered_trend, confidence


# ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½ç¾¤ï¼ˆultimate_advanced_analysis.pyã‹ã‚‰çµ±åˆï¼‰

@njit(fastmath=True, cache=True)
def unscented_kalman_filter(
    prices: np.ndarray, 
    volatility: np.ndarray,
    alpha: float = 0.001,
    beta: float = 2.0,
    kappa: float = 0.0
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Unscented Kalman Filterï¼ˆç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼‰
    éç·šå½¢ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ãŸé«˜åº¦ãªã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    
    Returns:
        (filtered_prices, trend_estimate, uncertainty)
    """
    n = len(prices)
    if n < 10:
        return prices.copy(), prices.copy(), np.ones(n)
    
    # çŠ¶æ…‹ã®æ¬¡å…ƒï¼ˆä¾¡æ ¼ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦ï¼‰
    L = 3
    
    # UKFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lambda_param = alpha * alpha * (L + kappa) - L
    
    # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®é‡ã¿
    Wm = np.zeros(2 * L + 1)  # å¹³å‡ç”¨ã®é‡ã¿
    Wc = np.zeros(2 * L + 1)  # å…±åˆ†æ•£ç”¨ã®é‡ã¿
    
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    for i in range(1, 2 * L + 1):
        Wm[i] = 0.5 / (L + lambda_param)
        Wc[i] = 0.5 / (L + lambda_param)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.array([prices[0], 0.0, 0.0])  # [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦]
    P = np.eye(L) * 1.0  # åˆæœŸå…±åˆ†æ•£
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    Q = np.array([[0.01, 0.0, 0.0],
                  [0.0, 0.01, 0.0],
                  [0.0, 0.0, 0.01]])
    
    filtered_prices = np.full(n, np.nan)
    trend_estimate = np.full(n, np.nan)
    uncertainty = np.full(n, np.nan)
    
    for t in range(n):
        if t == 0:
            filtered_prices[t] = prices[t]
            trend_estimate[t] = 0.0
            uncertainty[t] = 1.0
            continue
        
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®ç”Ÿæˆ
        sqrt_LP = np.sqrt(L + lambda_param)
        sigma_points = np.zeros((2 * L + 1, L))
        
        # å¹³æ–¹æ ¹è¡Œåˆ—ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        sqrt_P = np.zeros((L, L))
        for i in range(L):
            for j in range(L):
                if i == j:
                    sqrt_P[i, j] = math.sqrt(max(P[i, j], 0.001))
        
        sigma_points[0] = x
        for i in range(L):
            sigma_points[i + 1] = x + sqrt_LP * sqrt_P[i]
            sigma_points[i + 1 + L] = x - sqrt_LP * sqrt_P[i]
        
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        # çŠ¶æ…‹é·ç§»é–¢æ•° f(x) = [x[0] + x[1] + 0.5*x[2], x[1] + x[2], x[2]]
        predicted_sigma = np.zeros((2 * L + 1, L))
        for i in range(2 * L + 1):
            sp = sigma_points[i]
            predicted_sigma[i, 0] = sp[0] + sp[1] + 0.5 * sp[2]  # ä¾¡æ ¼
            predicted_sigma[i, 1] = sp[1] + sp[2]  # é€Ÿåº¦
            predicted_sigma[i, 2] = sp[2]  # åŠ é€Ÿåº¦
        
        # äºˆæ¸¬å¹³å‡
        x_pred = np.zeros(L)
        for i in range(2 * L + 1):
            x_pred += Wm[i] * predicted_sigma[i]
        
        # äºˆæ¸¬å…±åˆ†æ•£
        P_pred = np.zeros((L, L))
        for i in range(2 * L + 1):
            diff = predicted_sigma[i] - x_pred
            P_pred += Wc[i] * np.outer(diff, diff)
        P_pred += Q
        
        # è¦³æ¸¬æ›´æ–°
        # è¦³æ¸¬é–¢æ•° h(x) = x[0] (ä¾¡æ ¼ã®ã¿ã‚’è¦³æ¸¬)
        predicted_obs = np.zeros(2 * L + 1)
        for i in range(2 * L + 1):
            predicted_obs[i] = predicted_sigma[i, 0]
        
        # äºˆæ¸¬è¦³æ¸¬å€¤
        y_pred = np.sum(Wm * predicted_obs)
        
        # è¦³æ¸¬ãƒã‚¤ã‚º
        R = max(volatility[t] ** 2, 0.001)
        
        # è¦³æ¸¬å…±åˆ†æ•£
        Pyy = 0.0
        for i in range(2 * L + 1):
            diff_obs = predicted_obs[i] - y_pred
            Pyy += Wc[i] * diff_obs * diff_obs
        Pyy += R
        
        # äº¤å·®å…±åˆ†æ•£
        Pxy = np.zeros(L)
        for i in range(2 * L + 1):
            diff_state = predicted_sigma[i] - x_pred
            diff_obs = predicted_obs[i] - y_pred
            Pxy += Wc[i] * diff_state * diff_obs
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        if Pyy > 0:
            K = Pxy / Pyy
        else:
            K = np.zeros(L)
        
        # çŠ¶æ…‹æ›´æ–°
        innovation = prices[t] - y_pred
        x = x_pred + K * innovation
        P = P_pred - np.outer(K, K) * Pyy
        
        filtered_prices[t] = x[0]
        trend_estimate[t] = x[1]  # é€Ÿåº¦ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
        uncertainty[t] = math.sqrt(max(P[0, 0], 0))
    
    return filtered_prices, trend_estimate, uncertainty


@njit(fastmath=True, cache=True)
def garch_volatility_model(
    returns: np.ndarray,
    alpha: float = 0.1,
    beta: float = 0.85,
    omega: float = 0.01
) -> Tuple[np.ndarray, np.ndarray]:
    """
    GARCH(1,1)ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
    
    ÏƒÂ²(t) = Ï‰ + Î± * ÎµÂ²(t-1) + Î² * ÏƒÂ²(t-1)
    
    Returns:
        (conditional_variance, volatility)
    """
    n = len(returns)
    if n < 10:
        return np.full(n, 1.0), np.full(n, 1.0)
    
    # åˆæœŸå€¤
    unconditional_var = np.var(returns)
    
    conditional_variance = np.full(n, unconditional_var)
    volatility = np.full(n, math.sqrt(unconditional_var))
    
    for t in range(1, n):
        # GARCH(1,1)ã®æ›´æ–°å¼
        lagged_return_sq = returns[t-1] ** 2
        lagged_variance = conditional_variance[t-1]
        
        conditional_variance[t] = omega + alpha * lagged_return_sq + beta * lagged_variance
        conditional_variance[t] = max(conditional_variance[t], 1e-6)  # ä¸‹é™è¨­å®š
        
        volatility[t] = math.sqrt(conditional_variance[t])
    
    return conditional_variance, volatility


@njit(fastmath=True, cache=True)
def regime_switching_detection(
    prices: np.ndarray,
    window: int = 50,
    n_regimes: int = 3
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆæ¤œå‡ºï¼ˆãƒãƒ«ã‚³ãƒ•åˆ‡ã‚Šæ›¿ãˆãƒ¢ãƒ‡ãƒ«ã®ç°¡æ˜“ç‰ˆï¼‰
    
    Returns:
        (regime_probabilities, most_likely_regime)
    """
    n = len(prices)
    if n < window * 2:
        return np.zeros((n, n_regimes)), np.zeros(n)
    
    regime_probs = np.zeros((n, n_regimes))
    most_likely = np.zeros(n)
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ ã®ç‰¹å¾´ã‚’å®šç¾©
    # 0: ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆãƒ¬ãƒ³ã‚¸ï¼‰
    # 1: ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
    # 2: ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
    
    for i in range(window, n):
        price_window = prices[i-window:i]
        returns = np.diff(price_window)
        
        if len(returns) == 0:
            continue
        
        # çµ±è¨ˆé‡ã®è¨ˆç®—
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        # æ­£è¦åŒ–
        if std_return > 0:
            normalized_return = mean_return / std_return
        else:
            normalized_return = 0
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ ç¢ºç‡ã®è¨ˆç®—ï¼ˆå˜ç´”åŒ–ã—ãŸãƒ™ã‚¤ã‚ºæ¨å®šï¼‰
        # ãƒ¬ãƒ³ã‚¸ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã€ä½å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ï¼‰
        range_score = math.exp(-0.5 * (normalized_return ** 2 + (std_return - 0.01) ** 2))
        
        # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆæ­£ã®ãƒªã‚¿ãƒ¼ãƒ³ã€ä¸­ç¨‹åº¦ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
        uptrend_score = math.exp(-0.5 * ((normalized_return - 1.0) ** 2 + (std_return - 0.02) ** 2)) if normalized_return > 0 else 0
        
        # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆè² ã®ãƒªã‚¿ãƒ¼ãƒ³ã€ä¸­ç¨‹åº¦ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
        downtrend_score = math.exp(-0.5 * ((normalized_return + 1.0) ** 2 + (std_return - 0.02) ** 2)) if normalized_return < 0 else 0
        
        # æ­£è¦åŒ–
        total_score = range_score + uptrend_score + downtrend_score
        if total_score > 0:
            regime_probs[i, 0] = range_score / total_score
            regime_probs[i, 1] = uptrend_score / total_score
            regime_probs[i, 2] = downtrend_score / total_score
        else:
            regime_probs[i, :] = 1.0 / n_regimes
        
        # æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒ¬ã‚¸ãƒ¼ãƒ 
        max_prob = regime_probs[i, 0]
        max_idx = 0
        for j in range(1, n_regimes):
            if regime_probs[i, j] > max_prob:
                max_prob = regime_probs[i, j]
                max_idx = j
        most_likely[i] = max_idx
    
    return regime_probs, most_likely


@njit(fastmath=True, cache=True)
def spectral_analysis(
    prices: np.ndarray,
    window: int = 64
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æï¼ˆç°¡æ˜“ç‰ˆFFTï¼‰
    
    Returns:
        (dominant_frequency, spectral_power, trend_component)
    """
    n = len(prices)
    if n < window * 2:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    dominant_freq = np.full(n, np.nan)
    spectral_power = np.full(n, np.nan)
    trend_component = np.full(n, np.nan)
    
    for i in range(window, n):
        price_segment = prices[i-window:i]
        
        # ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ã®é™¤å»
        x_vals = np.arange(window)
        n_points = len(x_vals)
        sum_x = np.sum(x_vals)
        sum_y = np.sum(price_segment)
        sum_xy = np.sum(x_vals * price_segment)
        sum_x2 = np.sum(x_vals * x_vals)
        
        # ç·šå½¢å›å¸°ã®å‚¾ã
        denom = n_points * sum_x2 - sum_x * sum_x
        if denom != 0:
            slope = (n_points * sum_xy - sum_x * sum_y) / denom
            intercept = (sum_y - slope * sum_x) / n_points
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰é™¤å»
            detrended = price_segment - (slope * x_vals + intercept)
            trend_component[i] = slope
        else:
            detrended = price_segment - np.mean(price_segment)
            trend_component[i] = 0
        
        # ç°¡æ˜“DFTï¼ˆé›¢æ•£ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ï¼‰
        # ä¸»è¦å‘¨æ³¢æ•°æˆåˆ†ã®ã¿ã‚’è¨ˆç®—
        max_power = 0.0
        dominant_f = 0.0
        
        for k in range(1, min(window // 2, 16)):  # ä½å‘¨æ³¢æ•°æˆåˆ†ã®ã¿
            # DFTã®è¨ˆç®—
            real_part = 0.0
            imag_part = 0.0
            
            for j in range(window):
                angle = -2.0 * math.pi * k * j / window
                real_part += detrended[j] * math.cos(angle)
                imag_part += detrended[j] * math.sin(angle)
            
            # ãƒ‘ãƒ¯ãƒ¼ã®è¨ˆç®—
            power = real_part * real_part + imag_part * imag_part
            
            if power > max_power:
                max_power = power
                dominant_f = k / window  # æ­£è¦åŒ–å‘¨æ³¢æ•°
        
        dominant_freq[i] = dominant_f
        spectral_power[i] = max_power / (window * window)  # æ­£è¦åŒ–
    
    return dominant_freq, spectral_power, trend_component


@njit(fastmath=True, cache=True)
def multiscale_entropy(
    prices: np.ndarray,
    max_scale: int = 10,
    pattern_length: int = 2,
    tolerance_ratio: float = 0.15
) -> np.ndarray:
    """
    ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    è¤‡æ•°ã®æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®è¤‡é›‘æ€§ã‚’æ¸¬å®š
    
    Returns:
        ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å€¤ã®é…åˆ—
    """
    n = len(prices)
    if n < 50:
        return np.full(n, np.nan)
    
    mse_values = np.full(n, np.nan)
    
    for i in range(50, n):
        price_segment = prices[max(0, i-50):i]
        
        # å„ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—
        entropy_sum = 0.0
        valid_scales = 0
        
        for scale in range(1, min(max_scale + 1, len(price_segment) // 4)):
            # ã‚³ãƒ¼ã‚¹ãƒ»ã‚°ãƒ¬ã‚¤ãƒ‹ãƒ³ã‚°
            if scale == 1:
                coarse_grained = price_segment
            else:
                coarse_length = len(price_segment) // scale
                coarse_grained = np.zeros(coarse_length)
                for j in range(coarse_length):
                    start_idx = j * scale
                    end_idx = min(start_idx + scale, len(price_segment))
                    coarse_grained[j] = np.mean(price_segment[start_idx:end_idx])
            
            # ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
            if len(coarse_grained) > pattern_length + 1:
                tolerance = tolerance_ratio * np.std(coarse_grained)
                sample_entropy = calculate_sample_entropy(coarse_grained, pattern_length, tolerance)
                
                if not np.isnan(sample_entropy):
                    entropy_sum += sample_entropy
                    valid_scales += 1
        
        if valid_scales > 0:
            mse_values[i] = entropy_sum / valid_scales
        else:
            mse_values[i] = 0.5
    
    return mse_values


@njit(fastmath=True, cache=True)
def calculate_sample_entropy(
    data: np.ndarray,
    pattern_length: int,
    tolerance: float
) -> float:
    """
    ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
    """
    n = len(data)
    if n <= pattern_length:
        return np.nan
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    matches_m = 0
    matches_m_plus_1 = 0
    
    for i in range(n - pattern_length):
        # é•·ã•mã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern_m = data[i:i + pattern_length]
        
        # é•·ã•m+1ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        if i + pattern_length < n:
            pattern_m_plus_1 = data[i:i + pattern_length + 1]
        else:
            continue
        
        for j in range(i + 1, n - pattern_length):
            # é•·ã•mã®æ¯”è¼ƒ
            candidate_m = data[j:j + pattern_length]
            
            # æœ€å¤§å·®ã‚’è¨ˆç®—
            max_diff_m = 0.0
            for k in range(pattern_length):
                diff = abs(pattern_m[k] - candidate_m[k])
                if diff > max_diff_m:
                    max_diff_m = diff
            
            if max_diff_m <= tolerance:
                matches_m += 1
                
                # é•·ã•m+1ã®æ¯”è¼ƒ
                if j + pattern_length < n:
                    candidate_m_plus_1 = data[j:j + pattern_length + 1]
                    
                    max_diff_m_plus_1 = 0.0
                    for k in range(pattern_length + 1):
                        diff = abs(pattern_m_plus_1[k] - candidate_m_plus_1[k])
                        if diff > max_diff_m_plus_1:
                            max_diff_m_plus_1 = diff
                    
                    if max_diff_m_plus_1 <= tolerance:
                        matches_m_plus_1 += 1
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
    if matches_m > 0 and matches_m_plus_1 > 0:
        relative_prevalence = matches_m_plus_1 / matches_m
        if relative_prevalence > 0:
            return -math.log(relative_prevalence)
        else:
            return float('inf')
    else:
        return np.nan


@njit(fastmath=True, cache=True)
def nonlinear_dynamics_analysis(
    prices: np.ndarray,
    embedding_dim: int = 3,
    delay: int = 1,
    window: int = 50
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    éç·šå½¢åŠ›å­¦ç³»è§£æ
    
    Returns:
        (correlation_dimension, largest_lyapunov, predictability)
    """
    n = len(prices)
    if n < window * 2:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    correlation_dim = np.full(n, np.nan)
    lyapunov_exp = np.full(n, np.nan)
    predictability = np.full(n, np.nan)
    
    for i in range(window, n):
        price_segment = prices[i-window:i]
        
        # æ™‚é–“é…ã‚ŒåŸ‹ã‚è¾¼ã¿
        max_idx = window - (embedding_dim - 1) * delay
        if max_idx <= 0:
            continue
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ§‹ç¯‰
        embedded_points = np.zeros((max_idx, embedding_dim))
        for j in range(max_idx):
            for k in range(embedding_dim):
                embedded_points[j, k] = price_segment[j + k * delay]
        
        # ç›¸é–¢æ¬¡å…ƒã®æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if max_idx > 5:
            # è¿‘å‚ç‚¹ã®ã‚«ã‚¦ãƒ³ãƒˆ
            radius = np.std(price_segment) * 0.1
            correlation_sum = 0.0
            total_pairs = 0
            
            for j in range(max_idx):
                for k in range(j + 1, max_idx):
                    # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
                    dist = 0.0
                    for dim in range(embedding_dim):
                        diff = embedded_points[j, dim] - embedded_points[k, dim]
                        dist += diff * diff
                    dist = math.sqrt(dist)
                    
                    if dist < radius:
                        correlation_sum += 1.0
                    total_pairs += 1
            
            if total_pairs > 0:
                correlation_integral = correlation_sum / total_pairs
                if correlation_integral > 0:
                    correlation_dim[i] = math.log(correlation_integral) / math.log(radius)
                    correlation_dim[i] = min(max(correlation_dim[i], 0), embedding_dim)
        
        # æœ€å¤§ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ã®æ¨å®š
        if max_idx > 10:
            divergence_sum = 0.0
            divergence_count = 0
            
            for j in range(5, max_idx - 5):
                # æœ€è¿‘å‚ç‚¹ã‚’æ¢ã™
                min_dist = float('inf')
                nearest_idx = -1
                
                for k in range(max_idx):
                    if abs(k - j) < 3:  # æ™‚é–“çš„ã«è¿‘ã„ç‚¹ã¯é™¤å¤–
                        continue
                    
                    dist = 0.0
                    for dim in range(embedding_dim):
                        diff = embedded_points[j, dim] - embedded_points[k, dim]
                        dist += diff * diff
                    dist = math.sqrt(dist)
                    
                    if dist < min_dist:
                        min_dist = dist
                        nearest_idx = k
                
                # ç™ºæ•£ã®è¨ˆç®—
                if nearest_idx >= 0 and j + 3 < max_idx and nearest_idx + 3 < max_idx:
                    future_dist = 0.0
                    for dim in range(embedding_dim):
                        diff = embedded_points[j + 3, dim] - embedded_points[nearest_idx + 3, dim]
                        future_dist += diff * diff
                    future_dist = math.sqrt(future_dist)
                    
                    if min_dist > 0 and future_dist > 0:
                        divergence = math.log(future_dist / min_dist) / 3.0
                        divergence_sum += divergence
                        divergence_count += 1
            
            if divergence_count > 0:
                lyapunov_exp[i] = divergence_sum / divergence_count
        
        # äºˆæ¸¬å¯èƒ½æ€§ï¼ˆã‚«ã‚ªã‚¹æ€§ã®é€†ï¼‰
        if not np.isnan(correlation_dim[i]) and not np.isnan(lyapunov_exp[i]):
            # ä½æ¬¡å…ƒãƒ»è² ã®ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•° â†’ é«˜ã„äºˆæ¸¬å¯èƒ½æ€§
            dim_factor = max(0, 1 - correlation_dim[i] / embedding_dim)
            lyap_factor = max(0, 1 - max(lyapunov_exp[i], 0))
            predictability[i] = (dim_factor + lyap_factor) / 2
        else:
            predictability[i] = 0.5
    
    return correlation_dim, lyapunov_exp, predictability


@njit(fastmath=True, cache=True)
def information_entropy_trend(prices: np.ndarray, window: int = 20) -> np.ndarray:
    """
    æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
    
    Returns:
        ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™
    """
    n = len(prices)
    if n < window * 2:
        return np.full(n, np.nan)
    
    entropy_trend = np.full(n, np.nan)
    
    for i in range(window, n):
        # ä¾¡æ ¼å¤‰åŒ–ã®è¨ˆç®—
        price_changes = np.diff(prices[i-window:i+1])
        
        if len(price_changes) == 0:
            continue
            
        # å¤‰åŒ–ã‚’é›¢æ•£åŒ–ï¼ˆãƒ“ãƒ³åˆ†å‰²ï¼‰
        bins = 10
        hist_range = np.max(price_changes) - np.min(price_changes)
        if hist_range <= 0:
            entropy_trend[i] = 0.5
            continue
            
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®è¨ˆç®—
        bin_width = hist_range / bins
        histogram = np.zeros(bins)
        
        for change in price_changes:
            bin_idx = int((change - np.min(price_changes)) / bin_width)
            bin_idx = min(max(bin_idx, 0), bins - 1)
            histogram[bin_idx] += 1
        
        # ç¢ºç‡åˆ†å¸ƒã¸ã®å¤‰æ›
        total = np.sum(histogram)
        if total > 0:
            probabilities = histogram / total
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
            entropy = 0.0
            for p in probabilities:
                if p > 0:
                    entropy -= p * math.log2(p)
            
            # æ­£è¦åŒ–ï¼ˆæœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§å‰²ã‚‹ï¼‰
            max_entropy = math.log2(bins)
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ï¼ˆä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
            entropy_trend[i] = 1.0 - normalized_entropy
        else:
            entropy_trend[i] = 0.5
    
    return entropy_trend


@njit(fastmath=True, cache=True)
def chaos_theory_indicators(prices: np.ndarray, window: int = 20) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™ã®è¨ˆç®—
    
    Returns:
        (hurst_exponent, largest_lyapunov_exponent)
    """
    n = len(prices)
    if n < window * 3:
        return np.full(n, np.nan), np.full(n, np.nan)
    
    hurst = np.full(n, np.nan)
    lyapunov = np.full(n, np.nan)
    
    for i in range(window * 2, n):
        price_series = prices[i-window*2:i]
        
        # HurstæŒ‡æ•°ã®è¨ˆç®—ï¼ˆR/Sçµ±è¨ˆï¼‰
        returns = np.diff(price_series)
        if len(returns) < window:
            continue
            
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        if std_return > 0:
            # ç´¯ç©åå·®
            cumdev = np.cumsum(returns - mean_return)
            R = np.max(cumdev) - np.min(cumdev)  # ãƒ¬ãƒ³ã‚¸
            S = std_return  # æ¨™æº–åå·®
            
            if S > 0 and R > 0:
                rs_ratio = R / S
                hurst[i] = math.log(rs_ratio) / math.log(len(returns))
                hurst[i] = min(max(hurst[i], 0.0), 1.0)
            else:
                hurst[i] = 0.5
        else:
            hurst[i] = 0.5
        
        # ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ã®è¿‘ä¼¼è¨ˆç®—
        if len(returns) >= 10:
            # è¿‘å‚ç‚¹ã®ç™ºæ•£ç‡ã‚’è¨ˆç®—
            divergence_sum = 0.0
            count = 0
            
            for j in range(5, len(returns) - 5):
                # è¿‘å‚ç‚¹ã‚’æ¢ã™
                base_point = returns[j]
                
                for k in range(max(0, j-5), min(len(returns), j+5)):
                    if k != j and abs(returns[k] - base_point) < std_return * 0.1:
                        # ç™ºæ•£ã®è¨ˆç®—
                        future_steps = 3
                        if j + future_steps < len(returns) and k + future_steps < len(returns):
                            initial_dist = abs(returns[k] - returns[j])
                            final_dist = abs(returns[k + future_steps] - returns[j + future_steps])
                            
                            if initial_dist > 0 and final_dist > 0:
                                divergence = math.log(final_dist / initial_dist)
                                divergence_sum += divergence
                                count += 1
            
            if count > 0:
                lyapunov[i] = divergence_sum / count
            else:
                lyapunov[i] = 0.0
        else:
            lyapunov[i] = 0.0
    
    return hurst, lyapunov


@njit(fastmath=True, cache=True)
def adaptive_threshold_calculation(
    signals: np.ndarray, 
    volatility: np.ndarray, 
    window: int = 50
) -> np.ndarray:
    """
    é©å¿œçš„ã—ãã„å€¤ã®è¨ˆç®—
    
    Returns:
        é©å¿œã—ãã„å€¤ã®é…åˆ—
    """
    n = len(signals)
    if n < window:
        return np.full(n, 0.5)
    
    adaptive_threshold = np.full(n, 0.5)
    
    for i in range(window, n):
        # éå»ã®ã‚·ã‚°ãƒŠãƒ«ã®çµ±è¨ˆ
        signal_window = signals[i-window:i]
        vol_window = volatility[i-window:i]
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´æ¸ˆã¿æ¨™æº–åå·®
        signal_std = np.std(signal_window)
        avg_vol = np.mean(vol_window)
        
        # é©å¿œä¿‚æ•°
        vol_factor = min(max(avg_vol, 0.1), 2.0)
        
        # ã—ãã„å€¤ã®è¨ˆç®—
        base_threshold = 0.5
        volatility_adjustment = signal_std * vol_factor * 0.5
        
        adaptive_threshold[i] = base_threshold + volatility_adjustment
        adaptive_threshold[i] = min(max(adaptive_threshold[i], 0.2), 0.8)
    
    return adaptive_threshold


@njit(fastmath=True, cache=True)
def ensemble_voting_system(
    hilbert_sig: np.ndarray,
    fractal_sig: np.ndarray,
    wavelet_sig: np.ndarray,
    kalman_sig: np.ndarray,
    entropy_sig: np.ndarray,
    chaos_sig: np.ndarray,
    confidence_weights: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨ãªã‚¼ãƒ­é™¤ç®—å¯¾ç­–ç‰ˆï¼‰
    
    Returns:
        (ensemble_signal, ensemble_confidence)
    """
    n = len(hilbert_sig)
    ensemble_signal = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
    ensemble_confidence = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
    
    # å„ã‚·ã‚°ãƒŠãƒ«ã®é‡ã¿ï¼ˆåˆè¨ˆ1.0ï¼‰
    base_weights = np.array([0.25, 0.20, 0.15, 0.15, 0.15, 0.10])
    
    for i in range(n):
        # å®‰å…¨ãªã‚·ã‚°ãƒŠãƒ«å€¤ã®å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5ã€ç¯„å›²0-1ã«åˆ¶é™ï¼‰
        sig_values = np.array([
            min(max(hilbert_sig[i] if not np.isnan(hilbert_sig[i]) and np.isfinite(hilbert_sig[i]) else 0.5, 0.0), 1.0),
            min(max(fractal_sig[i] if not np.isnan(fractal_sig[i]) and np.isfinite(fractal_sig[i]) else 0.5, 0.0), 1.0),
            min(max(wavelet_sig[i] if not np.isnan(wavelet_sig[i]) and np.isfinite(wavelet_sig[i]) else 0.5, 0.0), 1.0),
            min(max(kalman_sig[i] if not np.isnan(kalman_sig[i]) and np.isfinite(kalman_sig[i]) else 0.5, 0.0), 1.0),
            min(max(entropy_sig[i] if not np.isnan(entropy_sig[i]) and np.isfinite(entropy_sig[i]) else 0.5, 0.0), 1.0),
            min(max(chaos_sig[i] if not np.isnan(chaos_sig[i]) and np.isfinite(chaos_sig[i]) else 0.5, 0.0), 1.0)
        ])
        
        # ä¿¡é ¼åº¦é‡ã¿ã®å®‰å…¨ãªå–å¾—
        if i < len(confidence_weights):
            conf_weight = confidence_weights[i] if (not np.isnan(confidence_weights[i]) and 
                                                   np.isfinite(confidence_weights[i]) and 
                                                   confidence_weights[i] > 0) else 1.0
        else:
            conf_weight = 1.0
        
        # ä¿¡é ¼åº¦é‡ã¿ã‚’å®‰å…¨ãªç¯„å›²ã«åˆ¶é™
        conf_weight = min(max(conf_weight, 0.01), 10.0)
        
        # åŠ¹æœçš„ãªé‡ã¿ã®è¨ˆç®—
        effective_weights = base_weights * conf_weight
        
        # é‡ã¿ä»˜ãå¹³å‡ã®è¨ˆç®—ï¼ˆå®Œå…¨ã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
        weighted_sum = 0.0
        weight_sum = 0.0
        
        for j in range(len(sig_values)):
            if j < len(effective_weights) and effective_weights[j] > 0:
                weighted_sum += sig_values[j] * effective_weights[j]
                weight_sum += effective_weights[j]
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚°ãƒŠãƒ«ã®è¨ˆç®—
        if weight_sum > 1e-15:  # æ¥µå°å€¤ãƒã‚§ãƒƒã‚¯
            ensemble_signal[i] = weighted_sum / weight_sum
        else:
            ensemble_signal[i] = 0.5  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
        
        # æœ€çµ‚ç¯„å›²åˆ¶é™
        ensemble_signal[i] = min(max(ensemble_signal[i], 0.0), 1.0)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆå®‰å…¨ç‰ˆï¼‰
        try:
            # æ‰‹å‹•ã§åˆ†æ•£ã‚’è¨ˆç®—ã—ã¦ã‚¼ãƒ­é™¤ç®—ã‚’å®Œå…¨å›é¿
            mean_val = np.mean(sig_values)
            variance_sum = 0.0
            
            for val in sig_values:
                diff = val - mean_val
                variance_sum += diff * diff
            
            signal_variance = variance_sum / len(sig_values) if len(sig_values) > 0 else 0.0
            
            # åˆ†æ•£ãŒæ¥µå°ã®å ´åˆã®å‡¦ç†
            if signal_variance < 1e-15:
                ensemble_confidence[i] = 0.95  # é«˜ã„ä¿¡é ¼åº¦ï¼ˆå…¨ã‚·ã‚°ãƒŠãƒ«ãŒä¸€è‡´ï¼‰
            else:
                confidence_val = 1.0 / (1.0 + signal_variance)
                ensemble_confidence[i] = min(max(confidence_val, 0.1), 1.0)
        except:
            ensemble_confidence[i] = 0.5  # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    return ensemble_signal, ensemble_confidence


@njit(fastmath=True, cache=True)
def predictive_analysis(
    prices: np.ndarray,
    trend_signals: np.ndarray,
    volatility: np.ndarray,
    lookforward: int = 3
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    äºˆæ¸¬åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (predictive_signal, momentum_forecast, volatility_forecast)
    """
    n = len(prices)
    if n < lookforward * 3:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    predictive_signal = np.full(n, np.nan)
    momentum_forecast = np.full(n, np.nan)
    volatility_forecast = np.full(n, np.nan)
    
    for i in range(lookforward * 2, n - lookforward):
        # éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        past_prices = prices[i-lookforward*2:i]
        past_trends = trend_signals[i-lookforward*2:i]
        past_vols = volatility[i-lookforward*2:i]
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ äºˆæ¸¬ï¼ˆç·šå½¢å›å¸°ã®å‚¾ãï¼‰
        x_vals = np.arange(len(past_prices))
        if len(past_prices) > 2:
            # å˜ç´”ãªç·šå½¢å›å¸°
            n_points = len(past_prices)
            sum_x = np.sum(x_vals)
            sum_y = np.sum(past_prices)
            sum_xy = np.sum(x_vals * past_prices)
            sum_x2 = np.sum(x_vals * x_vals)
            
            denom = n_points * sum_x2 - sum_x * sum_x
            if abs(denom) > 1e-15:  # ã‚¼ãƒ­é™¤ç®—ã®å®Œå…¨å›é¿
                slope = (n_points * sum_xy - sum_x * sum_y) / denom
                momentum_forecast[i] = min(max(slope, -1.0), 1.0)  # ç¯„å›²åˆ¶é™
            else:
                momentum_forecast[i] = 0.0
        else:
            momentum_forecast[i] = 0
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ï¼ˆEWMAï¼‰
        if len(past_vols) > 0:
            # æŒ‡æ•°é‡ã¿ä»˜ãç§»å‹•å¹³å‡
            alpha = 0.2
            vol_forecast = past_vols[-1]
            for j in range(len(past_vols)):
                weight = alpha * (1 - alpha) ** j
                vol_forecast += weight * past_vols[-(j+1)]
            volatility_forecast[i] = vol_forecast
        else:
            volatility_forecast[i] = 0
        
        # äºˆæ¸¬ã‚·ã‚°ãƒŠãƒ«ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®çµ„ã¿åˆã‚ã›ï¼‰
        current_trend = trend_signals[i] if not np.isnan(trend_signals[i]) else 0.5
        momentum_component = momentum_forecast[i] if not np.isnan(momentum_forecast[i]) else 0
        
        # ã‚·ã‚°ãƒŠãƒ«ã®çµ±åˆï¼ˆå®‰å…¨ãªè¨ˆç®—ï¼‰
        if np.isfinite(momentum_component):
            momentum_bounded = min(max(momentum_component * 10, -50), 50)  # tanhå…¥åŠ›å€¤åˆ¶é™
            momentum_normalized = math.tanh(momentum_bounded) * 0.5 + 0.5
        else:
            momentum_normalized = 0.5
        
        predictive_signal[i] = 0.7 * current_trend + 0.3 * momentum_normalized
        predictive_signal[i] = min(max(predictive_signal[i], 0.0), 1.0)
    
    return predictive_signal, momentum_forecast, volatility_forecast


@njit(fastmath=True, cache=True)
def calculate_trend_direction_classification(
    trend_index: np.ndarray,
    trend_signals: np.ndarray,
    prices: np.ndarray,
    lookback_period: int = 5
) -> Tuple[np.ndarray, np.ndarray]:
    """
    æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ†é¡ã‚’è¨ˆç®—ã™ã‚‹
    
    Args:
        trend_index: Ultimate ChopTrendã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-1ï¼‰
        trend_signals: ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«ï¼ˆ-1ï½1ï¼‰
        prices: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        lookback_period: ä¾¡æ ¼ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šç”¨ã®ãƒ«ãƒƒã‚¯ãƒãƒƒã‚¯æœŸé–“
    
    Returns:
        Tuple[np.ndarray, np.ndarray]:
            - trend_direction: 1=ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰, 0=ãƒ¬ãƒ³ã‚¸, -1=ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰
            - trend_strength: ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¼·ã•ï¼ˆ0-1ï¼‰
    """
    n = len(trend_index)
    trend_direction = np.zeros(n, dtype=np.int8)
    trend_strength = np.zeros(n)
    
    # åˆæœŸæœŸé–“ã‚‚å«ã‚ã¦å…¨æœŸé–“ã‚’å‡¦ç†
    for i in range(n):
        # Ultimate ChopTrendã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚‹åŸºæœ¬åˆ¤å®š
        trend_val = trend_index[i] if not np.isnan(trend_index[i]) else 0.5
        signal_val = trend_signals[i] if not np.isnan(trend_signals[i]) else 0.0
        
        # ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºèªï¼ˆå‹•çš„æœŸé–“ï¼‰
        price_trend = 0.0
        effective_lookback = min(lookback_period, i + 1)  # åˆ©ç”¨å¯èƒ½ãªæœŸé–“ã‚’ä½¿ç”¨
        
        if effective_lookback > 1:
            start_idx = max(0, i - effective_lookback + 1)
            price_change = prices[i] - prices[start_idx]
            price_slice = prices[start_idx:i + 1]
            price_volatility = np.std(price_slice) if len(price_slice) > 1 else 1e-10
            
            if price_volatility > 1e-10:
                price_trend = price_change / (price_volatility * effective_lookback)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã®è¨ˆç®—
        trend_strength[i] = abs(trend_val - 0.5) * 2.0  # 0.5ã‹ã‚‰ã®è·é›¢ã‚’2å€ã—ã¦0-1ã«æ­£è¦åŒ–
        
        # çµ±åˆåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´ç‰ˆï¼šãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚’å¢—ã‚„ã™ï¼‰
        uptrend_score = 0.0
        downtrend_score = 0.0
        
        # 1. ã‚·ã‚°ãƒŠãƒ«ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢ï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰
        if signal_val > 0.18:  # 0.2ã‹ã‚‰0.18ã«èª¿æ•´ï¼ˆã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã‚’å°‘ã—ç·©å’Œï¼‰
            uptrend_score += 0.3
        elif signal_val < -0.2:  # ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å³ã—ãç¶­æŒ
            downtrend_score += 0.3
        
        # 2. ä¾¡æ ¼ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚¹ã‚³ã‚¢ï¼ˆå°‘ã—å³ã—ãï¼‰
        if price_trend > 0.07:  # 0.05ã‹ã‚‰0.07ã«å¤‰æ›´ï¼ˆå³ã—ãï¼‰
            uptrend_score += 0.4
        elif price_trend < -0.07:  # -0.05ã‹ã‚‰-0.07ã«å¤‰æ›´ï¼ˆå³ã—ãï¼‰
            downtrend_score += 0.4
        
        # 3. ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚‹è¿½åŠ ã‚¹ã‚³ã‚¢ï¼ˆå³ã—ãï¼‰
        if trend_val > 0.6:  # 0.55ã‹ã‚‰0.6ã«æˆ»ã™
            if signal_val > 0.1:  # 0.05ã‹ã‚‰0.1ã«å¤‰æ›´
                uptrend_score += 0.2
            elif signal_val < -0.1:  # -0.05ã‹ã‚‰-0.1ã«å¤‰æ›´
                downtrend_score += 0.2
        
        # 4. å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ã®å ´åˆã®è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
        if trend_val > 0.75:  # 0.7ã‹ã‚‰0.75ã«å¤‰æ›´ï¼ˆã‚ˆã‚Šå³ã—ãï¼‰
            if signal_val > 0.15:  # 0.1ã‹ã‚‰0.15ã«å¤‰æ›´
                uptrend_score += 0.3
            elif signal_val < -0.15:  # -0.1ã‹ã‚‰-0.15ã«å¤‰æ›´
                downtrend_score += 0.3
        
        # 5. ä¾¡æ ¼å¤‰å‹•ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå³ã—ãï¼‰
        if abs(price_trend) > 0.05:  # 0.03ã‹ã‚‰0.05ã«å¤‰æ›´ï¼ˆæ˜ç¢ºãªä¾¡æ ¼å¤‰å‹•ãŒå¿…è¦ï¼‰
            if price_trend > 0 and signal_val > 0.1:  # 0ã‹ã‚‰0.1ã«å¤‰æ›´
                uptrend_score += 0.2  # ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
            elif price_trend < 0 and signal_val < -0.1:  # 0ã‹ã‚‰-0.1ã«å¤‰æ›´
                downtrend_score += 0.2  # ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
        
        # 6. ãƒ¬ãƒ³ã‚¸åˆ¤å®šã®å¼·åŒ–ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        # å¼±ã„ã‚·ã‚°ãƒŠãƒ«ã‹ã¤ä½ã„ä¾¡æ ¼å¤‰å‹•ã®å ´åˆã¯ãƒ¬ãƒ³ã‚¸å‚¾å‘
        range_score = 0.0
        if abs(signal_val) < 0.15 and abs(price_trend) < 0.05:
            range_score += 0.3
        if trend_val > 0.4 and trend_val < 0.7:  # ä¸­ç¨‹åº¦ã®ãƒˆãƒ¬ãƒ³ãƒ‰å€¤
            range_score += 0.2
        
        # æœ€çµ‚åˆ¤å®šï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿ï¼šå…¨æ–¹å‘ã®åˆ¤å®šã‚’é©åˆ‡ã«ï¼‰
        if uptrend_score > downtrend_score and uptrend_score > max(downtrend_score + 0.05, 0.35):
            trend_direction[i] = 1  # ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰
        elif downtrend_score > uptrend_score and downtrend_score > max(uptrend_score + 0.05, 0.35):
            trend_direction[i] = -1  # ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆæ¡ä»¶ã‚’ç·©å’Œï¼‰
        else:
            trend_direction[i] = 0  # ãƒ¬ãƒ³ã‚¸
    
    return trend_direction, trend_strength


class UltimateChopTrend(Indicator):
    """
    Ultimate Chop Trend - å®‡å®™æœ€å¼·ã®ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ ğŸš€
    
    æœ€æ–°ã®æ•°å­¦ãƒ»çµ±è¨ˆå­¦ãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆï¼š
    
    ã€åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‘
    ğŸ§  Hilbert Transform Analysis - ç¬æ™‚ä½ç›¸ãƒ»å‘¨æ³¢æ•°è§£æ
    ğŸ“ Fractal Adaptive Moving Average - ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡
    ğŸŒŠ Wavelet Multi-Resolution Analysis - ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æ
    ğŸ¯ Extended Kalman Filtering - æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    ğŸ“Š Information Entropy Analysis - æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è§£æ
    ğŸŒ€ Chaos Theory Indicators - ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™
    
    ã€ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½ - NEW!ã€‘
    ğŸ”¬ Unscented Kalman Filter - ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆéç·šå½¢ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
    ğŸ“ˆ GARCH Volatility Model - æ¡ä»¶ä»˜ãåˆ†æ•£å‹•çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
    ğŸ”„ Regime Switching Detection - ãƒãƒ«ã‚³ãƒ•åˆ‡ã‚Šæ›¿ãˆãƒ¢ãƒ‡ãƒ«
    ğŸ“¡ Spectral Analysis - å‘¨æ³¢æ•°ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ»å‘¨æœŸæ€§æ¤œå‡º
    ğŸ” Multiscale Entropy - è¤‡æ•°æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«è¤‡é›‘æ€§æ¸¬å®š
    ğŸŒŒ Nonlinear Dynamics - ç›¸é–¢æ¬¡å…ƒãƒ»ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ãƒ»äºˆæ¸¬å¯èƒ½æ€§
    
    ã€çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã€‘
    ğŸ¯ Advanced Ensemble Voting - é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ 
    ğŸ”® Predictive Analysis - æ¬¡ä¸–ä»£äºˆæ¸¬åˆ†æ
    ğŸ† Ultimate Performance - åœ§å€’çš„ç²¾åº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    """
    
    def __init__(
        self,
        # ã‚³ã‚¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        analysis_period: int = 21,
        ensemble_window: int = 50,
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°ï¼ˆå…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹ - æœ€çµ‚æ®µéšãƒ†ã‚¹ãƒˆï¼‰
        enable_hilbert: bool = True,   # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›è§£æ
        enable_fractal: bool = True,   # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡
        enable_wavelet: bool = True,   # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æ
        enable_kalman: bool = True,    # æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        enable_entropy: bool = True,   # æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
        enable_chaos: bool = True,     # ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™
        
        # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        enable_unscented_kalman: bool = True,  # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        enable_garch: bool = True,             # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
        enable_regime_switching: bool = True,  # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆæ¤œå‡º
        enable_spectral: bool = True,          # ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ
        enable_multiscale_entropy: bool = True, # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        enable_nonlinear_dynamics: bool = True, # éç·šå½¢åŠ›å­¦ç³»è§£æ
        
        # äºˆæ¸¬è¨­å®š
        enable_prediction: bool = True,
        prediction_horizon: int = 3,
        
        # ã—ãã„å€¤è¨­å®š
        trend_threshold: float = 0.65,
        strong_trend_threshold: float = 0.8,
        
        # å¾“æ¥ã®ChopTrendã¨ã®çµ±åˆ
        use_legacy_chop: bool = False,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç„¡åŠ¹åŒ–ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
        chop_weight: float = 0.3
    ):
        """
        å®‡å®™æœ€å¼·ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ - æœ€çµ‚æ®µéšæ§‹æˆ
        
        å…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆã—ãŸå®Œå…¨ç‰ˆï¼š
        ğŸ§  Hilbert Transform: ç¬æ™‚ä½ç›¸ãƒ»å‘¨æ³¢æ•°è§£æ
        ğŸ“ Fractal Adaptive MA: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡
        ğŸŒŠ Wavelet Analysis: ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æ  
        ğŸ¯ Extended Kalman: æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        ğŸ“Š Information Entropy: æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
        ğŸŒ€ Chaos Theory: ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™ï¼ˆHurstæŒ‡æ•°ã€ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ï¼‰
        """
        super().__init__(f"UltimateChopTrend(P={analysis_period},W={ensemble_window})")
        
        self.analysis_period = analysis_period
        self.ensemble_window = ensemble_window
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹åŒ–
        self.enable_hilbert = enable_hilbert
        self.enable_fractal = enable_fractal
        self.enable_wavelet = enable_wavelet
        self.enable_kalman = enable_kalman
        self.enable_entropy = enable_entropy
        self.enable_chaos = enable_chaos
        
        # ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½æœ‰åŠ¹åŒ–
        self.enable_unscented_kalman = enable_unscented_kalman
        self.enable_garch = enable_garch
        self.enable_regime_switching = enable_regime_switching
        self.enable_spectral = enable_spectral
        self.enable_multiscale_entropy = enable_multiscale_entropy
        self.enable_nonlinear_dynamics = enable_nonlinear_dynamics
        
        # äºˆæ¸¬è¨­å®š
        self.enable_prediction = enable_prediction
        self.prediction_horizon = prediction_horizon
        
        # ã—ãã„å€¤
        self.trend_threshold = trend_threshold
        self.strong_trend_threshold = strong_trend_threshold
        
        # å¾“æ¥ã®ChopTrendã¨ã®çµ±åˆ
        self.use_legacy_chop = use_legacy_chop
        self.chop_weight = chop_weight
        
        # å¾“æ¥ã®ChopTrendã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        if self.use_legacy_chop:
            try:
                from .chop_trend import ChopTrend
                self.legacy_chop = ChopTrend()
            except (ImportError, Exception):
                # ChopTrendãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç„¡åŠ¹åŒ–
                self.legacy_chop = None
                self.use_legacy_chop = False
                self.logger.warning("ChopTrendãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ¬ã‚¬ã‚·ãƒ¼çµ±åˆã‚’ç„¡åŠ¹åŒ–ã—ã¾ã—ãŸã€‚")
        
        self._cache = {}
        self._result: Optional[UltimateChopTrendResult] = None
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateChopTrendResult:
        """
        Ultimate ChopTrendã‚’è¨ˆç®—ã™ã‚‹
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if len(data) == 0:
                return self._create_empty_result(0)
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            if isinstance(data, pd.DataFrame):
                prices = np.asarray(data['close'].values, dtype=np.float64)
                high = np.asarray(data['high'].values, dtype=np.float64)
                low = np.asarray(data['low'].values, dtype=np.float64)
            else:
                prices = np.asarray(data[:, 3], dtype=np.float64)
                high = np.asarray(data[:, 1], dtype=np.float64)
                low = np.asarray(data[:, 2], dtype=np.float64)
            
            n = len(prices)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®è¨ˆç®—ï¼ˆATRè¿‘ä¼¼ï¼‰
            volatility = np.zeros(n)
            for i in range(1, n):
                tr = max(
                    high[i] - low[i],
                    abs(high[i] - prices[i-1]),
                    abs(low[i] - prices[i-1])
                )
                if i < 14:
                    volatility[i] = max(tr, 1e-10)  # æœ€å°å€¤åˆ¶é™
                else:
                    volatility[i] = max((volatility[i-1] * 13 + tr) / 14, 1e-10)  # æœ€å°å€¤åˆ¶é™
            
            # åˆæœŸå€¤ã‚‚è¨­å®š
            if n > 0:
                volatility[0] = max(high[0] - low[0], 1e-10)
            
            # å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè¡Œ
            components = {}
            
            if self.enable_hilbert:
                phase, freq, amp, hilbert_trend = hilbert_transform_analysis(prices)
                components['hilbert'] = hilbert_trend
            else:
                components['hilbert'] = np.full(n, 0.5)
            
            if self.enable_fractal:
                components['fractal'] = fractal_adaptive_moving_average(prices, self.analysis_period)
                # FRAMAã‚’0-1ã‚¹ã‚±ãƒ¼ãƒ«ã«æ­£è¦åŒ–
                frama_normalized = np.full(n, 0.5)
                for i in range(self.analysis_period, n):
                    if not np.isnan(components['fractal'][i]):
                        vol_safe = max(volatility[i], 1e-10)  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                        price_change = (prices[i] - components['fractal'][i]) / vol_safe
                        # å®‰å…¨ãªtanhè¨ˆç®—
                        price_change_bounded = min(max(price_change, -50), 50)
                        frama_normalized[i] = math.tanh(price_change_bounded) * 0.5 + 0.5
                components['fractal'] = frama_normalized
            else:
                components['fractal'] = np.full(n, 0.5)
            
            if self.enable_wavelet:
                denoised, detail = wavelet_denoising(prices)
                # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæˆåˆ†ã‚’æ­£è¦åŒ–
                wavelet_signal = np.full(n, 0.5)
                for i in range(10, n):
                    vol_safe = max(volatility[i], 1e-10)  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                    trend_strength = abs(denoised[i] - denoised[i-5]) / (vol_safe * 5)
                    wavelet_signal[i] = min(max(trend_strength, 0), 1)
                components['wavelet'] = wavelet_signal
            else:
                components['wavelet'] = np.full(n, 0.5)
            
            if self.enable_kalman:
                if self.enable_unscented_kalman:
                    # ğŸš€ é«˜åº¦ç‰ˆ: ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    ukf_filtered, ukf_trend, ukf_uncertainty = unscented_kalman_filter(prices, volatility)
                    kalman_signal = np.full(n, 0.5)
                    for i in range(5, n):
                        if not np.isnan(ukf_trend[i]):
                            # ãƒˆãƒ¬ãƒ³ãƒ‰é€Ÿåº¦ã‚’æ­£è¦åŒ–
                            trend_dir_bounded = min(max(ukf_trend[i] * 10, -50), 50)
                            kalman_signal[i] = math.tanh(trend_dir_bounded) * 0.5 + 0.5
                    components['kalman'] = kalman_signal
                    kalman_conf = 1.0 / (1.0 + ukf_uncertainty)  # ä¸ç¢ºå®Ÿæ€§ã®é€†æ•°ã§ä¿¡é ¼åº¦è¨ˆç®—
                else:
                    # å¾“æ¥ç‰ˆ: æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    kalman_trend, kalman_conf = extended_kalman_filter(prices, volatility)
                    # ã‚«ãƒ«ãƒãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ­£è¦åŒ–
                    kalman_signal = np.full(n, 0.5)
                    for i in range(5, n):
                        if not np.isnan(kalman_trend[i]):
                            vol_safe = max(volatility[i], 1e-10)  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                            trend_dir = (kalman_trend[i] - kalman_trend[i-3]) / vol_safe
                            # å®‰å…¨ãªtanhè¨ˆç®—
                            trend_dir_bounded = min(max(trend_dir * 2, -50), 50)
                            kalman_signal[i] = math.tanh(trend_dir_bounded) * 0.5 + 0.5
                    components['kalman'] = kalman_signal
            else:
                components['kalman'] = np.full(n, 0.5)
                kalman_conf = np.ones(n)
            
            if self.enable_entropy:
                if self.enable_multiscale_entropy:
                    # ğŸš€ é«˜åº¦ç‰ˆ: ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                    components['entropy'] = multiscale_entropy(prices, max_scale=10, pattern_length=2)
                else:
                    # å¾“æ¥ç‰ˆ: å˜ä¸€ã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                    components['entropy'] = information_entropy_trend(prices, self.analysis_period)
            else:
                components['entropy'] = np.full(n, 0.5)
            
            if self.enable_chaos:
                hurst, lyapunov = chaos_theory_indicators(prices, self.analysis_period)
                # ã‚«ã‚ªã‚¹æŒ‡æ¨™ã‚’çµ±åˆ
                chaos_signal = np.full(n, 0.5)
                for i in range(n):
                    if not np.isnan(hurst[i]):
                        # Hurst > 0.5 ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ã€< 0.5 ã¯å¹³å‡å›å¸°
                        chaos_signal[i] = hurst[i]
                components['chaos'] = chaos_signal
            else:
                components['chaos'] = np.full(n, 0.5)
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
            try:
                ensemble_signal, ensemble_confidence = ensemble_voting_system(
                    components['hilbert'],
                    components['fractal'],
                    components['wavelet'],
                    components['kalman'],
                    components['entropy'],
                    components['chaos'],
                    kalman_conf
                )
            except Exception as e:
                self.logger.error(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ã§ã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªå¹³å‡
                ensemble_signal = np.full(n, 0.5)
                ensemble_confidence = np.full(n, 0.5)
                
                for i in range(n):
                    valid_signals = []
                    for comp_name, comp_values in components.items():
                        if not np.isnan(comp_values[i]) and np.isfinite(comp_values[i]):
                            valid_signals.append(comp_values[i])
                    
                    if len(valid_signals) > 0:
                        ensemble_signal[i] = np.mean(valid_signals)
                        ensemble_confidence[i] = min(len(valid_signals) / 6.0, 1.0)  # æœ‰åŠ¹ã‚·ã‚°ãƒŠãƒ«æ•°ã«åŸºã¥ãä¿¡é ¼åº¦
            
            # ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½ã®è¨ˆç®—
            advanced_components = {}
            
            # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã®ä¿å­˜
            if self.enable_unscented_kalman and self.enable_kalman:
                ukf_filtered, ukf_trend, ukf_uncertainty = unscented_kalman_filter(prices, volatility)
                advanced_components['unscented_kalman'] = ukf_filtered
            else:
                advanced_components['unscented_kalman'] = np.full(n, np.nan)
            
            # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
            if self.enable_garch:
                returns = np.diff(prices)
                returns = np.concatenate([np.array([0]), returns])  # æœ€åˆã®å€¤ã‚’0ã§åˆæœŸåŒ–
                garch_var, garch_vol = garch_volatility_model(returns)
                advanced_components['garch_volatility'] = garch_vol
            else:
                advanced_components['garch_volatility'] = np.full(n, np.nan)
            
            # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆæ¤œå‡º
            if self.enable_regime_switching:
                regime_probs, most_likely_regime = regime_switching_detection(prices, window=50, n_regimes=3)
                advanced_components['regime_switching_probs'] = regime_probs
                advanced_components['most_likely_regime'] = most_likely_regime
            else:
                advanced_components['regime_switching_probs'] = np.zeros((n, 3))
                advanced_components['most_likely_regime'] = np.zeros(n)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ
            if self.enable_spectral:
                dominant_freq, spectral_power, spectral_trend = spectral_analysis(prices, window=64)
                advanced_components['dominant_frequency'] = dominant_freq
                advanced_components['spectral_power'] = spectral_power
                advanced_components['spectral_trend'] = spectral_trend
            else:
                advanced_components['dominant_frequency'] = np.full(n, np.nan)
                advanced_components['spectral_power'] = np.full(n, np.nan)
                advanced_components['spectral_trend'] = np.full(n, np.nan)
            
            # éç·šå½¢åŠ›å­¦ç³»è§£æ
            if self.enable_nonlinear_dynamics:
                correlation_dim, lyapunov_exp, predictability = nonlinear_dynamics_analysis(prices)
                advanced_components['correlation_dimension'] = correlation_dim
                advanced_components['lyapunov_exponent'] = lyapunov_exp
                advanced_components['predictability_score'] = predictability
            else:
                advanced_components['correlation_dimension'] = np.full(n, np.nan)
                advanced_components['lyapunov_exponent'] = np.full(n, np.nan)
                advanced_components['predictability_score'] = np.full(n, np.nan)

            # å¾“æ¥ã®ChopTrendã¨ã®çµ±åˆ
            if self.use_legacy_chop and self.legacy_chop is not None:
                try:
                    legacy_result = self.legacy_chop.calculate(data)
                    legacy_values = legacy_result.values
                    # çµ±åˆ
                    final_signal = (1 - self.chop_weight) * ensemble_signal + self.chop_weight * legacy_values
                except:
                    final_signal = ensemble_signal
            else:
                final_signal = ensemble_signal
            
            # é©å¿œã—ãã„å€¤
            adaptive_threshold = adaptive_threshold_calculation(final_signal, volatility, self.ensemble_window)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«ã®ç”Ÿæˆ
            trend_signals = np.zeros(n)
            trend_strength = np.zeros(n)
            regime_state = np.zeros(n)
            
            for i in range(n):
                signal = final_signal[i]
                threshold = adaptive_threshold[i]
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
                trend_strength[i] = abs(signal - 0.5) * 2
                
                # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š
                if trend_strength[i] > self.strong_trend_threshold:
                    regime_state[i] = 2  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ
                elif trend_strength[i] > self.trend_threshold:
                    regime_state[i] = 1  # é€šå¸¸ã®ãƒˆãƒ¬ãƒ³ãƒ‰
                else:
                    regime_state[i] = 0  # ãƒ¬ãƒ³ã‚¸
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«
                if signal > threshold + 0.1:
                    if signal > self.strong_trend_threshold:
                        trend_signals[i] = 1.0  # å¼·ã„ä¸Šæ˜‡
                    else:
                        trend_signals[i] = 0.5  # å¼±ã„ä¸Šæ˜‡
                elif signal < threshold - 0.1:
                    if signal < (1 - self.strong_trend_threshold):
                        trend_signals[i] = -1.0  # å¼·ã„ä¸‹é™
                    else:
                        trend_signals[i] = -0.5  # å¼±ã„ä¸‹é™
                else:
                    trend_signals[i] = 0.0  # ãƒ¬ãƒ³ã‚¸
            
            # äºˆæ¸¬åˆ†æ
            if self.enable_prediction:
                predictive_signal, momentum_forecast, volatility_forecast = predictive_analysis(
                    prices, final_signal, volatility, self.prediction_horizon
                )
                
                # ğŸš€ GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ã§å‘ä¸Š
                if self.enable_garch and 'garch_volatility' in advanced_components:
                    garch_vol = advanced_components['garch_volatility']
                    # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã§å¾“æ¥ã®äºˆæ¸¬ã‚’è£œå¼·
                    for i in range(len(volatility_forecast)):
                        if not np.isnan(garch_vol[i]) and not np.isnan(volatility_forecast[i]):
                            # GARCHäºˆæ¸¬ã¨å¾“æ¥äºˆæ¸¬ã®åŠ é‡å¹³å‡
                            volatility_forecast[i] = 0.7 * garch_vol[i] + 0.3 * volatility_forecast[i]
                        elif not np.isnan(garch_vol[i]):
                            volatility_forecast[i] = garch_vol[i]
            else:
                predictive_signal = np.full(n, np.nan)
                momentum_forecast = np.full(n, np.nan)
                volatility_forecast = np.full(n, np.nan)
            
            # æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ†é¡ï¼ˆãƒãƒ£ãƒ¼ãƒˆèƒŒæ™¯è‰²ç”¨ï¼‰
            trend_direction, direction_strength = calculate_trend_direction_classification(
                final_signal, trend_signals, prices, lookback_period=5
            )
            
            # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            market_efficiency = np.zeros(n)
            information_ratio = np.zeros(n)
            trend_probability = np.zeros(n)
            regime_probability = np.zeros(n)
            
            for i in range(self.analysis_period, n):
                # å¸‚å ´åŠ¹ç‡æ€§ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã‹ã‚‰ã®ä¹–é›¢ï¼‰
                returns = np.diff(prices[i-self.analysis_period:i+1])
                if len(returns) > 0:
                    autocorr = np.corrcoef(returns[:-1], returns[1:])[0, 1] if len(returns) > 1 else 0
                    market_efficiency[i] = 1 - abs(autocorr) if not np.isnan(autocorr) else 1
                
                # æƒ…å ±æ¯”ç‡
                avg_return = np.mean(returns) if len(returns) > 0 else 0
                std_return = np.std(returns) if len(returns) > 0 else 1e-10  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                std_return = max(std_return, 1e-10)  # æœ€å°å€¤åˆ¶é™
                information_ratio[i] = avg_return / std_return
                
                # ç¢ºç‡è¨ˆç®—
                trend_probability[i] = trend_strength[i]
                regime_probability[i] = regime_state[i] / 2.0
            
            # ç¾åœ¨çŠ¶æ…‹ã®åˆ¤å®š
            latest_signal = trend_signals[-1] if len(trend_signals) > 0 else 0
            latest_strength = trend_strength[-1] if len(trend_strength) > 0 else 0
            latest_confidence = ensemble_confidence[-1] if len(ensemble_confidence) > 0 else 0
            
            if latest_signal > 0.7:
                current_trend = "strong_uptrend"
            elif latest_signal > 0.2:
                current_trend = "uptrend"
            elif latest_signal < -0.7:
                current_trend = "strong_downtrend"
            elif latest_signal < -0.2:
                current_trend = "downtrend"
            else:
                current_trend = "range"
            
            # çµæœä½œæˆï¼ˆğŸš€ å®‡å®™æœ€å¼·ç‰ˆï¼‰
            result = UltimateChopTrendResult(
                ultimate_trend_index=final_signal,
                trend_signals=trend_signals,
                trend_strength=trend_strength,
                regime_state=regime_state,
                trend_direction=trend_direction,
                direction_strength=direction_strength,
                confidence_score=ensemble_confidence,
                trend_probability=trend_probability,
                regime_probability=regime_probability,
                predictive_signal=predictive_signal,
                momentum_forecast=momentum_forecast,
                volatility_forecast=volatility_forecast,
                hilbert_component=components['hilbert'],
                fractal_component=components['fractal'],
                wavelet_component=components['wavelet'],
                kalman_component=components['kalman'],
                entropy_component=components['entropy'],
                chaos_component=components['chaos'],
                # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£ææˆåˆ†
                unscented_kalman_component=advanced_components['unscented_kalman'],
                garch_volatility=advanced_components['garch_volatility'],
                regime_switching_probs=advanced_components['regime_switching_probs'],
                spectral_power=advanced_components['spectral_power'],
                dominant_frequency=advanced_components['dominant_frequency'],
                multiscale_entropy=components['entropy'] if self.enable_multiscale_entropy else np.full(n, np.nan),
                nonlinear_dynamics=advanced_components['predictability_score'],
                predictability_score=advanced_components['predictability_score'],
                market_efficiency=market_efficiency,
                information_ratio=information_ratio,
                adaptive_threshold=adaptive_threshold,
                correlation_dimension=advanced_components['correlation_dimension'],
                lyapunov_exponent=advanced_components['lyapunov_exponent'],
                current_trend=current_trend,
                current_strength=latest_strength,
                current_confidence=latest_confidence
            )
            
            self._result = result
            self._values = final_signal
            
            return result
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            self.logger.error(f"UltimateChopTrendè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\nè©³ç´°:\n{error_details}")
            return self._create_empty_result(len(data))
    
    def _create_empty_result(self, length: int) -> UltimateChopTrendResult:
        """ç©ºã®çµæœã‚’ä½œæˆï¼ˆğŸš€ å®‡å®™æœ€å¼·ç‰ˆï¼‰"""
        return UltimateChopTrendResult(
            ultimate_trend_index=np.full(length, np.nan),
            trend_signals=np.zeros(length),
            trend_strength=np.zeros(length),
            regime_state=np.zeros(length),
            trend_direction=np.zeros(length),
            direction_strength=np.zeros(length),
            confidence_score=np.zeros(length),
            trend_probability=np.zeros(length),
            regime_probability=np.zeros(length),
            predictive_signal=np.full(length, np.nan),
            momentum_forecast=np.full(length, np.nan),
            volatility_forecast=np.full(length, np.nan),
            hilbert_component=np.full(length, np.nan),
            fractal_component=np.full(length, np.nan),
            wavelet_component=np.full(length, np.nan),
            kalman_component=np.full(length, np.nan),
            entropy_component=np.full(length, np.nan),
            chaos_component=np.full(length, np.nan),
            # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£ææˆåˆ†
            unscented_kalman_component=np.full(length, np.nan),
            garch_volatility=np.full(length, np.nan),
            regime_switching_probs=np.zeros((length, 3)),
            spectral_power=np.full(length, np.nan),
            dominant_frequency=np.full(length, np.nan),
            multiscale_entropy=np.full(length, np.nan),
            nonlinear_dynamics=np.full(length, np.nan),
            predictability_score=np.full(length, np.nan),
            market_efficiency=np.zeros(length),
            information_ratio=np.zeros(length),
            adaptive_threshold=np.full(length, 0.5),
            correlation_dimension=np.full(length, np.nan),
            lyapunov_exponent=np.full(length, np.nan),
            current_trend="range",
            current_strength=0.0,
            current_confidence=0.0
        )
    
    def get_values(self) -> Optional[np.ndarray]:
        """ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™å€¤ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.ultimate_trend_index.copy()
        return None
    
    def get_result(self) -> Optional[UltimateChopTrendResult]:
        """å®Œå…¨ãªçµæœã‚’å–å¾—"""
        return self._result
    
    def reset(self) -> None:
        """ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        if self.use_legacy_chop and self.legacy_chop is not None:
            self.legacy_chop.reset()
        self._result = None
        self._cache = {} 