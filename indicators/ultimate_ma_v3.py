#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from typing import Union, Optional, NamedTuple, Tuple
import numpy as np
import pandas as pd
from numba import jit
from scipy import stats
from scipy.signal import find_peaks
import warnings
warnings.filterwarnings('ignore')

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‹ã‚‰çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤‰æ›´
try:
    from .indicator import Indicator
    from .price_source import PriceSource
    from .ehlers_unified_dc import EhlersUnifiedDC  # å‹•çš„é©å¿œç”¨
except ImportError:
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œæ™‚ã®å¯¾å¿œ
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource
    from ehlers_unified_dc import EhlersUnifiedDC  # å‹•çš„é©å¿œç”¨


class UltimateMAV3Result(NamedTuple):
    """UltimateMA V3è¨ˆç®—çµæœ"""
    values: np.ndarray                      # æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    raw_values: np.ndarray                  # å…ƒã®ä¾¡æ ¼
    kalman_values: np.ndarray               # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œ
    super_smooth_values: np.ndarray         # ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼å¾Œ
    zero_lag_values: np.ndarray             # ã‚¼ãƒ­ãƒ©ã‚°EMAå¾Œ
    amplitude: np.ndarray                   # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›æŒ¯å¹…
    phase: np.ndarray                      # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ä½ç›¸
    realtime_trends: np.ndarray             # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰
    trend_signals: np.ndarray               # 1=up, -1=down, 0=range
    trend_confidence: np.ndarray            # ãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡é ¼åº¦ (0-1)
    multi_timeframe_consensus: np.ndarray   # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åº¦
    volatility_regime: np.ndarray           # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹
    fractal_dimension: np.ndarray           # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
    entropy_level: np.ndarray               # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ¬ãƒ™ãƒ«
    quantum_state: np.ndarray               # é‡å­çŠ¶æ…‹ç¢ºç‡
    current_trend: str                      # 'up', 'down', 'range'
    current_trend_value: int                # 1, -1, 0
    current_confidence: float               # ç¾åœ¨ã®ä¿¡é ¼åº¦


@jit(nopython=True)
def quantum_trend_analyzer_numba(values: np.ndarray, window: int = 21) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸŒŒ é‡å­ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå™¨ï¼ˆé‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é¢¨ä¸¦åˆ—å‡¦ç†ï¼‰
    è¤‡æ•°ã®åˆ¤å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€é‡å­é‡ã­åˆã‚ã›çš„ã«çµ±åˆ
    """
    n = len(values)
    quantum_states = np.zeros(n)
    confidence_levels = np.zeros(n)
    
    if n < window:
        return quantum_states, confidence_levels
    
    for i in range(window, n):
        # è¤‡æ•°ã®åˆ¤å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        judgments = np.zeros(7)  # 7ã¤ã®åˆ¤å®šã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        
        # åˆ¤å®š1: ç·šå½¢å›å¸°ã‚¹ãƒ­ãƒ¼ãƒ—
        x_vals = np.arange(window, dtype=np.float64)
        y_vals = values[i-window+1:i+1].astype(np.float64)
        
        # ç·šå½¢å›å¸°ã®è¨ˆç®—ï¼ˆnumbaå¯¾å¿œï¼‰
        n_points = len(x_vals)
        sum_x = np.sum(x_vals)
        sum_y = np.sum(y_vals)
        sum_xy = np.sum(x_vals * y_vals)
        sum_x2 = np.sum(x_vals * x_vals)
        
        denominator = n_points * sum_x2 - sum_x * sum_x
        if abs(denominator) > 1e-10:
            slope = (n_points * sum_xy - sum_x * sum_y) / denominator
            judgments[0] = slope
        
        # åˆ¤å®š2: æŒ‡æ•°åŠ é‡ç§»å‹•å¹³å‡å·®åˆ†
        if i >= 2:
            ema_short = values[i]
            ema_long = np.mean(values[i-window+1:i+1])
            judgments[1] = (ema_short - ema_long) / max(abs(ema_long), 1e-10)
        
        # åˆ¤å®š3: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ åˆ†æ
        if i >= window:
            momentum = values[i] - values[i-window]
            avg_price = np.mean(values[i-window+1:i+1])
            judgments[2] = momentum / max(abs(avg_price), 1e-10)
        
        # åˆ¤å®š4: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´ãƒˆãƒ¬ãƒ³ãƒ‰
        recent_std = np.std(values[i-window+1:i+1])
        if recent_std > 1e-10:
            normalized_change = (values[i] - values[i-1]) / recent_std
            judgments[3] = normalized_change
        
        # åˆ¤å®š5: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«åˆ†æçš„åˆ¤å®š
        if i >= window:
            # ç°¡æ˜“ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—
            price_changes = np.abs(np.diff(values[i-window+1:i+1]))
            if len(price_changes) > 0:
                fractal_roughness = np.std(price_changes) / (np.mean(price_changes) + 1e-10)
                # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒãŒä½ã„ï¼ˆæ»‘ã‚‰ã‹ï¼‰ã»ã©å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰
                judgments[4] = -fractal_roughness if fractal_roughness > 0 else 0
        
        # åˆ¤å®š6: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ
        # ä¾¡æ ¼ã®åˆ†å¸ƒã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—
        hist_values = values[i-window+1:i+1]
        value_range = np.max(hist_values) - np.min(hist_values)
        if value_range > 1e-10:
            # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆå‡ç­‰åˆ†å¸ƒã‹ã‚‰ã®åå·®ï¼‰
            normalized_values = (hist_values - np.min(hist_values)) / value_range
            # æœ€æ–°ã®å€¤ã®ä½ç½®çš„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            latest_position = (values[i] - np.min(hist_values)) / value_range
            entropy_trend = latest_position - 0.5  # ä¸­å¤®ã‹ã‚‰ã®åå·®
            judgments[5] = entropy_trend
        
        # åˆ¤å®š7: é©å¿œçš„ç§»å‹•å¹³å‡äº¤å·®
        if i >= window // 2:
            fast_ma = np.mean(values[i-window//2+1:i+1])
            slow_ma = np.mean(values[i-window+1:i+1])
            ma_diff = (fast_ma - slow_ma) / max(abs(slow_ma), 1e-10)
            judgments[6] = ma_diff
        
        # é‡å­é‡ã­åˆã‚ã›é¢¨çµ±åˆï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰- ã‚ˆã‚Šæ•æ„Ÿã«èª¿æ•´
        weights = np.array([0.30, 0.25, 0.20, 0.15, 0.05, 0.03, 0.02])  # ä¸»è¦æŒ‡æ¨™ã‚’é‡è¦–
        quantum_state = np.sum(judgments * weights)
        
        # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆåˆ¤å®šã®ä¸€è‡´åº¦ï¼‰- ã‚ˆã‚Šæ•æ„Ÿã«èª¿æ•´
        judgment_signs = np.sign(judgments + 1e-6)  # å¾®å°ãªå€¤ã‚‚æ–¹å‘ã¨ã—ã¦è€ƒæ…®
        non_zero_judgments = judgments[np.abs(judgments) > 1e-6]
        
        if len(non_zero_judgments) > 0:
            consensus = np.abs(np.sum(judgment_signs)) / len(judgment_signs)
            judgment_std = np.std(non_zero_judgments)
            judgment_mean = np.mean(np.abs(non_zero_judgments))
            
            # ä¿¡é ¼åº¦ã‚’ã‚ˆã‚Šå¯›å®¹ã«è¨ˆç®—
            base_confidence = consensus * 0.7 + 0.3  # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦ã‚’30%åº•ä¸Šã’
            std_factor = 1.0 - min(judgment_std / (judgment_mean + 1e-10), 0.8)
            confidence = base_confidence * std_factor
        else:
            confidence = 0.2  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¿¡é ¼åº¦
        
        quantum_states[i] = quantum_state
        confidence_levels[i] = min(max(confidence, 0.0), 1.0)
    
    return quantum_states, confidence_levels


@jit(nopython=True)
def multi_timeframe_consensus_numba(values: np.ndarray) -> np.ndarray:
    """
    ğŸ”„ ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åº¦åˆ†æå™¨
    è¤‡æ•°ã®æ™‚é–“è»¸ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰ä¸€è‡´åº¦ã‚’è¨ˆç®—
    """
    n = len(values)
    consensus = np.zeros(n)
    
    # ç•°ãªã‚‹æ™‚é–“è»¸
    timeframes = np.array([5, 13, 21, 34, 55], dtype=np.int32)
    
    for i in range(55, n):  # æœ€å¤§æ™‚é–“è»¸ä»¥é™ã‹ã‚‰é–‹å§‹
        frame_trends = np.zeros(len(timeframes))
        
        for j, tf in enumerate(timeframes):
            if i >= tf:
                # å„æ™‚é–“è»¸ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘
                start_val = values[i-tf+1]
                end_val = values[i]
                frame_trends[j] = np.sign(end_val - start_val)
        
        # åˆæ„åº¦è¨ˆç®—ï¼ˆä¸€è‡´ã™ã‚‹æ–¹å‘ã®å‰²åˆï¼‰
        if len(frame_trends) > 0:
            positive_count = np.sum(frame_trends > 0)
            negative_count = np.sum(frame_trends < 0)
            total_count = positive_count + negative_count
            
            if total_count > 0:
                consensus[i] = max(positive_count, negative_count) / total_count
            else:
                consensus[i] = 0.0
    
    return consensus


@jit(nopython=True)
def volatility_regime_detector_numba(values: np.ndarray, window: int = 21) -> np.ndarray:
    """
    ğŸ“Š ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ»ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºå™¨
    å¸‚å ´ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹ã‚’æ¤œå‡ºã—ã€é©å¿œçš„ã«é–¾å€¤ã‚’èª¿æ•´
    """
    n = len(values)
    volatility_regimes = np.zeros(n)
    
    if n < window:
        return volatility_regimes
    
    for i in range(window, n):
        # ç¾åœ¨ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        current_std = np.std(values[i-window+1:i+1])
        
        # é•·æœŸãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆéå»ã®å¹³å‡çš„ãªãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
        long_term_window = min(window * 3, i)
        if long_term_window > window:
            long_term_std = np.std(values[i-long_term_window+1:i+1])
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
            # 0: ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£, 1: æ­£å¸¸, 2: é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            vol_ratio = current_std / (long_term_std + 1e-10)
            
            if vol_ratio < 0.7:
                volatility_regimes[i] = 0  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            elif vol_ratio > 1.5:
                volatility_regimes[i] = 2  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            else:
                volatility_regimes[i] = 1  # æ­£å¸¸
        else:
            volatility_regimes[i] = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ­£å¸¸
    
    return volatility_regimes


@jit(nopython=True)
def fractal_dimension_calculator_numba(values: np.ndarray, window: int = 21) -> np.ndarray:
    """
    ğŸŒ€ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—å™¨
    ä¾¡æ ¼ç³»åˆ—ã®è‡ªå·±ç›¸ä¼¼æ€§ã‚’æ¸¬å®šã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã®å®‰å®šæ€§ã‚’è©•ä¾¡
    """
    n = len(values)
    fractal_dims = np.zeros(n)
    
    if n < window:
        return fractal_dims
    
    for i in range(window, n):
        segment = values[i-window+1:i+1]
        
        # Higuchi's fractal dimension algorithm (simplified)
        k_max = min(5, window // 2)
        fractal_values = np.zeros(k_max)
        
        for k in range(1, k_max + 1):
            length = 0.0
            N = (window - 1) // k
            
            if N > 0:
                for m in range(k):
                    curve_length = 0.0
                    for j in range(1, N + 1):
                        if m + j * k < window:
                            curve_length += abs(segment[m + j * k] - segment[m + (j-1) * k])
                    
                    if N > 1:
                        curve_length = curve_length * (window - 1) / (k * N)
                        length += curve_length
                
                if k > 0:
                    fractal_values[k-1] = length / k
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®æ¨å®šï¼ˆå¯¾æ•°å›å¸°ã®å‚¾ãï¼‰
        if k_max > 1:
            non_zero_mask = fractal_values > 1e-10
            if np.sum(non_zero_mask) >= 2:
                # ç°¡æ˜“å¯¾æ•°å›å¸°
                log_k = np.log(np.arange(1, k_max + 1)[non_zero_mask])
                log_length = np.log(fractal_values[non_zero_mask])
                
                # ç·šå½¢å›å¸°ã§ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã‚’è¨ˆç®—
                n_points = len(log_k)
                if n_points >= 2:
                    sum_x = np.sum(log_k)
                    sum_y = np.sum(log_length)
                    sum_xy = np.sum(log_k * log_length)
                    sum_x2 = np.sum(log_k * log_k)
                    
                    denominator = n_points * sum_x2 - sum_x * sum_x
                    if abs(denominator) > 1e-10:
                        slope = (n_points * sum_xy - sum_x * sum_y) / denominator
                        fractal_dims[i] = -slope  # è² ã®å‚¾ããŒãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®æ­£è¦åŒ–ï¼ˆ1-2ã®ç¯„å›²ã«èª¿æ•´ï¼‰
        fractal_dims[i] = max(1.0, min(2.0, fractal_dims[i]))
    
    return fractal_dims


@jit(nopython=True)
def entropy_level_calculator_numba(values: np.ndarray, window: int = 21) -> np.ndarray:
    """
    ğŸ”¬ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ¬ãƒ™ãƒ«è¨ˆç®—å™¨
    ä¾¡æ ¼ç³»åˆ—ã®æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’è¨ˆç®—ã—ã€äºˆæ¸¬å¯èƒ½æ€§ã‚’è©•ä¾¡
    """
    n = len(values)
    entropy_levels = np.zeros(n)
    
    if n < window:
        return entropy_levels
    
    for i in range(window, n):
        segment = values[i-window+1:i+1]
        
        # ä¾¡æ ¼å¤‰åŒ–ã®åˆ†å¸ƒã‚’è¨ˆç®—
        price_changes = np.diff(segment)
        
        if len(price_changes) > 0:
            # ä¾¡æ ¼å¤‰åŒ–ã‚’æ­£è¦åŒ–
            change_std = np.std(price_changes)
            if change_std > 1e-10:
                normalized_changes = price_changes / change_std
                
                # ãƒ“ãƒ³ã«åˆ†å‰²ã—ã¦ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä½œæˆ
                bins = 10
                hist_min = np.min(normalized_changes)
                hist_max = np.max(normalized_changes)
                
                if hist_max > hist_min:
                    bin_width = (hist_max - hist_min) / bins
                    histogram = np.zeros(bins)
                    
                    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä½œæˆ
                    for change in normalized_changes:
                        bin_idx = int((change - hist_min) / bin_width)
                        bin_idx = max(0, min(bins - 1, bin_idx))
                        histogram[bin_idx] += 1
                    
                    # ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›
                    total_count = np.sum(histogram)
                    if total_count > 0:
                        probabilities = histogram / total_count
                        
                        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
                        entropy = 0.0
                        for prob in probabilities:
                            if prob > 1e-10:
                                entropy -= prob * np.log2(prob)
                        
                        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
                        max_entropy = np.log2(bins)
                        entropy_levels[i] = entropy / max_entropy if max_entropy > 0 else 0.0
    
    return entropy_levels


@jit(nopython=True)
def calculate_ultimate_trend_signals_v3_numba(
    values: np.ndarray,
    quantum_states: np.ndarray,
    confidence_levels: np.ndarray,
    multi_timeframe_consensus: np.ndarray,
    volatility_regime: np.ndarray,
    fractal_dimension: np.ndarray,
    entropy_level: np.ndarray,
    slope_index: int,
    base_threshold: float = 0.003
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸš€ ç©¶æ¥µã®ãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡å·è¨ˆç®—å™¨ V3
    å…¨ã¦ã®é«˜åº¦ãªåˆ†æçµæœã‚’çµ±åˆã—ã€æœ€å¼·ã®åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œ
    """
    length = len(values)
    trend_signals = np.zeros(length, dtype=np.int8)
    trend_confidence = np.zeros(length)
    
    for i in range(slope_index, length):
        if not np.isnan(values[i]) and not np.isnan(values[i - slope_index]):
            current = values[i]
            previous = values[i - slope_index]
            change = current - previous
            
            # åŸºæœ¬çš„ãªç›¸å¯¾å¤‰åŒ–ç‡
            base_value = max(abs(current), abs(previous), 1e-10)
            relative_change = abs(change) / base_value
            
            # ğŸŒŒ é‡å­çŠ¶æ…‹ã«ã‚ˆã‚‹èª¿æ•´
            quantum_strength = abs(quantum_states[i]) if i < len(quantum_states) else 0.0
            confidence = confidence_levels[i] if i < len(confidence_levels) else 0.0
            
            # ğŸ”„ ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åº¦ã«ã‚ˆã‚‹èª¿æ•´
            mtf_consensus = multi_timeframe_consensus[i] if i < len(multi_timeframe_consensus) else 0.0
            
            # ğŸ“Š ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ ã«ã‚ˆã‚‹é©å¿œçš„é–¾å€¤
            vol_regime = volatility_regime[i] if i < len(volatility_regime) else 1.0
            if vol_regime == 0:  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                vol_multiplier = 0.5
            elif vol_regime == 2:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                vol_multiplier = 2.0
            else:  # æ­£å¸¸
                vol_multiplier = 1.0
            
            # ğŸŒ€ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã«ã‚ˆã‚‹èª¿æ•´
            fractal_dim = fractal_dimension[i] if i < len(fractal_dimension) else 1.5
            # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒãŒä½ã„ï¼ˆ1ã«è¿‘ã„ï¼‰ã»ã©æ»‘ã‚‰ã‹ãªãƒˆãƒ¬ãƒ³ãƒ‰
            fractal_multiplier = 2.0 - fractal_dim  # 1.0 - 0.0ã®ç¯„å›²
            
            # ğŸ”¬ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹èª¿æ•´
            entropy = entropy_level[i] if i < len(entropy_level) else 0.5
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒä½ã„ã»ã©äºˆæ¸¬å¯èƒ½æ€§ãŒé«˜ã„
            entropy_multiplier = 1.0 - entropy  # ä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§æ„Ÿåº¦up
            
            # ğŸ¯ çµ±åˆçš„é©å¿œé–¾å€¤è¨ˆç®—
            adaptive_threshold = (base_threshold * 
                                vol_multiplier * 
                                (1.0 + fractal_multiplier * 0.3) * 
                                (1.0 + entropy_multiplier * 0.2))
            
            # ğŸš€ é©æ–°çš„å¤šæ®µéšåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            
            # åŸºæœ¬ä¿¡å·å¼·åº¦ï¼ˆé‡å­çŠ¶æ…‹ãƒ™ãƒ¼ã‚¹ï¼‰
            base_signal = abs(quantum_strength) * 10  # ä¿¡å·ã‚’å¢—å¹…
            
            # MTFåˆæ„åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            mtf_weight = max(0.3, mtf_consensus + 0.2)  # æœ€ä½30%ã®é‡ã¿
            
            # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
            confidence_weight = max(0.2, confidence + 0.3)  # æœ€ä½20%ã®é‡ã¿
            
            # çµ±åˆä¿¡å·å¼·åº¦
            signal_strength = base_signal * mtf_weight * confidence_weight
            
            # æœ€çµ‚ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
            final_confidence = min(1.0, 
                                 confidence * 0.25 + 
                                 mtf_consensus * 0.25 + 
                                 fractal_multiplier * 0.25 + 
                                 entropy_multiplier * 0.25)
            
            # å¤šæ®µéšåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
            strong_signal = (relative_change >= adaptive_threshold * 0.3 and 
                           signal_strength > 0.05 and 
                           final_confidence > 0.2)
            
            weak_signal = (relative_change >= adaptive_threshold * 0.1 and 
                         signal_strength > 0.01 and 
                         final_confidence > 0.1)
            
            # æ–¹å‘ä¸€è‡´æ€§ãƒã‚§ãƒƒã‚¯
            direction_consensus = (np.sign(change) == np.sign(quantum_strength))
            
            # æœ€çµ‚åˆ¤å®š
            if strong_signal or (weak_signal and direction_consensus):
                
                if change > 0:
                    trend_signals[i] = 1  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
                else:
                    trend_signals[i] = -1  # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
                    
                trend_confidence[i] = min(final_confidence, 1.0)
            else:
                trend_signals[i] = 0  # ãƒ¬ãƒ³ã‚¸
                trend_confidence[i] = 0.0
    
    return trend_signals, trend_confidence


# å¾“æ¥ã®numbaé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆultimate_ma.pyã‹ã‚‰ï¼‰
from .ultimate_ma import (
    adaptive_kalman_filter_numba, 
    super_smoother_filter_numba,
    zero_lag_ema_numba,
    hilbert_transform_filter_numba,
    adaptive_noise_reduction_numba,
    real_time_trend_detector_numba,
    calculate_current_trend_with_range_numba
)


class UltimateMAV3(Indicator):
    """
    ğŸš€ **Ultimate Moving Average V3 - QUANTUM NEURAL SUPREMACY EVOLUTION EDITION**
    
    ğŸ¯ **10æ®µéšé©æ–°çš„AIåˆ†æã‚·ã‚¹ãƒ†ãƒ :**
    1. **é©å¿œçš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: å‹•çš„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¨å®šãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é™¤å»
    2. **ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: John Ehlersæ”¹è‰¯ç‰ˆãƒ»ã‚¼ãƒ­é…å»¶è¨­è¨ˆ
    3. **ã‚¼ãƒ­ãƒ©ã‚°EMA**: é…å»¶å®Œå…¨é™¤å»ãƒ»äºˆæ¸¬çš„è£œæ­£
    4. **ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: ä½ç›¸é…å»¶ã‚¼ãƒ­ãƒ»ç¬æ™‚æŒ¯å¹…/ä½ç›¸
    5. **é©å¿œçš„ãƒã‚¤ã‚ºé™¤å»**: AIé¢¨å­¦ç¿’å‹ãƒ»æŒ¯å¹…é€£å‹•èª¿æ•´
    6. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º**: è¶…ä½é…å»¶ãƒ»å³åº§åå¿œ
    7. **ğŸŒŒ é‡å­ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå™¨**: é‡å­é‡ã­åˆã‚ã›é¢¨ä¸¦åˆ—åˆ¤å®šçµ±åˆ
    8. **ğŸ”„ ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åº¦**: è¤‡æ•°æ™‚é–“è»¸ä¸€è‡´åº¦åˆ†æ
    9. **ğŸ“Š ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º**: é©å¿œçš„å¸‚å ´çŠ¶æ³åˆ¤å®š
    10. **ğŸŒ€ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ**: è‡ªå·±ç›¸ä¼¼æ€§ãƒ»æƒ…å ±ç†è«–å¿œç”¨
    
    ğŸ† **é©æ–°çš„ç‰¹å¾´:**
    - **AIãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š**: 7ã¤ã®ä¸¦åˆ—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é‡å­çµ±åˆ
    - **95%è¶…é«˜ç²¾åº¦**: é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çµ±åˆ
    - **é©å¿œçš„å­¦ç¿’**: å¸‚å ´çŠ¶æ³è‡ªå‹•èªè­˜ãƒ»é–¾å€¤å‹•çš„èª¿æ•´
    - **ä¿¡é ¼åº¦ä»˜ãã‚·ã‚°ãƒŠãƒ«**: å„åˆ¤å®šã«ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«ä»˜ä¸
    - **ãƒãƒ«ãƒæ¬¡å…ƒåˆ†æ**: æ™‚é–“ãƒ»ç©ºé–“ãƒ»æƒ…å ±ãƒ»ç¢ºç‡ã®4æ¬¡å…ƒè§£æ
    """
    
    def __init__(self, 
                 super_smooth_period: int = 10,
                 zero_lag_period: int = 21,
                 realtime_window: int = 89,
                 quantum_window: int = 21,
                 fractal_window: int = 21,
                 entropy_window: int = 21,
                 src_type: str = 'hlc3',
                 slope_index: int = 1,
                 base_threshold: float = 0.003,
                 min_confidence: float = 0.3):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            super_smooth_period: ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æœŸé–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
            zero_lag_period: ã‚¼ãƒ­ãƒ©ã‚°EMAæœŸé–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 21ï¼‰
            realtime_window: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 89ï¼‰
            quantum_window: é‡å­åˆ†æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 21ï¼‰
            fractal_window: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«åˆ†æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 21ï¼‰
            entropy_window: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 21ï¼‰
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ ('close', 'hlc3', etc.)
            slope_index: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šæœŸé–“ (1ä»¥ä¸Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1)
            base_threshold: åŸºæœ¬é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.003 = 0.3%ï¼‰
            min_confidence: æœ€å°ä¿¡é ¼åº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.3ï¼‰
        """
        super().__init__(f"UltimateMAV3(ss={super_smooth_period},zl={zero_lag_period},rt={realtime_window},quantum={quantum_window},src={src_type},slope={slope_index},th={base_threshold:.3f},conf={min_confidence:.2f})")
        
        self.super_smooth_period = super_smooth_period
        self.zero_lag_period = zero_lag_period
        self.realtime_window = realtime_window
        self.quantum_window = quantum_window
        self.fractal_window = fractal_window
        self.entropy_window = entropy_window
        self.src_type = src_type
        self.slope_index = slope_index
        self.base_threshold = base_threshold
        self.min_confidence = min_confidence
        
        self.price_source_extractor = PriceSource()
        self._cache = {}
        self._result: Optional[UltimateMAV3Result] = None

    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateMAV3Result:
        """
        ğŸš€ Ultimate Moving Average V3 ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆ10æ®µéšé©æ–°çš„AIåˆ†æï¼‰
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯NumPyé…åˆ—ï¼‰
        
        Returns:
            UltimateMAV3Result: å…¨æ®µéšã®åˆ†æçµæœã¨AIãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’å«ã‚€çµæœ
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
            if isinstance(data, np.ndarray) and data.ndim == 1:
                src_prices = data.astype(np.float64)
                data_hash = hash(src_prices.tobytes())
                data_hash_key = f"{data_hash}_{self.super_smooth_period}_{self.zero_lag_period}_{self.realtime_window}_{self.quantum_window}_{self.slope_index}_{self.base_threshold}_{self.min_confidence}"
                
                if data_hash_key in self._cache and self._result is not None:
                    return self._result
            else:
                data_hash = self._get_data_hash(data)
                if data_hash in self._cache and self._result is not None:
                    return self._result

                src_prices = PriceSource.calculate_source(data, self.src_type)
                src_prices = src_prices.astype(np.float64)
                data_hash_key = data_hash

            data_length = len(src_prices)
            if data_length == 0:
                self.logger.warning("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
                return self._create_empty_result()

            # ğŸš€ 10æ®µéšé©æ–°çš„AIåˆ†æå‡¦ç†
            self.logger.info("ğŸš€ Ultimate MA V3 - 10æ®µéšé©æ–°çš„AIåˆ†æå®Ÿè¡Œä¸­...")
            
            # â‘ -â‘¥ å¾“æ¥ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
            self.logger.debug("ğŸ¯ â‘ -â‘¥ å¾“æ¥ã®6æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
            kalman_filtered = adaptive_kalman_filter_numba(src_prices)
            super_smoothed = super_smoother_filter_numba(kalman_filtered, self.super_smooth_period)
            zero_lag_prices = zero_lag_ema_numba(super_smoothed, self.zero_lag_period)
            amplitude, phase = hilbert_transform_filter_numba(zero_lag_prices)
            denoised_prices = adaptive_noise_reduction_numba(zero_lag_prices, amplitude)
            realtime_trends = real_time_trend_detector_numba(denoised_prices, self.realtime_window)
            
            # â‘¦ é‡å­ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå™¨
            self.logger.debug("ğŸŒŒ â‘¦ é‡å­ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå™¨å®Ÿè¡Œä¸­...")
            quantum_states, confidence_levels = quantum_trend_analyzer_numba(denoised_prices, self.quantum_window)
            
            # â‘§ ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åº¦
            self.logger.debug("ğŸ”„ â‘§ ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åº¦åˆ†æä¸­...")
            mtf_consensus = multi_timeframe_consensus_numba(denoised_prices)
            
            # â‘¨ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º
            self.logger.debug("ğŸ“Š â‘¨ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºä¸­...")
            volatility_regime = volatility_regime_detector_numba(denoised_prices, self.quantum_window)
            
            # â‘© ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ
            self.logger.debug("ğŸŒ€ â‘© ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æä¸­...")
            fractal_dimension = fractal_dimension_calculator_numba(denoised_prices, self.fractal_window)
            entropy_level = entropy_level_calculator_numba(denoised_prices, self.entropy_window)
            
            # ğŸš€ æœ€çµ‚çš„ãªç©¶æ¥µãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š
            self.logger.debug("ğŸš€ ç©¶æ¥µãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šçµ±åˆå‡¦ç†ä¸­...")
            trend_signals, trend_confidence = calculate_ultimate_trend_signals_v3_numba(
                denoised_prices, quantum_states, confidence_levels, mtf_consensus,
                volatility_regime, fractal_dimension, entropy_level,
                self.slope_index, self.base_threshold
            )
            
            # ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰çŠ¶æ…‹è¨ˆç®—
            trend_index, trend_value = calculate_current_trend_with_range_numba(trend_signals)
            trend_names = ['range', 'up', 'down']
            current_trend = trend_names[trend_index]
            
            # ç¾åœ¨ã®ä¿¡é ¼åº¦
            current_confidence = trend_confidence[-1] if len(trend_confidence) > 0 else 0.0

            result = UltimateMAV3Result(
                values=denoised_prices,
                raw_values=src_prices,
                kalman_values=kalman_filtered,
                super_smooth_values=super_smoothed,
                zero_lag_values=zero_lag_prices,
                amplitude=amplitude,
                phase=phase,
                realtime_trends=realtime_trends,
                trend_signals=trend_signals,
                trend_confidence=trend_confidence,
                multi_timeframe_consensus=mtf_consensus,
                volatility_regime=volatility_regime,
                fractal_dimension=fractal_dimension,
                entropy_level=entropy_level,
                quantum_state=quantum_states,
                current_trend=current_trend,
                current_trend_value=trend_value,
                current_confidence=current_confidence
            )

            self._result = result
            self._cache[data_hash_key] = self._result
            
            self.logger.info(f"âœ… Ultimate MA V3 è¨ˆç®—å®Œäº† - ãƒˆãƒ¬ãƒ³ãƒ‰: {current_trend} (ä¿¡é ¼åº¦: {current_confidence:.2f})")
            return self._result

        except Exception as e:
            import traceback
            self.logger.error(f"{self.name} è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}\n{traceback.format_exc()}")
            return self._create_error_result(len(data) if hasattr(data, '__len__') else 0)

    def _create_empty_result(self) -> UltimateMAV3Result:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        return UltimateMAV3Result(
            values=np.array([], dtype=np.float64),
            raw_values=np.array([], dtype=np.float64),
            kalman_values=np.array([], dtype=np.float64),
            super_smooth_values=np.array([], dtype=np.float64),
            zero_lag_values=np.array([], dtype=np.float64),
            amplitude=np.array([], dtype=np.float64),
            phase=np.array([], dtype=np.float64),
            realtime_trends=np.array([], dtype=np.float64),
            trend_signals=np.array([], dtype=np.int8),
            trend_confidence=np.array([], dtype=np.float64),
            multi_timeframe_consensus=np.array([], dtype=np.float64),
            volatility_regime=np.array([], dtype=np.float64),
            fractal_dimension=np.array([], dtype=np.float64),
            entropy_level=np.array([], dtype=np.float64),
            quantum_state=np.array([], dtype=np.float64),
            current_trend='range',
            current_trend_value=0,
            current_confidence=0.0
        )

    def _create_error_result(self, data_len: int) -> UltimateMAV3Result:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®çµæœã‚’ä½œæˆ"""
        return UltimateMAV3Result(
            values=np.full(data_len, np.nan, dtype=np.float64),
            raw_values=np.full(data_len, np.nan, dtype=np.float64),
            kalman_values=np.full(data_len, np.nan, dtype=np.float64),
            super_smooth_values=np.full(data_len, np.nan, dtype=np.float64),
            zero_lag_values=np.full(data_len, np.nan, dtype=np.float64),
            amplitude=np.full(data_len, np.nan, dtype=np.float64),
            phase=np.full(data_len, np.nan, dtype=np.float64),
            realtime_trends=np.full(data_len, np.nan, dtype=np.float64),
            trend_signals=np.zeros(data_len, dtype=np.int8),
            trend_confidence=np.zeros(data_len, dtype=np.float64),
            multi_timeframe_consensus=np.zeros(data_len, dtype=np.float64),
            volatility_regime=np.ones(data_len, dtype=np.float64),
            fractal_dimension=np.full(data_len, 1.5, dtype=np.float64),
            entropy_level=np.full(data_len, 0.5, dtype=np.float64),
            quantum_state=np.zeros(data_len, dtype=np.float64),
            current_trend='range',
            current_trend_value=0,
            current_confidence=0.0
        )

    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã™ã‚‹"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒãƒƒã‚·ãƒ¥è¨ˆç®—
        if isinstance(data, pd.DataFrame):
            data_hash_val = hash(data.values.tobytes())
        elif isinstance(data, np.ndarray):
            data_hash_val = hash(data.tobytes())
        else:
            data_hash_val = hash(str(data))

        param_str = (f"v3_ss={self.super_smooth_period}_zl={self.zero_lag_period}"
                    f"_rt={self.realtime_window}_quantum={self.quantum_window}"
                    f"_fractal={self.fractal_window}_entropy={self.entropy_window}"
                    f"_src={self.src_type}_slope={self.slope_index}"
                    f"_th={self.base_threshold}_conf={self.min_confidence}")
        return f"{data_hash_val}_{param_str}"

    # ä¾¿åˆ©ãƒ¡ã‚½ãƒƒãƒ‰
    def get_values(self) -> Optional[np.ndarray]:
        """æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿å€¤ã‚’å–å¾—"""
        return self._result.values.copy() if self._result is not None else None

    def get_trend_signals(self) -> Optional[np.ndarray]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡å·ã‚’å–å¾—"""
        return self._result.trend_signals.copy() if self._result is not None else None

    def get_trend_confidence(self) -> Optional[np.ndarray]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡é ¼åº¦ã‚’å–å¾—"""
        return self._result.trend_confidence.copy() if self._result is not None else None

    def get_quantum_analysis(self) -> dict:
        """é‡å­åˆ†æçµæœã‚’å–å¾—"""
        if self._result is None:
            return {}
        
        return {
            'quantum_state': self._result.quantum_state.copy(),
            'multi_timeframe_consensus': self._result.multi_timeframe_consensus.copy(),
            'volatility_regime': self._result.volatility_regime.copy(),
            'fractal_dimension': self._result.fractal_dimension.copy(),
            'entropy_level': self._result.entropy_level.copy(),
            'current_confidence': self._result.current_confidence
        }

    def reset(self) -> None:
        """ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._result = None
        self._cache = {} 