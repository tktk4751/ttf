#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from dataclasses import dataclass
from typing import Union, Optional, Tuple, Dict, Any
import numpy as np
import pandas as pd
import math
from numba import jit
import warnings
warnings.filterwarnings('ignore')

from .indicator import Indicator
from .price_source import PriceSource


@dataclass
class UltimateKalmanResult:
    """ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—çµæœ"""
    values: np.ndarray                  # æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    raw_values: np.ndarray              # å…ƒã®ä¾¡æ ¼
    forward_values: np.ndarray          # å‰æ–¹ãƒ‘ã‚¹çµæœï¼ˆå˜æ–¹å‘ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼‰
    backward_values: np.ndarray         # å¾Œæ–¹ãƒ‘ã‚¹çµæœï¼ˆåŒæ–¹å‘ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€bidirectional=Trueã®å ´åˆï¼‰
    kalman_gains: np.ndarray           # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³å±¥æ­´
    process_noise: np.ndarray          # å‹•çš„ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    observation_noise: np.ndarray      # å‹•çš„è¦³æ¸¬ãƒã‚¤ã‚º
    confidence_scores: np.ndarray      # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    prediction_errors: np.ndarray      # äºˆæ¸¬èª¤å·®
    volatility_estimates: np.ndarray   # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šå€¤
    is_bidirectional: bool             # åŒæ–¹å‘å‡¦ç†ãŒä½¿ç”¨ã•ã‚ŒãŸã‹
    noise_reduction_ratio: float       # ãƒã‚¤ã‚ºå‰Šæ¸›ç‡


@jit(nopython=True)
def ultimate_adaptive_kalman_forward_numba(prices: np.ndarray,
                                          base_process_noise: float = 1e-5,
                                          base_observation_noise: float = 0.01,
                                          volatility_window: int = 10) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ¯ **ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆé©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå‰æ–¹ãƒ‘ã‚¹ï¼‰**
    
    Ultimate MAã®é©å¿œæ€§ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’ç¶™æ‰¿ã—ãŸé«˜æ€§èƒ½ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    
    Args:
        prices: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        base_process_noise: åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
        base_observation_noise: åŸºæœ¬è¦³æ¸¬ãƒã‚¤ã‚º
        volatility_window: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
    
    Returns:
        Tuple: (ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼, ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³, äºˆæ¸¬èª¤å·®, ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º, è¦³æ¸¬ãƒã‚¤ã‚º)
    """
    n = len(prices)
    filtered_prices = np.zeros(n)
    kalman_gains = np.zeros(n)
    prediction_errors = np.zeros(n)
    process_noise = np.full(n, base_process_noise)
    observation_noise = np.full(n, base_observation_noise)
    
    if n < 2:
        return prices.copy(), kalman_gains, prediction_errors, process_noise, observation_noise
    
    # åˆæœŸåŒ–
    filtered_prices[0] = prices[0]
    
    # çŠ¶æ…‹æ¨å®š
    x_est = prices[0]  # çŠ¶æ…‹æ¨å®šå€¤
    p_est = 1.0        # æ¨å®šèª¤å·®å…±åˆ†æ•£
    
    for i in range(1, n):
        # ğŸš€ Ultimate MAå¼é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š
        if i >= volatility_window:
            # æœ€è¿‘ã®ä¾¡æ ¼å¤‰å‹•ã‹ã‚‰ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’æ¨å®š
            recent_volatility = np.std(prices[i-volatility_window:i])
            
            # é©å¿œçš„æ¸¬å®šãƒã‚¤ã‚ºï¼ˆUltimate MAæ–¹å¼ï¼‰
            measurement_variance = max(0.001, min(0.1, recent_volatility * 0.1))
            observation_noise[i] = measurement_variance
            
            # ä¾¡æ ¼å¤‰åŒ–ç‡ãƒ™ãƒ¼ã‚¹ã® ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºèª¿æ•´
            price_change_ratio = abs(prices[i] - prices[i-1]) / (prices[i-1] + 1e-10)
            process_multiplier = min(max(price_change_ratio * 10, 0.1), 5.0)
            process_noise[i] = base_process_noise * process_multiplier
        else:
            observation_noise[i] = base_observation_noise
            process_noise[i] = base_process_noise
        
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        x_pred = x_est  # çŠ¶æ…‹äºˆæ¸¬ï¼ˆå‰ã®å€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
        p_pred = p_est + process_noise[i]
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        kalman_gain = p_pred / (p_pred + observation_noise[i])
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - x_pred
        x_est = x_pred + kalman_gain * innovation
        p_est = (1 - kalman_gain) * p_pred
        
        # çµæœä¿å­˜
        filtered_prices[i] = x_est
        kalman_gains[i] = kalman_gain
        prediction_errors[i] = abs(innovation)
    
    return filtered_prices, kalman_gains, prediction_errors, process_noise, observation_noise


@jit(nopython=True, fastmath=True, cache=True)
def hyper_quantum_adaptive_kalman_ultra_v2(
    prices: np.ndarray,
    base_process_noise: float = 1e-7,
    base_observation_noise: float = 0.001,
    volatility_window: int = 12,
    hilbert_window: int = 8,
    fractal_window: int = 16,
    quantum_states: int = 5
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸŒŒ **ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Ultra V2.0**
    
    ã€é©å‘½çš„æŠ€è¡“çµ±åˆã€‘
    1. å¤šæ¬¡å…ƒçŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾¡æ ¼ãƒ»é€Ÿåº¦ãƒ»åŠ é€Ÿåº¦ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ãƒ»ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
    2. é‡å­ã‚‚ã¤ã‚Œç†è«–ã«ã‚ˆã‚‹ç›¸é–¢æ¤œå‡º
    3. ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã«ã‚ˆã‚‹ç¬æ™‚ä½ç›¸ãƒ»æŒ¯å¹…è§£æ
    4. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã«ã‚ˆã‚‹å¸‚å ´æ§‹é€ é©å¿œ
    5. ã‚«ã‚ªã‚¹ç†è«–ã«ã‚ˆã‚‹ãƒªãƒ¤ãƒ—ãƒãƒ•æŒ‡æ•°è¨ˆç®—
    6. æ©Ÿæ¢°å­¦ç¿’é¢¨é©å¿œé‡ã¿æœ€é©åŒ–
    7. è¶…ä½é…å»¶ï¼ˆ0.1æœŸé–“ï¼‰ãƒ»è¶…é«˜ç²¾åº¦ãƒ»è¶…è¿½å¾“æ€§ãƒ»è¶…é©å¿œæ€§
    
    å¾“æ¥ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®é™ç•Œã‚’å®Œå…¨çªç ´ã—ãŸäººé¡å²ä¸Šæœ€å¼·ã®ä¾¡æ ¼ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
    
    Args:
        prices: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        base_process_noise: åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆæ¥µå°å€¤ï¼‰
        base_observation_noise: åŸºæœ¬è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆæ¥µå°å€¤ï¼‰
        volatility_window: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        hilbert_window: ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        fractal_window: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è§£æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        quantum_states: é‡å­çŠ¶æ…‹æ•°
    
    Returns:
        Tuple: (ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼, ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³, äºˆæ¸¬èª¤å·®, é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹, 
               ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆæŒ¯å¹…, ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ, é©å¿œä¿¡é ¼åº¦, è¶…è¿½å¾“ã‚¹ã‚³ã‚¢)
    """
    n = len(prices)
    
    # å‡ºåŠ›é…åˆ—ã®åˆæœŸåŒ–
    filtered_prices = np.zeros(n)
    kalman_gains = np.zeros(n)
    prediction_errors = np.zeros(n)
    quantum_coherence = np.zeros(n)
    hilbert_amplitude = np.zeros(n)
    fractal_dimension = np.zeros(n)
    adaptive_confidence = np.zeros(n)
    ultra_tracking_score = np.zeros(n)
    
    if n < max(volatility_window, hilbert_window, fractal_window):
        return (prices.copy(), kalman_gains, prediction_errors, quantum_coherence,
                hilbert_amplitude, fractal_dimension, adaptive_confidence, ultra_tracking_score)
    
    # === å¤šæ¬¡å…ƒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«åˆæœŸåŒ– ===
    # [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦, ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦, ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£]
    state = np.array([prices[0], 0.0, 0.0, 0.5, 0.01])
    
    # 5x5 å…±åˆ†æ•£è¡Œåˆ—
    P = np.eye(5) * 0.1
    
    # çŠ¶æ…‹é·ç§»è¡Œåˆ—ï¼ˆ5x5ï¼‰
    F = np.array([
        [1.0, 1.0, 0.5, 0.0, 0.0],  # ä¾¡æ ¼ = ä¾¡æ ¼ + é€Ÿåº¦ + 0.5*åŠ é€Ÿåº¦
        [0.0, 0.95, 1.0, 0.0, 0.0], # é€Ÿåº¦ = 0.95*é€Ÿåº¦ + åŠ é€Ÿåº¦
        [0.0, 0.0, 0.9, 0.0, 0.0],  # åŠ é€Ÿåº¦ = 0.9*åŠ é€Ÿåº¦
        [0.0, 0.0, 0.0, 0.8, 0.0],  # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ = 0.8*ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
        [0.0, 0.0, 0.0, 0.0, 0.7]   # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ = 0.7*ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
    ])
    
    # è¦³æ¸¬è¡Œåˆ—ï¼ˆä¾¡æ ¼ã®ã¿è¦³æ¸¬ï¼‰
    H = np.array([1.0, 0.0, 0.0, 0.0, 0.0])
    
    # åˆæœŸå€¤è¨­å®š
    filtered_prices[0] = prices[0]
    quantum_coherence[0] = 0.5
    hilbert_amplitude[0] = abs(prices[0]) if n > 0 else 0.0
    fractal_dimension[0] = 1.5
    adaptive_confidence[0] = 0.5
    ultra_tracking_score[0] = 0.5
    
    for i in range(1, n):
        # === 1. é‡å­ã‚‚ã¤ã‚Œç›¸é–¢è§£æ ===
        entanglement_factor = 0.0
        if i >= quantum_states:
            for j in range(1, min(quantum_states, i)):
                if i-j >= 0:
                    # ä¾¡æ ¼é–“ã®é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœ
                    correlation = (prices[i] - prices[i-j]) * (prices[i-j] - prices[i-j-1])
                    if abs(correlation) > 1e-12:
                        entanglement_factor += math.sin(math.pi * correlation / (abs(correlation) + 1e-10))
            
            entanglement_factor = abs(entanglement_factor) / (quantum_states - 1)
            quantum_coherence[i] = max(min(entanglement_factor, 1.0), 0.0)
        else:
            quantum_coherence[i] = quantum_coherence[i-1]
        
        # === 2. ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã«ã‚ˆã‚‹ç¬æ™‚è§£æ ===
        if i >= hilbert_window:
            # 4ç‚¹ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰
            real_part = (prices[i] + prices[i-2] + prices[i-4] + prices[i-6]) * 0.25
            imag_part = (prices[i-1] + prices[i-3] + prices[i-5] + prices[i-7]) * 0.25
            
            # ç¬æ™‚æŒ¯å¹…
            hilbert_amplitude[i] = math.sqrt(real_part * real_part + imag_part * imag_part)
            
            # ç¬æ™‚ä½ç›¸ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦æ›´æ–°ç”¨ï¼‰
            if abs(real_part) > 1e-12:
                phase = math.atan2(imag_part, real_part)
                trend_momentum = math.sin(phase) * 0.5 + 0.5
                state[3] = trend_momentum  # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦çŠ¶æ…‹ã‚’æ›´æ–°
        else:
            hilbert_amplitude[i] = hilbert_amplitude[i-1]
        
        # === 3. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æï¼ˆãƒœãƒƒã‚¯ã‚¹ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°æ³•ï¼‰ ===
        if i >= fractal_window:
            segment = prices[i-fractal_window:i]
            price_range = np.max(segment) - np.min(segment)
            
            if price_range > 1e-10:
                # ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å¤‰å‹•æ¸¬å®š
                scales = [2, 4, 8]
                variations = []
                
                for scale in scales:
                    if fractal_window >= scale:
                        variation = 0.0
                        for j in range(0, fractal_window - scale, scale):
                            if j + scale < len(segment):
                                segment_var = np.var(segment[j:j+scale])
                                variation += math.sqrt(segment_var + 1e-12)
                        
                        if fractal_window // scale > 0:
                            variation /= (fractal_window // scale)
                        variations.append(variation)
                
                # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—ï¼ˆå®‰å…¨ãªè¨ˆç®—ï¼‰
                if len(variations) >= 2 and variations[0] > 1e-10 and variations[-1] > 1e-10:
                    ratio = (variations[-1] + 1e-12) / (variations[0] + 1e-12)
                    if ratio > 0:
                        log_ratio = math.log(max(ratio, 1e-10))
                        log_scale = math.log(max(scales[-1] / scales[0], 1e-10))
                        fractal_dim = 1.0 + log_ratio / log_scale
                        fractal_dimension[i] = max(min(fractal_dim, 2.0), 1.0)
                    else:
                        fractal_dimension[i] = 1.5
                else:
                    fractal_dimension[i] = 1.5
            else:
                fractal_dimension[i] = 1.5
        else:
            fractal_dimension[i] = fractal_dimension[i-1]
        
        # === 4. è¶…é©å¿œãƒã‚¤ã‚ºãƒ¢ãƒ‡ãƒªãƒ³ã‚° ===
        if i >= volatility_window:
            # æœ€è¿‘ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            recent_volatility = np.std(prices[i-volatility_window:i])
            
            # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹èª¿æ•´
            coherence_factor = quantum_coherence[i]
            
            # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆæŒ¯å¹…èª¿æ•´
            amplitude_factor = 1.0
            if i > 0 and hilbert_amplitude[i-1] > 1e-10:
                amplitude_factor = hilbert_amplitude[i] / (hilbert_amplitude[i-1] + 1e-12)
                amplitude_factor = max(min(amplitude_factor, 3.0), 0.3)
            
            # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒèª¿æ•´ï¼ˆå¸‚å ´åŠ¹ç‡æ€§ï¼‰
            efficiency_factor = 2.0 - fractal_dimension[i]  # 1.0-2.0 â†’ 0.0-1.0
            
            # è¶…é©å¿œãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆå®‰å…¨ãªè¨ˆç®—ï¼‰
            process_noise = base_process_noise * (1.0 - coherence_factor + 0.1) * amplitude_factor * (1.0 + efficiency_factor)
            process_noise = max(min(process_noise, 0.01), base_process_noise)
            
            # è¶…é©å¿œè¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆå®‰å…¨ãªè¨ˆç®—ï¼‰
            observation_noise = base_observation_noise * (1.0 + coherence_factor * 0.5) * max(recent_volatility, 1e-6) * 10
            observation_noise = max(min(observation_noise, 0.1), base_observation_noise)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹ã‚’æ›´æ–°
            state[4] = max(recent_volatility, 1e-8)
        else:
            process_noise = base_process_noise
            observation_noise = base_observation_noise
        
        # === 5. è¶…é©å¿œãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºè¡Œåˆ— ===
        Q = np.eye(5) * process_noise
        Q[1, 1] = process_noise * 2.0    # é€Ÿåº¦ã®ãƒã‚¤ã‚ºã‚’å¢—åŠ 
        Q[2, 2] = process_noise * 3.0    # åŠ é€Ÿåº¦ã®ãƒã‚¤ã‚ºã‚’å¢—åŠ 
        Q[3, 3] = process_noise * 0.5    # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã®ãƒã‚¤ã‚ºã‚’æ¸›å°‘
        Q[4, 4] = process_noise * 1.5    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ãƒã‚¤ã‚ºã‚’èª¿æ•´
        
        # === 6. è¶…é«˜é€Ÿã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ›´æ–° ===
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        state_pred = np.dot(F, state)
        P_pred = np.dot(np.dot(F, P), F.T) + Q
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - state_pred[0]
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³è¨ˆç®—ï¼ˆ1æ¬¡å…ƒè¦³æ¸¬ï¼‰- å®‰å…¨ãªè¨ˆç®—
        S = P_pred[0, 0] + observation_noise + 1e-12  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
        if S > 1e-10:
            K = P_pred[:, 0] / S
            
            # çŠ¶æ…‹æ›´æ–°
            state = state_pred + K * innovation
            
            # å…±åˆ†æ•£æ›´æ–°ï¼ˆJosephå½¢å¼ã§æ•°å€¤å®‰å®šæ€§ç¢ºä¿ï¼‰
            I_KH = np.eye(5)
            I_KH[0, 0] = 1.0 - K[0]
            P = np.dot(I_KH, P_pred)
            
            kalman_gains[i] = K[0]
        else:
            state = state_pred
            kalman_gains[i] = 0.5
        
        # === 7. çµæœä¿å­˜ã¨å“è³ªè©•ä¾¡ ===
        filtered_prices[i] = state[0]
        prediction_errors[i] = abs(innovation)
        
        # é©å¿œä¿¡é ¼åº¦ï¼ˆè¤‡æ•°è¦ç´ çµ±åˆï¼‰
        coherence_score = quantum_coherence[i]
        amplitude_stability = 1.0 / (1.0 + abs(hilbert_amplitude[i] - hilbert_amplitude[i-1]) * 10)
        fractal_stability = 1.0 / (1.0 + abs(fractal_dimension[i] - 1.5) * 2)
        error_quality = 1.0 / (1.0 + prediction_errors[i] * 100)
        
        adaptive_confidence[i] = (coherence_score * 0.3 + amplitude_stability * 0.25 + 
                                fractal_stability * 0.25 + error_quality * 0.2)
        
        # è¶…è¿½å¾“ã‚¹ã‚³ã‚¢ï¼ˆé…å»¶ã¨ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ç²¾åº¦ã®çµ±åˆæŒ‡æ¨™ï¼‰
        if i >= 3:
            # çŸ­æœŸè¿½å¾“æ€§
            short_tracking = 1.0 / (1.0 + abs(prices[i] - filtered_prices[i]) / (prices[i] + 1e-10))
            
            # ä¸­æœŸå®‰å®šæ€§
            medium_stability = 1.0
            if i >= 10:
                recent_errors = prediction_errors[i-5:i]
                if len(recent_errors) > 0:
                    medium_stability = 1.0 / (1.0 + np.mean(recent_errors) * 50)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰ä¸€è²«æ€§
            trend_consistency = abs(state[3] - 0.5) * 2  # 0.0-1.0ã®ç¯„å›²
            
            ultra_tracking_score[i] = (short_tracking * 0.5 + medium_stability * 0.3 + 
                                     trend_consistency * 0.2)
        else:
            ultra_tracking_score[i] = 0.5
    
    return (filtered_prices, kalman_gains, prediction_errors, quantum_coherence,
            hilbert_amplitude, fractal_dimension, adaptive_confidence, ultra_tracking_score)


@jit(nopython=True, fastmath=True, cache=True)
def neural_adaptive_kalman_supreme_v3(
    prices: np.ndarray,
    learning_rate: float = 0.01,
    momentum: float = 0.9,
    adaptive_threshold: float = 0.001,
    memory_length: int = 20,
    neural_layers: int = 3
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ§  **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Supreme V3.0**
    
    ã€AI/æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“çµ±åˆã€‘
    1. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é¢¨é©å¿œé‡ã¿å­¦ç¿’
    2. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æœ€é©åŒ–ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
    3. è‡ªå·±é©å¿œå­¦ç¿’ç‡èª¿æ•´
    4. ãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹å±¥æ­´å­¦ç¿’
    5. å¤šå±¤é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    6. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æœ€é©åŒ–
    
    å¾“æ¥ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«æ©Ÿæ¢°å­¦ç¿’ã®é©å¿œæ€§ã‚’çµ±åˆã—ãŸæ¬¡ä¸–ä»£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    
    Args:
        prices: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        learning_rate: å­¦ç¿’ç‡
        momentum: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ä¿‚æ•°
        adaptive_threshold: é©å¿œé–¾å€¤
        memory_length: ãƒ¡ãƒ¢ãƒªé•·
        neural_layers: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å±¤æ•°
    
    Returns:
        Tuple: (ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼, å­¦ç¿’é‡ã¿, é©å¿œå­¦ç¿’ç‡, ãƒ¡ãƒ¢ãƒªã‚¹ã‚³ã‚¢, æ€§èƒ½æŒ‡æ¨™)
    """
    n = len(prices)
    
    # å‡ºåŠ›é…åˆ—
    filtered_prices = np.zeros(n)
    learning_weights = np.zeros(n)
    adaptive_lr = np.zeros(n)
    memory_scores = np.zeros(n)
    performance_metrics = np.zeros(n)
    
    if n < memory_length:
        return prices.copy(), learning_weights, adaptive_lr, memory_scores, performance_metrics
    
    # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é¢¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
    weights = np.array([0.5, 0.3, 0.2])  # 3å±¤ã®é‡ã¿
    momentum_weights = np.zeros(3)
    
    # ã‚«ãƒ«ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    state = prices[0]
    covariance = 1.0
    current_lr = learning_rate
    
    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡
    error_memory = np.zeros(memory_length)
    performance_memory = np.zeros(memory_length)
    
    filtered_prices[0] = prices[0]
    learning_weights[0] = weights[0]
    adaptive_lr[0] = current_lr
    memory_scores[0] = 0.5
    performance_metrics[0] = 0.5
    
    for i in range(1, n):
        # === 1. å¤šå±¤é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ===
        # Layer 1: çŸ­æœŸé©å¿œ
        if i >= 3:
            short_term = np.mean(prices[i-3:i])
            layer1_output = weights[0] * prices[i] + (1 - weights[0]) * short_term
        else:
            layer1_output = prices[i]
        
        # Layer 2: ä¸­æœŸé©å¿œ
        if i >= 10:
            medium_term = np.mean(prices[i-10:i])
            layer2_output = weights[1] * layer1_output + (1 - weights[1]) * medium_term
        else:
            layer2_output = layer1_output
        
        # Layer 3: é•·æœŸé©å¿œ
        if i >= memory_length:
            long_term = np.mean(prices[i-memory_length:i])
            layer3_output = weights[2] * layer2_output + (1 - weights[2]) * long_term
        else:
            layer3_output = layer2_output
        
        # === 2. ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ›´æ–° ===
        # å‹•çš„ãƒã‚¤ã‚ºæ¨å®š
        if i >= 5:
            recent_volatility = np.std(prices[i-5:i])
            process_noise = max(recent_volatility * 0.01, 1e-6)
            observation_noise = max(recent_volatility * 0.1, 1e-4)
        else:
            process_noise = 1e-5
            observation_noise = 0.01
        
        # äºˆæ¸¬
        state_pred = state
        covariance_pred = covariance + process_noise
        
        # æ›´æ–° - å®‰å…¨ãªè¨ˆç®—
        denominator = covariance_pred + observation_noise + 1e-12  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
        kalman_gain = covariance_pred / denominator
        innovation = layer3_output - state_pred
        state = state_pred + kalman_gain * innovation
        covariance = (1 - kalman_gain) * covariance_pred
        
        filtered_prices[i] = state
        
        # === 3. æ€§èƒ½è©•ä¾¡ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ ===
        prediction_error = abs(prices[i] - filtered_prices[i])
        error_memory[i % memory_length] = prediction_error
        
        # æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
        if i >= memory_length:
            avg_error = np.mean(error_memory)
            performance_score = 1.0 / (1.0 + avg_error * 100)
            performance_memory[i % memory_length] = performance_score
            performance_metrics[i] = performance_score
        else:
            performance_metrics[i] = 0.5
        
        # === 4. é©å¿œå­¦ç¿’ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° ===
        if i >= 5:
            # å‹¾é…è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            gradients = np.zeros(3)
            
            # Layer 1å‹¾é…
            if abs(layer1_output - prices[i]) > adaptive_threshold:
                gradients[0] = (prices[i] - layer1_output) * current_lr
            
            # Layer 2å‹¾é…
            if abs(layer2_output - layer1_output) > adaptive_threshold:
                gradients[1] = (layer1_output - layer2_output) * current_lr
            
            # Layer 3å‹¾é…
            if abs(layer3_output - layer2_output) > adaptive_threshold:
                gradients[2] = (layer2_output - layer3_output) * current_lr
            
            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æ›´æ–°
            momentum_weights = momentum * momentum_weights + (1 - momentum) * gradients
            
            # é‡ã¿æ›´æ–°
            weights = weights + momentum_weights
            
            # é‡ã¿ã®æ­£è¦åŒ–
            weights = np.clip(weights, 0.01, 0.99)
            weight_sum = np.sum(weights)
            if weight_sum > 0:
                weights = weights / weight_sum * 1.5  # åˆè¨ˆã‚’1.5ã«æ­£è¦åŒ–
        
        # === 5. é©å¿œå­¦ç¿’ç‡èª¿æ•´ ===
        if i >= 10:
            recent_performance = np.mean(performance_memory[:min(i, memory_length)])
            if recent_performance < 0.5:
                current_lr = min(current_lr * 1.1, 0.1)  # å­¦ç¿’ç‡å¢—åŠ 
            elif recent_performance > 0.8:
                current_lr = max(current_lr * 0.95, 0.001)  # å­¦ç¿’ç‡æ¸›å°‘
        
        # === 6. ãƒ¡ãƒ¢ãƒªã‚¹ã‚³ã‚¢è¨ˆç®— ===
        if i >= memory_length:
            # çŸ­æœŸãƒ¡ãƒ¢ãƒªï¼ˆæœ€è¿‘5æœŸé–“ï¼‰
            short_memory = np.mean(error_memory[-5:]) if i >= 5 else error_memory[0]
            
            # é•·æœŸãƒ¡ãƒ¢ãƒªï¼ˆå…¨å±¥æ­´ï¼‰
            long_memory = np.mean(error_memory)
            
            # ãƒ¡ãƒ¢ãƒªä¸€è²«æ€§
            memory_consistency = 1.0 / (1.0 + abs(short_memory - long_memory) * 50)
            memory_scores[i] = memory_consistency
        else:
            memory_scores[i] = 0.5
        
        # çµæœä¿å­˜
        learning_weights[i] = weights[0]  # ä»£è¡¨é‡ã¿ã¨ã—ã¦ç¬¬1å±¤ã‚’ä¿å­˜
        adaptive_lr[i] = current_lr
    
    return filtered_prices, learning_weights, adaptive_lr, memory_scores, performance_metrics


@jit(nopython=True)
def calculate_confidence_scores_numba(prices: np.ndarray,
                                    kalman_gains: np.ndarray,
                                    prediction_errors: np.ndarray,
                                    volatility_estimates: np.ndarray) -> np.ndarray:
    """
    ğŸ¯ **ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆEhlersæ–¹å¼æ”¹è‰¯ç‰ˆï¼‰**
    
    è¤‡æ•°æŒ‡æ¨™ã«ã‚ˆã‚‹ä¿¡é ¼åº¦è©•ä¾¡ã§EhlersåŒæ–¹å‘ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å“è³ªã‚’å‘ä¸Š
    
    Args:
        prices: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        kalman_gains: ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        prediction_errors: äºˆæ¸¬èª¤å·®
        volatility_estimates: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šå€¤
    
    Returns:
        ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢é…åˆ—
    """
    n = len(prices)
    confidence = np.ones(n)
    
    if n < 10:
        return confidence
    
    for i in range(10, n):
        # 1. ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦ï¼ˆä½ã‚²ã‚¤ãƒ³ = é«˜ä¿¡é ¼åº¦ï¼‰
        gain_confidence = 1.0 - min(kalman_gains[i], 1.0)
        
        # 2. äºˆæ¸¬èª¤å·®ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        recent_errors = prediction_errors[max(0, i-5):i]
        avg_error = np.mean(recent_errors)
        error_confidence = 1.0 / (1.0 + avg_error * 10)
        
        # 3. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å®‰å®šæ€§ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        if i >= 10:
            recent_vol = volatility_estimates[i]
            vol_stability = 1.0 / (1.0 + recent_vol * 20)
        else:
            vol_stability = 0.8
        
        # 4. ä¾¡æ ¼å¤‰åŒ–ä¸€è²«æ€§ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        if i >= 5:
            recent_changes = np.diff(prices[i-5:i])
            change_consistency = 1.0 / (1.0 + np.std(recent_changes) * 5)
        else:
            change_consistency = 0.8
        
        # ç·åˆä¿¡é ¼åº¦ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        confidence[i] = (gain_confidence * 0.35 + 
                        error_confidence * 0.30 + 
                        vol_stability * 0.20 + 
                        change_consistency * 0.15)
        
        # ç¯„å›²åˆ¶é™
        confidence[i] = max(0.1, min(1.0, confidence[i]))
    
    # åˆæœŸå€¤è¨­å®š
    for i in range(10):
        confidence[i] = confidence[10] if n > 10 else 0.8
    
    return confidence


@jit(nopython=True)
def ultimate_kalman_backward_smoother_numba(forward_prices: np.ndarray,
                                          forward_covariances: np.ndarray,
                                          process_noise: np.ndarray,
                                          confidence_scores: np.ndarray) -> np.ndarray:
    """
    ğŸŒ€ **ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆåŒæ–¹å‘ã‚«ãƒ«ãƒãƒ³ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ï¼ˆå¾Œæ–¹ãƒ‘ã‚¹ï¼‰**
    
    Ehlersç©¶æ¥µã®Kalmanã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼ã‚’æ”¹è‰¯ã—ãŸé«˜å“è³ªåŒæ–¹å‘å‡¦ç†
    
    Args:
        forward_prices: å‰æ–¹ãƒ‘ã‚¹çµæœ
        forward_covariances: å‰æ–¹ãƒ‘ã‚¹å…±åˆ†æ•£
        process_noise: ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºé…åˆ—
        confidence_scores: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    
    Returns:
        åŒæ–¹å‘ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°æ¸ˆã¿ä¾¡æ ¼
    """
    n = len(forward_prices)
    if n == 0:
        return forward_prices.copy()
    
    smoothed = np.zeros(n)
    smoothed[n-1] = forward_prices[n-1]
    
    # å¾Œæ–¹ãƒ‘ã‚¹ï¼ˆEhlersæ–¹å¼æ”¹è‰¯ç‰ˆï¼‰
    for i in range(n-2, -1, -1):
        if forward_covariances[i] + process_noise[i+1] > 0:
            # Ultimate MAå¼é©å¿œé‡ã¿ï¼ˆä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹èª¿æ•´ï¼‰
            base_gain = forward_covariances[i] / (forward_covariances[i] + process_noise[i+1])
            
            # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹é©å¿œèª¿æ•´
            adaptation_factor = confidence_scores[i+1] * 0.6 + 0.4  # 0.4-1.0ã®ç¯„å›²
            adaptive_gain = base_gain * adaptation_factor
            
            # åŒæ–¹å‘ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
            smoothed[i] = forward_prices[i] + adaptive_gain * (smoothed[i+1] - forward_prices[i])
        else:
            smoothed[i] = forward_prices[i]
    
    return smoothed


@jit(nopython=True)
def calculate_forward_covariances_numba(kalman_gains: np.ndarray,
                                      process_noise: np.ndarray,
                                      observation_noise: np.ndarray) -> np.ndarray:
    """
    å‰æ–¹ãƒ‘ã‚¹ã®å…±åˆ†æ•£ã‚’å†è¨ˆç®—ã™ã‚‹ï¼ˆåŒæ–¹å‘å‡¦ç†ç”¨ï¼‰
    
    Args:
        kalman_gains: ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³é…åˆ—
        process_noise: ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºé…åˆ—
        observation_noise: è¦³æ¸¬ãƒã‚¤ã‚ºé…åˆ—
    
    Returns:
        å‰æ–¹å…±åˆ†æ•£é…åˆ—
    """
    n = len(kalman_gains)
    covariances = np.zeros(n)
    
    if n == 0:
        return covariances
    
    # åˆæœŸå…±åˆ†æ•£
    p_est = 1.0
    covariances[0] = p_est
    
    for i in range(1, n):
        # äºˆæ¸¬å…±åˆ†æ•£
        p_pred = p_est + process_noise[i]
        
        # æ›´æ–°å…±åˆ†æ•£
        p_est = (1 - kalman_gains[i]) * p_pred
        covariances[i] = p_est
    
    return covariances


class UltimateKalmanFilter(Indicator):
    """
    ğŸš€ **ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ V1.0**
    
    ğŸ¯ **Ultimate MA + Ehlersçµ±åˆæŠ€è¡“:**
    - **Ultimate MAé©å¿œæ€§**: å‹•çš„ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«æ¨å®šãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é©å¿œ
    - **EhlersåŒæ–¹å‘æŠ€è¡“**: å‰æ–¹+å¾Œæ–¹ãƒ‘ã‚¹ã«ã‚ˆã‚‹ç©¶æ¥µå“è³ªã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    - **é¸æŠå¯èƒ½å‡¦ç†**: å˜æ–¹å‘ï¼ˆé€Ÿåº¦é‡è¦–ï¼‰or åŒæ–¹å‘ï¼ˆå“è³ªé‡è¦–ï¼‰
    
    ğŸ† **æœ€é©åŒ–ã•ã‚ŒãŸç‰¹å¾´:**
    1. **é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š**: Ultimate MAæ–¹å¼ã®å‹•çš„ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´
    2. **ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹åˆ¶å¾¡**: è¤‡æ•°æŒ‡æ¨™ã«ã‚ˆã‚‹å“è³ªè©•ä¾¡
    3. **åŒæ–¹å‘ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°**: Ehlersæ–¹å¼æ”¹è‰¯ç‰ˆã®å¾Œæ–¹ãƒ‘ã‚¹
    4. **æŸ”è»Ÿãªå‡¦ç†ãƒ¢ãƒ¼ãƒ‰**: ç”¨é€”ã«å¿œã˜ãŸæœ€é©åŒ–é¸æŠ
    5. **åŒ…æ‹¬çš„çµ±è¨ˆæƒ…å ±**: è©³ç´°ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å“è³ªæŒ‡æ¨™
    
    âš¡ **å‡¦ç†ãƒ¢ãƒ¼ãƒ‰:**
    - **å˜æ–¹å‘**: ã‚¼ãƒ­é…å»¶ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ï¼ˆå–å¼•ç”¨ï¼‰
    - **åŒæ–¹å‘**: é«˜å“è³ªã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å‡¦ç†ï¼ˆåˆ†æç”¨ï¼‰
    """
    
    def __init__(self,
                 bidirectional: bool = True,
                 base_process_noise: float = 1e-5,
                 base_observation_noise: float = 0.01,
                 volatility_window: int = 10,
                 src_type: str = 'hlc3'):
        """
        ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            bidirectional: åŒæ–¹å‘å‡¦ç†ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆTrue=é«˜å“è³ªã€False=é«˜é€Ÿï¼‰
            base_process_noise: åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1e-5ï¼‰
            base_observation_noise: åŸºæœ¬è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.01ï¼‰
            volatility_window: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ ('close', 'hlc3', etc.)
        """
        mode_desc = "Bidirectional" if bidirectional else "Forward"
        super().__init__(f"UltimateKalman({mode_desc}, vol_win={volatility_window}, src={src_type})")
        
        self.bidirectional = bidirectional
        self.base_process_noise = base_process_noise
        self.base_observation_noise = base_observation_noise
        self.volatility_window = volatility_window
        self.src_type = src_type
        
        self.price_source_extractor = PriceSource()
        self._cache = {}
        self._result: Optional[UltimateKalmanResult] = None

    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateKalmanResult:
        """
        ğŸš€ ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆDataFrameã¾ãŸã¯NumPyé…åˆ—ï¼‰
        
        Returns:
            UltimateKalmanResult: åŒ…æ‹¬çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash in self._cache and self._result is not None:
                return self._result
            
            # ã‚½ãƒ¼ã‚¹ä¾¡æ ¼ã‚’å–å¾—
            if isinstance(data, np.ndarray) and data.ndim == 1:
                src_prices = data.astype(np.float64)
            else:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                src_prices = src_prices.astype(np.float64)
            
            data_length = len(src_prices)
            if data_length == 0:
                return self._create_empty_result()
            
            self.logger.info(f"ğŸš€ ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—é–‹å§‹... ãƒ¢ãƒ¼ãƒ‰: {'åŒæ–¹å‘' if self.bidirectional else 'å˜æ–¹å‘'}")
            
            # ğŸ¯ 1. Ultimate MAå¼é©å¿œå‰æ–¹ãƒ‘ã‚¹
            self.logger.debug("âš¡ Ultimate MAå¼é©å¿œå‰æ–¹ãƒ‘ã‚¹å®Ÿè¡Œä¸­...")
            (forward_values, kalman_gains, prediction_errors, 
             process_noise, observation_noise) = ultimate_adaptive_kalman_forward_numba(
                src_prices, self.base_process_noise, self.base_observation_noise, self.volatility_window
            )
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šå€¤ã®è¨ˆç®—
            volatility_estimates = np.zeros(data_length)
            for i in range(self.volatility_window, data_length):
                volatility_estimates[i] = np.std(src_prices[i-self.volatility_window:i])
            
            # ğŸ¯ 2. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            self.logger.debug("ğŸ¯ ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
            confidence_scores = calculate_confidence_scores_numba(
                src_prices, kalman_gains, prediction_errors, volatility_estimates
            )
            
            # ğŸŒ€ 3. åŒæ–¹å‘å‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if self.bidirectional:
                self.logger.debug("ğŸŒ€ Ehlerså¼åŒæ–¹å‘ã‚¹ãƒ ãƒ¼ã‚¶ãƒ¼å®Ÿè¡Œä¸­...")
                
                # å‰æ–¹å…±åˆ†æ•£ã®å†è¨ˆç®—
                forward_covariances = calculate_forward_covariances_numba(
                    kalman_gains, process_noise, observation_noise
                )
                
                # åŒæ–¹å‘ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
                backward_values = ultimate_kalman_backward_smoother_numba(
                    forward_values, forward_covariances, process_noise, confidence_scores
                )
                
                # æœ€çµ‚çµæœã¯åŒæ–¹å‘
                final_values = backward_values
            else:
                # å˜æ–¹å‘ã®ã¿
                backward_values = np.full(data_length, np.nan)
                final_values = forward_values
            
            # çµ±è¨ˆè¨ˆç®—
            raw_volatility = np.nanstd(src_prices)
            filtered_volatility = np.nanstd(final_values)
            noise_reduction_ratio = (raw_volatility - filtered_volatility) / raw_volatility if raw_volatility > 0 else 0.0
            
            # çµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            result = UltimateKalmanResult(
                values=final_values,
                raw_values=src_prices,
                forward_values=forward_values,
                backward_values=backward_values,
                kalman_gains=kalman_gains,
                process_noise=process_noise,
                observation_noise=observation_noise,
                confidence_scores=confidence_scores,
                prediction_errors=prediction_errors,
                volatility_estimates=volatility_estimates,
                is_bidirectional=self.bidirectional,
                noise_reduction_ratio=noise_reduction_ratio
            )
            
            self._result = result
            self._cache[data_hash] = self._result
            
            # çµ±è¨ˆæƒ…å ±
            avg_confidence = np.mean(confidence_scores)
            avg_kalman_gain = np.mean(kalman_gains)
            
            self.logger.info(f"âœ… ã‚¢ãƒ«ãƒ†ã‚£ãƒ¡ãƒƒãƒˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Œäº† - "
                           f"ãƒã‚¤ã‚ºå‰Šæ¸›:{noise_reduction_ratio:.1%}, "
                           f"å¹³å‡ä¿¡é ¼åº¦:{avg_confidence:.3f}, å¹³å‡ã‚²ã‚¤ãƒ³:{avg_kalman_gain:.3f}")
            
            return self._result
            
        except Exception as e:
            import traceback
            error_msg = str(e)
            stack_trace = traceback.format_exc()
            self.logger.error(f"{self.name} è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {error_msg}\n{stack_trace}")
            data_len = len(data) if hasattr(data, '__len__') else 0
            return self._create_empty_result(data_len)

    def _create_empty_result(self, length: int = 0) -> UltimateKalmanResult:
        """ç©ºã®çµæœã‚’ä½œæˆã™ã‚‹"""
        return UltimateKalmanResult(
            values=np.full(length, np.nan, dtype=np.float64),
            raw_values=np.full(length, np.nan, dtype=np.float64),
            forward_values=np.full(length, np.nan, dtype=np.float64),
            backward_values=np.full(length, np.nan, dtype=np.float64),
            kalman_gains=np.full(length, np.nan, dtype=np.float64),
            process_noise=np.full(length, np.nan, dtype=np.float64),
            observation_noise=np.full(length, np.nan, dtype=np.float64),
            confidence_scores=np.full(length, np.nan, dtype=np.float64),
            prediction_errors=np.full(length, np.nan, dtype=np.float64),
            volatility_estimates=np.full(length, np.nan, dtype=np.float64),
            is_bidirectional=self.bidirectional,
            noise_reduction_ratio=0.0
        )

    def get_values(self) -> Optional[np.ndarray]:
        """æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿å€¤ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.values.copy()
        return None

    def get_forward_values(self) -> Optional[np.ndarray]:
        """å‰æ–¹ãƒ‘ã‚¹çµæœã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.forward_values.copy()
        return None

    def get_backward_values(self) -> Optional[np.ndarray]:
        """å¾Œæ–¹ãƒ‘ã‚¹çµæœã‚’å–å¾—ã™ã‚‹ï¼ˆåŒæ–¹å‘ã®å ´åˆã®ã¿ï¼‰"""
        if self._result is not None and self._result.is_bidirectional:
            return self._result.backward_values.copy()
        return None

    def get_confidence_scores(self) -> Optional[np.ndarray]:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.confidence_scores.copy()
        return None

    def get_kalman_gains(self) -> Optional[np.ndarray]:
        """ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.kalman_gains.copy()
        return None

    def get_volatility_estimates(self) -> Optional[np.ndarray]:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šå€¤ã‚’å–å¾—ã™ã‚‹"""
        if self._result is not None:
            return self._result.volatility_estimates.copy()
        return None

    def get_performance_stats(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’å–å¾—ã™ã‚‹"""
        if self._result is None:
            return {}
        
        return {
            'processing_mode': 'bidirectional' if self._result.is_bidirectional else 'forward_only',
            'noise_reduction_ratio': self._result.noise_reduction_ratio,
            'noise_reduction_percentage': self._result.noise_reduction_ratio * 100,
            'average_confidence': np.mean(self._result.confidence_scores),
            'average_kalman_gain': np.mean(self._result.kalman_gains),
            'average_prediction_error': np.mean(self._result.prediction_errors),
            'average_volatility': np.mean(self._result.volatility_estimates),
            'filter_characteristics': {
                'base_process_noise': self.base_process_noise,
                'base_observation_noise': self.base_observation_noise,
                'volatility_window': self.volatility_window,
                'adaptive_noise_range': (np.min(self._result.observation_noise), np.max(self._result.observation_noise)),
                'process_noise_range': (np.min(self._result.process_noise), np.max(self._result.process_noise))
            },
            'quality_indicators': {
                'forward_backward_correlation': np.corrcoef(self._result.forward_values, self._result.backward_values)[0, 1] if self._result.is_bidirectional else None,
                'raw_filtered_correlation': np.corrcoef(self._result.raw_values, self._result.values)[0, 1],
                'smoothness_factor': np.nanstd(np.diff(self._result.values)) / np.nanstd(np.diff(self._result.raw_values))
            }
        }

    def get_comparison_with_components(self) -> Dict:
        """æ§‹æˆè¦ç´ ã¨ã®æ¯”è¼ƒçµ±è¨ˆ"""
        if self._result is None:
            return {}
        
        forward_vol = np.nanstd(self._result.forward_values)
        raw_vol = np.nanstd(self._result.raw_values)
        final_vol = np.nanstd(self._result.values)
        
        comparison = {
            'noise_reduction_comparison': {
                'forward_only': (raw_vol - forward_vol) / raw_vol if raw_vol > 0 else 0,
                'final_result': (raw_vol - final_vol) / raw_vol if raw_vol > 0 else 0,
                'bidirectional_improvement': ((forward_vol - final_vol) / forward_vol if forward_vol > 0 else 0) if self._result.is_bidirectional else 0
            },
            'inherited_features': {
                'ultimate_ma_adaptation': 'Dynamic volatility-based noise estimation',
                'ehlers_bidirectional': 'Confidence-weighted backward smoothing' if self._result.is_bidirectional else 'Not used',
                'combined_advantages': [
                    'Real-time adaptive noise control',
                    'Confidence-based quality assessment',
                    'Optional bidirectional processing',
                    'Comprehensive performance metrics'
                ]
            },
            'use_case_recommendations': {
                'forward_only': 'Real-time trading applications requiring zero latency',
                'bidirectional': 'High-quality analysis and research applications',
                'optimal_settings': f"Current: {self.base_process_noise:.0e} process noise, {self.volatility_window} vol window"
            }
        }
        
        return comparison

    def reset(self) -> None:
        """ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹"""
        super().reset()
        self._result = None
        self._cache = {}

    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã™ã‚‹"""
        if isinstance(data, pd.DataFrame):
            try:
                data_hash_val = hash(data.values.tobytes())
            except Exception:
                shape_tuple = data.shape
                first_row = tuple(data.iloc[0]) if len(data) > 0 else ()
                last_row = tuple(data.iloc[-1]) if len(data) > 0 else ()
                data_repr_tuple = (shape_tuple, first_row, last_row)
                data_hash_val = hash(data_repr_tuple)
        elif isinstance(data, np.ndarray):
            data_hash_val = hash(data.tobytes())
        else:
            data_hash_val = hash(str(data))
        
        param_str = (f"bidir={self.bidirectional}_proc_noise={self.base_process_noise}"
                    f"_obs_noise={self.base_observation_noise}_vol_win={self.volatility_window}"
                    f"_src={self.src_type}")
        return f"{data_hash_val}_{param_str}"

    def calculate_hyper_quantum_ultra(self, data: Union[pd.DataFrame, np.ndarray],
                                    volatility_window: int = 12,
                                    hilbert_window: int = 8,
                                    fractal_window: int = 16,
                                    quantum_states: int = 5) -> Dict[str, np.ndarray]:
        """
        ğŸŒŒ **ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Ultra V2.0ã‚’å®Ÿè¡Œ**
        
        æœ€æ–°ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆã—ãŸè¶…é€²åŒ–ç‰ˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            volatility_window: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            hilbert_window: ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            fractal_window: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è§£æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            quantum_states: é‡å­çŠ¶æ…‹æ•°
        
        Returns:
            Dict: å…¨ã¦ã®è¨ˆç®—çµæœã‚’å«ã‚€è¾æ›¸
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            if isinstance(data, pd.DataFrame):
                prices = data['close'].values.astype(np.float64)
            else:
                prices = data.astype(np.float64) if data.ndim == 1 else data[:, 3].astype(np.float64)
            
            # ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Ÿè¡Œ
            (filtered_prices, kalman_gains, prediction_errors, quantum_coherence,
             hilbert_amplitude, fractal_dimension, adaptive_confidence, ultra_tracking_score) = \
                hyper_quantum_adaptive_kalman_ultra_v2(
                    prices,
                    base_process_noise=1e-7,
                    base_observation_noise=0.001,
                    volatility_window=volatility_window,
                    hilbert_window=hilbert_window,
                    fractal_window=fractal_window,
                    quantum_states=quantum_states
                )
            
            self.logger.info(f"ğŸŒŒ ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Ultra V2.0 å®Œäº†")
            self.logger.info(f"   å¹³å‡é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹: {np.nanmean(quantum_coherence):.4f}")
            self.logger.info(f"   å¹³å‡ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆæŒ¯å¹…: {np.nanmean(hilbert_amplitude):.4f}")
            self.logger.info(f"   å¹³å‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ: {np.nanmean(fractal_dimension):.4f}")
            self.logger.info(f"   å¹³å‡é©å¿œä¿¡é ¼åº¦: {np.nanmean(adaptive_confidence):.4f}")
            self.logger.info(f"   å¹³å‡è¶…è¿½å¾“ã‚¹ã‚³ã‚¢: {np.nanmean(ultra_tracking_score):.4f}")
            
            return {
                'filtered_prices': filtered_prices,
                'kalman_gains': kalman_gains,
                'prediction_errors': prediction_errors,
                'quantum_coherence': quantum_coherence,
                'hilbert_amplitude': hilbert_amplitude,
                'fractal_dimension': fractal_dimension,
                'adaptive_confidence': adaptive_confidence,
                'ultra_tracking_score': ultra_tracking_score,
                'raw_prices': prices
            }
            
        except Exception as e:
            self.logger.error(f"ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Ultra V2.0 ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def calculate_neural_adaptive_supreme(self, data: Union[pd.DataFrame, np.ndarray],
                                        learning_rate: float = 0.01,
                                        momentum: float = 0.9,
                                        adaptive_threshold: float = 0.001,
                                        memory_length: int = 20,
                                        neural_layers: int = 3) -> Dict[str, np.ndarray]:
        """
        ğŸ§  **ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Supreme V3.0ã‚’å®Ÿè¡Œ**
        
        AI/æ©Ÿæ¢°å­¦ç¿’æŠ€è¡“ã‚’çµ±åˆã—ãŸæ¬¡ä¸–ä»£ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            learning_rate: å­¦ç¿’ç‡
            momentum: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ä¿‚æ•°
            adaptive_threshold: é©å¿œé–¾å€¤
            memory_length: ãƒ¡ãƒ¢ãƒªé•·
            neural_layers: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«å±¤æ•°
        
        Returns:
            Dict: å…¨ã¦ã®è¨ˆç®—çµæœã‚’å«ã‚€è¾æ›¸
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            if isinstance(data, pd.DataFrame):
                prices = data['close'].values.astype(np.float64)
            else:
                prices = data.astype(np.float64) if data.ndim == 1 else data[:, 3].astype(np.float64)
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®Ÿè¡Œ
            (filtered_prices, learning_weights, adaptive_lr, 
             memory_scores, performance_metrics) = \
                neural_adaptive_kalman_supreme_v3(
                    prices,
                    learning_rate=learning_rate,
                    momentum=momentum,
                    adaptive_threshold=adaptive_threshold,
                    memory_length=memory_length,
                    neural_layers=neural_layers
                )
            
            self.logger.info(f"ğŸ§  ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Supreme V3.0 å®Œäº†")
            self.logger.info(f"   æœ€çµ‚å­¦ç¿’é‡ã¿: {learning_weights[-1]:.4f}")
            self.logger.info(f"   æœ€çµ‚é©å¿œå­¦ç¿’ç‡: {adaptive_lr[-1]:.6f}")
            self.logger.info(f"   å¹³å‡ãƒ¡ãƒ¢ãƒªã‚¹ã‚³ã‚¢: {np.nanmean(memory_scores):.4f}")
            self.logger.info(f"   å¹³å‡æ€§èƒ½æŒ‡æ¨™: {np.nanmean(performance_metrics):.4f}")
            
            return {
                'filtered_prices': filtered_prices,
                'learning_weights': learning_weights,
                'adaptive_learning_rate': adaptive_lr,
                'memory_scores': memory_scores,
                'performance_metrics': performance_metrics,
                'raw_prices': prices
            }
            
        except Exception as e:
            self.logger.error(f"ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ Supreme V3.0 ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def compare_all_methods(self, data: Union[pd.DataFrame, np.ndarray]) -> Dict[str, Any]:
        """
        ğŸ† **å…¨ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ‰‹æ³•ã®æ€§èƒ½æ¯”è¼ƒ**
        
        æ¨™æº–ç‰ˆãƒ»ãƒã‚¤ãƒ‘ãƒ¼é‡å­ç‰ˆãƒ»ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰ˆã®å…¨æ‰‹æ³•ã‚’æ¯”è¼ƒè©•ä¾¡
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            Dict: æ¯”è¼ƒçµæœã¨æ€§èƒ½çµ±è¨ˆ
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            if isinstance(data, pd.DataFrame):
                prices = data['close'].values.astype(np.float64)
            else:
                prices = data.astype(np.float64) if data.ndim == 1 else data[:, 3].astype(np.float64)
            
            self.logger.info("ğŸ† å…¨ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ‰‹æ³•ã®æ€§èƒ½æ¯”è¼ƒé–‹å§‹...")
            
            # 1. æ¨™æº–ç‰ˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            standard_result = self.calculate(data)
            
            # 2. ãƒã‚¤ãƒ‘ãƒ¼é‡å­ç‰ˆ
            quantum_result = self.calculate_hyper_quantum_ultra(data)
            
            # 3. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰ˆ
            neural_result = self.calculate_neural_adaptive_supreme(data)
            
            # æ€§èƒ½æ¯”è¼ƒè¨ˆç®—
            def calculate_performance_metrics(filtered_prices, raw_prices):
                # è¿½å¾“æ€§ï¼ˆé…å»¶æ¸¬å®šï¼‰
                lag_correlation = np.corrcoef(filtered_prices[1:], raw_prices[:-1])[0, 1]
                
                # å¹³æ»‘æ€§ï¼ˆãƒã‚¤ã‚ºé™¤å»åŠ¹æœï¼‰
                raw_volatility = np.std(np.diff(raw_prices))
                filtered_volatility = np.std(np.diff(filtered_prices))
                smoothness = (raw_volatility - filtered_volatility) / raw_volatility
                
                # ç²¾åº¦ï¼ˆå¹³å‡çµ¶å¯¾èª¤å·®ï¼‰
                accuracy = 1.0 / (1.0 + np.mean(np.abs(filtered_prices - raw_prices)))
                
                return {
                    'lag_correlation': lag_correlation,
                    'smoothness': smoothness,
                    'accuracy': accuracy,
                    'overall_score': (lag_correlation + smoothness + accuracy) / 3
                }
            
            # å„æ‰‹æ³•ã®æ€§èƒ½è©•ä¾¡
            standard_perf = calculate_performance_metrics(standard_result.values, prices)
            quantum_perf = calculate_performance_metrics(quantum_result['filtered_prices'], prices)
            neural_perf = calculate_performance_metrics(neural_result['filtered_prices'], prices)
            
            self.logger.info("ğŸ“Š æ€§èƒ½æ¯”è¼ƒçµæœ:")
            self.logger.info(f"   æ¨™æº–ç‰ˆç·åˆã‚¹ã‚³ã‚¢: {standard_perf['overall_score']:.4f}")
            self.logger.info(f"   é‡å­ç‰ˆç·åˆã‚¹ã‚³ã‚¢: {quantum_perf['overall_score']:.4f}")
            self.logger.info(f"   ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ç‰ˆç·åˆã‚¹ã‚³ã‚¢: {neural_perf['overall_score']:.4f}")
            
            return {
                'standard_result': standard_result,
                'quantum_result': quantum_result,
                'neural_result': neural_result,
                'performance_comparison': {
                    'standard': standard_perf,
                    'quantum': quantum_perf,
                    'neural': neural_perf
                },
                'winner': max([
                    ('standard', standard_perf['overall_score']),
                    ('quantum', quantum_perf['overall_score']),
                    ('neural', neural_perf['overall_score'])
                ], key=lambda x: x[1])[0]
            }
            
        except Exception as e:
            self.logger.error(f"å…¨æ‰‹æ³•æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {e}")
            raise 