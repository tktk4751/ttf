#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultimate Chop Trend - å®‡å®™æœ€å¼·ã®ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼

æœ€æ–°ã®æ•°å­¦ãƒ»çµ±è¨ˆå­¦ãƒ»æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆã—ãŸè¶…ä½é…å»¶ãƒ»è¶…é«˜ç²¾åº¦ã‚·ã‚¹ãƒ†ãƒ ï¼š
- Hilbert Transform Instantaneous Analysisï¼ˆç¬æ™‚ä½ç›¸ãƒ»å‘¨æ³¢æ•°è§£æï¼‰
- Fractal Adaptive Filteringï¼ˆãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
- Wavelet Multi-Resolution Analysisï¼ˆã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æï¼‰
- Dynamic Kalman Filteringï¼ˆå‹•çš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
- Information Entropy Trend Detectionï¼ˆæƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºï¼‰
- Machine Learning Ensemble Votingï¼ˆæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ï¼‰
- Chaos Theory Indicatorsï¼ˆã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™ï¼‰
- Multi-Timeframe Regime Detectionï¼ˆå¤šé‡æ™‚é–“è»¸ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºï¼‰
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Any, Optional, NamedTuple, List
import numpy as np
import pandas as pd
from numba import jit, njit, prange
import traceback
import math
import warnings
warnings.filterwarnings("ignore")

# Assuming these base classes exist
try:
    from .indicator import Indicator
    from .atr import ATR
    from .ehlers_unified_dc import EhlersUnifiedDC
except ImportError:
    print("Warning: Could not import from relative path. Assuming base classes are available.")
    class Indicator:
        def __init__(self, name): self.name = name; self.logger = self._get_logger()
        def reset(self): pass
        def _get_logger(self): import logging; return logging.getLogger(self.__class__.__name__)
    class ATR:
        def __init__(self, **kwargs): pass
        def calculate(self, data): return None
        def get_values(self): return np.array([])
        def reset(self): pass
    class EhlersUnifiedDC:
        def __init__(self, **kwargs): pass
        def calculate(self, data): return np.full(len(data), 10.0)
        def reset(self): pass


class UltimateChopTrendResult(NamedTuple):
    """Ultimate ChopTrendè¨ˆç®—çµæœ - å®‡å®™æœ€å¼·ç‰ˆ"""
    # ã‚³ã‚¢æŒ‡æ¨™
    ultimate_trend_index: np.ndarray      # æœ€çµ‚çµ±åˆãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ•°ï¼ˆ0-1ï¼‰
    trend_signals: np.ndarray             # 1=å¼·ä¸Šæ˜‡, 0.5=å¼±ä¸Šæ˜‡, 0=ãƒ¬ãƒ³ã‚¸, -0.5=å¼±ä¸‹é™, -1=å¼·ä¸‹é™
    trend_strength: np.ndarray            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆ0-1ï¼‰
    regime_state: np.ndarray              # ãƒ¬ã‚¸ãƒ¼ãƒ çŠ¶æ…‹ï¼ˆ0=ãƒ¬ãƒ³ã‚¸ã€1=ãƒˆãƒ¬ãƒ³ãƒ‰ã€2=ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆï¼‰
    
    # æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ¤å®šï¼ˆãƒãƒ£ãƒ¼ãƒˆèƒŒæ™¯è‰²ç”¨ï¼‰
    trend_direction: np.ndarray           # 1=ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰, 0=ãƒ¬ãƒ³ã‚¸, -1=ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰
    direction_strength: np.ndarray        # æ–¹å‘æ€§ã®å¼·ã•ï¼ˆ0-1ï¼‰
    
    # ä¿¡é ¼åº¦ãƒ»ç¢ºç‡
    confidence_score: np.ndarray          # äºˆæ¸¬ä¿¡é ¼åº¦ï¼ˆ0-1ï¼‰
    trend_probability: np.ndarray         # ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºç‡ï¼ˆ0-1ï¼‰
    regime_probability: np.ndarray        # ãƒ¬ã‚¸ãƒ¼ãƒ ç¢ºç‡
    
    # å…ˆè¡ŒæŒ‡æ¨™
    predictive_signal: np.ndarray         # äºˆæ¸¬ã‚·ã‚°ãƒŠãƒ«
    momentum_forecast: np.ndarray         # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ äºˆæ¸¬
    volatility_forecast: np.ndarray       # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆåˆ†
    hilbert_component: np.ndarray         # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›æˆåˆ†
    fractal_component: np.ndarray         # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æˆåˆ†
    wavelet_component: np.ndarray         # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæˆåˆ†
    kalman_component: np.ndarray          # ã‚«ãƒ«ãƒãƒ³æˆåˆ†
    entropy_component: np.ndarray         # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æˆåˆ†
    chaos_component: np.ndarray           # ã‚«ã‚ªã‚¹æˆåˆ†
    
    # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£ææˆåˆ†
    unscented_kalman_component: np.ndarray  # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æˆåˆ†
    garch_volatility: np.ndarray            # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
    regime_switching_probs: np.ndarray      # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆç¢ºç‡ï¼ˆ3æ¬¡å…ƒ: n_samples x n_regimesï¼‰
    spectral_power: np.ndarray              # ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ‘ãƒ¯ãƒ¼
    dominant_frequency: np.ndarray          # æ”¯é…çš„å‘¨æ³¢æ•°
    multiscale_entropy: np.ndarray          # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
    nonlinear_dynamics: np.ndarray          # éç·šå½¢åŠ›å­¦ç³»æŒ‡æ¨™
    predictability_score: np.ndarray       # äºˆæ¸¬å¯èƒ½æ€§ã‚¹ã‚³ã‚¢
    
    # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    market_efficiency: np.ndarray         # å¸‚å ´åŠ¹ç‡æ€§
    information_ratio: np.ndarray         # æƒ…å ±æ¯”ç‡
    adaptive_threshold: np.ndarray        # é©å¿œã—ãã„å€¤
    correlation_dimension: np.ndarray     # ç›¸é–¢æ¬¡å…ƒ
    lyapunov_exponent: np.ndarray         # ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°
    
    # ç¾åœ¨çŠ¶æ…‹
    current_trend: str
    current_strength: float
    current_confidence: float


@njit(fastmath=True, cache=True)
def hilbert_transform_analysis(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã«ã‚ˆã‚‹ç¬æ™‚è§£æ
    
    Returns:
        (instantaneous_phase, instantaneous_frequency, instantaneous_amplitude, trend_component)
    """
    n = len(prices)
    if n < 50:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã®è¿‘ä¼¼å®Ÿè£…
    phase = np.zeros(n)
    frequency = np.zeros(n)
    amplitude = np.zeros(n)
    trend_component = np.zeros(n)
    
    # ä½ç›¸å·®åˆ†ã‚’ä½¿ã£ãŸç¬æ™‚å‘¨æ³¢æ•°ã®è¿‘ä¼¼
    for i in range(7, n):
        # 4ã¤ã®ä½ç›¸æˆåˆ†ã‚’è¨ˆç®—
        real_part = (prices[i] + prices[i-2] + prices[i-4] + prices[i-6]) / 4.0
        
        # 90åº¦ä½ç›¸ã‚’ãšã‚‰ã—ãŸè™šæ•°éƒ¨ã®è¿‘ä¼¼
        imag_part = (prices[i-1] + prices[i-3] + prices[i-5] + prices[i-7]) / 4.0
        
        # ç¬æ™‚ä½ç›¸
        if real_part != 0:
            phase[i] = math.atan2(imag_part, real_part)
        
        # ç¬æ™‚æŒ¯å¹…
        amplitude[i] = math.sqrt(real_part * real_part + imag_part * imag_part)
        
        # ç¬æ™‚å‘¨æ³¢æ•°ï¼ˆä½ç›¸ã®å·®åˆ†ï¼‰
        if i > 7:
            freq_diff = phase[i] - phase[i-1]
            # ä½ç›¸ã®å·»ãæˆ»ã—ã‚’ä¿®æ­£
            if freq_diff > math.pi:
                freq_diff -= 2 * math.pi
            elif freq_diff < -math.pi:
                freq_diff += 2 * math.pi
            frequency[i] = abs(freq_diff)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†ï¼ˆä½ç›¸ã®æ–¹å‘æ€§ï¼‰
        if i > 14:
            phase_trend = 0.0
            for j in range(7):
                phase_trend += math.sin(phase[i-j])
            trend_component[i] = phase_trend / 7.0
    
    return phase, frequency, amplitude, trend_component


@njit(fastmath=True, cache=True)
def fractal_adaptive_moving_average(prices: np.ndarray, period: int = 21) -> np.ndarray:
    """
    ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡ï¼ˆFRAMAï¼‰
    
    Returns:
        FRAMAå€¤ã®é…åˆ—
    """
    n = len(prices)
    if n < period * 2:
        return np.full(n, np.nan)
    
    frama = np.full(n, np.nan)
    half_period = period // 2
    
    for i in range(period, n):
        # ä¸ŠåŠåˆ†ã¨ä¸‹åŠåˆ†ã®æœ€é«˜å€¤ãƒ»æœ€å®‰å€¤ã‚’å–å¾—
        h1 = np.max(prices[i-period:i-half_period])
        l1 = np.min(prices[i-period:i-half_period])
        h2 = np.max(prices[i-half_period:i])
        l2 = np.min(prices[i-half_period:i])
        
        # å…¨æœŸé–“ã®æœ€é«˜å€¤ãƒ»æœ€å®‰å€¤
        h_total = np.max(prices[i-period:i])
        l_total = np.min(prices[i-period:i])
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®è¨ˆç®—
        n1 = (h1 - l1) / half_period
        n2 = (h2 - l2) / half_period
        n3 = (h_total - l_total) / period
        
        if n1 > 0 and n2 > 0 and n3 > 0:
            # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
            fractal_dim = (math.log(n1 + n2) - math.log(n3)) / math.log(2.0)
            fractal_dim = min(max(fractal_dim, 1.0), 2.0)  # 1-2ã®ç¯„å›²ã«åˆ¶é™
            
            # é©å¿œä¿‚æ•°
            alpha = math.exp(-4.6 * (fractal_dim - 1.0))
            alpha = min(max(alpha, 0.01), 1.0)  # 0.01-1.0ã®ç¯„å›²ã«åˆ¶é™
        else:
            alpha = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        # FRAMAã®è¨ˆç®—
        if i == period:
            frama[i] = prices[i]
        else:
            frama[i] = alpha * prices[i] + (1.0 - alpha) * frama[i-1]
    
    return frama


@njit(fastmath=True, cache=True)
def wavelet_denoising(prices: np.ndarray, levels: int = 3) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆãƒ»ãƒã‚¤ã‚ºé™¤å»ï¼ˆHaar wavelet approximationï¼‰
    
    Returns:
        (denoised_signal, detail_component)
    """
    n = len(prices)
    if n < 8:
        return prices.copy(), np.zeros(n)
    
    # ã‚·ãƒ³ãƒ—ãƒ«ãªHaarã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè¿‘ä¼¼
    signal = prices.copy()
    detail = np.zeros(n)
    
    for level in range(levels):
        step = 2 ** level
        if step >= n // 2:
            break
            
        # ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¨ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è¿‘ä¼¼
        for i in range(step, n - step, step * 2):
            # ãƒ­ãƒ¼ãƒ‘ã‚¹ï¼ˆå¹³å‡ï¼‰
            low = (signal[i-step] + signal[i]) * 0.5
            # ãƒã‚¤ãƒ‘ã‚¹ï¼ˆå·®åˆ†ï¼‰
            high = (signal[i-step] - signal[i]) * 0.5
            
            # ãƒã‚¤ã‚ºé™¤å»ã®ãŸã‚ã®ã—ãã„å€¤
            threshold = np.std(signal[max(0, i-step*4):i+step*4]) * 0.1
            if abs(high) < threshold:
                high = 0
            
            detail[i] += high
            signal[i-step] = low
            signal[i] = low
    
    denoised = signal - detail
    return denoised, detail


@njit(fastmath=True, cache=True)
def extended_kalman_filter(prices: np.ndarray, volatility: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®šç”¨ï¼‰
    
    Returns:
        (filtered_trend, prediction_confidence)
    """
    n = len(prices)
    if n < 10:
        return prices.copy(), np.ones(n)
    
    # çŠ¶æ…‹å¤‰æ•°ï¼š[ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦]
    state = np.array([prices[0], 0.0, 0.0])
    
    # å…±åˆ†æ•£è¡Œåˆ—
    P = np.eye(3) * 1.0
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    Q = np.array([[0.01, 0.0, 0.0],
                  [0.0, 0.01, 0.0],
                  [0.0, 0.0, 0.01]])
    
    # çŠ¶æ…‹é·ç§»è¡Œåˆ—
    F = np.array([[1.0, 1.0, 0.5],
                  [0.0, 1.0, 1.0],
                  [0.0, 0.0, 1.0]])
    
    filtered_trend = np.full(n, np.nan)
    confidence = np.full(n, np.nan)
    
    for i in range(n):
        if i == 0:
            filtered_trend[i] = prices[i]
            confidence[i] = 1.0
            continue
            
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        state_pred = np.dot(F, state)
        P_pred = np.dot(np.dot(F, P), F.T) + Q
        
        # è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ï¼‰
        R = max(volatility[i] ** 2, 0.001)
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        H = np.array([1.0, 0.0, 0.0])  # ä¾¡æ ¼ã®ã¿ã‚’è¦³æ¸¬
        K_denom = np.dot(np.dot(H, P_pred), H.T) + R
        if K_denom > 0:
            K = np.dot(P_pred, H.T) / K_denom
        else:
            K = np.zeros(3)
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - state_pred[0]
        state = state_pred + K * innovation
        P = P_pred - np.outer(K, np.dot(H, P_pred))
        
        filtered_trend[i] = state[0]
        confidence[i] = 1.0 / (1.0 + P[0, 0])  # ç¢ºä¿¡åº¦
    
    return filtered_trend, confidence


# ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½ç¾¤ï¼ˆultimate_advanced_analysis.pyã‹ã‚‰çµ±åˆï¼‰

@njit(fastmath=True, cache=True)
def unscented_kalman_filter(
    prices: np.ndarray, 
    volatility: np.ndarray,
    alpha: float = 0.001,
    beta: float = 2.0,
    kappa: float = 0.0
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Unscented Kalman Filterï¼ˆç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼‰
    éç·šå½¢ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ãŸé«˜åº¦ãªã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    
    Returns:
        (filtered_prices, trend_estimate, uncertainty)
    """
    n = len(prices)
    if n < 10:
        return prices.copy(), prices.copy(), np.ones(n)
    
    # çŠ¶æ…‹ã®æ¬¡å…ƒï¼ˆä¾¡æ ¼ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦ï¼‰
    L = 3
    
    # UKFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lambda_param = alpha * alpha * (L + kappa) - L
    
    # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®é‡ã¿
    Wm = np.zeros(2 * L + 1)  # å¹³å‡ç”¨ã®é‡ã¿
    Wc = np.zeros(2 * L + 1)  # å…±åˆ†æ•£ç”¨ã®é‡ã¿
    
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    for i in range(1, 2 * L + 1):
        Wm[i] = 0.5 / (L + lambda_param)
        Wc[i] = 0.5 / (L + lambda_param)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.array([prices[0], 0.0, 0.0])  # [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦]
    P = np.eye(L) * 1.0  # åˆæœŸå…±åˆ†æ•£
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    Q = np.array([[0.01, 0.0, 0.0],
                  [0.0, 0.01, 0.0],
                  [0.0, 0.0, 0.01]])
    
    filtered_prices = np.full(n, np.nan)
    trend_estimate = np.full(n, np.nan)
    uncertainty = np.full(n, np.nan)
    
    for t in range(n):
        if t == 0:
            filtered_prices[t] = prices[t]
            trend_estimate[t] = 0.0
            uncertainty[t] = 1.0
            continue
        
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®ç”Ÿæˆ
        sqrt_LP = np.sqrt(L + lambda_param)
        sigma_points = np.zeros((2 * L + 1, L))
        
        # å¹³æ–¹æ ¹è¡Œåˆ—ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        sqrt_P = np.zeros((L, L))
        for i in range(L):
            for j in range(L):
                if i == j:
                    sqrt_P[i, j] = math.sqrt(max(P[i, j], 0.001))
        
        sigma_points[0] = x
        for i in range(L):
            sigma_points[i + 1] = x + sqrt_LP * sqrt_P[i]
            sigma_points[i + 1 + L] = x - sqrt_LP * sqrt_P[i]
        
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        # çŠ¶æ…‹é·ç§»é–¢æ•° f(x) = [x[0] + x[1] + 0.5*x[2], x[1] + x[2], x[2]]
        predicted_sigma = np.zeros((2 * L + 1, L))
        for i in range(2 * L + 1):
            sp = sigma_points[i]
            predicted_sigma[i, 0] = sp[0] + sp[1] + 0.5 * sp[2]  # ä¾¡æ ¼
            predicted_sigma[i, 1] = sp[1] + sp[2]  # é€Ÿåº¦
            predicted_sigma[i, 2] = sp[2]  # åŠ é€Ÿåº¦
        
        # äºˆæ¸¬å¹³å‡
        x_pred = np.zeros(L)
        for i in range(2 * L + 1):
            x_pred += Wm[i] * predicted_sigma[i]
        
        # äºˆæ¸¬å…±åˆ†æ•£
        P_pred = np.zeros((L, L))
        for i in range(2 * L + 1):
            diff = predicted_sigma[i] - x_pred
            P_pred += Wc[i] * np.outer(diff, diff)
        P_pred += Q
        
        # è¦³æ¸¬æ›´æ–°
        # è¦³æ¸¬é–¢æ•° h(x) = x[0] (ä¾¡æ ¼ã®ã¿ã‚’è¦³æ¸¬)
        predicted_obs = np.zeros(2 * L + 1)
        for i in range(2 * L + 1):
            predicted_obs[i] = predicted_sigma[i, 0]
        
        # äºˆæ¸¬è¦³æ¸¬å€¤
        y_pred = np.sum(Wm * predicted_obs)
        
        # è¦³æ¸¬ãƒã‚¤ã‚º
        R = max(volatility[t] ** 2, 0.001)
        
        # è¦³æ¸¬å…±åˆ†æ•£
        Pyy = 0.0
        for i in range(2 * L + 1):
            diff_obs = predicted_obs[i] - y_pred
            Pyy += Wc[i] * diff_obs * diff_obs
        Pyy += R
        
        # äº¤å·®å…±åˆ†æ•£
        Pxy = np.zeros(L)
        for i in range(2 * L + 1):
            diff_state = predicted_sigma[i] - x_pred
            diff_obs = predicted_obs[i] - y_pred
            Pxy += Wc[i] * diff_state * diff_obs
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        if Pyy > 0:
            K = Pxy / Pyy
        else:
            K = np.zeros(L)
        
        # çŠ¶æ…‹æ›´æ–°
        innovation = prices[t] - y_pred
        x = x_pred + K * innovation
        P = P_pred - np.outer(K, K) * Pyy
        
        filtered_prices[t] = x[0]
        trend_estimate[t] = x[1]  # é€Ÿåº¦ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
        uncertainty[t] = math.sqrt(max(P[0, 0], 0))
    
    return filtered_prices, trend_estimate, uncertainty


@njit(fastmath=True, cache=True)
def garch_volatility_model(
    returns: np.ndarray,
    alpha: float = 0.1,
    beta: float = 0.85,
    omega: float = 0.01
) -> Tuple[np.ndarray, np.ndarray]:
    """
    GARCH(1,1)ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
    
    ÏƒÂ²(t) = Ï‰ + Î± * ÎµÂ²(t-1) + Î² * ÏƒÂ²(t-1)
    
    Returns:
        (conditional_variance, volatility)
    """
    n = len(returns)
    if n < 10:
        return np.full(n, 1.0), np.full(n, 1.0)
    
    # åˆæœŸå€¤
    unconditional_var = np.var(returns)
    
    conditional_variance = np.full(n, unconditional_var)
    volatility = np.full(n, math.sqrt(unconditional_var))
    
    for t in range(1, n):
        # GARCH(1,1)ã®æ›´æ–°å¼
        lagged_return_sq = returns[t-1] ** 2
        lagged_variance = conditional_variance[t-1]
        
        conditional_variance[t] = omega + alpha * lagged_return_sq + beta * lagged_variance
        conditional_variance[t] = max(conditional_variance[t], 1e-6)  # ä¸‹é™è¨­å®š
        
        volatility[t] = math.sqrt(conditional_variance[t])
    
    return conditional_variance, volatility


@njit(fastmath=True, cache=True)
def regime_switching_detection(
    prices: np.ndarray,
    window: int = 50,
    n_regimes: int = 3
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆæ¤œå‡ºï¼ˆãƒãƒ«ã‚³ãƒ•åˆ‡ã‚Šæ›¿ãˆãƒ¢ãƒ‡ãƒ«ã®ç°¡æ˜“ç‰ˆï¼‰
    
    Returns:
        (regime_probabilities, most_likely_regime)
    """
    n = len(prices)
    if n < window * 2:
        return np.zeros((n, n_regimes)), np.zeros(n)
    
    regime_probs = np.zeros((n, n_regimes))
    most_likely = np.zeros(n)
    
    # ãƒ¬ã‚¸ãƒ¼ãƒ ã®ç‰¹å¾´ã‚’å®šç¾©
    # 0: ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆãƒ¬ãƒ³ã‚¸ï¼‰
    # 1: ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰
    # 2: ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰
    
    for i in range(window, n):
        price_window = prices[i-window:i]
        returns = np.diff(price_window)
        
        if len(returns) == 0:
            continue
        
        # çµ±è¨ˆé‡ã®è¨ˆç®—
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        # æ­£è¦åŒ–
        if std_return > 0:
            normalized_return = mean_return / std_return
        else:
            normalized_return = 0
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ ç¢ºç‡ã®è¨ˆç®—ï¼ˆå˜ç´”åŒ–ã—ãŸãƒ™ã‚¤ã‚ºæ¨å®šï¼‰
        # ãƒ¬ãƒ³ã‚¸ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã€ä½å¹³å‡ãƒªã‚¿ãƒ¼ãƒ³ï¼‰
        range_score = math.exp(-0.5 * (normalized_return ** 2 + (std_return - 0.01) ** 2))
        
        # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆæ­£ã®ãƒªã‚¿ãƒ¼ãƒ³ã€ä¸­ç¨‹åº¦ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
        uptrend_score = math.exp(-0.5 * ((normalized_return - 1.0) ** 2 + (std_return - 0.02) ** 2)) if normalized_return > 0 else 0
        
        # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆè² ã®ãƒªã‚¿ãƒ¼ãƒ³ã€ä¸­ç¨‹åº¦ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
        downtrend_score = math.exp(-0.5 * ((normalized_return + 1.0) ** 2 + (std_return - 0.02) ** 2)) if normalized_return < 0 else 0
        
        # æ­£è¦åŒ–
        total_score = range_score + uptrend_score + downtrend_score
        if total_score > 0:
            regime_probs[i, 0] = range_score / total_score
            regime_probs[i, 1] = uptrend_score / total_score
            regime_probs[i, 2] = downtrend_score / total_score
        else:
            regime_probs[i, :] = 1.0 / n_regimes
        
        # æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒ¬ã‚¸ãƒ¼ãƒ 
        max_prob = regime_probs[i, 0]
        max_idx = 0
        for j in range(1, n_regimes):
            if regime_probs[i, j] > max_prob:
                max_prob = regime_probs[i, j]
                max_idx = j
        most_likely[i] = max_idx
    
    return regime_probs, most_likely


@njit(fastmath=True, cache=True)
def spectral_analysis(
    prices: np.ndarray,
    window: int = 64
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æï¼ˆç°¡æ˜“ç‰ˆFFTï¼‰
    
    Returns:
        (dominant_frequency, spectral_power, trend_component)
    """
    n = len(prices)
    if n < window * 2:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    dominant_freq = np.full(n, np.nan)
    spectral_power = np.full(n, np.nan)
    trend_component = np.full(n, np.nan)
    
    for i in range(window, n):
        price_segment = prices[i-window:i]
        
        # ç·šå½¢ãƒˆãƒ¬ãƒ³ãƒ‰ã®é™¤å»
        x_vals = np.arange(window)
        n_points = len(x_vals)
        sum_x = np.sum(x_vals)
        sum_y = np.sum(price_segment)
        sum_xy = np.sum(x_vals * price_segment)
        sum_x2 = np.sum(x_vals * x_vals)
        
        # ç·šå½¢å›å¸°ã®å‚¾ã
        denom = n_points * sum_x2 - sum_x * sum_x
        if denom != 0:
            slope = (n_points * sum_xy - sum_x * sum_y) / denom
            intercept = (sum_y - slope * sum_x) / n_points
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰é™¤å»
            detrended = price_segment - (slope * x_vals + intercept)
            trend_component[i] = slope
        else:
            detrended = price_segment - np.mean(price_segment)
            trend_component[i] = 0
        
        # ç°¡æ˜“DFTï¼ˆé›¢æ•£ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›ï¼‰
        # ä¸»è¦å‘¨æ³¢æ•°æˆåˆ†ã®ã¿ã‚’è¨ˆç®—
        max_power = 0.0
        dominant_f = 0.0
        
        for k in range(1, min(window // 2, 16)):  # ä½å‘¨æ³¢æ•°æˆåˆ†ã®ã¿
            # DFTã®è¨ˆç®—
            real_part = 0.0
            imag_part = 0.0
            
            for j in range(window):
                angle = -2.0 * math.pi * k * j / window
                real_part += detrended[j] * math.cos(angle)
                imag_part += detrended[j] * math.sin(angle)
            
            # ãƒ‘ãƒ¯ãƒ¼ã®è¨ˆç®—
            power = real_part * real_part + imag_part * imag_part
            
            if power > max_power:
                max_power = power
                dominant_f = k / window  # æ­£è¦åŒ–å‘¨æ³¢æ•°
        
        dominant_freq[i] = dominant_f
        spectral_power[i] = max_power / (window * window)  # æ­£è¦åŒ–
    
    return dominant_freq, spectral_power, trend_component


@njit(fastmath=True, cache=True)
def multiscale_entropy(
    prices: np.ndarray,
    max_scale: int = 5,
    pattern_length: int = 2,
    tolerance_ratio: float = 0.2
) -> np.ndarray:
    """
    ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    è¤‡æ•°ã®æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®è¤‡é›‘æ€§ã‚’æ¸¬å®š
    
    Returns:
        ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å€¤ã®é…åˆ—ï¼ˆ0-1æ­£è¦åŒ–ï¼‰
    """
    n = len(prices)
    if n < 20:
        return np.full(n, 0.5)
    
    mse_values = np.full(n, 0.5)
    
    for i in range(20, n):
        window_size = min(30, i)
        price_segment = prices[i-window_size:i]
        
        if len(price_segment) < 10:
            mse_values[i] = 0.5
            continue
        
        # ä¾¡æ ¼å¤‰åŒ–ç‡ã‚’è¨ˆç®—
        returns = np.diff(price_segment)
        if len(returns) < 5:
            mse_values[i] = 0.5
            continue
        
        # ç°¡æ˜“ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        entropy_sum = 0.0
        valid_scales = 0
        
        for scale in range(1, min(max_scale + 1, len(returns) // 3)):
            # ã‚¹ã‚±ãƒ¼ãƒ«ã”ã¨ã®ç²—è¦–åŒ–
            if scale == 1:
                scaled_data = returns
            else:
                scaled_length = len(returns) // scale
                if scaled_length < 3:
                    continue
                scaled_data = np.zeros(scaled_length)
                for j in range(scaled_length):
                    start_idx = j * scale
                    end_idx = min(start_idx + scale, len(returns))
                    scaled_data[j] = np.mean(returns[start_idx:end_idx])
            
            # ç°¡æ˜“ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆåˆ†æ•£ãƒ™ãƒ¼ã‚¹ï¼‰
            if len(scaled_data) > 2:
                data_std = np.std(scaled_data)
                data_mean = np.mean(scaled_data)
                
                # æ­£è¦åŒ–ã•ã‚ŒãŸåˆ†æ•£ã‚’ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®ä»£ç†ã¨ã—ã¦ä½¿ç”¨
                if data_std > 1e-10:
                    normalized_variance = data_std / (abs(data_mean) + data_std + 1e-10)
                    entropy_sum += normalized_variance
                    valid_scales += 1
        
        if valid_scales > 0:
            avg_entropy = entropy_sum / valid_scales
            # 0-1ç¯„å›²ã«æ­£è¦åŒ–
            mse_values[i] = min(max(avg_entropy, 0.0), 1.0)
        else:
            mse_values[i] = 0.5
    
    return mse_values


@njit(fastmath=True, cache=True)
def calculate_sample_entropy(
    data: np.ndarray,
    pattern_length: int,
    tolerance: float
) -> float:
    """
    ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
    """
    n = len(data)
    if n <= pattern_length:
        return np.nan
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    matches_m = 0
    matches_m_plus_1 = 0
    
    for i in range(n - pattern_length):
        # é•·ã•mã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        pattern_m = data[i:i + pattern_length]
        
        # é•·ã•m+1ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        if i + pattern_length < n:
            pattern_m_plus_1 = data[i:i + pattern_length + 1]
        else:
            continue
        
        for j in range(i + 1, n - pattern_length):
            # é•·ã•mã®æ¯”è¼ƒ
            candidate_m = data[j:j + pattern_length]
            
            # æœ€å¤§å·®ã‚’è¨ˆç®—
            max_diff_m = 0.0
            for k in range(pattern_length):
                diff = abs(pattern_m[k] - candidate_m[k])
                if diff > max_diff_m:
                    max_diff_m = diff
            
            if max_diff_m <= tolerance:
                matches_m += 1
                
                # é•·ã•m+1ã®æ¯”è¼ƒ
                if j + pattern_length < n:
                    candidate_m_plus_1 = data[j:j + pattern_length + 1]
                    
                    max_diff_m_plus_1 = 0.0
                    for k in range(pattern_length + 1):
                        diff = abs(pattern_m_plus_1[k] - candidate_m_plus_1[k])
                        if diff > max_diff_m_plus_1:
                            max_diff_m_plus_1 = diff
                    
                    if max_diff_m_plus_1 <= tolerance:
                        matches_m_plus_1 += 1
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
    if matches_m > 0 and matches_m_plus_1 > 0:
        relative_prevalence = matches_m_plus_1 / matches_m
        if relative_prevalence > 0:
            return -math.log(relative_prevalence)
        else:
            return float('inf')
    else:
        return np.nan


@njit(fastmath=True, cache=True)
def nonlinear_dynamics_analysis(
    prices: np.ndarray,
    embedding_dim: int = 3,
    delay: int = 1,
    window: int = 50
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    éç·šå½¢åŠ›å­¦ç³»è§£æ
    
    Returns:
        (correlation_dimension, largest_lyapunov, predictability)
    """
    n = len(prices)
    if n < window * 2:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    correlation_dim = np.full(n, np.nan)
    lyapunov_exp = np.full(n, np.nan)
    predictability = np.full(n, np.nan)
    
    for i in range(window, n):
        price_segment = prices[i-window:i]
        
        # æ™‚é–“é…ã‚ŒåŸ‹ã‚è¾¼ã¿
        max_idx = window - (embedding_dim - 1) * delay
        if max_idx <= 0:
            continue
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ§‹ç¯‰
        embedded_points = np.zeros((max_idx, embedding_dim))
        for j in range(max_idx):
            for k in range(embedding_dim):
                embedded_points[j, k] = price_segment[j + k * delay]
        
        # ç›¸é–¢æ¬¡å…ƒã®æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if max_idx > 5:
            # è¿‘å‚ç‚¹ã®ã‚«ã‚¦ãƒ³ãƒˆ
            radius = np.std(price_segment) * 0.1
            correlation_sum = 0.0
            total_pairs = 0
            
            for j in range(max_idx):
                for k in range(j + 1, max_idx):
                    # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
                    dist = 0.0
                    for dim in range(embedding_dim):
                        diff = embedded_points[j, dim] - embedded_points[k, dim]
                        dist += diff * diff
                    dist = math.sqrt(dist)
                    
                    if dist < radius:
                        correlation_sum += 1.0
                    total_pairs += 1
            
            if total_pairs > 0:
                correlation_integral = correlation_sum / total_pairs
                if correlation_integral > 0:
                    correlation_dim[i] = math.log(correlation_integral) / math.log(radius)
                    correlation_dim[i] = min(max(correlation_dim[i], 0), embedding_dim)
        
        # æœ€å¤§ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ã®æ¨å®š
        if max_idx > 10:
            divergence_sum = 0.0
            divergence_count = 0
            
            for j in range(5, max_idx - 5):
                # æœ€è¿‘å‚ç‚¹ã‚’æ¢ã™
                min_dist = float('inf')
                nearest_idx = -1
                
                for k in range(max_idx):
                    if abs(k - j) < 3:  # æ™‚é–“çš„ã«è¿‘ã„ç‚¹ã¯é™¤å¤–
                        continue
                    
                    dist = 0.0
                    for dim in range(embedding_dim):
                        diff = embedded_points[j, dim] - embedded_points[k, dim]
                        dist += diff * diff
                    dist = math.sqrt(dist)
                    
                    if dist < min_dist:
                        min_dist = dist
                        nearest_idx = k
                
                # ç™ºæ•£ã®è¨ˆç®—
                if nearest_idx >= 0 and j + 3 < max_idx and nearest_idx + 3 < max_idx:
                    future_dist = 0.0
                    for dim in range(embedding_dim):
                        diff = embedded_points[j + 3, dim] - embedded_points[nearest_idx + 3, dim]
                        future_dist += diff * diff
                    future_dist = math.sqrt(future_dist)
                    
                    if min_dist > 0 and future_dist > 0:
                        divergence = math.log(future_dist / min_dist) / 3.0
                        divergence_sum += divergence
                        divergence_count += 1
            
            if divergence_count > 0:
                lyapunov_exp[i] = divergence_sum / divergence_count
        
        # äºˆæ¸¬å¯èƒ½æ€§ï¼ˆã‚«ã‚ªã‚¹æ€§ã®é€†ï¼‰
        if not np.isnan(correlation_dim[i]) and not np.isnan(lyapunov_exp[i]):
            # ä½æ¬¡å…ƒãƒ»è² ã®ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•° â†’ é«˜ã„äºˆæ¸¬å¯èƒ½æ€§
            dim_factor = max(0, 1 - correlation_dim[i] / embedding_dim)
            lyap_factor = max(0, 1 - max(lyapunov_exp[i], 0))
            predictability[i] = (dim_factor + lyap_factor) / 2
        else:
            predictability[i] = 0.5
    
    return correlation_dim, lyapunov_exp, predictability


@njit(fastmath=True, cache=True)
def information_entropy_trend(prices: np.ndarray, window: int = 20) -> np.ndarray:
    """
    æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
    
    Returns:
        ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™
    """
    n = len(prices)
    if n < window * 2:
        return np.full(n, np.nan)
    
    entropy_trend = np.full(n, np.nan)
    
    for i in range(window, n):
        # ä¾¡æ ¼å¤‰åŒ–ã®è¨ˆç®—
        price_changes = np.diff(prices[i-window:i+1])
        
        if len(price_changes) == 0:
            continue
            
        # å¤‰åŒ–ã‚’é›¢æ•£åŒ–ï¼ˆãƒ“ãƒ³åˆ†å‰²ï¼‰
        bins = 10
        hist_range = np.max(price_changes) - np.min(price_changes)
        if hist_range <= 0:
            entropy_trend[i] = 0.5
            continue
            
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®è¨ˆç®—
        bin_width = hist_range / bins
        histogram = np.zeros(bins)
        
        for change in price_changes:
            bin_idx = int((change - np.min(price_changes)) / bin_width)
            bin_idx = min(max(bin_idx, 0), bins - 1)
            histogram[bin_idx] += 1
        
        # ç¢ºç‡åˆ†å¸ƒã¸ã®å¤‰æ›
        total = np.sum(histogram)
        if total > 0:
            probabilities = histogram / total
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
            entropy = 0.0
            for p in probabilities:
                if p > 0:
                    entropy -= p * math.log2(p)
            
            # æ­£è¦åŒ–ï¼ˆæœ€å¤§ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§å‰²ã‚‹ï¼‰
            max_entropy = math.log2(bins)
            normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ï¼ˆä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ = å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰
            entropy_trend[i] = 1.0 - normalized_entropy
        else:
            entropy_trend[i] = 0.5
    
    return entropy_trend


@njit(fastmath=True, cache=True)
def chaos_theory_indicators(prices: np.ndarray, window: int = 20) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™ã®è¨ˆç®—
    
    Returns:
        (hurst_exponent, largest_lyapunov_exponent)
    """
    n = len(prices)
    if n < window * 3:
        return np.full(n, np.nan), np.full(n, np.nan)
    
    hurst = np.full(n, np.nan)
    lyapunov = np.full(n, np.nan)
    
    for i in range(window * 2, n):
        price_series = prices[i-window*2:i]
        
        # HurstæŒ‡æ•°ã®è¨ˆç®—ï¼ˆR/Sçµ±è¨ˆï¼‰
        returns = np.diff(price_series)
        if len(returns) < window:
            continue
            
        mean_return = np.mean(returns)
        std_return = np.std(returns)
        
        if std_return > 0:
            # ç´¯ç©åå·®
            cumdev = np.cumsum(returns - mean_return)
            R = np.max(cumdev) - np.min(cumdev)  # ãƒ¬ãƒ³ã‚¸
            S = std_return  # æ¨™æº–åå·®
            
            if S > 0 and R > 0:
                rs_ratio = R / S
                hurst[i] = math.log(rs_ratio) / math.log(len(returns))
                hurst[i] = min(max(hurst[i], 0.0), 1.0)
            else:
                hurst[i] = 0.5
        else:
            hurst[i] = 0.5
        
        # ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ã®è¿‘ä¼¼è¨ˆç®—
        if len(returns) >= 10:
            # è¿‘å‚ç‚¹ã®ç™ºæ•£ç‡ã‚’è¨ˆç®—
            divergence_sum = 0.0
            count = 0
            
            for j in range(5, len(returns) - 5):
                # è¿‘å‚ç‚¹ã‚’æ¢ã™
                base_point = returns[j]
                
                for k in range(max(0, j-5), min(len(returns), j+5)):
                    if k != j and abs(returns[k] - base_point) < std_return * 0.1:
                        # ç™ºæ•£ã®è¨ˆç®—
                        future_steps = 3
                        if j + future_steps < len(returns) and k + future_steps < len(returns):
                            initial_dist = abs(returns[k] - returns[j])
                            final_dist = abs(returns[k + future_steps] - returns[j + future_steps])
                            
                            if initial_dist > 0 and final_dist > 0:
                                divergence = math.log(final_dist / initial_dist)
                                divergence_sum += divergence
                                count += 1
            
            if count > 0:
                lyapunov[i] = divergence_sum / count
            else:
                lyapunov[i] = 0.0
        else:
            lyapunov[i] = 0.0
    
    return hurst, lyapunov


@njit(fastmath=True, cache=True)
def adaptive_threshold_calculation(
    signals: np.ndarray, 
    volatility: np.ndarray, 
    window: int = 50
) -> np.ndarray:
    """
    é©å¿œçš„ã—ãã„å€¤ã®è¨ˆç®—ï¼ˆãƒ¬ãƒ³ã‚¸å„ªå…ˆç‰ˆï¼‰
    
    Returns:
        é©å¿œã—ãã„å€¤ã®é…åˆ—
    """
    n = len(signals)
    if n < window:
        return np.full(n, 0.5)
    
    adaptive_threshold = np.full(n, 0.5)
    
    for i in range(window, n):
        # éå»ã®ã‚·ã‚°ãƒŠãƒ«ã®çµ±è¨ˆ
        signal_window = signals[i-window:i]
        vol_window = volatility[i-window:i]
        
        # ã‚·ã‚°ãƒŠãƒ«ã®çµ±è¨ˆå€¤
        signal_mean = np.mean(signal_window)
        signal_std = np.std(signal_window)
        avg_vol = np.mean(vol_window)
        
        # ãƒ¬ãƒ³ã‚¸ç›¸å ´ã®æ¤œå‡º
        range_indicators = 0
        
        # 1. ã‚·ã‚°ãƒŠãƒ«ãŒä¸­å¤®å€¤ä»˜è¿‘ã«é›†ä¸­ã—ã¦ã„ã‚‹
        if abs(signal_mean - 0.5) < 0.08:  # 0.1ã‹ã‚‰0.08ã«å³æ ¼åŒ–
            range_indicators += 1
        
        # 2. ã‚·ã‚°ãƒŠãƒ«ã®æ¨™æº–åå·®ãŒå°ã•ã„
        if signal_std < 0.12:  # 0.15ã‹ã‚‰0.12ã«å³æ ¼åŒ–
            range_indicators += 1
        
        # 3. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãŒä½ã„
        if avg_vol < 0.015:  # 0.02ã‹ã‚‰0.015ã«å³æ ¼åŒ–
            range_indicators += 1
        
        # 4. ã‚·ã‚°ãƒŠãƒ«ã®ç¯„å›²ãŒç‹­ã„
        signal_range = np.max(signal_window) - np.min(signal_window)
        if signal_range < 0.25:  # 0.3ã‹ã‚‰0.25ã«å³æ ¼åŒ–
            range_indicators += 1
        
        # ãƒ¬ãƒ³ã‚¸åº¦åˆã„ã«å¿œã˜ã¦ã—ãã„å€¤ã‚’èª¿æ•´
        range_strength = range_indicators / 4.0  # 0-1ã®ç¯„å›²
        
        # é©å¿œä¿‚æ•°ï¼ˆãƒ¬ãƒ³ã‚¸ç›¸å ´ã§ã¯é«˜ã„ã—ãã„å€¤ï¼‰
        vol_factor = min(max(avg_vol, 0.05), 1.5)
        
        # ãƒ™ãƒ¼ã‚¹ã—ãã„å€¤ã‚’ãƒ¬ãƒ³ã‚¸å¼·åº¦ã«å¿œã˜ã¦èª¿æ•´ï¼ˆãƒãƒ©ãƒ³ã‚¹ç‰ˆï¼‰
        base_threshold = 0.5 + range_strength * 0.06  # 0.1ã‹ã‚‰0.06ã«å‰Šæ¸›
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã«ã‚ˆã‚‹èª¿æ•´
        volatility_adjustment = signal_std * vol_factor * 0.4  # 0.3ã‹ã‚‰0.4ã«å¢—åŠ 
        
        # ãƒ¬ãƒ³ã‚¸ç›¸å ´è¿½åŠ èª¿æ•´ï¼ˆæ§ãˆã‚ã«ï¼‰
        range_adjustment = range_strength * 0.05  # 0.1ã‹ã‚‰0.05ã«å‰Šæ¸›
        
        adaptive_threshold[i] = base_threshold + volatility_adjustment + range_adjustment
        
        # å®Ÿè·µçš„ãªç¯„å›²ã«åˆ¶é™ï¼ˆãƒãƒ©ãƒ³ã‚¹é‡è¦–ï¼‰
        adaptive_threshold[i] = min(max(adaptive_threshold[i], 0.25), 0.7)  # 0.3-0.75ã‹ã‚‰0.25-0.7ã«èª¿æ•´
        
        # æ¥µç«¯ãªãƒ¬ãƒ³ã‚¸ç›¸å ´ã®å ´åˆã®ã¿è¿½åŠ èª¿æ•´
        if range_indicators == 4:  # å…¨æŒ‡æ¨™ãŒæƒã£ãŸå ´åˆã®ã¿
            adaptive_threshold[i] = min(adaptive_threshold[i] + 0.03, 0.72)  # 0.05ã‹ã‚‰0.03ã«å‰Šæ¸›
    
    return adaptive_threshold


@njit(fastmath=True, cache=True)
def ensemble_voting_system(
    hilbert_sig: np.ndarray,
    fractal_sig: np.ndarray,
    wavelet_sig: np.ndarray,
    kalman_sig: np.ndarray,
    entropy_sig: np.ndarray,
    chaos_sig: np.ndarray,
    confidence_weights: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨ãªã‚¼ãƒ­é™¤ç®—å¯¾ç­–ç‰ˆï¼‰
    
    Returns:
        (ensemble_signal, ensemble_confidence)
    """
    n = len(hilbert_sig)
    ensemble_signal = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
    ensemble_confidence = np.full(n, 0.5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§åˆæœŸåŒ–
    
    # å„ã‚·ã‚°ãƒŠãƒ«ã®é‡ã¿ï¼ˆåˆè¨ˆ1.0ï¼‰
    base_weights = np.array([0.25, 0.20, 0.15, 0.15, 0.15, 0.10])
    
    for i in range(n):
        # å®‰å…¨ãªã‚·ã‚°ãƒŠãƒ«å€¤ã®å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.5ã€ç¯„å›²0-1ã«åˆ¶é™ï¼‰
        sig_values = np.array([
            min(max(hilbert_sig[i] if not np.isnan(hilbert_sig[i]) and np.isfinite(hilbert_sig[i]) else 0.5, 0.0), 1.0),
            min(max(fractal_sig[i] if not np.isnan(fractal_sig[i]) and np.isfinite(fractal_sig[i]) else 0.5, 0.0), 1.0),
            min(max(wavelet_sig[i] if not np.isnan(wavelet_sig[i]) and np.isfinite(wavelet_sig[i]) else 0.5, 0.0), 1.0),
            min(max(kalman_sig[i] if not np.isnan(kalman_sig[i]) and np.isfinite(kalman_sig[i]) else 0.5, 0.0), 1.0),
            min(max(entropy_sig[i] if not np.isnan(entropy_sig[i]) and np.isfinite(entropy_sig[i]) else 0.5, 0.0), 1.0),
            min(max(chaos_sig[i] if not np.isnan(chaos_sig[i]) and np.isfinite(chaos_sig[i]) else 0.5, 0.0), 1.0)
        ])
        
        # ä¿¡é ¼åº¦é‡ã¿ã®å®‰å…¨ãªå–å¾—
        if i < len(confidence_weights):
            conf_weight = confidence_weights[i] if (not np.isnan(confidence_weights[i]) and 
                                                   np.isfinite(confidence_weights[i]) and 
                                                   confidence_weights[i] > 0) else 1.0
        else:
            conf_weight = 1.0
        
        # ä¿¡é ¼åº¦é‡ã¿ã‚’å®‰å…¨ãªç¯„å›²ã«åˆ¶é™
        conf_weight = min(max(conf_weight, 0.01), 10.0)
        
        # åŠ¹æœçš„ãªé‡ã¿ã®è¨ˆç®—
        effective_weights = base_weights * conf_weight
        
        # é‡ã¿ä»˜ãå¹³å‡ã®è¨ˆç®—ï¼ˆå®Œå…¨ã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
        weighted_sum = 0.0
        weight_sum = 0.0
        
        for j in range(len(sig_values)):
            if j < len(effective_weights) and effective_weights[j] > 0:
                weighted_sum += sig_values[j] * effective_weights[j]
                weight_sum += effective_weights[j]
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚°ãƒŠãƒ«ã®è¨ˆç®—
        if weight_sum > 1e-15:  # æ¥µå°å€¤ãƒã‚§ãƒƒã‚¯
            ensemble_signal[i] = weighted_sum / weight_sum
        else:
            ensemble_signal[i] = 0.5  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
        
        # æœ€çµ‚ç¯„å›²åˆ¶é™
        ensemble_signal[i] = min(max(ensemble_signal[i], 0.0), 1.0)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¿¡é ¼åº¦ã®è¨ˆç®—ï¼ˆå®‰å…¨ç‰ˆï¼‰
        try:
            # æ‰‹å‹•ã§åˆ†æ•£ã‚’è¨ˆç®—ã—ã¦ã‚¼ãƒ­é™¤ç®—ã‚’å®Œå…¨å›é¿
            mean_val = np.mean(sig_values)
            variance_sum = 0.0
            
            for val in sig_values:
                diff = val - mean_val
                variance_sum += diff * diff
            
            signal_variance = variance_sum / len(sig_values) if len(sig_values) > 0 else 0.0
            
            # åˆ†æ•£ãŒæ¥µå°ã®å ´åˆã®å‡¦ç†
            if signal_variance < 1e-15:
                ensemble_confidence[i] = 0.95  # é«˜ã„ä¿¡é ¼åº¦ï¼ˆå…¨ã‚·ã‚°ãƒŠãƒ«ãŒä¸€è‡´ï¼‰
            else:
                confidence_val = 1.0 / (1.0 + signal_variance)
                ensemble_confidence[i] = min(max(confidence_val, 0.1), 1.0)
        except:
            ensemble_confidence[i] = 0.5  # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    return ensemble_signal, ensemble_confidence


@njit(fastmath=True, cache=True)
def predictive_analysis(
    prices: np.ndarray,
    trend_signals: np.ndarray,
    volatility: np.ndarray,
    lookforward: int = 3
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    äºˆæ¸¬åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (predictive_signal, momentum_forecast, volatility_forecast)
    """
    n = len(prices)
    if n < lookforward * 3:
        return np.full(n, np.nan), np.full(n, np.nan), np.full(n, np.nan)
    
    predictive_signal = np.full(n, np.nan)
    momentum_forecast = np.full(n, np.nan)
    volatility_forecast = np.full(n, np.nan)
    
    for i in range(lookforward * 2, n - lookforward):
        # éå»ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        past_prices = prices[i-lookforward*2:i]
        past_trends = trend_signals[i-lookforward*2:i]
        past_vols = volatility[i-lookforward*2:i]
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ äºˆæ¸¬ï¼ˆç·šå½¢å›å¸°ã®å‚¾ãï¼‰
        x_vals = np.arange(len(past_prices))
        if len(past_prices) > 2:
            # å˜ç´”ãªç·šå½¢å›å¸°
            n_points = len(past_prices)
            sum_x = np.sum(x_vals)
            sum_y = np.sum(past_prices)
            sum_xy = np.sum(x_vals * past_prices)
            sum_x2 = np.sum(x_vals * x_vals)
            
            denom = n_points * sum_x2 - sum_x * sum_x
            if abs(denom) > 1e-15:  # ã‚¼ãƒ­é™¤ç®—ã®å®Œå…¨å›é¿
                slope = (n_points * sum_xy - sum_x * sum_y) / denom
                momentum_forecast[i] = min(max(slope, -1.0), 1.0)  # ç¯„å›²åˆ¶é™
            else:
                momentum_forecast[i] = 0.0
        else:
            momentum_forecast[i] = 0
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ï¼ˆEWMAï¼‰
        if len(past_vols) > 0:
            # æŒ‡æ•°é‡ã¿ä»˜ãç§»å‹•å¹³å‡
            alpha = 0.2
            vol_forecast = past_vols[-1]
            for j in range(len(past_vols)):
                weight = alpha * (1 - alpha) ** j
                vol_forecast += weight * past_vols[-(j+1)]
            volatility_forecast[i] = vol_forecast
        else:
            volatility_forecast[i] = 0
        
        # äºˆæ¸¬ã‚·ã‚°ãƒŠãƒ«ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®çµ„ã¿åˆã‚ã›ï¼‰
        current_trend = trend_signals[i] if not np.isnan(trend_signals[i]) else 0.5
        momentum_component = momentum_forecast[i] if not np.isnan(momentum_forecast[i]) else 0
        
        # ã‚·ã‚°ãƒŠãƒ«ã®çµ±åˆï¼ˆå®‰å…¨ãªè¨ˆç®—ï¼‰
        if np.isfinite(momentum_component):
            momentum_bounded = min(max(momentum_component * 10, -50), 50)  # tanhå…¥åŠ›å€¤åˆ¶é™
            momentum_normalized = math.tanh(momentum_bounded) * 0.5 + 0.5
        else:
            momentum_normalized = 0.5
        
        predictive_signal[i] = 0.7 * current_trend + 0.3 * momentum_normalized
        predictive_signal[i] = min(max(predictive_signal[i], 0.0), 1.0)
    
    return predictive_signal, momentum_forecast, volatility_forecast


@njit(fastmath=True, cache=True)
def zero_lag_ema(prices: np.ndarray, period: int = 14) -> np.ndarray:
    """
    Zero-Lag Exponential Moving Average (è¶…ä½é…å»¶)
    
    Returns:
        Zero-lag EMAå€¤ã®é…åˆ—
    """
    n = len(prices)
    if n == 0:
        return np.zeros(0, dtype=np.float64)
    
    alpha = 2.0 / (period + 1)
    zlema = np.zeros(n)
    ema1 = np.zeros(n)
    ema2 = np.zeros(n)
    
    # åˆæœŸå€¤è¨­å®š
    zlema[0] = prices[0]
    ema1[0] = prices[0]
    ema2[0] = prices[0]
    
    for i in range(1, n):
        # æ¨™æº–EMA
        ema1[i] = alpha * prices[i] + (1 - alpha) * ema1[i-1]
        # EMAã®EMA
        ema2[i] = alpha * ema1[i] + (1 - alpha) * ema2[i-1]
        # Zero-lagè¨ˆç®—
        zlema[i] = 2 * ema1[i] - ema2[i]
    
    return zlema


@njit(fastmath=True, cache=True)
def ehlers_instantaneous_trendline(prices: np.ndarray, alpha: float = 0.07) -> Tuple[np.ndarray, np.ndarray]:
    """
    Ehlers' Instantaneous Trendline (ç¬æ™‚ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³)
    
    Returns:
        (trendline, momentum)
    """
    n = len(prices)
    if n < 4:
        return prices.copy(), np.zeros(n)
    
    trendline = np.zeros(n)
    momentum = np.zeros(n)
    
    # åˆæœŸå€¤è¨­å®š
    trendline[0] = prices[0]
    trendline[1] = prices[1]
    
    for i in range(2, n):
        # Ehlers' Instantaneous Trendline formula
        trendline[i] = (alpha - 0.25 * alpha * alpha) * prices[i] + \
                      0.5 * alpha * alpha * prices[i-1] - \
                      (alpha - 0.75 * alpha * alpha) * prices[i-2] + \
                      2 * (1 - alpha) * trendline[i-1] - \
                      (1 - alpha) * (1 - alpha) * trendline[i-2]
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ è¨ˆç®—
        if i >= 3:
            momentum[i] = trendline[i] - trendline[i-3]
    
    return trendline, momentum


@njit(fastmath=True, cache=True)
def adaptive_cyber_cycle(prices: np.ndarray, alpha: float = 0.07) -> Tuple[np.ndarray, np.ndarray]:
    """
    Adaptive Cyber Cycle (é©å¿œã‚µã‚¤ãƒãƒ¼ã‚µã‚¤ã‚¯ãƒ«)
    
    Returns:
        (cycle, trend_signal)
    """
    n = len(prices)
    if n < 7:
        return np.zeros(n), np.zeros(n)
    
    cycle = np.zeros(n)
    trend_signal = np.zeros(n)
    smooth = zero_lag_ema(prices, period=7)
    
    for i in range(6, n):
        # Cyber Cycle calculation
        cycle[i] = (1 - 0.5 * alpha) * (1 - 0.5 * alpha) * (smooth[i] - 2 * smooth[i-1] + smooth[i-2]) + \
                  2 * (1 - alpha) * cycle[i-1] - (1 - alpha) * (1 - alpha) * cycle[i-2]
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«ï¼ˆã‚µã‚¤ã‚¯ãƒ«ã®æ–¹å‘æ€§ï¼‰
        if i >= 7:
            if cycle[i] > cycle[i-1] and cycle[i-1] <= cycle[i-2]:
                trend_signal[i] = 1  # ã‚µã‚¤ã‚¯ãƒ«åº•ã‹ã‚‰ã®åè»¢ï¼ˆä¸Šæ˜‡ï¼‰
            elif cycle[i] < cycle[i-1] and cycle[i-1] >= cycle[i-2]:
                trend_signal[i] = -1  # ã‚µã‚¤ã‚¯ãƒ«å¤©äº•ã‹ã‚‰ã®åè»¢ï¼ˆä¸‹é™ï¼‰
            else:
                trend_signal[i] = trend_signal[i-1]  # å‰ã®çŠ¶æ…‹ã‚’ç¶­æŒ
    
    return cycle, trend_signal


@njit(fastmath=True, cache=True)
def online_variance_estimator(prices: np.ndarray, alpha: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æ•£æ¨å®šå™¨ï¼ˆWelford's algorithmæ”¹è‰¯ç‰ˆï¼‰
    
    Returns:
        (running_mean, running_variance)
    """
    n = len(prices)
    running_mean = np.zeros(n)
    running_variance = np.zeros(n)
    
    if n == 0:
        return running_mean, running_variance
    
    running_mean[0] = prices[0]
    running_variance[0] = 0.0
    
    for i in range(1, n):
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¹³å‡æ›´æ–°
        delta = prices[i] - running_mean[i-1]
        running_mean[i] = running_mean[i-1] + alpha * delta
        
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æ•£æ›´æ–°
        delta2 = prices[i] - running_mean[i]
        running_variance[i] = (1 - alpha) * running_variance[i-1] + alpha * delta * delta2
        running_variance[i] = max(running_variance[i], 1e-10)  # æœ€å°å€¤åˆ¶é™
    
    return running_mean, running_variance


@njit(fastmath=True, cache=True)
def streaming_trend_detector(
    prices: np.ndarray,
    fast_alpha: float = 0.15,
    slow_alpha: float = 0.05,
    threshold: float = 0.02
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºå™¨ï¼ˆè¶…ä½é…å»¶ï¼‰
    
    Returns:
        (trend_direction, trend_strength, confidence)
    """
    n = len(prices)
    trend_direction = np.zeros(n)
    trend_strength = np.zeros(n)
    confidence = np.zeros(n)
    
    if n < 2:
        return trend_direction, trend_strength, confidence
    
    fast_ema = np.zeros(n)
    slow_ema = np.zeros(n)
    
    # åˆæœŸå€¤
    fast_ema[0] = prices[0]
    slow_ema[0] = prices[0]
    
    for i in range(1, n):
        # é«˜é€Ÿãƒ»ä½é€ŸEMA
        fast_ema[i] = fast_alpha * prices[i] + (1 - fast_alpha) * fast_ema[i-1]
        slow_ema[i] = slow_alpha * prices[i] + (1 - slow_alpha) * slow_ema[i-1]
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
        ema_diff = fast_ema[i] - slow_ema[i]
        ema_diff_normalized = ema_diff / max(slow_ema[i], 1e-10)
        
        # æ–¹å‘åˆ¤å®š
        if abs(ema_diff_normalized) > threshold:
            trend_direction[i] = 1 if ema_diff_normalized > 0 else -1
            trend_strength[i] = min(abs(ema_diff_normalized) / threshold, 2.0)
            confidence[i] = min(trend_strength[i] / 2.0, 1.0)
        else:
            trend_direction[i] = 0
            trend_strength[i] = abs(ema_diff_normalized) / threshold
            confidence[i] = 0.1  # ä½ä¿¡é ¼åº¦
    
    return trend_direction, trend_strength, confidence


@njit(fastmath=True, cache=True)
def advanced_market_regime_detection(
    prices: np.ndarray,
    volatility: np.ndarray,
    window: int = 20
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    è¶…ä½é…å»¶ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
    
    Returns:
        (trend_strength, trend_direction, regime_confidence)
    """
    n = len(prices)
    trend_strength = np.zeros(n)
    trend_direction = np.zeros(n)
    regime_confidence = np.zeros(n)
    
    # 1. ã‚¼ãƒ­ãƒ©ã‚°EMAãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³
    zlema = zero_lag_ema(prices, period=14)
    
    # 2. Ehlersç¬æ™‚ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³
    ehlers_trend, ehlers_momentum = ehlers_instantaneous_trendline(prices, alpha=0.07)
    
    # 3. é©å¿œã‚µã‚¤ãƒãƒ¼ã‚µã‚¤ã‚¯ãƒ«
    cyber_cycle, cycle_signal = adaptive_cyber_cycle(prices, alpha=0.07)
    
    # 4. ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æ•£æ¨å®š
    running_mean, running_variance = online_variance_estimator(prices, alpha=0.1)
    
    # 5. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
    stream_direction, stream_strength, stream_confidence = streaming_trend_detector(
        prices, fast_alpha=0.15, slow_alpha=0.05, threshold=0.015
    )
    
    # 6. çµ±åˆåˆ¤å®šï¼ˆè¶…ä½é…å»¶ï¼‰
    for i in range(n):
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿
        weights = np.array([0.3, 0.25, 0.2, 0.15, 0.1])  # å„æ‰‹æ³•ã®é‡ã¿
        scores = np.zeros(5)
        
        # A. Zero-lag EMAãƒˆãƒ¬ãƒ³ãƒ‰ (30%)
        if i > 0:
            zlema_change = zlema[i] - zlema[i-1] if zlema[i-1] != 0 else 0
            zlema_trend = zlema_change / max(abs(zlema[i-1]), 1e-10)
            if abs(zlema_trend) > 0.005:  # 0.5%ä»¥ä¸Šã®å¤‰åŒ–
                scores[0] = min(abs(zlema_trend) * 100, 1.0)
                trend_direction[i] = 1 if zlema_trend > 0 else -1
        
        # B. Ehlersç¬æ™‚ãƒˆãƒ¬ãƒ³ãƒ‰ (25%)
        if i < len(ehlers_momentum):
            ehlers_norm = ehlers_momentum[i] / max(abs(prices[i]), 1e-10)
            if abs(ehlers_norm) > 0.01:
                scores[1] = min(abs(ehlers_norm) * 50, 1.0)
                if trend_direction[i] == 0:
                    trend_direction[i] = 1 if ehlers_norm > 0 else -1
        
        # C. ã‚µã‚¤ãƒãƒ¼ã‚µã‚¤ã‚¯ãƒ« (20%)
        if i < len(cycle_signal) and cycle_signal[i] != 0:
            scores[2] = 0.7
            if trend_direction[i] == 0:
                trend_direction[i] = int(cycle_signal[i])
        
        # D. ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åˆ†æ•£ãƒ™ãƒ¼ã‚¹ (15%)
        if i > 5 and running_variance[i] > 0:
            price_change = abs(prices[i] - running_mean[i])
            volatility_ratio = price_change / math.sqrt(running_variance[i])
            if volatility_ratio > 1.5:  # 1.5æ¨™æº–åå·®ä»¥ä¸Š
                scores[3] = min(volatility_ratio / 3.0, 1.0)
        
        # E. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¤œå‡º (10%)
        if i < len(stream_direction):
            if stream_direction[i] != 0:
                scores[4] = stream_confidence[i]
                if trend_direction[i] == 0:
                    trend_direction[i] = int(stream_direction[i])
        
        # æœ€çµ‚çµ±åˆ
        trend_strength[i] = np.sum(weights * scores)
        regime_confidence[i] = trend_strength[i]
        
        # ãƒ¬ãƒ³ã‚¸åˆ¤å®šï¼ˆè¶…å³æ ¼ï¼‰
        if trend_strength[i] < 0.3:
            trend_direction[i] = 0
    
    return trend_strength, trend_direction, regime_confidence


@njit(fastmath=True, cache=True)
def incremental_regression_stats(
    prices: np.ndarray,
    alpha: float = 0.12
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    å¢—åˆ†å›å¸°çµ±è¨ˆï¼ˆè¶…ä½é…å»¶ï¼‰
    
    Returns:
        (trend_slope, trend_significance, confidence_score)
    """
    n = len(prices)
    trend_slope = np.zeros(n)
    trend_significance = np.zeros(n)
    confidence_score = np.zeros(n)
    
    if n < 3:
        return trend_slope, trend_significance, confidence_score
    
    # å¢—åˆ†çµ±è¨ˆã®åˆæœŸåŒ–
    sum_x = 0.0
    sum_y = 0.0  
    sum_xy = 0.0
    sum_x2 = 0.0
    sum_y2 = 0.0
    count = 0.0
    
    for i in range(n):
        # ç¾åœ¨ã®ç‚¹ã‚’è¿½åŠ 
        x_val = float(i)
        y_val = prices[i]
        
        # æŒ‡æ•°é‡ã¿ä»˜ãæ›´æ–°
        if i > 0:
            decay = 1.0 - alpha
            sum_x = decay * sum_x + alpha * x_val
            sum_y = decay * sum_y + alpha * y_val
            sum_xy = decay * sum_xy + alpha * x_val * y_val
            sum_x2 = decay * sum_x2 + alpha * x_val * x_val
            sum_y2 = decay * sum_y2 + alpha * y_val * y_val
            count = decay * count + alpha
        else:
            sum_x = x_val
            sum_y = y_val
            sum_xy = x_val * y_val
            sum_x2 = x_val * x_val
            sum_y2 = y_val * y_val
            count = 1.0
        
        # å›å¸°ä¿‚æ•°è¨ˆç®—ï¼ˆ2ç‚¹ä»¥ä¸Šã§ï¼‰
        if i >= 2 and count > 1e-10:
            # å‚¾ãè¨ˆç®—
            x_mean = sum_x / count
            y_mean = sum_y / count
            
            numerator = sum_xy / count - x_mean * y_mean
            denominator = sum_x2 / count - x_mean * x_mean
            
            if abs(denominator) > 1e-10:
                slope = numerator / denominator
                trend_slope[i] = slope
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰æœ‰æ„æ€§ï¼ˆæ­£è¦åŒ–ï¼‰
                y_variance = sum_y2 / count - y_mean * y_mean
                if y_variance > 1e-10:
                    # RÂ²ã®è¿‘ä¼¼
                    r_squared_approx = (numerator * numerator) / (denominator * y_variance)
                    trend_significance[i] = min(r_squared_approx, 1.0)
                    
                    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
                    slope_normalized = abs(slope) / max(abs(y_mean), 1e-10)
                    confidence_score[i] = min(slope_normalized * r_squared_approx * 8, 1.0)
    
    return trend_slope, trend_significance, confidence_score


@njit(fastmath=True, cache=True)
def statistical_trend_significance(
    prices: np.ndarray,
    window: int = 30,
    confidence_level: float = 0.95
) -> Tuple[np.ndarray, np.ndarray]:
    """
    è¶…é«˜é€Ÿçµ±è¨ˆçš„ãƒˆãƒ¬ãƒ³ãƒ‰æœ‰æ„æ€§ãƒ†ã‚¹ãƒˆ
    
    Returns:
        (trend_significance, p_values)
    """
    n = len(prices)
    trend_significance = np.zeros(n)
    p_values = np.ones(n)
    
    # å¢—åˆ†å›å¸°çµ±è¨ˆã‚’ä½¿ç”¨ï¼ˆè¶…ä½é…å»¶ï¼‰
    trend_slope, sig_score, confidence_score = incremental_regression_stats(prices, alpha=0.12)
    
    for i in range(n):
        if i >= 3:
            # ç°¡æ˜“çµ±è¨ˆçš„æœ‰æ„æ€§åˆ¤å®š
            slope_magnitude = abs(trend_slope[i])
            significance = sig_score[i]
            
            # çµ±è¨ˆçš„æœ‰æ„æ€§ã‚¹ã‚³ã‚¢
            if significance > 0.15:  # æœ€å°æœ‰æ„æ€§é–¾å€¤
                stat_score = slope_magnitude * significance * math.sqrt(float(i + 1))
                
                # é«˜é€Ÿpå€¤è¿‘ä¼¼
                if stat_score > 3.0:  # æ¥µã‚ã¦é«˜ã„æœ‰æ„æ€§
                    p_values[i] = 0.001
                    trend_significance[i] = 2.0 if trend_slope[i] > 0 else -2.0
                elif stat_score > 2.0:  # é«˜ã„æœ‰æ„æ€§
                    p_values[i] = 0.01
                    trend_significance[i] = 1.5 if trend_slope[i] > 0 else -1.5
                elif stat_score > 1.2:  # ä¸­ç¨‹åº¦ã®æœ‰æ„æ€§
                    p_values[i] = 0.05
                    trend_significance[i] = 1.0 if trend_slope[i] > 0 else -1.0
                elif stat_score > 0.8:  # ä½ã„æœ‰æ„æ€§
                    p_values[i] = 0.15
                    trend_significance[i] = 0.5 if trend_slope[i] > 0 else -0.5
                else:
                    p_values[i] = 0.5
                    trend_significance[i] = 0.0  # æœ‰æ„ã§ãªã„
    
    return trend_significance, p_values


@njit(fastmath=True, cache=True)
def adaptive_multi_timeframe_consensus(
    prices: np.ndarray,
    fast_alpha: float = 0.3,
    medium_alpha: float = 0.15,
    slow_alpha: float = 0.05
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    é©å¿œãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ï¼ˆè¶…ä½é…å»¶ï¼‰
    
    Returns:
        (consensus_direction, consensus_strength, timeframe_agreement)
    """
    n = len(prices)
    consensus_direction = np.zeros(n)
    consensus_strength = np.zeros(n)
    timeframe_agreement = np.zeros(n)
    
    if n < 2:
        return consensus_direction, consensus_strength, timeframe_agreement
    
    # é©å¿œEMAåˆæœŸåŒ–
    fast_ema = np.zeros(n)
    medium_ema = np.zeros(n)
    slow_ema = np.zeros(n)
    
    # åˆæœŸå€¤
    fast_ema[0] = prices[0]
    medium_ema[0] = prices[0]
    slow_ema[0] = prices[0]
    
    for i in range(1, n):
        # é©å¿œEMAæ›´æ–°
        fast_ema[i] = fast_alpha * prices[i] + (1 - fast_alpha) * fast_ema[i-1]
        medium_ema[i] = medium_alpha * prices[i] + (1 - medium_alpha) * medium_ema[i-1]
        slow_ema[i] = slow_alpha * prices[i] + (1 - slow_alpha) * slow_ema[i-1]
        
        # å„ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ–¹å‘åˆ¤å®š
        fast_trend = 1 if fast_ema[i] > fast_ema[i-1] else -1
        medium_trend = 1 if medium_ema[i] > medium_ema[i-1] else -1
        slow_trend = 1 if slow_ema[i] > slow_ema[i-1] else -1
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è¨ˆç®—
        fast_strength = abs(fast_ema[i] - fast_ema[i-1]) / max(fast_ema[i-1], 1e-10)
        medium_strength = abs(medium_ema[i] - medium_ema[i-1]) / max(medium_ema[i-1], 1e-10)
        slow_strength = abs(slow_ema[i] - slow_ema[i-1]) / max(slow_ema[i-1], 1e-10)
        
        # é‡ã¿ä»˜ãã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ï¼ˆçŸ­æœŸé‡è¦–ï¼‰
        trend_sum = fast_trend * 0.5 + medium_trend * 0.3 + slow_trend * 0.2
        strength_sum = fast_strength * 0.5 + medium_strength * 0.3 + slow_strength * 0.2
        
        # æ–¹å‘åˆ¤å®š
        if abs(trend_sum) > 0.6:  # å¼·ã„ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹
            consensus_direction[i] = 1 if trend_sum > 0 else -1
            consensus_strength[i] = min(strength_sum * 100, 1.0)
        elif abs(trend_sum) > 0.2:  # å¼±ã„ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹
            consensus_direction[i] = 1 if trend_sum > 0 else -1
            consensus_strength[i] = min(strength_sum * 50, 0.6)
        else:  # ãƒ¬ãƒ³ã‚¸
            consensus_direction[i] = 0
            consensus_strength[i] = 0.1
        
        # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ä¸€è‡´åº¦
        agreement_count = 0
        if fast_trend == medium_trend:
            agreement_count += 1
        if medium_trend == slow_trend:
            agreement_count += 1
        if fast_trend == slow_trend:
            agreement_count += 1
        
        timeframe_agreement[i] = agreement_count / 3.0
    
    return consensus_direction, consensus_strength, timeframe_agreement


@njit(fastmath=True, cache=True)
def multi_timeframe_trend_consensus(
    prices: np.ndarray,
    short_window: int = 10,
    medium_window: int = 20,
    long_window: int = 50
) -> Tuple[np.ndarray, np.ndarray]:
    """
    è¶…ä½é…å»¶è¤‡æ•°æ™‚é–“è»¸ãƒˆãƒ¬ãƒ³ãƒ‰ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹
    
    Returns:
        (consensus_direction, consensus_strength)
    """
    # é©å¿œç‰ˆã‚’ä½¿ç”¨ã—ã¦ä½é…å»¶åŒ–
    consensus_dir, consensus_str, agreement = adaptive_multi_timeframe_consensus(
        prices, fast_alpha=0.25, medium_alpha=0.12, slow_alpha=0.04
    )
    
    return consensus_dir, consensus_str


@njit(fastmath=True, cache=True)
def streaming_volatility_regime(
    volatility: np.ndarray,
    prices: np.ndarray,
    alpha: float = 0.1
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºï¼ˆè¶…ä½é…å»¶ï¼‰
    
    Returns:
        (volatility_regime, volatility_score)
    """
    n = len(volatility)
    vol_regime = np.zeros(n)
    vol_score = np.zeros(n)
    
    if n < 2:
        return vol_regime, vol_score
    
    # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµ±è¨ˆ
    running_mean = 0.0
    running_var = 1.0  # åˆæœŸåˆ†æ•£
    
    for i in range(n):
        current_vol = volatility[i]
        
        if i == 0:
            running_mean = current_vol
            vol_regime[i] = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸­ç¨‹åº¦
            vol_score[i] = 0.5
        else:
            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¹³å‡ã¨åˆ†æ•£ã®æ›´æ–°
            delta = current_vol - running_mean
            running_mean += alpha * delta
            running_var = (1 - alpha) * running_var + alpha * delta * delta
            running_var = max(running_var, 1e-10)  # æœ€å°å€¤åˆ¶é™
            
            # Z-scoreãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šï¼ˆé«˜é€Ÿï¼‰
            z_score = delta / math.sqrt(running_var)
            
            # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆã‚ˆã‚Šç´°ã‹ã„åˆ†é¡ï¼‰
            if z_score > 1.5:  # æ¥µé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                vol_regime[i] = 3
                vol_score[i] = min(abs(z_score) / 2.0, 1.0)
            elif z_score > 0.8:  # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                vol_regime[i] = 2
                vol_score[i] = min(abs(z_score) / 1.5, 0.8)
            elif z_score > -0.3:  # ä¸­ç¨‹åº¦
                vol_regime[i] = 1
                vol_score[i] = 0.5
            else:  # ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                vol_regime[i] = 0
                vol_score[i] = max(0.2, 1.0 - abs(z_score) / 2.0)
    
    return vol_regime, vol_score


@njit(fastmath=True, cache=True)
def adaptive_volatility_regime(
    volatility: np.ndarray,
    prices: np.ndarray,
    window: int = 20
) -> np.ndarray:
    """
    è¶…ä½é…å»¶é©å¿œçš„ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º
    
    Returns:
        volatility_regime (0=ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£, 1=ä¸­ç¨‹åº¦, 2=é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£, 3=æ¥µé«˜)
    """
    vol_regime, vol_score = streaming_volatility_regime(volatility, prices, alpha=0.12)
    return vol_regime


@njit(fastmath=True, cache=True)
def calculate_trend_direction_classification(
    trend_index: np.ndarray,
    trend_signals: np.ndarray,
    prices: np.ndarray,
    lookback_period: int = 5
) -> Tuple[np.ndarray, np.ndarray]:
    """
    æ¬¡ä¸–ä»£é«˜åº¦ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒ¬ãƒ³ã‚¸åˆ¤åˆ¥ã‚·ã‚¹ãƒ†ãƒ 
    
    Args:
        trend_index: Ultimate ChopTrendã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0-1ï¼‰
        trend_signals: ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«ï¼ˆ-1ï½1ï¼‰
        prices: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        lookback_period: ä¾¡æ ¼ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šç”¨ã®ãƒ«ãƒƒã‚¯ãƒãƒƒã‚¯æœŸé–“
    
    Returns:
        Tuple[np.ndarray, np.ndarray]:
            - trend_direction: 1=ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰, 0=ãƒ¬ãƒ³ã‚¸, -1=ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰
            - trend_strength: ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¼·ã•ï¼ˆ0-1ï¼‰
    """
    n = len(trend_index)
    trend_direction = np.zeros(n, dtype=np.int8)
    trend_strength = np.zeros(n)
    
    # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®è¨ˆç®—ï¼ˆä¾¡æ ¼ã‹ã‚‰è¿‘ä¼¼ï¼‰
    volatility = np.zeros(n)
    for i in range(1, n):
        volatility[i] = abs(prices[i] - prices[i-1]) / max(prices[i-1], 1e-10)
    
    # 1. è¶…ä½é…å»¶ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º
    regime_strength, regime_direction, regime_confidence = advanced_market_regime_detection(
        prices, volatility, window=12  # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºå‰Šæ¸›
    )
    
    # 2. é«˜é€Ÿçµ±è¨ˆçš„æœ‰æ„æ€§ãƒ†ã‚¹ãƒˆ
    stat_significance, p_values = statistical_trend_significance(
        prices, window=20, confidence_level=0.80  # ä¿¡é ¼åº¦ã‚’å®Ÿè·µçš„ã«èª¿æ•´
    )
    
    # 3. é©å¿œãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹
    consensus_direction, consensus_strength, timeframe_agreement = adaptive_multi_timeframe_consensus(
        prices, fast_alpha=0.25, medium_alpha=0.12, slow_alpha=0.04
    )
    
    # 4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ 
    vol_regime, vol_score = streaming_volatility_regime(volatility, prices, alpha=0.12)
    
    # 5. çµ±åˆåˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
    for i in range(n):
        # === ä½é…å»¶é‡è¦–çµ±åˆã‚¹ã‚³ã‚¢è¨ˆç®— ===
        uptrend_score = 0.0
        downtrend_score = 0.0
        range_score = 0.0
        
        # é…å»¶è£œæ­£ä¿‚æ•°ï¼ˆæ—©æœŸã®ãƒ‡ãƒ¼ã‚¿ã»ã©é‡è¦–ï¼‰
        latency_factor = min(1.0, (i + 1) / 8.0)  # æœ€åˆ8æœŸé–“ã¯æ®µéšçš„ã«é‡ã¿å¢—åŠ 
        
        # === ä½é…å»¶é‡è¦–é‡ã¿é…åˆ† ===
        # A. é«˜é€Ÿãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º (é‡ã¿: 35% - æœ€é‡è¦ã€ä½é…å»¶)
        regime_weight = 0.35 * latency_factor
        
        # B. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµ±è¨ˆ (é‡ã¿: 25% - é«˜é€Ÿçµ±è¨ˆ)
        stat_weight = 0.25 * latency_factor
        
        # C. é©å¿œãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ  (é‡ã¿: 25% - EMAãƒ™ãƒ¼ã‚¹é«˜é€Ÿ)
        consensus_weight = 0.25 * latency_factor
        
        # D. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ (é‡ã¿: 10% - è£œåŠ©æŒ‡æ¨™)
        vol_weight = 0.10 * latency_factor
        
        # E. å¾“æ¥ChopTrend (é‡ã¿: 5% - é…å»¶å¤§ã®ãŸã‚æœ€å°)
        legacy_weight = 0.05 * latency_factor
        
        # === å„æŒ‡æ¨™ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ–°é‡ã¿é…åˆ†ï¼‰ ===
        
        # A. é«˜é€Ÿãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºã‚¹ã‚³ã‚¢
        if i < len(regime_strength):
            regime_str = regime_strength[i]
            regime_dir = regime_direction[i]
            
            if regime_dir > 0 and regime_str > 0.2:
                uptrend_score += regime_weight * regime_str
            elif regime_dir < 0 and regime_str > 0.2:
                downtrend_score += regime_weight * regime_str
            else:
                range_score += regime_weight * 0.8  # ãƒ¬ãƒ³ã‚¸å¯„ã‚Š
        
        # B. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµ±è¨ˆã‚¹ã‚³ã‚¢
        if i < len(stat_significance):
            stat_sig = stat_significance[i]
            
            if stat_sig > 0.5:  # ä¸Šæ˜‡æœ‰æ„
                uptrend_score += stat_weight * min(stat_sig, 1.0)
            elif stat_sig < -0.5:  # ä¸‹é™æœ‰æ„
                downtrend_score += stat_weight * min(abs(stat_sig), 1.0)
            else:  # æœ‰æ„ã§ãªã„
                range_score += stat_weight * 0.7
        
        # C. é©å¿œãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã‚¹ã‚³ã‚¢
        if i < len(consensus_direction):
            consensus_dir = consensus_direction[i]
            consensus_str = consensus_strength[i]
            
            if consensus_dir > 0 and consensus_str > 0.3:
                uptrend_score += consensus_weight * consensus_str
            elif consensus_dir < 0 and consensus_str > 0.3:
                downtrend_score += consensus_weight * consensus_str
            else:
                range_score += consensus_weight * 0.6
        
        # D. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢ï¼ˆè£œåŠ©ï¼‰
        if i < len(vol_regime):
            vol_reg = vol_regime[i]
            vol_sc = vol_score[i] if i < len(vol_score) else 0.5
            
            # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ç¶™ç¶šã‚’ç¤ºå”†
            if vol_reg >= 2:  # é«˜ãƒ»æ¥µé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                # æ—¢å­˜ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å¼·åŒ–
                if uptrend_score > downtrend_score:
                    uptrend_score += vol_weight * vol_sc * 0.7
                elif downtrend_score > uptrend_score:
                    downtrend_score += vol_weight * vol_sc * 0.7
                else:
                    range_score += vol_weight * 0.3  # ä¸ç¢ºå®Ÿæ€§
            else:  # ä½ãƒ»ä¸­ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                range_score += vol_weight * 0.6
        
        # E. å¾“æ¥ChopTrendã‚¹ã‚³ã‚¢ï¼ˆæœ€å°é‡ã¿ï¼‰
        trend_val = trend_index[i] if not np.isnan(trend_index[i]) else 0.5
        signal_val = trend_signals[i] if not np.isnan(trend_signals[i]) else 0.0
        
        if signal_val > 0.1:  # ä¸Šæ˜‡ã‚·ã‚°ãƒŠãƒ«
            uptrend_score += legacy_weight * signal_val
        elif signal_val < -0.1:  # ä¸‹é™ã‚·ã‚°ãƒŠãƒ«
            downtrend_score += legacy_weight * abs(signal_val)
        else:  # ãƒ¬ãƒ³ã‚¸ã‚·ã‚°ãƒŠãƒ«
            range_score += legacy_weight * 0.5
        
        # === è¶…ä½é…å»¶æœ€çµ‚çµ±åˆåˆ¤å®š ===
        total_score = uptrend_score + downtrend_score + range_score
        
        # æ­£è¦åŒ–ï¼ˆã‚¹ã‚³ã‚¢å®‰å®šåŒ–ï¼‰
        if total_score > 1e-10:
            uptrend_score /= total_score
            downtrend_score /= total_score
            range_score /= total_score
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - å‡ç­‰åˆ†é…
            uptrend_score = downtrend_score = range_score = 1.0/3.0
        
        # ä½é…å»¶åˆ¤å®šã—ãã„å€¤ï¼ˆç©æ¥µçš„ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡ºï¼‰
        strong_trend_threshold = 0.35  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚ˆã‚Šç©æ¥µçš„ï¼‰
        weak_trend_threshold = 0.25    # å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆã‚ˆã‚Šç©æ¥µçš„ï¼‰
        trend_advantage = 0.03         # ä»–ã‚¹ã‚³ã‚¢ã«å¯¾ã™ã‚‹æœ€å°å„ªä½æ€§ï¼ˆç·©å’Œï¼‰
        
        # æ—©æœŸãƒˆãƒ¬ãƒ³ãƒ‰è­¦å‘Šã‚·ã‚¹ãƒ†ãƒ ï¼ˆä½é…å»¶ç‰¹åŒ–ï¼‰
        max_trend_score = max(uptrend_score, downtrend_score)
        trend_dominance = max_trend_score - range_score
        trend_consensus = abs(uptrend_score - downtrend_score)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è¨ˆç®—
        trend_strength[i] = max_trend_score * trend_dominance * latency_factor
        trend_strength[i] = min(trend_strength[i], 1.0)
        
        # è¶…é«˜é€Ÿæ–¹å‘åˆ¤å®šï¼ˆä½é…å»¶é‡è¦–ï¼‰
        if (max_trend_score > strong_trend_threshold and 
            trend_dominance > trend_advantage and 
            trend_consensus > 0.05):  # æ˜ç¢ºãªæ–¹å‘æ€§ï¼ˆç·©å’Œï¼‰
            
            if uptrend_score > downtrend_score:
                trend_direction[i] = 1  # å¼·ã„ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰
            else:
                trend_direction[i] = -1  # å¼·ã„ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰
                
        elif (max_trend_score > weak_trend_threshold and 
              trend_dominance > trend_advantage * 0.5 and
              trend_consensus > 0.03):  # å¼±ã„ãŒæ˜ç¢ºãªæ–¹å‘æ€§ï¼ˆç·©å’Œï¼‰
            
            if uptrend_score > downtrend_score:
                trend_direction[i] = 1  # å¼±ã„ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ³ãƒ‰
            else:
                trend_direction[i] = -1  # å¼±ã„ãƒ€ã‚¦ãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰
                
        else:
            trend_direction[i] = 0  # ãƒ¬ãƒ³ã‚¸ï¼ˆä¸ç¢ºå®Ÿæ€§å„ªå…ˆï¼‰
    
    return trend_direction, trend_strength


@njit(fastmath=True, cache=True)
def advanced_ensemble_voting_system(
    # åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆ6ç¨®é¡ï¼‰
    hilbert_sig: np.ndarray,
    fractal_sig: np.ndarray,
    wavelet_sig: np.ndarray,
    kalman_sig: np.ndarray,
    entropy_sig: np.ndarray,
    chaos_sig: np.ndarray,
    # é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆ6ç¨®é¡ï¼‰
    unscented_kalman_sig: np.ndarray,
    garch_sig: np.ndarray,
    regime_switching_sig: np.ndarray,
    spectral_sig: np.ndarray,
    multiscale_entropy_sig: np.ndarray,
    nonlinear_dynamics_sig: np.ndarray,
    confidence_weights: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ12ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çµ±åˆç‰ˆï¼‰
    
    Returns:
        (ensemble_signal, ensemble_confidence)
    """
    n = len(hilbert_sig)
    ensemble_signal = np.full(n, 0.5)
    ensemble_confidence = np.full(n, 0.5)
    
    # 12ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é‡ã¿é…åˆ†ï¼ˆåˆè¨ˆ1.0ï¼‰
    # åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : 50%, é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : 50%
    base_weights = np.array([
        # åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆ50%ï¼‰
        0.125,  # Hilbert Transform (12.5%)
        0.100,  # Fractal Adaptive (10.0%)
        0.075,  # Wavelet (7.5%)
        0.075,  # Kalman (7.5%)
        0.075,  # Entropy (7.5%)
        0.050,  # Chaos (5.0%)
        # é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆ50%ï¼‰
        0.140,  # Unscented Kalman (14.0%)
        0.105,  # GARCH (10.5%)
        0.085,  # Regime Switching (8.5%)
        0.070,  # Spectral (7.0%)
        0.060,  # Multiscale Entropy (6.0%)
        0.040   # Nonlinear Dynamics (4.0%)
    ])
    
    for i in range(n):
        # å…¨12ã‚·ã‚°ãƒŠãƒ«ã®å®‰å…¨ãªå–å¾—
        sig_values = np.array([
            # åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
            min(max(hilbert_sig[i] if not np.isnan(hilbert_sig[i]) and np.isfinite(hilbert_sig[i]) else 0.5, 0.0), 1.0),
            min(max(fractal_sig[i] if not np.isnan(fractal_sig[i]) and np.isfinite(fractal_sig[i]) else 0.5, 0.0), 1.0),
            min(max(wavelet_sig[i] if not np.isnan(wavelet_sig[i]) and np.isfinite(wavelet_sig[i]) else 0.5, 0.0), 1.0),
            min(max(kalman_sig[i] if not np.isnan(kalman_sig[i]) and np.isfinite(kalman_sig[i]) else 0.5, 0.0), 1.0),
            min(max(entropy_sig[i] if not np.isnan(entropy_sig[i]) and np.isfinite(entropy_sig[i]) else 0.5, 0.0), 1.0),
            min(max(chaos_sig[i] if not np.isnan(chaos_sig[i]) and np.isfinite(chaos_sig[i]) else 0.5, 0.0), 1.0),
            # é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
            min(max(unscented_kalman_sig[i] if not np.isnan(unscented_kalman_sig[i]) and np.isfinite(unscented_kalman_sig[i]) else 0.5, 0.0), 1.0),
            min(max(garch_sig[i] if not np.isnan(garch_sig[i]) and np.isfinite(garch_sig[i]) else 0.5, 0.0), 1.0),
            min(max(regime_switching_sig[i] if not np.isnan(regime_switching_sig[i]) and np.isfinite(regime_switching_sig[i]) else 0.5, 0.0), 1.0),
            min(max(spectral_sig[i] if not np.isnan(spectral_sig[i]) and np.isfinite(spectral_sig[i]) else 0.5, 0.0), 1.0),
            min(max(multiscale_entropy_sig[i] if not np.isnan(multiscale_entropy_sig[i]) and np.isfinite(multiscale_entropy_sig[i]) else 0.5, 0.0), 1.0),
            min(max(nonlinear_dynamics_sig[i] if not np.isnan(nonlinear_dynamics_sig[i]) and np.isfinite(nonlinear_dynamics_sig[i]) else 0.5, 0.0), 1.0)
        ])
        
        # ä¿¡é ¼åº¦é‡ã¿ã®å®‰å…¨ãªå–å¾—
        if i < len(confidence_weights):
            conf_weight = confidence_weights[i] if (not np.isnan(confidence_weights[i]) and 
                                                   np.isfinite(confidence_weights[i]) and 
                                                   confidence_weights[i] > 0) else 1.0
        else:
            conf_weight = 1.0
        
        conf_weight = min(max(conf_weight, 0.01), 10.0)
        
        # åŠ¹æœçš„ãªé‡ã¿ã®è¨ˆç®—
        effective_weights = base_weights * conf_weight
        
        # é‡ã¿ä»˜ãå¹³å‡ã®è¨ˆç®—
        weighted_sum = 0.0
        weight_sum = 0.0
        
        for j in range(len(sig_values)):
            if j < len(effective_weights) and effective_weights[j] > 0:
                weighted_sum += sig_values[j] * effective_weights[j]
                weight_sum += effective_weights[j]
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚°ãƒŠãƒ«ã®è¨ˆç®—
        if weight_sum > 1e-15:
            ensemble_signal[i] = weighted_sum / weight_sum
        else:
            ensemble_signal[i] = 0.5
        
        ensemble_signal[i] = min(max(ensemble_signal[i], 0.0), 1.0)
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¿¡é ¼åº¦ã®è¨ˆç®—
        try:
            mean_val = np.mean(sig_values)
            variance_sum = 0.0
            
            for val in sig_values:
                diff = val - mean_val
                variance_sum += diff * diff
            
            signal_variance = variance_sum / len(sig_values) if len(sig_values) > 0 else 0.0
            
            if signal_variance < 1e-15:
                ensemble_confidence[i] = 0.98  # ã‚ˆã‚Šé«˜ã„ä¿¡é ¼åº¦ï¼ˆ12ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¸€è‡´ï¼‰
            else:
                confidence_val = 1.0 / (1.0 + signal_variance)
                ensemble_confidence[i] = min(max(confidence_val, 0.1), 1.0)
        except:
            ensemble_confidence[i] = 0.5
    
    return ensemble_signal, ensemble_confidence


class UltimateChopTrend(Indicator):
    """
    Ultimate Chop Trend - å®‡å®™æœ€å¼·ã®ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ ğŸš€
    
    æœ€æ–°ã®æ•°å­¦ãƒ»çµ±è¨ˆå­¦ãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆï¼š
    
    ã€åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‘
    ğŸ§  Hilbert Transform Analysis - ç¬æ™‚ä½ç›¸ãƒ»å‘¨æ³¢æ•°è§£æ
    ğŸ“ Fractal Adaptive Moving Average - ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡
    ğŸŒŠ Wavelet Multi-Resolution Analysis - ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æ
    ğŸ¯ Extended Kalman Filtering - æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    ğŸ“Š Information Entropy Analysis - æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è§£æ
    ğŸŒ€ Chaos Theory Indicators - ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™
    
    ã€ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½ - NEW!ã€‘
    ğŸ”¬ Unscented Kalman Filter - ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆéç·šå½¢ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰
    ğŸ“ˆ GARCH Volatility Model - æ¡ä»¶ä»˜ãåˆ†æ•£å‹•çš„ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
    ğŸ”„ Regime Switching Detection - ãƒãƒ«ã‚³ãƒ•åˆ‡ã‚Šæ›¿ãˆãƒ¢ãƒ‡ãƒ«
    ğŸ“¡ Spectral Analysis - å‘¨æ³¢æ•°ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æãƒ»å‘¨æœŸæ€§æ¤œå‡º
    ğŸ” Multiscale Entropy - è¤‡æ•°æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«è¤‡é›‘æ€§æ¸¬å®š
    ğŸŒŒ Nonlinear Dynamics - ç›¸é–¢æ¬¡å…ƒãƒ»ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ãƒ»äºˆæ¸¬å¯èƒ½æ€§
    
    ã€çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã€‘
    ğŸ¯ Advanced Ensemble Voting - é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ 
    ğŸ”® Predictive Analysis - æ¬¡ä¸–ä»£äºˆæ¸¬åˆ†æ
    ğŸ† Ultimate Performance - åœ§å€’çš„ç²¾åº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    """
    
    def __init__(
        self,
        # ã‚³ã‚¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        analysis_period: int = 21,
        ensemble_window: int = 50,
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹åŒ–ãƒ•ãƒ©ã‚°ï¼ˆå…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹ - æœ€çµ‚æ®µéšãƒ†ã‚¹ãƒˆï¼‰
        enable_hilbert: bool = True,   # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›è§£æ
        enable_fractal: bool = True,   # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡
        enable_wavelet: bool = True,   # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æ
        enable_kalman: bool = True,    # æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        enable_entropy: bool = True,   # æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
        enable_chaos: bool = True,     # ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™
        
        # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        enable_unscented_kalman: bool = True,  # ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        enable_garch: bool = True,             # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
        enable_regime_switching: bool = True,  # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆæ¤œå‡º
        enable_spectral: bool = True,          # ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ
        enable_multiscale_entropy: bool = True, # ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        enable_nonlinear_dynamics: bool = True, # éç·šå½¢åŠ›å­¦ç³»è§£æ
        
        # äºˆæ¸¬è¨­å®š
        enable_prediction: bool = True,
        prediction_horizon: int = 3,
        
        # ã—ãã„å€¤è¨­å®š
        trend_threshold: float = 0.65,
        strong_trend_threshold: float = 0.8,
        
        # å¾“æ¥ã®ChopTrendã¨ã®çµ±åˆ
        use_legacy_chop: bool = False,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç„¡åŠ¹åŒ–ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
        chop_weight: float = 0.3
    ):
        """
        å®‡å®™æœ€å¼·ãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸åˆ¤å®šã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ - æœ€çµ‚æ®µéšæ§‹æˆ
        
        å…¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆã—ãŸå®Œå…¨ç‰ˆï¼š
        ğŸ§  Hilbert Transform: ç¬æ™‚ä½ç›¸ãƒ»å‘¨æ³¢æ•°è§£æ
        ğŸ“ Fractal Adaptive MA: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡
        ğŸŒŠ Wavelet Analysis: ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šé‡è§£åƒåº¦è§£æ  
        ğŸ¯ Extended Kalman: æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        ğŸ“Š Information Entropy: æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
        ğŸŒ€ Chaos Theory: ã‚«ã‚ªã‚¹ç†è«–æŒ‡æ¨™ï¼ˆHurstæŒ‡æ•°ã€ãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ï¼‰
        """
        super().__init__(f"UltimateChopTrend(P={analysis_period},W={ensemble_window})")
        
        self.analysis_period = analysis_period
        self.ensemble_window = ensemble_window
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ‰åŠ¹åŒ–
        self.enable_hilbert = enable_hilbert
        self.enable_fractal = enable_fractal
        self.enable_wavelet = enable_wavelet
        self.enable_kalman = enable_kalman
        self.enable_entropy = enable_entropy
        self.enable_chaos = enable_chaos
        
        # ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½æœ‰åŠ¹åŒ–
        self.enable_unscented_kalman = enable_unscented_kalman
        self.enable_garch = enable_garch
        self.enable_regime_switching = enable_regime_switching
        self.enable_spectral = enable_spectral
        self.enable_multiscale_entropy = enable_multiscale_entropy
        self.enable_nonlinear_dynamics = enable_nonlinear_dynamics
        
        # äºˆæ¸¬è¨­å®š
        self.enable_prediction = enable_prediction
        self.prediction_horizon = prediction_horizon
        
        # ã—ãã„å€¤
        self.trend_threshold = trend_threshold
        self.strong_trend_threshold = strong_trend_threshold
        
        # å¾“æ¥ã®ChopTrendã¨ã®çµ±åˆ
        self.use_legacy_chop = use_legacy_chop
        self.chop_weight = chop_weight
        
        # å¾“æ¥ã®ChopTrendã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        if self.use_legacy_chop:
            try:
                from .chop_trend import ChopTrend
                self.legacy_chop = ChopTrend()
            except (ImportError, Exception):
                # ChopTrendãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç„¡åŠ¹åŒ–
                self.legacy_chop = None
                self.use_legacy_chop = False
                self.logger.warning("ChopTrendãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ¬ã‚¬ã‚·ãƒ¼çµ±åˆã‚’ç„¡åŠ¹åŒ–ã—ã¾ã—ãŸã€‚")
        
        self._cache = {}
        self._result: Optional[UltimateChopTrendResult] = None
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateChopTrendResult:
        """
        Ultimate ChopTrendã‚’è¨ˆç®—ã™ã‚‹
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if len(data) == 0:
                return self._create_empty_result(0)
            
            # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
            if isinstance(data, pd.DataFrame):
                prices = np.asarray(data['close'].values, dtype=np.float64)
                high = np.asarray(data['high'].values, dtype=np.float64)
                low = np.asarray(data['low'].values, dtype=np.float64)
            else:
                prices = np.asarray(data[:, 3], dtype=np.float64)
                high = np.asarray(data[:, 1], dtype=np.float64)
                low = np.asarray(data[:, 2], dtype=np.float64)
            
            n = len(prices)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®è¨ˆç®—ï¼ˆATRè¿‘ä¼¼ï¼‰
            volatility = np.zeros(n)
            for i in range(1, n):
                tr = max(
                    high[i] - low[i],
                    abs(high[i] - prices[i-1]),
                    abs(low[i] - prices[i-1])
                )
                if i < 14:
                    volatility[i] = max(tr, 1e-10)  # æœ€å°å€¤åˆ¶é™
                else:
                    volatility[i] = max((volatility[i-1] * 13 + tr) / 14, 1e-10)  # æœ€å°å€¤åˆ¶é™
            
            # åˆæœŸå€¤ã‚‚è¨­å®š
            if n > 0:
                volatility[0] = max(high[0] - low[0], 1e-10)
            
            # å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè¡Œ
            components = {}
            
            if self.enable_hilbert:
                phase, freq, amp, hilbert_trend = hilbert_transform_analysis(prices)
                components['hilbert'] = hilbert_trend
            else:
                components['hilbert'] = np.full(n, 0.5)
            
            if self.enable_fractal:
                components['fractal'] = fractal_adaptive_moving_average(prices, self.analysis_period)
                # FRAMAã‚’0-1ã‚¹ã‚±ãƒ¼ãƒ«ã«æ­£è¦åŒ–
                frama_normalized = np.full(n, 0.5)
                for i in range(self.analysis_period, n):
                    if not np.isnan(components['fractal'][i]):
                        vol_safe = max(volatility[i], 1e-10)  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                        price_change = (prices[i] - components['fractal'][i]) / vol_safe
                        # å®‰å…¨ãªtanhè¨ˆç®—
                        price_change_bounded = min(max(price_change, -50), 50)
                        frama_normalized[i] = math.tanh(price_change_bounded) * 0.5 + 0.5
                components['fractal'] = frama_normalized
            else:
                components['fractal'] = np.full(n, 0.5)
            
            if self.enable_wavelet:
                denoised, detail = wavelet_denoising(prices)
                # ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆæˆåˆ†ã‚’æ­£è¦åŒ–
                wavelet_signal = np.full(n, 0.5)
                for i in range(10, n):
                    vol_safe = max(volatility[i], 1e-10)  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                    trend_strength = abs(denoised[i] - denoised[i-5]) / (vol_safe * 5)
                    wavelet_signal[i] = min(max(trend_strength, 0), 1)
                components['wavelet'] = wavelet_signal
            else:
                components['wavelet'] = np.full(n, 0.5)
            
            if self.enable_kalman:
                if self.enable_unscented_kalman:
                    # ğŸš€ é«˜åº¦ç‰ˆ: ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    ukf_filtered, ukf_trend, ukf_uncertainty = unscented_kalman_filter(prices, volatility)
                    kalman_signal = np.full(n, 0.5)
                    for i in range(5, n):
                        if not np.isnan(ukf_trend[i]):
                            # ãƒˆãƒ¬ãƒ³ãƒ‰é€Ÿåº¦ã‚’æ­£è¦åŒ–
                            trend_dir_bounded = min(max(ukf_trend[i] * 10, -50), 50)
                            kalman_signal[i] = math.tanh(trend_dir_bounded) * 0.5 + 0.5
                    components['kalman'] = kalman_signal
                    kalman_conf = 1.0 / (1.0 + ukf_uncertainty)  # ä¸ç¢ºå®Ÿæ€§ã®é€†æ•°ã§ä¿¡é ¼åº¦è¨ˆç®—
                else:
                    # å¾“æ¥ç‰ˆ: æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    kalman_trend, kalman_conf = extended_kalman_filter(prices, volatility)
                    # ã‚«ãƒ«ãƒãƒ³ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ­£è¦åŒ–
                    kalman_signal = np.full(n, 0.5)
                    for i in range(5, n):
                        if not np.isnan(kalman_trend[i]):
                            vol_safe = max(volatility[i], 1e-10)  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                            trend_dir = (kalman_trend[i] - kalman_trend[i-3]) / vol_safe
                            # å®‰å…¨ãªtanhè¨ˆç®—
                            trend_dir_bounded = min(max(trend_dir * 2, -50), 50)
                            kalman_signal[i] = math.tanh(trend_dir_bounded) * 0.5 + 0.5
                    components['kalman'] = kalman_signal
            else:
                components['kalman'] = np.full(n, 0.5)
                kalman_conf = np.ones(n)
            
            if self.enable_entropy:
                if self.enable_multiscale_entropy:
                    # ğŸš€ é«˜åº¦ç‰ˆ: ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                    components['entropy'] = multiscale_entropy(prices, max_scale=10, pattern_length=2)
                else:
                    # å¾“æ¥ç‰ˆ: å˜ä¸€ã‚¹ã‚±ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
                    components['entropy'] = information_entropy_trend(prices, self.analysis_period)
            else:
                components['entropy'] = np.full(n, 0.5)
            
            if self.enable_chaos:
                hurst, lyapunov = chaos_theory_indicators(prices, self.analysis_period)
                # ã‚«ã‚ªã‚¹æŒ‡æ¨™ã‚’çµ±åˆ
                chaos_signal = np.full(n, 0.5)
                for i in range(n):
                    if not np.isnan(hurst[i]):
                        # Hurst > 0.5 ã¯ãƒˆãƒ¬ãƒ³ãƒ‰ã€< 0.5 ã¯å¹³å‡å›å¸°
                        chaos_signal[i] = hurst[i]
                components['chaos'] = chaos_signal
            else:
                components['chaos'] = np.full(n, 0.5)
            
            # ğŸš€ é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ
            advanced_signals = {}
            
            # Unscented Kalman Filter ã‚·ã‚°ãƒŠãƒ«
            if self.enable_unscented_kalman:
                ukf_filtered, ukf_trend, ukf_uncertainty = unscented_kalman_filter(prices, volatility)
                ukf_signal = np.full(n, 0.5)
                for i in range(5, n):
                    if not np.isnan(ukf_trend[i]):
                        trend_dir_bounded = min(max(ukf_trend[i] * 10, -50), 50)
                        ukf_signal[i] = math.tanh(trend_dir_bounded) * 0.5 + 0.5
                advanced_signals['unscented_kalman'] = ukf_signal
            else:
                advanced_signals['unscented_kalman'] = np.full(n, 0.5)
            
            # GARCH ã‚·ã‚°ãƒŠãƒ«
            if self.enable_garch:
                returns = np.diff(prices)
                returns = np.concatenate([np.array([0]), returns])
                garch_var, garch_vol = garch_volatility_model(returns)
                garch_signal = np.full(n, 0.5)
                for i in range(10, n):
                    if not np.isnan(garch_vol[i]) and garch_vol[i] > 0:
                        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®å¤‰åŒ–ç‡ã‚’ã‚·ã‚°ãƒŠãƒ«ã«å¤‰æ›
                        vol_change = (garch_vol[i] - garch_vol[i-5]) / garch_vol[i-5] if garch_vol[i-5] > 0 else 0
                        vol_change_bounded = min(max(vol_change * 5, -50), 50)
                        garch_signal[i] = math.tanh(vol_change_bounded) * 0.5 + 0.5
                advanced_signals['garch'] = garch_signal
            else:
                advanced_signals['garch'] = np.full(n, 0.5)
            
            # Regime Switching ã‚·ã‚°ãƒŠãƒ«
            if self.enable_regime_switching:
                regime_probs, most_likely_regime = regime_switching_detection(prices, window=50, n_regimes=3)
                regime_signal = np.full(n, 0.5)
                for i in range(n):
                    if not np.isnan(most_likely_regime[i]):
                        # ãƒ¬ã‚¸ãƒ¼ãƒ 0=ä¸‹é™ã€1=ãƒ¬ãƒ³ã‚¸ã€2=ä¸Šæ˜‡ã¨ã—ã¦æ­£è¦åŒ–
                        regime_signal[i] = most_likely_regime[i] / 2.0
                advanced_signals['regime_switching'] = regime_signal
            else:
                advanced_signals['regime_switching'] = np.full(n, 0.5)
            
            # Spectral Analysis ã‚·ã‚°ãƒŠãƒ«
            if self.enable_spectral:
                dominant_freq, spectral_power, spectral_trend = spectral_analysis(prices, window=64)
                spectral_signal = np.full(n, 0.5)
                for i in range(n):
                    if not np.isnan(spectral_trend[i]):
                        trend_bounded = min(max(spectral_trend[i] * 2, -50), 50)
                        spectral_signal[i] = math.tanh(trend_bounded) * 0.5 + 0.5
                advanced_signals['spectral'] = spectral_signal
            else:
                advanced_signals['spectral'] = np.full(n, 0.5)
            
            # Multiscale Entropy ã‚·ã‚°ãƒŠãƒ«ï¼ˆæ—¢ã«0-1æ­£è¦åŒ–æ¸ˆã¿ï¼‰
            if self.enable_multiscale_entropy:
                advanced_signals['multiscale_entropy'] = multiscale_entropy(prices, max_scale=10, pattern_length=2)
            else:
                advanced_signals['multiscale_entropy'] = np.full(n, 0.5)
            
            # Nonlinear Dynamics ã‚·ã‚°ãƒŠãƒ«
            if self.enable_nonlinear_dynamics:
                correlation_dim, lyapunov_exp, predictability = nonlinear_dynamics_analysis(prices)
                nonlinear_signal = np.full(n, 0.5)
                for i in range(n):
                    if not np.isnan(predictability[i]):
                        # äºˆæ¸¬å¯èƒ½æ€§ã‚’0-1ç¯„å›²ã«æ­£è¦åŒ–
                        nonlinear_signal[i] = min(max(predictability[i], 0.0), 1.0)
                advanced_signals['nonlinear_dynamics'] = nonlinear_signal
            else:
                advanced_signals['nonlinear_dynamics'] = np.full(n, 0.5)

            # ğŸš€ é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ç”¨
            try:
                ensemble_signal, ensemble_confidence = advanced_ensemble_voting_system(
                    # åŸºæœ¬ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                    components['hilbert'],
                    components['fractal'],
                    components['wavelet'],
                    components['kalman'],
                    components['entropy'],
                    components['chaos'],
                    # é«˜åº¦è§£æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                    advanced_signals['unscented_kalman'],
                    advanced_signals['garch'],
                    advanced_signals['regime_switching'],
                    advanced_signals['spectral'],
                    advanced_signals['multiscale_entropy'],
                    advanced_signals['nonlinear_dynamics'],
                    kalman_conf
                )
                
                # ã‚·ãƒ³ãƒ—ãƒ«ãªå¾Œå‡¦ç†ï¼ˆæœ€å°é™ã®èª¿æ•´ï¼‰
                for i in range(n):
                    signal_val = ensemble_signal[i]
                    
                    # æ¥µç«¯ã«ä¿å®ˆçš„ã™ãã‚‹èª¿æ•´ã‚’å‰Šé™¤ã—ã€è»½å¾®ãªèª¿æ•´ã®ã¿
                    # æ¥µç«¯ãªå€¤ã®ã¿ã‚’ä¸­å¤®å€¤ã«è¿‘ã¥ã‘ã‚‹
                    if abs(signal_val - 0.5) > 0.4:  # éå¸¸ã«æ¥µç«¯ãªå ´åˆã®ã¿
                        # è»½å¾®ãªèª¿æ•´
                        ensemble_signal[i] = 0.5 + (signal_val - 0.5) * 0.9  # 10%å‰Šæ¸›ã®ã¿
                
            except Exception as e:
                self.logger.error(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã‚·ã‚¹ãƒ†ãƒ ã§ã‚¨ãƒ©ãƒ¼: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ¬ãƒ³ã‚¸å„ªå…ˆã®ã‚·ãƒ³ãƒ—ãƒ«ãªå¹³å‡
                ensemble_signal = np.full(n, 0.5)
                ensemble_confidence = np.full(n, 0.5)
                
                for i in range(n):
                    valid_signals = []
                    for comp_name, comp_values in components.items():
                        if not np.isnan(comp_values[i]) and np.isfinite(comp_values[i]):
                            valid_signals.append(comp_values[i])
                    
                    if len(valid_signals) > 0:
                        # ã‚·ãƒ³ãƒ—ãƒ«ãªå¹³å‡ï¼ˆãƒã‚¤ã‚¢ã‚¹é™¤å»ï¼‰
                        ensemble_signal[i] = np.mean(valid_signals)
                        ensemble_confidence[i] = min(len(valid_signals) / 6.0, 1.0)
            
            # ğŸš€ é«˜åº¦è§£ææ©Ÿèƒ½ã®çµæœä¿å­˜ï¼ˆé‡è¤‡è¨ˆç®—ã‚’é¿ã‘ã‚‹ãŸã‚æ—¢å­˜ã®çµæœã‚’ä½¿ç”¨ï¼‰
            advanced_components = {}
            
            # æ—¢ã«è¨ˆç®—æ¸ˆã¿ã®çµæœã‚’ä½¿ç”¨
            if self.enable_unscented_kalman:
                # Unscented Kalman Filterã®çµæœã‚’ä¿å­˜
                ukf_filtered, _, _ = unscented_kalman_filter(prices, volatility)
                advanced_components['unscented_kalman'] = ukf_filtered
            else:
                advanced_components['unscented_kalman'] = np.full(n, np.nan)
            
            # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
            if self.enable_garch:
                returns = np.diff(prices)
                returns = np.concatenate([np.array([0]), returns])
                _, garch_vol = garch_volatility_model(returns)
                advanced_components['garch_volatility'] = garch_vol
            else:
                advanced_components['garch_volatility'] = np.full(n, np.nan)
            
            # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ‡ã‚Šæ›¿ãˆæ¤œå‡º
            if self.enable_regime_switching:
                regime_probs, most_likely_regime = regime_switching_detection(prices, window=50, n_regimes=3)
                advanced_components['regime_switching_probs'] = regime_probs
                advanced_components['most_likely_regime'] = most_likely_regime
            else:
                advanced_components['regime_switching_probs'] = np.zeros((n, 3))
                advanced_components['most_likely_regime'] = np.zeros(n)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ
            if self.enable_spectral:
                dominant_freq, spectral_power, spectral_trend = spectral_analysis(prices, window=64)
                advanced_components['dominant_frequency'] = dominant_freq
                advanced_components['spectral_power'] = spectral_power
                advanced_components['spectral_trend'] = spectral_trend
            else:
                advanced_components['dominant_frequency'] = np.full(n, np.nan)
                advanced_components['spectral_power'] = np.full(n, np.nan)
                advanced_components['spectral_trend'] = np.full(n, np.nan)
            
            # éç·šå½¢åŠ›å­¦ç³»è§£æ
            if self.enable_nonlinear_dynamics:
                correlation_dim, lyapunov_exp, predictability = nonlinear_dynamics_analysis(prices)
                advanced_components['correlation_dimension'] = correlation_dim
                advanced_components['lyapunov_exponent'] = lyapunov_exp
                advanced_components['predictability_score'] = predictability
            else:
                advanced_components['correlation_dimension'] = np.full(n, np.nan)
                advanced_components['lyapunov_exponent'] = np.full(n, np.nan)
                advanced_components['predictability_score'] = np.full(n, np.nan)

            # å¾“æ¥ã®ChopTrendã¨ã®çµ±åˆ
            if self.use_legacy_chop and self.legacy_chop is not None:
                try:
                    legacy_result = self.legacy_chop.calculate(data)
                    legacy_values = legacy_result.values
                    # çµ±åˆ
                    final_signal = (1 - self.chop_weight) * ensemble_signal + self.chop_weight * legacy_values
                except:
                    final_signal = ensemble_signal
            else:
                final_signal = ensemble_signal
            
            # é©å¿œã—ãã„å€¤
            adaptive_threshold = adaptive_threshold_calculation(final_signal, volatility, self.ensemble_window)
            
            # ğŸš€ æ¬¡ä¸–ä»£ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆï¼ˆå®Ÿè·µçš„ãƒãƒ©ãƒ³ã‚¹ç‰ˆï¼‰
            trend_signals = np.zeros(n)
            trend_strength = np.zeros(n)
            regime_state = np.zeros(n)
            
            for i in range(n):
                signal = final_signal[i]
                threshold = adaptive_threshold[i]
                
                # ã‚·ãƒ³ãƒ—ãƒ«ã§å®Ÿè·µçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
                current_vol = volatility[i] if i < len(volatility) else 0.01
                
                # ğŸ”§ ä¿®æ­£ã•ã‚ŒãŸãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è¨ˆç®—
                # ä¸­å¤®å€¤ï¼ˆ0.5ï¼‰ã‹ã‚‰ã®è·é›¢ã‚’åŸºæœ¬ã¨ã—ã€ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã§èª¿æ•´
                base_strength = abs(signal - 0.5) * 2.0
                
                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´ï¼ˆé«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ™‚ã¯ã‚ˆã‚Šå¼·ã„ã‚·ã‚°ãƒŠãƒ«ãŒå¿…è¦ï¼‰
                vol_adjustment = min(current_vol * 10, 1.0)  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´ä¿‚æ•°
                adjusted_strength = base_strength * (1 + vol_adjustment)
                
                # ä¾¡æ ¼å¤‰å‹•ã®å‹¢ã„ã‚‚è€ƒæ…®ï¼ˆç›´è¿‘ã®ä¾¡æ ¼å¤‰åŒ–ï¼‰
                if i > 0:
                    price_momentum = abs(prices[i] - prices[i-1]) / max(prices[i-1], 1e-10)
                    momentum_boost = min(price_momentum * 50, 0.5)  # æœ€å¤§50%ã®ãƒ–ãƒ¼ã‚¹ãƒˆ
                    adjusted_strength += momentum_boost
                
                trend_strength[i] = min(adjusted_strength, 1.0)  # æœ€å¤§å€¤1.0ã«åˆ¶é™
                
                # ğŸ”§ ä¿®æ­£ã•ã‚ŒãŸãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆã‚ˆã‚Šå®Ÿè·µçš„ãªã—ãã„å€¤ï¼‰
                # ã—ãã„å€¤ã‚’å¤§å¹…ã«ç·©å’Œã—ã¦ã€ã‚ˆã‚Šå¤šãã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ¤œå‡º
                if trend_strength[i] > 0.4:  # 0.7 â†’ 0.4ã«ç·©å’Œ
                    regime_state[i] = 2  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰
                elif trend_strength[i] > 0.2:  # 0.4 â†’ 0.2ã«ç·©å’Œ
                    regime_state[i] = 1  # é€šå¸¸ã®ãƒˆãƒ¬ãƒ³ãƒ‰
                else:
                    regime_state[i] = 0  # ãƒ¬ãƒ³ã‚¸
                
                # å®Ÿè·µçš„ãªã—ãã„å€¤ï¼ˆå›ºå®šå€¤ã§å®‰å®šæ€§ç¢ºä¿ï¼‰
                upper_bound = 0.65  # ã‚·ãƒ³ãƒ—ãƒ«ãªä¸Šé™
                lower_bound = 0.35  # ã‚·ãƒ³ãƒ—ãƒ«ãªä¸‹é™
                
                strong_upper = 0.75  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ä¸Šé™
                strong_lower = 0.25  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ä¸‹é™
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚·ã‚°ãƒŠãƒ«ï¼ˆå®Ÿè·µçš„åˆ¤å®šï¼‰
                if signal > strong_upper:
                    trend_signals[i] = 1.0  # å¼·ã„ä¸Šæ˜‡
                elif signal > upper_bound:
                    trend_signals[i] = 0.5  # å¼±ã„ä¸Šæ˜‡
                elif signal > 0.55:  # å¾®å¼±ãªä¸Šæ˜‡ã‚‚æ¤œå‡º
                    trend_signals[i] = 0.25
                elif signal < strong_lower:
                    trend_signals[i] = -1.0  # å¼·ã„ä¸‹é™
                elif signal < lower_bound:
                    trend_signals[i] = -0.5  # å¼±ã„ä¸‹é™
                elif signal < 0.45:  # å¾®å¼±ãªä¸‹é™ã‚‚æ¤œå‡º
                    trend_signals[i] = -0.25
                else:
                    trend_signals[i] = 0.0  # ãƒ¬ãƒ³ã‚¸
            
            # äºˆæ¸¬åˆ†æ
            if self.enable_prediction:
                predictive_signal, momentum_forecast, volatility_forecast = predictive_analysis(
                    prices, final_signal, volatility, self.prediction_horizon
                )
                
                # ğŸš€ GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£äºˆæ¸¬ã§å‘ä¸Š
                if self.enable_garch and 'garch_volatility' in advanced_components:
                    garch_vol = advanced_components['garch_volatility']
                    # GARCHãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã§å¾“æ¥ã®äºˆæ¸¬ã‚’è£œå¼·
                    for i in range(len(volatility_forecast)):
                        if not np.isnan(garch_vol[i]) and not np.isnan(volatility_forecast[i]):
                            # GARCHäºˆæ¸¬ã¨å¾“æ¥äºˆæ¸¬ã®åŠ é‡å¹³å‡
                            volatility_forecast[i] = 0.7 * garch_vol[i] + 0.3 * volatility_forecast[i]
                        elif not np.isnan(garch_vol[i]):
                            volatility_forecast[i] = garch_vol[i]
            else:
                predictive_signal = np.full(n, np.nan)
                momentum_forecast = np.full(n, np.nan)
                volatility_forecast = np.full(n, np.nan)
            
            # ğŸš€ æ¬¡ä¸–ä»£é«˜åº¦ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ†é¡ï¼ˆãƒãƒ£ãƒ¼ãƒˆèƒŒæ™¯è‰²ç”¨ï¼‰
            trend_direction, direction_strength = calculate_trend_direction_classification(
                final_signal, trend_signals, prices, lookback_period=5
            )
            
            # è¿½åŠ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            market_efficiency = np.zeros(n)
            information_ratio = np.zeros(n)
            trend_probability = np.zeros(n)
            regime_probability = np.zeros(n)
            
            for i in range(self.analysis_period, n):
                # å¸‚å ´åŠ¹ç‡æ€§ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã‹ã‚‰ã®ä¹–é›¢ï¼‰
                returns = np.diff(prices[i-self.analysis_period:i+1])
                if len(returns) > 0:
                    autocorr = np.corrcoef(returns[:-1], returns[1:])[0, 1] if len(returns) > 1 else 0
                    market_efficiency[i] = 1 - abs(autocorr) if not np.isnan(autocorr) else 1
                
                # æƒ…å ±æ¯”ç‡
                avg_return = np.mean(returns) if len(returns) > 0 else 0
                std_return = np.std(returns) if len(returns) > 0 else 1e-10  # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                std_return = max(std_return, 1e-10)  # æœ€å°å€¤åˆ¶é™
                information_ratio[i] = avg_return / std_return
                
                # ç¢ºç‡è¨ˆç®—
                trend_probability[i] = trend_strength[i]
                regime_probability[i] = regime_state[i] / 2.0
            
            # ç¾åœ¨çŠ¶æ…‹ã®åˆ¤å®š
            latest_signal = trend_signals[-1] if len(trend_signals) > 0 else 0
            latest_strength = trend_strength[-1] if len(trend_strength) > 0 else 0
            latest_confidence = ensemble_confidence[-1] if len(ensemble_confidence) > 0 else 0
            
            if latest_signal > 0.7:
                current_trend = "strong_uptrend"
            elif latest_signal > 0.2:
                current_trend = "uptrend"
            elif latest_signal < -0.7:
                current_trend = "strong_downtrend"
            elif latest_signal < -0.2:
                current_trend = "downtrend"
            else:
                current_trend = "range"
            
            # çµæœä½œæˆï¼ˆğŸš€ å®‡å®™æœ€å¼·ç‰ˆï¼‰
            result = UltimateChopTrendResult(
                ultimate_trend_index=final_signal,
                trend_signals=trend_signals,
                trend_strength=trend_strength,
                regime_state=regime_state,
                trend_direction=trend_direction,
                direction_strength=direction_strength,
                confidence_score=ensemble_confidence,
                trend_probability=trend_probability,
                regime_probability=regime_probability,
                predictive_signal=predictive_signal,
                momentum_forecast=momentum_forecast,
                volatility_forecast=volatility_forecast,
                hilbert_component=components['hilbert'],
                fractal_component=components['fractal'],
                wavelet_component=components['wavelet'],
                kalman_component=components['kalman'],
                entropy_component=components['entropy'],
                chaos_component=components['chaos'],
                # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£ææˆåˆ†
                unscented_kalman_component=advanced_components['unscented_kalman'],
                garch_volatility=advanced_components['garch_volatility'],
                regime_switching_probs=advanced_components['regime_switching_probs'],
                spectral_power=advanced_components['spectral_power'],
                dominant_frequency=advanced_components['dominant_frequency'],
                multiscale_entropy=components['entropy'] if self.enable_multiscale_entropy else np.full(n, np.nan),
                nonlinear_dynamics=advanced_components['predictability_score'],
                predictability_score=advanced_components['predictability_score'],
                market_efficiency=market_efficiency,
                information_ratio=information_ratio,
                adaptive_threshold=adaptive_threshold,
                correlation_dimension=advanced_components['correlation_dimension'],
                lyapunov_exponent=advanced_components['lyapunov_exponent'],
                current_trend=current_trend,
                current_strength=latest_strength,
                current_confidence=latest_confidence
            )
            
            self._result = result
            self._values = final_signal
            
            return result
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            self.logger.error(f"UltimateChopTrendè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}\nè©³ç´°:\n{error_details}")
            return self._create_empty_result(len(data))
    
    def _create_empty_result(self, length: int) -> UltimateChopTrendResult:
        """ç©ºã®çµæœã‚’ä½œæˆï¼ˆğŸš€ å®‡å®™æœ€å¼·ç‰ˆï¼‰"""
        return UltimateChopTrendResult(
            ultimate_trend_index=np.full(length, np.nan),
            trend_signals=np.zeros(length),
            trend_strength=np.zeros(length),
            regime_state=np.zeros(length),
            trend_direction=np.zeros(length),
            direction_strength=np.zeros(length),
            confidence_score=np.zeros(length),
            trend_probability=np.zeros(length),
            regime_probability=np.zeros(length),
            predictive_signal=np.full(length, np.nan),
            momentum_forecast=np.full(length, np.nan),
            volatility_forecast=np.full(length, np.nan),
            hilbert_component=np.full(length, np.nan),
            fractal_component=np.full(length, np.nan),
            wavelet_component=np.full(length, np.nan),
            kalman_component=np.full(length, np.nan),
            entropy_component=np.full(length, np.nan),
            chaos_component=np.full(length, np.nan),
            # ğŸš€ æ–°æ©Ÿèƒ½: é«˜åº¦è§£ææˆåˆ†
            unscented_kalman_component=np.full(length, np.nan),
            garch_volatility=np.full(length, np.nan),
            regime_switching_probs=np.zeros((length, 3)),
            spectral_power=np.full(length, np.nan),
            dominant_frequency=np.full(length, np.nan),
            multiscale_entropy=np.full(length, np.nan),
            nonlinear_dynamics=np.full(length, np.nan),
            predictability_score=np.full(length, np.nan),
            market_efficiency=np.zeros(length),
            information_ratio=np.zeros(length),
            adaptive_threshold=np.full(length, 0.5),
            correlation_dimension=np.full(length, np.nan),
            lyapunov_exponent=np.full(length, np.nan),
            current_trend="range",
            current_strength=0.0,
            current_confidence=0.0
        )
    
    def get_values(self) -> Optional[np.ndarray]:
        """ãƒ¡ã‚¤ãƒ³æŒ‡æ¨™å€¤ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.ultimate_trend_index.copy()
        return None
    
    def get_result(self) -> Optional[UltimateChopTrendResult]:
        """å®Œå…¨ãªçµæœã‚’å–å¾—"""
        return self._result
    
    def reset(self) -> None:
        """ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        if self.use_legacy_chop and self.legacy_chop is not None:
            self.legacy_chop.reset()
        self._result = None
        self._cache = {} 