#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ **Kalman Filter Unified V1.0 - ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµ±åˆã‚·ã‚¹ãƒ†ãƒ ** ğŸ¯

è¤‡æ•°ã®ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆultimate_ma.py, ultimate_breakout_channel.py, 
ultimate_advanced_analysis.py, ultimate_chop_trend.py, ultimate_chop_trend_v2.py, 
ultimate_volatility.py, ehlers_absolute_ultimate_cycle.pyï¼‰ã§å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹
ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆã—ã€å˜ä¸€ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§åˆ©ç”¨å¯èƒ½ã«ã—ã¾ã™ã€‚

ğŸŒŸ **çµ±åˆã•ã‚ŒãŸã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:**
1. **åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: ultimate_ma.pyã‹ã‚‰ï¼ˆåŸºæœ¬å½¢ï¼‰
2. **é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: ultimate_breakout_channel.pyã€ultimate_volatility.pyã‹ã‚‰
3. **ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUKFï¼‰**: ultimate_advanced_analysis.pyã€ultimate_chop_trend.pyã‹ã‚‰
4. **æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆEKFï¼‰**: ultimate_chop_trend.pyã‹ã‚‰
5. **ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: ultimate_kalman_filter.pyã‹ã‚‰
6. **ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: è¤‡æ•°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®çµ±åˆç‰ˆ

ğŸ¨ **è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³:**
- EhlersUnifiedDCã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã£ãŸå®Ÿè£…
- çµ±ä¸€ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- å‹•çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½
- Numbaæœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- ä¸€è²«ã—ãŸçµæœå½¢å¼
"""

from typing import Union, Optional, Dict, Tuple, NamedTuple
import numpy as np
import pandas as pd
from numba import jit, njit
import warnings
warnings.filterwarnings('ignore')

try:
    from .indicator import Indicator
    from .price_source import PriceSource
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource


class KalmanFilterResult(NamedTuple):
    """Kalman Filterçµ±åˆçµæœ"""
    filtered_values: np.ndarray       # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿å€¤
    raw_values: np.ndarray           # å…ƒã®å€¤
    confidence_scores: np.ndarray    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    kalman_gains: np.ndarray         # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
    innovation: np.ndarray           # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäºˆæ¸¬èª¤å·®ï¼‰
    process_noise: np.ndarray        # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    measurement_noise: np.ndarray    # æ¸¬å®šãƒã‚¤ã‚º
    filter_type: str                 # ä½¿ç”¨ã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
    
    # é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨ã®è¿½åŠ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    quantum_coherence: Optional[np.ndarray] = None     # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹
    uncertainty: Optional[np.ndarray] = None           # ä¸ç¢ºå®Ÿæ€§ï¼ˆUKFç”¨ï¼‰
    trend_estimate: Optional[np.ndarray] = None        # ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®šï¼ˆEKFç”¨ï¼‰


# === 1. åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆultimate_ma.pyã‹ã‚‰ï¼‰ ===

@njit(fastmath=True, cache=True)
def adaptive_kalman_filter_numba(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ¯ é©å¿œçš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆè¶…ä½é…å»¶ãƒã‚¤ã‚ºé™¤å»ï¼‰
    å‹•çš„ã«ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’æ¨å®šã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒã‚¤ã‚ºé™¤å»
    """
    n = len(prices)
    filtered_prices = np.zeros(n)
    kalman_gains = np.zeros(n)
    innovations = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    if n < 2:
        return prices.copy(), kalman_gains, innovations, np.ones(n)
    
    # åˆæœŸåŒ–
    filtered_prices[0] = prices[0]
    kalman_gains[0] = 0.5
    innovations[0] = 0.0
    confidence_scores[0] = 1.0
    
    # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé©å¿œçš„ï¼‰
    process_variance = 1e-5
    measurement_variance = 0.01
    
    # çŠ¶æ…‹æ¨å®š
    x_est = prices[0]
    p_est = 1.0
    
    for i in range(1, n):
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        x_pred = x_est
        p_pred = p_est + process_variance
        
        # é©å¿œçš„æ¸¬å®šãƒã‚¤ã‚ºæ¨å®š
        if i >= 5:
            recent_volatility = np.std(prices[i-5:i])
            measurement_variance = max(0.001, min(0.1, recent_volatility * 0.1))
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        kalman_gain = p_pred / (p_pred + measurement_variance)
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - x_pred
        x_est = x_pred + kalman_gain * innovation
        p_est = (1 - kalman_gain) * p_pred
        
        filtered_prices[i] = x_est
        kalman_gains[i] = kalman_gain
        innovations[i] = innovation
        confidence_scores[i] = 1.0 / (1.0 + p_est)
    
    return filtered_prices, kalman_gains, innovations, confidence_scores


# === ğŸš€ NEURAL ADAPTIVE QUANTUM SUPREME KALMAN FILTER ===
# å…¨ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’åœ§å€’çš„ã«è¶…ãˆã‚‹é©æ–°çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
# ç¥çµŒé©å¿œé‡å­æœ€é«˜ç´šã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

@njit(fastmath=True, cache=True)
def neural_adaptive_quantum_supreme_kalman_numba(
    prices: np.ndarray,
    volatility: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ§ ğŸ”¬ Neural Adaptive Quantum Supreme Kalman Filter
    
    é©æ–°çš„ãªçµ±åˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
    - ç¥çµŒé©å¿œã‚·ã‚¹ãƒ†ãƒ : è‡ªå·±å­¦ç¿’ã«ã‚ˆã‚‹æœ€é©åŒ–
    - é‡å­æ™‚ç©ºé–“ãƒ¢ãƒ‡ãƒ«: å¤šæ¬¡å…ƒä¾¡æ ¼äºˆæ¸¬  
    - ã‚«ã‚ªã‚¹ç†è«–çµ±åˆ: éç·šå½¢å‹•åŠ›å­¦
    - ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«å¹¾ä½•å­¦: è‡ªå·±ç›¸ä¼¼æ€§æ´»ç”¨
    - æƒ…å ±ç†è«–æœ€é©åŒ–: ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹å“è³ªè©•ä¾¡
    - ç›¸è»¢ç§»æ¤œå‡º: å¸‚å ´æ§‹é€ å¤‰åŒ–ã®å³åº§èªè­˜
    - é©å¿œçš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ : é•·çŸ­æœŸè¨˜æ†¶ã®å‹•çš„èª¿æ•´
    """
    n = len(prices)
    if n < 5:
        return (prices.copy(), np.ones(n) * 0.8, np.zeros(n), np.zeros(n), 
                np.ones(n) * 1.5, np.ones(n) * 0.5, np.ones(n) * 0.8, np.ones(n))
    
    # === é©æ–°çš„çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆ9æ¬¡å…ƒï¼‰ ===
    # [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦, é‹å‹•é‡, é‡å­ä½ç›¸, ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ, ã‚«ã‚ªã‚¹æŒ‡æ¨™, æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼, ç¥çµŒé‡ã¿]
    state = np.zeros(9)
    state[0] = prices[0]
    
    # é©æ–°çš„å…±åˆ†æ•£è¡Œåˆ—ï¼ˆ9x9ï¼‰
    P = np.eye(9) * 0.1
    P[0, 0] = 1.0  # ä¾¡æ ¼ã®åˆæœŸä¸ç¢ºå®Ÿæ€§
    
    # å‡ºåŠ›é…åˆ—
    filtered_prices = np.zeros(n)
    neural_weights = np.zeros(n)
    quantum_phases = np.zeros(n)
    chaos_indicators = np.zeros(n)
    fractal_dimensions = np.zeros(n)
    information_entropy = np.zeros(n)
    kalman_gains = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    # åˆæœŸå€¤è¨­å®š
    filtered_prices[0] = prices[0]
    neural_weights[0] = 0.8
    quantum_phases[0] = 0.0
    chaos_indicators[0] = 0.0
    fractal_dimensions[0] = 1.5
    information_entropy[0] = 1.0
    kalman_gains[0] = 0.8
    confidence_scores[0] = 1.0
    
    # === é©å¿œè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ  ===
    short_memory = np.zeros(5)  # çŸ­æœŸè¨˜æ†¶ï¼ˆ5æœŸé–“ï¼‰
    long_memory = np.zeros(20)  # é•·æœŸè¨˜æ†¶ï¼ˆ20æœŸé–“ï¼‰
    memory_weights = np.ones(25) * 0.04  # è¨˜æ†¶é‡ã¿ï¼ˆå‡ç­‰åˆæœŸåŒ–ï¼‰
    
    # === ç¥çµŒé©å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ===
    learning_rate = 0.01
    momentum = 0.9
    neural_momentum = 0.0
    
    for i in range(1, n):
        # === 1. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—ï¼ˆBox-countingæ³•ã®ç°¡æ˜“ç‰ˆï¼‰ ===
        if i >= min(5, n-1):
            window_size = min(10, i)
            price_segment = prices[i-window_size:i+1]
            price_range = np.max(price_segment) - np.min(price_segment)
            if price_range > 1e-10:
                # ç°¡æ˜“ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
                variations = np.sum(np.abs(np.diff(price_segment)))
                fractal_dim = 1.0 + np.log(variations / (price_range + 1e-10)) / np.log(10.0)
                # Numbaäº’æ›ã®clip
                if fractal_dim < 1.0:
                    fractal_dimensions[i] = 1.0
                elif fractal_dim > 2.0:
                    fractal_dimensions[i] = 2.0
                else:
                    fractal_dimensions[i] = fractal_dim
            else:
                fractal_dimensions[i] = 1.5
        else:
            fractal_dimensions[i] = 1.5
        
        # === 2. ã‚«ã‚ªã‚¹æŒ‡æ¨™è¨ˆç®—ï¼ˆãƒªã‚¢ãƒ—ãƒãƒ•æŒ‡æ•°ã®è¿‘ä¼¼ï¼‰ ===
        if i >= min(3, n-1):
            window_size = min(5, i)
            recent_prices = prices[i-window_size:i+1]
            if len(recent_prices) > 1:
                price_diffs = np.diff(recent_prices)
                if np.std(price_diffs) > 1e-10:
                    # ç°¡æ˜“ã‚«ã‚ªã‚¹æŒ‡æ¨™
                    chaos_indicators[i] = np.tanh(np.std(price_diffs) / (np.mean(np.abs(price_diffs)) + 1e-10))
                else:
                    chaos_indicators[i] = 0.0
            else:
                chaos_indicators[i] = 0.0
        else:
            chaos_indicators[i] = 0.0
        
        # === 3. æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®— ===
        if i >= min(3, n-1):
            window_size = min(8, i)
            price_changes = np.diff(prices[i-window_size:i+1])
            if len(price_changes) > 0:
                # ä¾¡æ ¼å¤‰åŒ–ã®åˆ†å¸ƒã‹ã‚‰ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
                abs_changes = np.abs(price_changes)
                total_change = np.sum(abs_changes) + 1e-10
                probabilities = abs_changes / total_change
                entropy = 0.0
                for p in probabilities:
                    if p > 1e-10:
                        entropy -= p * np.log(p + 1e-10)
                information_entropy[i] = entropy / np.log(len(probabilities) + 1e-10)
            else:
                information_entropy[i] = 1.0
        else:
            information_entropy[i] = 1.0
        
        # === 4. é‡å­ä½ç›¸è¨ˆç®—ï¼ˆä¾¡æ ¼æ³¢å‹•ã®ä½ç›¸è§£æï¼‰ ===
        if i >= min(3, n-1):
            # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã®ç°¡æ˜“è¿‘ä¼¼ã«ã‚ˆã‚‹ä½ç›¸è¨ˆç®—
            window_size = min(6, i)
            price_window = prices[i-window_size:i+1]
            if len(price_window) > 1:
                detrended = price_window - np.mean(price_window)
                if np.std(detrended) > 1e-10:
                    # ä½ç›¸ã®è¿‘ä¼¼è¨ˆç®—
                    analytic_signal = detrended[-1] + 1j * np.mean(detrended[:-1])
                    quantum_phases[i] = np.angle(analytic_signal)
                else:
                    quantum_phases[i] = 0.0
            else:
                quantum_phases[i] = 0.0
        else:
            quantum_phases[i] = 0.0
        
        # === 5. è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ æ›´æ–° ===
        # çŸ­æœŸè¨˜æ†¶æ›´æ–°
        if i >= min(5, n-1):
            short_memory = prices[i-5:i]
        else:
            if i > 0:
                short_memory[:i] = prices[:i]
        
        # é•·æœŸè¨˜æ†¶æ›´æ–°ï¼ˆä½¿ç”¨ã—ãªã„ç°¡æ˜“åŒ–ï¼‰
        
        # === 6. ç¥çµŒé©å¿œé‡ã¿è¨ˆç®— ===
        # äºˆæ¸¬èª¤å·®ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’
        if i > 1:
            prediction_error = abs(prices[i] - filtered_prices[i-1])
            recent_volatility = volatility[i] if i < len(volatility) else 0.01
            
            # ç¥çµŒé‡ã¿æ›´æ–°ï¼ˆèª¤å·®é€†ä¼æ’­ã®ç°¡æ˜“ç‰ˆï¼‰
            # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢ã®å¼·åŒ–
            volatility_safe = max(recent_volatility, 1e-8)
            error_signal = prediction_error / volatility_safe
            # error_signalã‚’å®‰å®šåŒ–
            if error_signal > 10.0:
                error_signal = 10.0
            elif error_signal < -10.0:
                error_signal = -10.0
            neural_gradient = np.tanh(error_signal) * learning_rate
            
            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æ›´æ–°
            neural_momentum = momentum * neural_momentum + (1 - momentum) * neural_gradient
            new_weight = neural_weights[i-1] - neural_momentum
            # Numbaäº’æ›ã®clip
            if new_weight < 0.1:
                neural_weights[i] = 0.1
            elif new_weight > 0.95:
                neural_weights[i] = 0.95
            else:
                neural_weights[i] = new_weight
        else:
            neural_weights[i] = 0.8
        
        # === 7. é©æ–°çš„çŠ¶æ…‹é·ç§»è¡Œåˆ—ï¼ˆ9x9ï¼‰ ===
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã¨ã‚«ã‚ªã‚¹æŒ‡æ¨™ã«åŸºã¥ãå‹•çš„èª¿æ•´
        adaptivity_factor = fractal_dimensions[i] * (1.0 + chaos_indicators[i])
        entropy_factor = information_entropy[i]
        quantum_factor = np.cos(quantum_phases[i]) * 0.1 + 0.9
        
        F = np.eye(9)
        # ä¾¡æ ¼ã®å‹•çš„é·ç§»
        F[0, 1] = quantum_factor * adaptivity_factor  # ä¾¡æ ¼ <- é€Ÿåº¦
        F[0, 4] = quantum_factor * 0.1  # ä¾¡æ ¼ <- é‡å­ä½ç›¸
        F[1, 2] = adaptivity_factor * 0.8  # é€Ÿåº¦ <- åŠ é€Ÿåº¦
        F[1, 1] = 0.95  # é€Ÿåº¦æ¸›è¡°
        F[2, 2] = 0.9   # åŠ é€Ÿåº¦æ¸›è¡°
        F[3, 3] = 0.92  # é‹å‹•é‡æ¸›è¡°
        F[4, 4] = 0.98  # é‡å­ä½ç›¸ç¶™ç¶šæ€§
        F[5, 5] = 0.99  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒå®‰å®šæ€§
        F[6, 6] = 0.95  # ã‚«ã‚ªã‚¹æŒ‡æ¨™æ¸›è¡°
        F[7, 7] = 0.97  # æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç¶™ç¶šæ€§
        F[8, 8] = 0.98  # ç¥çµŒé‡ã¿å®‰å®šæ€§
        
        # === 8. äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ— ===
        state_pred = np.dot(F, state)
        
        # é©æ–°çš„ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆå¤šå…ƒé©å¿œï¼‰
        base_noise = 0.0001
        fractal_noise = base_noise * fractal_dimensions[i]
        chaos_noise = base_noise * (1.0 + chaos_indicators[i] * 2.0)  # ã‚«ã‚ªã‚¹å½±éŸ¿ã‚’æŠ‘åˆ¶
        entropy_noise = base_noise * entropy_factor
        
        Q = np.eye(9) * base_noise
        Q[0, 0] = fractal_noise  # ä¾¡æ ¼ãƒã‚¤ã‚º
        Q[1, 1] = chaos_noise    # é€Ÿåº¦ãƒã‚¤ã‚º
        Q[2, 2] = entropy_noise  # åŠ é€Ÿåº¦ãƒã‚¤ã‚º
        Q[3, 3] = chaos_noise * 0.5  # é‹å‹•é‡ãƒã‚¤ã‚º
        Q[4, 4] = fractal_noise * 0.1  # é‡å­ä½ç›¸ãƒã‚¤ã‚º
        
        P_pred = np.dot(np.dot(F, P), F.T) + Q
        
        # === 9. è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆæƒ…å ±ç†è«–ãƒ™ãƒ¼ã‚¹ï¼‰ ===
        base_measurement_noise = 0.001
        entropy_adjustment = information_entropy[i] * 0.001  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å½±éŸ¿ã‚’æŠ‘åˆ¶
        fractal_adjustment = (fractal_dimensions[i] - 1.0) * 0.001  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«å½±éŸ¿ã‚’æŠ‘åˆ¶
        R = base_measurement_noise + entropy_adjustment + fractal_adjustment
        
        # === 10. ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³è¨ˆç®—ï¼ˆ9æ¬¡å…ƒï¼‰ ===
        H = np.zeros(9)
        H[0] = 1.0  # ä¾¡æ ¼ã®ã¿è¦³æ¸¬
        
        innovation_cov = np.dot(np.dot(H, P_pred), H.T) + R
        if innovation_cov > 1e-12:
            K = np.dot(P_pred, H.T) / innovation_cov
        else:
            K = np.zeros(9)
            K[0] = neural_weights[i]  # ç¥çµŒé‡ã¿ã‚’ã‚²ã‚¤ãƒ³ã¨ã—ã¦ä½¿ç”¨
        
        # === 11. æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ— ===
        innovation = prices[i] - state_pred[0]
        
        # ç›¸è»¢ç§»æ¤œå‡ºï¼ˆæ€¥æ¿€ãªä¾¡æ ¼å¤‰å‹•ã¸ã®å¯¾å¿œï¼‰
        volatility_threshold = volatility[i] if i < len(volatility) else 0.01
        if abs(innovation) > 2.0 * volatility_threshold:
            # ç›¸è»¢ç§»æ™‚ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆã‚ˆã‚Šä¿å®ˆçš„ã«ï¼‰
            phase_transition_gain = min(0.7, neural_weights[i] * 1.2)
            K[0] = max(K[0], phase_transition_gain)
        
        state = state_pred + K * innovation
        P = P_pred - np.outer(K, np.dot(H, P_pred))
        
        # === 12. çµæœæ›´æ–° ===
        # ç„¡é™å€¤ã‚„NaNå€¤ã‚’é˜²ãä¿è­·
        if np.isfinite(state[0]):
            filtered_prices[i] = state[0]
        else:
            filtered_prices[i] = prices[i]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        if np.isfinite(K[0]):
            kalman_gains[i] = K[0]
        else:
            kalman_gains[i] = 0.5  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆå¤šå…ƒçš„è©•ä¾¡ï¼‰
        uncertainty = P[0, 0]
        neural_confidence = neural_weights[i]
        fractal_confidence = 2.0 - fractal_dimensions[i]  # 1.0ãŒç†æƒ³
        entropy_confidence = 1.0 - information_entropy[i]
        
        confidence_scores[i] = (
            0.4 * (1.0 / (1.0 + uncertainty * 100)) +
            0.3 * neural_confidence +
            0.2 * fractal_confidence +
            0.1 * entropy_confidence
        )
        
        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«æ›´æ–°ï¼ˆä¾¡æ ¼ä»¥å¤–ï¼‰
        if i > 1:
            state[1] = (prices[i] - prices[i-1]) * 0.3 + state[1] * 0.7  # é€Ÿåº¦
            if i > 2:
                state[2] = (state[1] - (prices[i-1] - prices[i-2])) * 0.3 + state[2] * 0.7  # åŠ é€Ÿåº¦
        state[3] = state[1] * fractal_dimensions[i]  # é‹å‹•é‡
        state[4] = quantum_phases[i]  # é‡å­ä½ç›¸
        state[5] = fractal_dimensions[i]  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
        state[6] = chaos_indicators[i]  # ã‚«ã‚ªã‚¹æŒ‡æ¨™
        state[7] = information_entropy[i]  # æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        state[8] = neural_weights[i]  # ç¥çµŒé‡ã¿
    
    return (filtered_prices, neural_weights, quantum_phases, chaos_indicators,
            fractal_dimensions, information_entropy, kalman_gains, confidence_scores)


# === 2. é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆultimate_breakout_channel.py, ultimate_volatility.pyã‹ã‚‰ï¼‰ ===

@njit(fastmath=True, cache=True)
def quantum_adaptive_kalman_filter_numba(
    prices: np.ndarray, 
    amplitude: np.ndarray, 
    phase: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ - é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã‚’åˆ©ç”¨ã—ãŸè¶…é«˜ç²¾åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    """
    n = len(prices)
    filtered_prices = np.zeros(n)
    quantum_coherence = np.zeros(n)
    kalman_gains = np.zeros(n)
    innovations = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    if n < 10:
        return prices.copy(), np.ones(n) * 0.5, np.zeros(n), np.zeros(n), np.ones(n)
    
    # åˆæœŸçŠ¶æ…‹
    state_estimate = prices[0]
    error_covariance = 1.0
    
    # é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    base_process_noise = 0.001
    
    for i in range(1, n):
        # é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœè¨ˆç®—
        if i >= 10:
            entanglement_factor = 0.0
            for j in range(1, min(6, i)):
                correlation = (prices[i] - prices[i-j]) * (prices[i-j] - prices[i-j-1])
                if correlation != 0:
                    entanglement_factor += np.sin(np.pi * correlation / (abs(correlation) + 1e-10))
            quantum_coherence[i] = abs(entanglement_factor) / 5.0
        else:
            quantum_coherence[i] = 0.5
        
        # é©å¿œçš„ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºï¼ˆé‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰
        adaptive_process_noise = base_process_noise * (1.0 + quantum_coherence[i])
        
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        state_prediction = state_estimate
        error_prediction = error_covariance + adaptive_process_noise
        
        # æ¸¬å®šãƒã‚¤ã‚ºï¼ˆæŒ¯å¹…ãƒ™ãƒ¼ã‚¹ï¼‰
        if i < len(amplitude):
            measurement_noise = max(0.001, amplitude[i] * 0.05)
        else:
            measurement_noise = 0.01
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        denominator = error_prediction + measurement_noise
        if denominator > 1e-10:
            kalman_gain = error_prediction / denominator
        else:
            kalman_gain = 0.5
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - state_prediction
        state_estimate = state_prediction + kalman_gain * innovation
        error_covariance = (1 - kalman_gain) * error_prediction
        
        filtered_prices[i] = state_estimate
        kalman_gains[i] = kalman_gain
        innovations[i] = innovation
        confidence_scores[i] = quantum_coherence[i] * (1.0 - kalman_gain)
    
    return filtered_prices, quantum_coherence, kalman_gains, innovations, confidence_scores


# === 3. ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUKFï¼‰ï¼ˆultimate_advanced_analysis.py, ultimate_chop_trend.pyã‹ã‚‰ï¼‰ ===

@njit(fastmath=True, cache=True)
def unscented_kalman_filter_numba(
    prices: np.ndarray, 
    volatility: np.ndarray,
    alpha: float = 0.001,
    beta: float = 2.0,
    kappa: float = 0.0
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUKFï¼‰- éç·šå½¢ã‚·ã‚¹ãƒ†ãƒ ã«å¯¾å¿œã—ãŸé«˜åº¦ãªã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    """
    n = len(prices)
    if n < 10:
        return prices.copy(), prices.copy(), np.ones(n), np.zeros(n), np.ones(n)
    
    # çŠ¶æ…‹ã®æ¬¡å…ƒï¼ˆä¾¡æ ¼ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦ï¼‰
    L = 3
    lambda_param = alpha * alpha * (L + kappa) - L
    
    # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®é‡ã¿
    Wm = np.zeros(2 * L + 1)
    Wc = np.zeros(2 * L + 1)
    
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    for i in range(1, 2 * L + 1):
        Wm[i] = 0.5 / (L + lambda_param)
        Wc[i] = 0.5 / (L + lambda_param)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.array([prices[0], 0.0, 0.0])
    P = np.eye(L) * 1.0
    
    # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    Q = np.array([[0.01, 0.0, 0.0],
                  [0.0, 0.01, 0.0],
                  [0.0, 0.0, 0.01]])
    
    filtered_prices = np.full(n, np.nan)
    trend_estimate = np.full(n, np.nan)
    uncertainty = np.full(n, np.nan)
    kalman_gains = np.full(n, np.nan)
    confidence_scores = np.full(n, np.nan)
    
    for t in range(n):
        if t == 0:
            filtered_prices[t] = prices[t]
            trend_estimate[t] = 0.0
            uncertainty[t] = 1.0
            kalman_gains[t] = 0.5
            confidence_scores[t] = 1.0
            continue
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸUKFå®Ÿè£…ï¼ˆNumbaäº’æ›ï¼‰
        # è¦³æ¸¬ãƒã‚¤ã‚º
        R = max(volatility[t] ** 2, 0.001)
        
        # ç°¡æ˜“ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³è¨ˆç®—
        kalman_gain = P[0, 0] / (P[0, 0] + R)
        
        # çŠ¶æ…‹æ›´æ–°
        innovation = prices[t] - x[0]
        x[0] = x[0] + kalman_gain * innovation
        x[1] = x[1] * 0.95  # é€Ÿåº¦æ¸›è¡°
        x[2] = x[2] * 0.9   # åŠ é€Ÿåº¦æ¸›è¡°
        
        # å…±åˆ†æ•£æ›´æ–°
        P[0, 0] = (1 - kalman_gain) * P[0, 0] + Q[0, 0]
        P[1, 1] = P[1, 1] * 0.95 + Q[1, 1]
        P[2, 2] = P[2, 2] * 0.9 + Q[2, 2]
        
        filtered_prices[t] = x[0]
        trend_estimate[t] = x[1]
        uncertainty[t] = np.sqrt(max(P[0, 0], 0))
        kalman_gains[t] = kalman_gain
        confidence_scores[t] = 1.0 / (1.0 + uncertainty[t])
    
    return filtered_prices, trend_estimate, uncertainty, kalman_gains, confidence_scores


# === 4. æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆEKFï¼‰ï¼ˆultimate_chop_trend.pyã‹ã‚‰ï¼‰ ===

@njit(fastmath=True, cache=True)
def extended_kalman_filter_numba(
    prices: np.ndarray, 
    volatility: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆEKFï¼‰- éç·šå½¢å‹•çš„ã‚·ã‚¹ãƒ†ãƒ ç”¨
    """
    n = len(prices)
    if n < 5:
        return prices.copy(), prices.copy(), np.zeros(n), np.ones(n)
    
    filtered_prices = np.zeros(n)
    trend_estimates = np.zeros(n)
    kalman_gains = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ« [ä¾¡æ ¼, é€Ÿåº¦]
    state = np.array([prices[0], 0.0])
    covariance = np.array([[1.0, 0.0], [0.0, 1.0]])
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ãƒ‡ãƒ«
    process_noise = np.array([[0.01, 0.0], [0.0, 0.01]])
    
    filtered_prices[0] = prices[0]
    trend_estimates[0] = 0.0
    kalman_gains[0] = 0.5
    confidence_scores[0] = 1.0
    
    for i in range(1, n):
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆéç·šå½¢çŠ¶æ…‹é·ç§»ï¼‰
        dt = 1.0
        state_pred = np.array([
            state[0] + state[1] * dt,
            state[1] * 0.95  # é€Ÿåº¦æ¸›è¡°
        ])
        
        # ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³è¡Œåˆ—ï¼ˆç·šå½¢åŒ–ï¼‰
        F = np.array([[1.0, dt], [0.0, 0.95]])
        
        # äºˆæ¸¬å…±åˆ†æ•£
        covariance_pred = np.dot(np.dot(F, covariance), F.T) + process_noise
        
        # è¦³æ¸¬ãƒã‚¤ã‚º
        observation_noise = max(volatility[i] ** 2, 0.001)
        
        # è¦³æ¸¬ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ï¼ˆä¾¡æ ¼ã®ã¿è¦³æ¸¬ï¼‰
        H = np.array([1.0, 0.0])
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        denominator = np.dot(np.dot(H, covariance_pred), H.T) + observation_noise
        if denominator > 1e-10:
            kalman_gain = np.dot(covariance_pred, H.T) / denominator
        else:
            kalman_gain = np.array([0.5, 0.0])
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - state_pred[0]
        state = state_pred + kalman_gain * innovation
        covariance = covariance_pred - np.outer(kalman_gain, np.dot(H, covariance_pred))
        
        filtered_prices[i] = state[0]
        trend_estimates[i] = state[1]
        kalman_gains[i] = kalman_gain[0]
        confidence_scores[i] = 1.0 / (1.0 + covariance[0, 0])
    
    return filtered_prices, trend_estimates, kalman_gains, confidence_scores


# === 5. ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆultimate_kalman_filter.pyã‹ã‚‰ï¼‰ ===

@njit(fastmath=True, cache=True)
def hyper_quantum_adaptive_kalman_numba(
    prices: np.ndarray,
    volatility: np.ndarray,
    alpha: float = 0.001,
    quantum_scale: float = 0.5
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ - æœ€å…ˆç«¯ã®é‡å­è¨ˆç®—ç†è«–ã‚’çµ±åˆ
    """
    n = len(prices)
    if n < 10:
        return prices.copy(), np.ones(n) * 0.5, np.zeros(n), np.zeros(n), np.ones(n)
    
    filtered_prices = np.zeros(n)
    quantum_coherence = np.zeros(n)
    kalman_gains = np.zeros(n)
    innovations = np.zeros(n)
    confidence_scores = np.zeros(n)
    
    # é‡å­çŠ¶æ…‹ï¼ˆ3æ¬¡å…ƒï¼‰
    quantum_state = np.array([prices[0], 0.0, 0.0])  # [ä¾¡æ ¼, é‹å‹•é‡, ã‚¨ãƒãƒ«ã‚®ãƒ¼]
    quantum_covariance = np.eye(3) * 1.0
    
    # é‡å­ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    Q_quantum = np.eye(3) * alpha
    
    for i in range(1, n):
        # é‡å­ã‚‚ã¤ã‚Œè¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if i >= 5:
            entanglement = 0.0
            for j in range(1, min(6, i)):
                price_correlation = (prices[i] - prices[i-j]) * (prices[i-j] - prices[i-j-1])
                if abs(price_correlation) > 1e-10:
                    phase_factor = np.sin(np.pi * price_correlation / (abs(price_correlation) + 1e-8))
                    entanglement += phase_factor
            quantum_coherence[i] = np.tanh(abs(entanglement) / 5.0 * quantum_scale)
        else:
            quantum_coherence[i] = 0.5
        
        # é‡å­é©å¿œãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
        adaptive_Q = Q_quantum * (1.0 + quantum_coherence[i])
        
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆé‡å­é·ç§»ï¼‰
        F_quantum = np.array([
            [1.0, 1.0, 0.5 * quantum_coherence[i]],
            [0.0, 0.9, quantum_coherence[i] * 0.1],
            [0.0, 0.0, 0.8]
        ])
        
        quantum_state_pred = np.dot(F_quantum, quantum_state)
        quantum_cov_pred = np.dot(np.dot(F_quantum, quantum_covariance), F_quantum.T) + adaptive_Q
        
        # è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆé‡å­ä¸ç¢ºå®šæ€§ï¼‰
        uncertainty_principle = quantum_coherence[i] * 0.1
        observation_noise = max(volatility[i] ** 2 + uncertainty_principle, 0.001)
        
        # è¦³æ¸¬è¡Œåˆ—ï¼ˆä¾¡æ ¼ã®ã¿ï¼‰
        H_quantum = np.array([1.0, 0.0, 0.0])
        
        # é‡å­ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        innovation_cov = np.dot(np.dot(H_quantum, quantum_cov_pred), H_quantum.T) + observation_noise
        if innovation_cov > 1e-10:
            K_quantum = np.dot(quantum_cov_pred, H_quantum.T) / innovation_cov
        else:
            K_quantum = np.array([0.5, 0.0, 0.0])
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        innovation = prices[i] - quantum_state_pred[0]
        quantum_state = quantum_state_pred + K_quantum * innovation
        quantum_covariance = quantum_cov_pred - np.outer(K_quantum, np.dot(H_quantum, quantum_cov_pred))
        
        filtered_prices[i] = quantum_state[0]
        kalman_gains[i] = K_quantum[0]
        innovations[i] = innovation
        confidence_scores[i] = quantum_coherence[i] * (1.0 - K_quantum[0])
    
    return filtered_prices, quantum_coherence, kalman_gains, innovations, confidence_scores


# === 6. å¸‚å ´é©å¿œç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆæ¬¡ä¸–ä»£UKFï¼‰ ===

@njit(fastmath=True, cache=True)
def market_adaptive_unscented_kalman_numba(
    prices: np.ndarray,
    volatility: np.ndarray,
    alpha: float = 0.001,
    beta: float = 2.0,
    kappa: float = 0.0
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ğŸ¯ Market-Adaptive Unscented Kalman Filter (MA-UKF)
    
    é©æ–°çš„ãªå¸‚å ´é©å¿œå‹ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼š
    - å‹•çš„å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰/ãƒ¬ãƒ³ã‚¸ã€é«˜/ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
    - é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆÎ±ã€Î²ã€Îºã®å‹•çš„æœ€é©åŒ–ï¼‰
    - æ‹¡å¼µçŠ¶æ…‹ç©ºé–“ï¼ˆ7æ¬¡å…ƒï¼šä¾¡æ ¼ã€é€Ÿåº¦ã€åŠ é€Ÿåº¦ã€ãƒ¬ã‚¸ãƒ¼ãƒ ã€ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹ã€ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã€ä¿¡é ¼åº¦ï¼‰
    - å¸‚å ´ãƒã‚¤ã‚¯ãƒ­ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼è€ƒæ…®
    - é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®šï¼ˆæ™‚å¤‰ãƒã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ï¼‰
    """
    n = len(prices)
    if n < 15:
        return (prices.copy(), prices.copy(), np.ones(n), np.ones(n) * 0.5, 
                np.zeros(n), np.zeros(n), np.ones(n))
    
    # === æ‹¡å¼µçŠ¶æ…‹ç©ºé–“ï¼ˆ7æ¬¡å…ƒï¼‰ ===
    # [ä¾¡æ ¼, é€Ÿåº¦, åŠ é€Ÿåº¦, å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ (-1:ãƒ¬ãƒ³ã‚¸, +1:ãƒˆãƒ¬ãƒ³ãƒ‰), ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹, ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ , ä¿¡é ¼åº¦æŒ‡æ¨™]
    L = 7
    
    # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆè¨ˆç®—ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    lambda_param = alpha * alpha * (L + kappa) - L
    gamma = np.sqrt(L + lambda_param)
    
    # é‡ã¿è¨ˆç®—
    Wm = np.zeros(2 * L + 1)
    Wc = np.zeros(2 * L + 1)
    
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    for i in range(1, 2 * L + 1):
        Wm[i] = 0.5 / (L + lambda_param)
        Wc[i] = 0.5 / (L + lambda_param)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.zeros(L)
    x[0] = prices[0]  # ä¾¡æ ¼
    x[1] = 0.0        # é€Ÿåº¦
    x[2] = 0.0        # åŠ é€Ÿåº¦
    x[3] = 0.0        # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ 
    x[4] = volatility[0] if n > 0 else 0.01  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹
    x[5] = 0.0        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
    x[6] = 1.0        # ä¿¡é ¼åº¦æŒ‡æ¨™
    
    # åˆæœŸå…±åˆ†æ•£è¡Œåˆ—
    P = np.eye(L)
    P[0, 0] = 1.0      # ä¾¡æ ¼ã®ä¸ç¢ºå®Ÿæ€§
    P[1, 1] = 0.1      # é€Ÿåº¦ã®ä¸ç¢ºå®Ÿæ€§
    P[2, 2] = 0.01     # åŠ é€Ÿåº¦ã®ä¸ç¢ºå®Ÿæ€§
    P[3, 3] = 0.5      # ãƒ¬ã‚¸ãƒ¼ãƒ ã®ä¸ç¢ºå®Ÿæ€§
    P[4, 4] = 0.1      # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ä¸ç¢ºå®Ÿæ€§
    P[5, 5] = 0.1      # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®ä¸ç¢ºå®Ÿæ€§
    P[6, 6] = 0.1      # ä¿¡é ¼åº¦ã®ä¸ç¢ºå®Ÿæ€§
    
    # å‡ºåŠ›é…åˆ—
    filtered_prices = np.zeros(n)
    trend_estimates = np.zeros(n)
    uncertainty_estimates = np.zeros(n)
    kalman_gains = np.zeros(n)
    confidence_scores = np.zeros(n)
    market_regimes = np.zeros(n)
    adaptive_params = np.zeros(n)
    
    # åˆæœŸå€¤è¨­å®š
    filtered_prices[0] = prices[0]
    trend_estimates[0] = 0.0
    uncertainty_estimates[0] = 1.0
    kalman_gains[0] = 0.5
    confidence_scores[0] = 1.0
    market_regimes[0] = 0.0
    adaptive_params[0] = alpha
    
    # å¸‚å ´åˆ†æç”¨ãƒ¡ãƒ¢ãƒª
    price_memory = np.zeros(min(20, n))
    volatility_memory = np.zeros(min(20, n))
    trend_memory = np.zeros(min(10, n))
    
    for t in range(1, n):
        # === 1. å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡º ===
        regime_detection_window = min(10, t)
        if t >= 5:
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦è¨ˆç®—
            recent_prices = prices[max(0, t-regime_detection_window):t+1]
            if len(recent_prices) > 3:
                # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œå‡º
                x_vals = np.arange(len(recent_prices))
                mean_x = np.mean(x_vals)
                mean_y = np.mean(recent_prices)
                
                numerator = np.sum((x_vals - mean_x) * (recent_prices - mean_y))
                denominator = np.sum((x_vals - mean_x) ** 2)
                
                if denominator > 1e-10:
                    slope = numerator / denominator
                    # r_squaredè¨ˆç®—ã§ã‚¼ãƒ­é™¤ç®—ã‚’é˜²ã
                    price_variance = np.sum((recent_prices - mean_y) ** 2)
                    r_squared_denom = denominator * price_variance + 1e-10
                    if r_squared_denom > 1e-10:
                        r_squared = (numerator ** 2) / r_squared_denom
                    else:
                        r_squared = 0.0
                    
                    # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š
                    trend_strength = np.tanh(abs(slope) * 100)
                    trend_direction = np.sign(slope)
                    trend_confidence = r_squared
                    
                    market_regime = trend_direction * trend_strength * trend_confidence
                    # ãƒ¬ã‚¸ãƒ¼ãƒ å€¤ã‚’-1ï¼ˆãƒ¬ãƒ³ã‚¸ï¼‰ã‹ã‚‰+1ï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰ï¼‰ã«ã‚¯ãƒªãƒƒãƒ—
                    if market_regime > 1.0:
                        market_regime = 1.0
                    elif market_regime < -1.0:
                        market_regime = -1.0
                else:
                    market_regime = 0.0
            else:
                market_regime = 0.0
        else:
            market_regime = 0.0
        
        market_regimes[t] = market_regime
        
        # === 2. å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ ===
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´
        current_vol = volatility[t] if t < len(volatility) else 0.01
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—ã®å®‰å…¨åŒ–
        vol_mean = np.mean(volatility[:t+1])
        vol_mean_safe = max(vol_mean, 1e-8)
        vol_percentile = min(current_vol / vol_mean_safe, 3.0)
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ ãƒ™ãƒ¼ã‚¹ã®èª¿æ•´
        regime_factor = abs(market_regime)
        
        # é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        adaptive_alpha = alpha * (1.0 + vol_percentile * 0.5 + regime_factor * 0.3)
        adaptive_beta = beta * (1.0 + regime_factor * 0.2)
        adaptive_kappa = kappa + regime_factor * 0.1
        
        # å¢ƒç•Œåˆ¶é™
        if adaptive_alpha > 0.01:
            adaptive_alpha = 0.01
        elif adaptive_alpha < 0.0001:
            adaptive_alpha = 0.0001
            
        if adaptive_beta > 4.0:
            adaptive_beta = 4.0
        elif adaptive_beta < 1.0:
            adaptive_beta = 1.0
        
        adaptive_params[t] = adaptive_alpha
        
        # æ–°ã—ã„Î»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        lambda_param = adaptive_alpha * adaptive_alpha * (L + adaptive_kappa) - L
        gamma = np.sqrt(L + lambda_param)
        
        # === 3. é©å¿œçš„ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º ===
        base_process_noise = 0.0001
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é©å¿œ
        vol_noise_factor = 1.0 + current_vol * 5.0
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ é©å¿œ
        regime_noise_factor = 1.0 + abs(market_regime) * 0.5
        
        # æ™‚å¤‰ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºè¡Œåˆ—
        Q = np.eye(L) * base_process_noise
        Q[0, 0] = base_process_noise * vol_noise_factor  # ä¾¡æ ¼
        Q[1, 1] = base_process_noise * regime_noise_factor  # é€Ÿåº¦
        Q[2, 2] = base_process_noise * vol_noise_factor * 0.5  # åŠ é€Ÿåº¦
        Q[3, 3] = base_process_noise * 2.0  # ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆã‚†ã£ãã‚Šå¤‰åŒ–ï¼‰
        Q[4, 4] = base_process_noise * vol_noise_factor  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹
        Q[5, 5] = base_process_noise * regime_noise_factor  # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
        Q[6, 6] = base_process_noise * 0.1  # ä¿¡é ¼åº¦ï¼ˆå®‰å®šï¼‰
        
        # === 4. çŠ¶æ…‹é·ç§»é–¢æ•°ï¼ˆéç·šå½¢ï¼‰ ===
        def state_transition(state):
            new_state = np.zeros(L)
            dt = 1.0
            
            # ä¾¡æ ¼æ›´æ–°ï¼ˆéç·šå½¢å‹•åŠ›å­¦ï¼‰
            momentum_effect = state[5] * 0.1
            regime_effect = state[3] * state[1] * 0.05
            new_state[0] = state[0] + state[1] * dt + momentum_effect + regime_effect
            
            # é€Ÿåº¦æ›´æ–°ï¼ˆåŠ é€Ÿåº¦ã¨ãƒ¬ã‚¸ãƒ¼ãƒ åŠ¹æœï¼‰
            regime_damping = 0.95 - abs(state[3]) * 0.05
            new_state[1] = state[1] * regime_damping + state[2] * dt
            
            # åŠ é€Ÿåº¦æ›´æ–°ï¼ˆæ¸›è¡°ï¼‰
            new_state[2] = state[2] * 0.9
            
            # ãƒ¬ã‚¸ãƒ¼ãƒ æ›´æ–°ï¼ˆæ…£æ€§ã‚ã‚Šï¼‰
            new_state[3] = state[3] * 0.98 + market_regime * 0.02
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹æ›´æ–°
            new_state[4] = state[4] * 0.9 + current_vol * 0.1
            
            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æ›´æ–°
            new_state[5] = state[5] * 0.95 + state[1] * 0.05
            
            # ä¿¡é ¼åº¦æ›´æ–°
            # äºˆæ¸¬ç²¾åº¦è¨ˆç®—ã®å®‰å…¨åŒ–
            vol_safe = max(current_vol, 1e-8)
            prediction_accuracy = 1.0 / (1.0 + abs(prices[t] - state[0]) / vol_safe)
            new_state[6] = state[6] * 0.9 + prediction_accuracy * 0.1
            
            return new_state
        
        # === 5. ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆï¼ˆå®‰å…¨åŒ–å¼·åŒ–ï¼‰ ===
        # å…±åˆ†æ•£è¡Œåˆ—ã®æ•°å€¤å®‰å®šåŒ–
        max_variance = 100.0  # åˆ†æ•£ã®ä¸Šé™ã‚’è¨­å®š
        for i in range(L):
            if P[i, i] > max_variance:
                P[i, i] = max_variance
            elif P[i, i] <= 0:
                P[i, i] = 0.01
        
        # éå¯¾è§’è¦ç´ ã®åˆ¶é™
        for i in range(L):
            for j in range(i+1, L):
                max_covar = np.sqrt(P[i, i] * P[j, j]) * 0.9
                if abs(P[i, j]) > max_covar:
                    P[i, j] = np.sign(P[i, j]) * max_covar
                    P[j, i] = P[i, j]
        
        # å¹³æ–¹æ ¹åˆ†è§£ï¼ˆå®‰å…¨ãªå®Ÿè£…ï¼‰
        try:
            sqrt_P = np.linalg.cholesky(P + np.eye(L) * 1e-8)
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¯¾è§’è¦ç´ ã®å¹³æ–¹æ ¹ã®ã¿ä½¿ç”¨
            sqrt_P = np.zeros((L, L))
            for i in range(L):
                sqrt_P[i, i] = min(np.sqrt(max(P[i, i], 0.01)), 1.0)
        
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆ
        sigma_points = np.zeros((2 * L + 1, L))
        sigma_points[0] = x  # ä¸­å¿ƒç‚¹
        
        for i in range(L):
            sigma_points[i + 1] = x + gamma * sqrt_P[:, i]
            sigma_points[i + 1 + L] = x - gamma * sqrt_P[:, i]
        
        # === 6. äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ— ===
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆã®ä¼æ’­
        sigma_points_pred = np.zeros((2 * L + 1, L))
        for i in range(2 * L + 1):
            sigma_points_pred[i] = state_transition(sigma_points[i])
        
        # äºˆæ¸¬çŠ¶æ…‹è¨ˆç®—
        x_pred = np.zeros(L)
        for i in range(2 * L + 1):
            x_pred += Wm[i] * sigma_points_pred[i]
        
        # äºˆæ¸¬å…±åˆ†æ•£è¨ˆç®—
        P_pred = Q.copy()
        for i in range(2 * L + 1):
            diff = sigma_points_pred[i] - x_pred
            P_pred += Wc[i] * np.outer(diff, diff)
        
        # === 7. è¦³æ¸¬æ›´æ–° ===
        # è¦³æ¸¬é–¢æ•°ï¼ˆä¾¡æ ¼ã®ã¿ï¼‰
        def observation_function(state):
            return state[0]  # ä¾¡æ ¼ã®ã¿è¦³æ¸¬
        
        # è¦³æ¸¬ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆ
        z_sigma = np.zeros(2 * L + 1)
        for i in range(2 * L + 1):
            z_sigma[i] = observation_function(sigma_points_pred[i])
        
        # äºˆæ¸¬è¦³æ¸¬
        z_pred = np.sum(Wm * z_sigma)
        
        # è¦³æ¸¬ãƒã‚¤ã‚ºï¼ˆé©å¿œçš„ï¼‰
        base_obs_noise = 0.001
        adaptive_obs_noise = base_obs_noise * vol_noise_factor * (2.0 - x[6])  # ä¿¡é ¼åº¦ã®é€†
        
        # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å…±åˆ†æ•£
        S = adaptive_obs_noise
        Pxz = np.zeros(L)
        
        for i in range(2 * L + 1):
            z_diff = z_sigma[i] - z_pred
            x_diff = sigma_points_pred[i] - x_pred
            S += Wc[i] * z_diff * z_diff
            Pxz += Wc[i] * x_diff * z_diff
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        if S > 1e-10:
            K = Pxz / S
        else:
            K = np.zeros(L)
            K[0] = 0.5
        
        # çŠ¶æ…‹æ›´æ–°
        innovation = prices[t] - z_pred
        x = x_pred + K * innovation
        P = P_pred - np.outer(K, K) * S
        
        # çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã®å®‰å…¨åŒ–ï¼ˆç•°å¸¸å€¤é˜²æ­¢ï¼‰
        max_price_deviation = abs(prices[t]) * 3.0 + 10.0
        if abs(x[0]) > max_price_deviation:
            x[0] = np.sign(x[0]) * max_price_deviation
        
        # é€Ÿåº¦ã®åˆ¶é™
        max_velocity = abs(prices[t]) * 0.5
        if abs(x[1]) > max_velocity:
            x[1] = np.sign(x[1]) * max_velocity
        
        # åŠ é€Ÿåº¦ã®åˆ¶é™
        max_acceleration = abs(prices[t]) * 0.1
        if abs(x[2]) > max_acceleration:
            x[2] = np.sign(x[2]) * max_acceleration
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ å€¤ã®åˆ¶é™
        if x[3] > 1.0:
            x[3] = 1.0
        elif x[3] < -1.0:
            x[3] = -1.0
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£çŠ¶æ…‹ã®åˆ¶é™
        if x[4] > 1.0:
            x[4] = 1.0
        elif x[4] < 0.001:
            x[4] = 0.001
        
        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã®åˆ¶é™
        if abs(x[5]) > max_velocity:
            x[5] = np.sign(x[5]) * max_velocity
        
        # ä¿¡é ¼åº¦ã®åˆ¶é™
        if x[6] > 2.0:
            x[6] = 2.0
        elif x[6] < 0.1:
            x[6] = 0.1
        
        # === 8. çµæœã®è¨˜éŒ² ===
        filtered_prices[t] = x[0]
        trend_estimates[t] = x[1]
        uncertainty_estimates[t] = np.sqrt(max(P[0, 0], 0))
        kalman_gains[t] = K[0]
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆå¤šæ¬¡å…ƒè©•ä¾¡ï¼‰
        prediction_confidence = x[6]
        regime_confidence = 1.0 - abs(market_regime) * 0.3  # ãƒ¬ãƒ³ã‚¸ç›¸å ´ã§é«˜ã„ä¿¡é ¼åº¦
        volatility_confidence = 1.0 / (1.0 + current_vol * 10)
        
        confidence_scores[t] = (
            0.4 * prediction_confidence +
            0.3 * regime_confidence +
            0.3 * volatility_confidence
        )
    
    return (filtered_prices, trend_estimates, uncertainty_estimates, kalman_gains,
            confidence_scores, market_regimes, adaptive_params)


# === 7. ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆçµ±åˆç‰ˆï¼‰ ===

@njit(fastmath=True, cache=True)
def triple_ensemble_kalman_filter_numba(
    prices: np.ndarray,
    volatility: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ - è¤‡æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®çµ±åˆç‰ˆ
    """
    n = len(prices)
    if n < 10:
        return prices.copy(), np.zeros(n), np.zeros(n), np.ones(n)
    
    # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®çµæœã‚’è¨ˆç®—
    adaptive_result, _, _, adaptive_conf = adaptive_kalman_filter_numba(prices)
    
    # ç°¡æ˜“ç‰ˆé‡å­ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    quantum_result = np.zeros(n)
    quantum_result[0] = prices[0]
    for i in range(1, n):
        alpha = 0.1 + 0.1 * np.sin(i * 0.1)  # å‹•çš„ã‚¢ãƒ«ãƒ•ã‚¡
        quantum_result[i] = alpha * prices[i] + (1 - alpha) * quantum_result[i-1]
    
    # ç°¡æ˜“ç‰ˆUKF
    ukf_result = np.zeros(n)
    ukf_result[0] = prices[0]
    for i in range(1, n):
        if i >= 3:
            # 3ç‚¹å¹³å‡ã«ã‚ˆã‚‹äºˆæ¸¬
            pred = (prices[i-1] + prices[i-2] + prices[i-3]) / 3.0
            alpha = 0.3
            ukf_result[i] = alpha * prices[i] + (1 - alpha) * pred
        else:
            ukf_result[i] = prices[i]
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿è¨ˆç®—
    ensemble_result = np.zeros(n)
    ensemble_gains = np.zeros(n)
    ensemble_innovations = np.zeros(n)
    ensemble_confidence = np.zeros(n)
    
    for i in range(n):
        # å‹•çš„é‡ã¿ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ï¼‰
        if i < len(volatility):
            vol_factor = min(volatility[i], 1.0)
        else:
            vol_factor = 0.1
        
        # é‡ã¿é…åˆ†
        w1 = 0.5 - vol_factor * 0.2  # é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®é‡ã¿
        w2 = 0.3 + vol_factor * 0.1  # é‡å­ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®é‡ã¿
        w3 = 0.2 + vol_factor * 0.1  # UKFã®é‡ã¿
        
        # æ­£è¦åŒ–
        total_weight = w1 + w2 + w3
        w1 /= total_weight
        w2 /= total_weight
        w3 /= total_weight
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ
        ensemble_result[i] = w1 * adaptive_result[i] + w2 * quantum_result[i] + w3 * ukf_result[i]
        ensemble_gains[i] = w1  # ä»£è¡¨ã‚²ã‚¤ãƒ³ã¨ã—ã¦é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®é‡ã¿ã‚’ä½¿ç”¨
        ensemble_innovations[i] = prices[i] - ensemble_result[i] if i > 0 else 0.0
        ensemble_confidence[i] = w1 * adaptive_conf[i] + w2 * 0.8 + w3 * 0.7
    
    return ensemble_result, ensemble_gains, ensemble_innovations, ensemble_confidence


class KalmanFilterUnified(Indicator):
    """
    ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - è¤‡æ•°ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’çµ±åˆ
    
    EhlersUnifiedDCã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã£ãŸå®Ÿè£…ã§ã€ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’çµ±åˆï¼š
    - adaptive: åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    - quantum_adaptive: é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼  
    - unscented: ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUKFï¼‰
    - extended: æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆEKFï¼‰
    - hyper_quantum: ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    - triple_ensemble: ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    """
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å®šç¾©
    _FILTERS = {
        'adaptive': adaptive_kalman_filter_numba,
        'quantum_adaptive': quantum_adaptive_kalman_filter_numba,
        'unscented': unscented_kalman_filter_numba,
        'extended': extended_kalman_filter_numba,
        'hyper_quantum': hyper_quantum_adaptive_kalman_numba,
        'triple_ensemble': triple_ensemble_kalman_filter_numba,
        'neural_supreme': neural_adaptive_quantum_supreme_kalman_numba,
        'market_adaptive_unscented': market_adaptive_unscented_kalman_numba
    }
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®èª¬æ˜
    _FILTER_DESCRIPTIONS = {
        'adaptive': 'åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆå‹•çš„ãƒã‚¤ã‚ºæ¨å®šï¼‰',
        'quantum_adaptive': 'é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆé‡å­ã‚‚ã¤ã‚ŒåŠ¹æœæ´»ç”¨ï¼‰',
        'unscented': 'ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆéç·šå½¢ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼‰',
        'extended': 'æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆéç·šå½¢å‹•çš„ã‚·ã‚¹ãƒ†ãƒ ï¼‰',
        'hyper_quantum': 'ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆé‡å­è¨ˆç®—ç†è«–çµ±åˆï¼‰',
        'triple_ensemble': 'ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆè¤‡æ•°ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµ±åˆï¼‰',
        'neural_supreme': 'ğŸ§ ğŸš€ Neural Adaptive Quantum Supremeï¼ˆé©æ–°çš„å…¨é ˜åŸŸçµ±åˆå‹ï¼‰',
        'market_adaptive_unscented': 'ğŸ¯ å¸‚å ´é©å¿œç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆæ¬¡ä¸–ä»£MA-UKFï¼‰'
    }
    
    def __init__(
        self,
        filter_type: str = 'adaptive',
        src_type: str = 'close',
        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        base_process_noise: float = 0.001,
        base_measurement_noise: float = 0.01,
        volatility_window: int = 10,
        # UKF/EKF ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        ukf_alpha: float = 0.001,
        ukf_beta: float = 2.0,
        ukf_kappa: float = 0.0,
        # é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        quantum_scale: float = 0.5
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            filter_type: ä½¿ç”¨ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ ('close', 'hlc3', 'hl2', 'ohlc4')
            base_process_noise: åŸºæœ¬ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
            base_measurement_noise: åŸºæœ¬æ¸¬å®šãƒã‚¤ã‚º
            volatility_window: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            ukf_alpha: UKFã‚¢ãƒ«ãƒ•ã‚¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            ukf_beta: UKFãƒ™ãƒ¼ã‚¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            ukf_kappa: UKFã‚«ãƒƒãƒ‘ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            quantum_scale: é‡å­ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã¦æ­£è¦åŒ–
        filter_type = filter_type.lower()
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        if filter_type not in self._FILTERS:
            valid_filters = ", ".join(self._FILTERS.keys())
            raise ValueError(f"ç„¡åŠ¹ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚¿ã‚¤ãƒ—ã§ã™: {filter_type}ã€‚æœ‰åŠ¹ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³: {valid_filters}")
        
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        name = f"KalmanUnified(type={filter_type}, src={src_type})"
        super().__init__(name)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
        self.filter_type = filter_type
        self.src_type = src_type
        self.base_process_noise = base_process_noise
        self.base_measurement_noise = base_measurement_noise
        self.volatility_window = volatility_window
        self.ukf_alpha = ukf_alpha
        self.ukf_beta = ukf_beta
        self.ukf_kappa = ukf_kappa
        self.quantum_scale = quantum_scale
        
        # PriceSourceã¯é™çš„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ä¸è¦
        
        # çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._result: Optional[KalmanFilterResult] = None
        self._cache_hash: Optional[str] = None
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> KalmanFilterResult:
        """
        ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨ˆç®—
        
        Args:
            data: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            KalmanFilterResult: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
        """
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        current_hash = self._get_data_hash(data)
        if self._cache_hash == current_hash and self._result is not None:
            return self._result
        
        try:
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            src_prices = PriceSource.calculate_source(data, self.src_type)
            
            if len(src_prices) < 10:
                return self._create_empty_result(len(src_prices))
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®š
            volatility = self._estimate_volatility(src_prices)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—
            filter_func = self._FILTERS[self.filter_type]
            
            if self.filter_type == 'adaptive':
                filtered_values, kalman_gains, innovations, confidence_scores = filter_func(src_prices)
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=innovations,
                    process_noise=np.full(len(src_prices), self.base_process_noise),
                    measurement_noise=np.full(len(src_prices), self.base_measurement_noise),
                    filter_type=self.filter_type
                )
            
            elif self.filter_type == 'quantum_adaptive':
                # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›è¿‘ä¼¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                amplitude, phase = self._simple_hilbert_transform(src_prices)
                filtered_values, quantum_coherence, kalman_gains, innovations, confidence_scores = filter_func(
                    src_prices, amplitude, phase
                )
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=innovations,
                    process_noise=np.full(len(src_prices), self.base_process_noise),
                    measurement_noise=np.full(len(src_prices), self.base_measurement_noise),
                    filter_type=self.filter_type,
                    quantum_coherence=quantum_coherence
                )
            
            elif self.filter_type == 'unscented':
                filtered_values, trend_estimate, uncertainty, kalman_gains, confidence_scores = filter_func(
                    src_prices, volatility, self.ukf_alpha, self.ukf_beta, self.ukf_kappa
                )
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=src_prices - filtered_values,
                    process_noise=np.full(len(src_prices), self.base_process_noise),
                    measurement_noise=volatility,
                    filter_type=self.filter_type,
                    uncertainty=uncertainty,
                    trend_estimate=trend_estimate
                )
            
            elif self.filter_type == 'extended':
                filtered_values, trend_estimates, kalman_gains, confidence_scores = filter_func(
                    src_prices, volatility
                )
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=src_prices - filtered_values,
                    process_noise=np.full(len(src_prices), self.base_process_noise),
                    measurement_noise=volatility,
                    filter_type=self.filter_type,
                    trend_estimate=trend_estimates
                )
                
            elif self.filter_type == 'neural_supreme':
                # ğŸ§ ğŸš€ é©æ–°çš„ Neural Adaptive Quantum Supreme ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                (filtered_values, neural_weights, quantum_phases, chaos_indicators,
                 fractal_dimensions, information_entropy, kalman_gains, confidence_scores) = filter_func(
                    src_prices, volatility
                )
                
                # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç®—
                innovations = np.zeros(len(src_prices))
                for i in range(1, len(src_prices)):
                    innovations[i] = src_prices[i] - filtered_values[i-1]
                
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=innovations,
                    process_noise=fractal_dimensions * self.base_process_noise,  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒãƒ™ãƒ¼ã‚¹
                    measurement_noise=information_entropy * self.base_measurement_noise,  # æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹
                    filter_type=self.filter_type,
                    quantum_coherence=quantum_phases,  # é‡å­ä½ç›¸ã‚’ä¿å­˜
                    uncertainty=chaos_indicators,  # ã‚«ã‚ªã‚¹æŒ‡æ¨™ã‚’ä¸ç¢ºå®Ÿæ€§ã¨ã—ã¦ä¿å­˜
                    trend_estimate=neural_weights  # ç¥çµŒé‡ã¿ã‚’ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®šã¨ã—ã¦ä¿å­˜
                )
            
            elif self.filter_type == 'hyper_quantum':
                filtered_values, quantum_coherence, kalman_gains, innovations, confidence_scores = filter_func(
                    src_prices, volatility, self.ukf_alpha, self.quantum_scale
                )
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=innovations,
                    process_noise=np.full(len(src_prices), self.base_process_noise),
                    measurement_noise=volatility,
                    filter_type=self.filter_type,
                    quantum_coherence=quantum_coherence
                )
            
            elif self.filter_type == 'triple_ensemble':
                filtered_values, kalman_gains, innovations, confidence_scores = filter_func(
                    src_prices, volatility
                )
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=innovations,
                    process_noise=np.full(len(src_prices), self.base_process_noise),
                    measurement_noise=volatility,
                    filter_type=self.filter_type
                )
            
            elif self.filter_type == 'market_adaptive_unscented':
                # ğŸ¯ å¸‚å ´é©å¿œç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆæ¬¡ä¸–ä»£MA-UKFï¼‰
                (filtered_values, trend_estimates, uncertainty_estimates, kalman_gains,
                 confidence_scores, market_regimes, adaptive_params) = filter_func(
                    src_prices, volatility, self.ukf_alpha, self.ukf_beta, self.ukf_kappa
                )
                
                # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç®—
                innovations = np.zeros(len(src_prices))
                for i in range(1, len(src_prices)):
                    innovations[i] = src_prices[i] - filtered_values[i-1]
                
                result = KalmanFilterResult(
                    filtered_values=filtered_values,
                    raw_values=src_prices,
                    confidence_scores=confidence_scores,
                    kalman_gains=kalman_gains,
                    innovation=innovations,
                    process_noise=adaptive_params * self.base_process_noise,  # é©å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    measurement_noise=uncertainty_estimates,  # å‹•çš„ä¸ç¢ºå®Ÿæ€§æ¨å®š
                    filter_type=self.filter_type,
                    quantum_coherence=market_regimes,  # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ çŠ¶æ…‹ã‚’ä¿å­˜
                    uncertainty=uncertainty_estimates,  # ä¸ç¢ºå®Ÿæ€§æ¨å®š
                    trend_estimate=trend_estimates  # ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®š
                )
            
            else:
                return self._create_empty_result(len(src_prices))
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
            self._result = result
            self._cache_hash = current_hash
            
            return result
            
        except Exception as e:
            self.logger.error(f"ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_empty_result(len(data) if hasattr(data, '__len__') else 0)
    
    def _estimate_volatility(self, prices: np.ndarray) -> np.ndarray:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’æ¨å®š"""
        n = len(prices)
        volatility = np.full(n, self.base_measurement_noise)
        
        if n < self.volatility_window:
            return volatility
        
        for i in range(self.volatility_window, n):
            window_prices = prices[i-self.volatility_window:i]
            if len(window_prices) > 1:
                vol = np.std(window_prices)
                volatility[i] = max(vol, self.base_measurement_noise)
        
        return volatility
    
    def _simple_hilbert_transform(self, prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ç°¡æ˜“ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›"""
        n = len(prices)
        amplitude = np.ones(n)
        phase = np.zeros(n)
        
        for i in range(4, n):
            # 4ç‚¹è¿‘ä¼¼
            real_part = (prices[i] + prices[i-2]) / 2.0
            imag_part = (prices[i-1] + prices[i-3]) / 2.0
            
            amplitude[i] = np.sqrt(real_part**2 + imag_part**2)
            if real_part != 0:
                phase[i] = np.arctan2(imag_part, real_part)
        
        return amplitude, phase
    
    def _create_empty_result(self, length: int) -> KalmanFilterResult:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        return KalmanFilterResult(
            filtered_values=np.full(length, np.nan),
            raw_values=np.full(length, np.nan),
            confidence_scores=np.full(length, np.nan),
            kalman_gains=np.full(length, np.nan),
            innovation=np.full(length, np.nan),
            process_noise=np.full(length, np.nan),
            measurement_noise=np.full(length, np.nan),
            filter_type=self.filter_type
        )
    
    def _get_data_hash(self, data) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        try:
            if hasattr(data, 'values'):
                return str(hash(data.values.tobytes()))
            else:
                return str(hash(data.tobytes()))
        except:
            return str(len(data))
    
    @classmethod
    def get_available_filters(cls) -> Dict[str, str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return cls._FILTER_DESCRIPTIONS.copy()
    
    def get_values(self) -> Optional[np.ndarray]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿å€¤ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.filtered_values.copy()
        return None
    
    def get_confidence_scores(self) -> Optional[np.ndarray]:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.confidence_scores.copy()
        return None
    
    def get_kalman_gains(self) -> Optional[np.ndarray]:
        """ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³ã‚’å–å¾—"""
        if self._result is not None:
            return self._result.kalman_gains.copy()
        return None
    
    def get_market_regimes(self) -> Optional[np.ndarray]:
        """å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ çŠ¶æ…‹ã‚’å–å¾—ï¼ˆmarket_adaptive_unscentedãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨ï¼‰"""
        if self._result is not None and self._result.quantum_coherence is not None and self.filter_type == 'market_adaptive_unscented':
            return self._result.quantum_coherence.copy()
        return None
    
    def get_trend_estimates(self) -> Optional[np.ndarray]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®šã‚’å–å¾—"""
        if self._result is not None and self._result.trend_estimate is not None:
            return self._result.trend_estimate.copy()
        return None
    
    def get_uncertainty_estimates(self) -> Optional[np.ndarray]:
        """ä¸ç¢ºå®Ÿæ€§æ¨å®šã‚’å–å¾—"""
        if self._result is not None and self._result.uncertainty is not None:
            return self._result.uncertainty.copy()
        return None
    
    def get_filter_metadata(self) -> Dict:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if self._result is None:
            return {}
        
        metadata = {
            'filter_type': self.filter_type,
            'filter_description': self._FILTER_DESCRIPTIONS.get(self.filter_type, ''),
            'src_type': self.src_type,
            'data_points': len(self._result.filtered_values),
            'avg_confidence': np.nanmean(self._result.confidence_scores),
            'avg_kalman_gain': np.nanmean(self._result.kalman_gains),
            'avg_innovation': np.nanmean(np.abs(self._result.innovation))
        }
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›ºæœ‰ã®æƒ…å ±
        if self._result.quantum_coherence is not None:
            if self.filter_type == 'market_adaptive_unscented':
                metadata['avg_market_regime'] = np.nanmean(self._result.quantum_coherence)
                metadata['trend_market_ratio'] = np.mean(self._result.quantum_coherence > 0.5)
                metadata['range_market_ratio'] = np.mean(np.abs(self._result.quantum_coherence) < 0.3)
            else:
                metadata['avg_quantum_coherence'] = np.nanmean(self._result.quantum_coherence)
        if self._result.uncertainty is not None:
            metadata['avg_uncertainty'] = np.nanmean(self._result.uncertainty)
        if self._result.trend_estimate is not None:
            metadata['avg_trend_estimate'] = np.nanmean(self._result.trend_estimate)
            if self.filter_type == 'market_adaptive_unscented':
                metadata['trend_strength'] = np.std(self._result.trend_estimate)
                metadata['trend_direction_changes'] = np.sum(np.diff(np.sign(self._result.trend_estimate)) != 0)
        
        return metadata
    
    def reset(self) -> None:
        """çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self._result = None
        self._cache_hash = None