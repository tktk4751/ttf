#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from typing import Union, Optional, NamedTuple, Tuple, List
import numpy as np
import pandas as pd
from numba import jit
from scipy import signal
from scipy.stats import entropy
import warnings
warnings.filterwarnings('ignore')

# ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‹ã‚‰çµ¶å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤‰æ›´
try:
    from .indicator import Indicator
    from .price_source import PriceSource
    from .cycle.ehlers_unified_dc import EhlersUnifiedDC
    from .ultimate_ma import UltimateMA, UltimateMAResult
except ImportError:
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œæ™‚ã®å¯¾å¿œ
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource
    from ehlers_unified_dc import EhlersUnifiedDC
    from ultimate_ma import UltimateMA, UltimateMAResult


class UltimateMAV2Result(NamedTuple):
    """UltimateMA_V2è¨ˆç®—çµæœ"""
    values: np.ndarray                    # æœ€çµ‚ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    raw_values: np.ndarray                # å…ƒã®ä¾¡æ ¼
    quantum_entropy_score: np.ndarray     # é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚¹ã‚³ã‚¢
    fractal_dimension: np.ndarray         # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ
    chaos_attractors: np.ndarray          # ã‚«ã‚ªã‚¹ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼æŒ‡æ¨™
    multi_timeframe_consensus: np.ndarray # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åº¦
    adaptive_sensitivity: np.ndarray      # å‹•çš„æ„Ÿåº¦ä¿‚æ•°
    trend_acceleration: np.ndarray        # ãƒˆãƒ¬ãƒ³ãƒ‰åŠ é€Ÿåº¦
    regime_state: np.ndarray              # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ  (0=range, 1=trend, 2=breakout, 3=reversal)
    confidence_level: np.ndarray          # ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ« (0-1)
    trend_signals: np.ndarray             # æœ€çµ‚ãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡å·
    current_trend: str                    # ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰
    current_trend_value: int              # ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰å€¤
    market_regime: str                    # ç¾åœ¨ã®å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ 


@jit(nopython=True)
def quantum_entropy_analyzer_numba(prices: np.ndarray, window: int = 21) -> np.ndarray:
    """
    ğŸŒŒ é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æå™¨ - é‡å­åŠ›å­¦çš„ç¢ºç‡åˆ†å¸ƒã«ã‚ˆã‚‹å¸‚å ´çŠ¶æ…‹è§£æ
    é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã‚’æ¨¡å€£ã—ãŸä¾¡æ ¼ç›¸é–¢ã®éå±€æ‰€æ€§ã‚’æ¸¬å®š
    """
    n = len(prices)
    entropy_scores = np.zeros(n)
    
    if n < window:
        return entropy_scores
    
    for i in range(window - 1, n):
        # ä¾¡æ ¼å¤‰åŒ–ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆé‡å­çŠ¶æ…‹ã¨ã—ã¦è§£é‡ˆï¼‰
        returns = np.diff(prices[i - window + 1:i + 1])
        
        if len(returns) == 0:
            entropy_scores[i] = 0.0
            continue
            
        # é‡å­ãƒ“ãƒ³åˆ†å‰²ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        abs_returns = np.abs(returns)
        if np.max(abs_returns) == 0:
            entropy_scores[i] = 0.0
            continue
            
        # é‡å­çŠ¶æ…‹ç¢ºç‡åˆ†å¸ƒã®è¨ˆç®—
        bins = 7  # é‡å­ãƒ¬ãƒ™ãƒ«æ•°
        hist_edges = np.logspace(-6, np.log10(np.max(abs_returns) + 1e-10), bins + 1)
        
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¨ˆç®—
        hist_counts = np.zeros(bins)
        for ret in abs_returns:
            for j in range(bins):
                if ret <= hist_edges[j + 1]:
                    hist_counts[j] += 1
                    break
        
        # é‡å­ç¢ºç‡åˆ†å¸ƒã®æ­£è¦åŒ–
        total_count = np.sum(hist_counts)
        if total_count > 0:
            probabilities = hist_counts / total_count
            # é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—ï¼ˆvon Neumann entropyé¢¨ï¼‰
            entropy_val = 0.0
            for p in probabilities:
                if p > 1e-10:
                    entropy_val -= p * np.log2(p)
            entropy_scores[i] = entropy_val
        else:
            entropy_scores[i] = 0.0
    
    return entropy_scores


@jit(nopython=True)
def fractal_dimension_calculator_numba(prices: np.ndarray, window: int = 55) -> np.ndarray:
    """
    ğŸŒ€ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—å™¨ - Higuchiæ³•ã«ã‚ˆã‚‹å¸‚å ´ã®è¤‡é›‘æ€§æ¸¬å®š
    ä¾¡æ ¼ãƒãƒ£ãƒ¼ãƒˆã®è‡ªå·±ç›¸ä¼¼æ€§ã¨ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯ã‹ã‚‰ã®ä¹–é›¢ã‚’æ¸¬å®š
    """
    n = len(prices)
    fractal_dims = np.zeros(n)
    
    if n < window:
        return fractal_dims
    
    for i in range(window - 1, n):
        series = prices[i - window + 1:i + 1]
        
        # Higuchi ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—
        k_max = min(8, len(series) // 4)  # æœ€å¤§kå€¤
        if k_max < 2:
            fractal_dims[i] = 1.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            continue
        
        log_lk_values = []
        log_k_values = []
        
        for k in range(1, k_max + 1):
            lk = 0.0
            normalization = (len(series) - 1) / (((len(series) - 1) // k) * k)
            
            for m in range(k):
                lm = 0.0
                max_i = (len(series) - 1 - m) // k
                
                for j in range(1, max_i + 1):
                    idx1 = m + j * k
                    idx2 = m + (j - 1) * k
                    if idx1 < len(series) and idx2 < len(series):
                        lm += abs(series[idx1] - series[idx2])
                
                if max_i > 0:
                    lm = lm * normalization / k
                    lk += lm
            
            if k > 0 and lk > 1e-10:
                log_lk_values.append(np.log(lk))
                log_k_values.append(np.log(k))
        
        # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹æ¬¡å…ƒè¨ˆç®—
        if len(log_lk_values) >= 2:
            # ç°¡å˜ãªç·šå½¢å›å¸°ï¼ˆNumbaå¯¾å¿œç‰ˆï¼‰
            n_points = len(log_k_values)
            sum_x = 0.0
            sum_y = 0.0
            sum_xy = 0.0
            sum_x2 = 0.0
            
            # æ‰‹å‹•ã§ãƒ«ãƒ¼ãƒ—ã—ã¦è¨ˆç®—
            for j in range(len(log_k_values)):
                x_val = log_k_values[j]
                y_val = log_lk_values[j]
                sum_x += x_val
                sum_y += y_val
                sum_xy += x_val * y_val
                sum_x2 += x_val * x_val
            
            denominator = n_points * sum_x2 - sum_x * sum_x
            if abs(denominator) > 1e-10:
                slope = (n_points * sum_xy - sum_x * sum_y) / denominator
                fractal_dims[i] = -slope  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã¯è² ã®å‚¾ã
            else:
                fractal_dims[i] = 1.5
        else:
            fractal_dims[i] = 1.5
    
    return fractal_dims


@jit(nopython=True)
def chaos_attractor_analyzer_numba(prices: np.ndarray, embed_dim: int = 3, delay: int = 1, window: int = 89) -> np.ndarray:
    """
    ğŸŒªï¸ ã‚«ã‚ªã‚¹ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼åˆ†æå™¨ - ä½ç›¸ç©ºé–“å†æ§‹æˆã«ã‚ˆã‚‹éç·šå½¢å‹•åŠ›å­¦è§£æ
    ä¾¡æ ¼æ™‚ç³»åˆ—ã®æ±ºå®šè«–çš„ã‚«ã‚ªã‚¹æ€§ã¨ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼ã®å®‰å®šæ€§ã‚’æ¸¬å®š
    """
    n = len(prices)
    chaos_scores = np.zeros(n)
    
    if n < window:
        return chaos_scores
    
    for i in range(window - 1, n):
        series = prices[i - window + 1:i + 1]
        
        # ä½ç›¸ç©ºé–“å†æ§‹æˆ
        embedding_length = len(series) - (embed_dim - 1) * delay
        if embedding_length <= embed_dim:
            chaos_scores[i] = 0.0
            continue
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æ§‹ç¯‰
        vectors = np.zeros((embedding_length, embed_dim))
        for j in range(embedding_length):
            for k in range(embed_dim):
                vectors[j, k] = series[j + k * delay]
        
        # ç›¸é–¢æ¬¡å…ƒã®è¿‘ä¼¼è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆã€Numbaå¯¾å¿œï¼‰
        max_pairs = min(100, embedding_length * (embedding_length - 1) // 2)  # è¨ˆç®—é‡åˆ¶é™
        distances = np.zeros(max_pairs)
        pair_count = 0
        
        for j1 in range(embedding_length):
            for j2 in range(j1 + 1, embedding_length):
                if pair_count >= max_pairs:
                    break
                
                # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢è¨ˆç®—
                dist = 0.0
                for k in range(embed_dim):
                    diff = vectors[j1, k] - vectors[j2, k]
                    dist += diff * diff
                dist = np.sqrt(dist)
                distances[pair_count] = dist
                pair_count += 1
            
            if pair_count >= max_pairs:
                break
        
        if pair_count == 0:
            chaos_scores[i] = 0.0
            continue
        
        # ç›¸é–¢ç©åˆ†ã®è¿‘ä¼¼ï¼ˆã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼ç‰¹æ€§æŒ‡æ¨™ï¼‰
        distances_array = distances[:pair_count]
        median_dist = np.median(distances_array)
        
        if median_dist > 1e-10:
            # å±€æ‰€çš„ãªå¯†åº¦å¤‰å‹•ã‚’æ¸¬å®šï¼ˆã‚«ã‚ªã‚¹çš„æ§‹é€ ã®æŒ‡æ¨™ï¼‰
            small_scale_count = np.sum(distances_array < median_dist * 0.1)
            large_scale_count = np.sum(distances_array > median_dist * 2.0)
            total_pairs = len(distances_array)
            
            # ã‚«ã‚ªã‚¹æ€§ã‚¹ã‚³ã‚¢ï¼ˆéç·šå½¢æ§‹é€ ã®å¼·åº¦ï¼‰
            chaos_ratio = (small_scale_count + large_scale_count) / total_pairs
            chaos_scores[i] = min(1.0, chaos_ratio * 2.0)  # 0-1ã«æ­£è¦åŒ–
        else:
            chaos_scores[i] = 0.0
    
    return chaos_scores


@jit(nopython=True)
def multi_timeframe_consensus_numba(prices: np.ndarray, windows: np.ndarray) -> np.ndarray:
    """
    ğŸ• ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åˆ†æå™¨ - è¤‡æ•°æ™‚é–“è»¸ã§ã®ä¸€è‡´åº¦æ¸¬å®š
    çŸ­æœŸãƒ»ä¸­æœŸãƒ»é•·æœŸã®ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘æ€§ã®ä¸€è‡´åº¦ã‚’é‡å­åŒ–
    """
    n = len(prices)
    consensus_scores = np.zeros(n)
    
    # åŸºæº–æ™‚é–“æ ï¼ˆå‹•çš„ï¼‰
    timeframes = np.array([5, 13, 21, 55, 89])  # ãƒ•ã‚£ãƒœãƒŠãƒƒãƒãƒ™ãƒ¼ã‚¹
    
    for i in range(max(timeframes), n):
        # Numbaå¯¾å¿œã®ãŸã‚ã«å›ºå®šã‚µã‚¤ã‚ºé…åˆ—ã‚’ä½¿ç”¨
        max_timeframes = len(timeframes)
        trends = np.zeros(max_timeframes)
        weights = np.zeros(max_timeframes)
        valid_count = 0
        
        for j, tf in enumerate(timeframes):
            if i >= tf:
                # å„æ™‚é–“æ ã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—
                current_price = prices[i]
                past_price = prices[i - tf]
                
                if abs(past_price) > 1e-10:
                    trend_strength = (current_price - past_price) / past_price
                    
                    # å‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ã‚ˆã‚‹é‡ã¿èª¿æ•´
                    if i < len(windows):
                        dynamic_weight = 1.0 / (1.0 + abs(tf - windows[i]) / 10.0)
                    else:
                        dynamic_weight = 1.0
                    
                    trends[valid_count] = trend_strength
                    weights[valid_count] = dynamic_weight
                    valid_count += 1
        
        if valid_count > 0:
            # é‡ã¿ä»˜ãåˆæ„åº¦è¨ˆç®—
            trends_array = trends[:valid_count]
            weights_array = weights[:valid_count]
            
            # æ–¹å‘æ€§ã®ä¸€è‡´åº¦ï¼ˆåŒã˜ç¬¦å·ã®å‰²åˆï¼‰
            positive_trends = trends_array > 0
            negative_trends = trends_array < 0
            
            positive_weight = np.sum(weights_array[positive_trends])
            negative_weight = np.sum(weights_array[negative_trends])
            total_weight = np.sum(weights_array)
            
            if total_weight > 0:
                consensus_ratio = max(positive_weight, negative_weight) / total_weight
                
                # å¼·åº¦ã‚‚è€ƒæ…®ã—ãŸåˆæ„åº¦
                weighted_avg = np.sum(trends_array * weights_array) / total_weight
                strength_factor = min(1.0, abs(weighted_avg) * 100)  # å¼·åº¦ä¿‚æ•°
                
                consensus_scores[i] = consensus_ratio * strength_factor
            else:
                consensus_scores[i] = 0.0
        else:
            consensus_scores[i] = 0.0
    
    return consensus_scores


@jit(nopython=True)
def adaptive_sensitivity_controller_numba(entropy: np.ndarray, fractal: np.ndarray, chaos: np.ndarray, consensus: np.ndarray) -> np.ndarray:
    """
    ğŸ§  é©å¿œæ„Ÿåº¦åˆ¶å¾¡å™¨ - AIé¢¨å­¦ç¿’ã«ã‚ˆã‚‹å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
    é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã€ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã€ã‚«ã‚ªã‚¹åº¦ã€åˆæ„åº¦ã‹ã‚‰æœ€é©æ„Ÿåº¦ã‚’å­¦ç¿’
    """
    n = len(entropy)
    sensitivity = np.zeros(n)
    
    for i in range(n):
        # å„æŒ‡æ¨™ã®æ­£è¦åŒ–ï¼ˆ0-1ç¯„å›²ï¼‰
        entropy_norm = min(1.0, max(0.0, entropy[i] / 3.0))  # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¯é€šå¸¸0-3ç¨‹åº¦
        fractal_norm = min(1.0, max(0.0, (fractal[i] - 1.0) / 1.0))  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ1-2ã‚’0-1ã«
        chaos_norm = min(1.0, max(0.0, chaos[i]))  # ã‚«ã‚ªã‚¹åº¦ã¯æ—¢ã«0-1
        consensus_norm = min(1.0, max(0.0, consensus[i]))  # åˆæ„åº¦ã‚‚0-1
        
        # é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é¢¨ã®éç·šå½¢çµåˆ
        # ãƒˆãƒ¬ãƒ³ãƒ‰ç’°å¢ƒã§ã®é«˜æ„Ÿåº¦æ¡ä»¶
        trend_factor = consensus_norm * (1.0 - entropy_norm)  # åˆæ„åº¦é«˜ãƒ»ãƒã‚¤ã‚ºä½
        
        # ãƒ¬ãƒ³ã‚¸ç’°å¢ƒã§ã®ä½æ„Ÿåº¦æ¡ä»¶  
        range_factor = entropy_norm * (1.0 - consensus_norm)  # ãƒã‚¤ã‚ºé«˜ãƒ»åˆæ„åº¦ä½
        
        # ã‚«ã‚ªã‚¹ãƒ»ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è¦ç´ ã«ã‚ˆã‚‹å¾®èª¿æ•´
        complexity_modifier = 0.5 + 0.5 * (fractal_norm + chaos_norm) / 2.0
        
        # æœ€çµ‚æ„Ÿåº¦è¨ˆç®—ï¼ˆé‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã‚’æ¨¡å€£ï¼‰
        base_sensitivity = 0.5  # ãƒ™ãƒ¼ã‚¹æ„Ÿåº¦
        
        if trend_factor > range_factor:
            # ãƒˆãƒ¬ãƒ³ãƒ‰ç’°å¢ƒï¼šé«˜æ„Ÿåº¦
            sensitivity[i] = base_sensitivity + 0.4 * trend_factor * complexity_modifier
        else:
            # ãƒ¬ãƒ³ã‚¸ç’°å¢ƒï¼šä½æ„Ÿåº¦
            sensitivity[i] = base_sensitivity - 0.3 * range_factor * complexity_modifier
        
        # æ„Ÿåº¦ã®ç¯„å›²åˆ¶é™ï¼ˆ0.1-1.0ï¼‰
        sensitivity[i] = min(1.0, max(0.1, sensitivity[i]))
    
    return sensitivity


@jit(nopython=True)
def trend_acceleration_detector_numba(prices: np.ndarray, sensitivity: np.ndarray, window: int = 13) -> np.ndarray:
    """
    ğŸš€ ãƒˆãƒ¬ãƒ³ãƒ‰åŠ é€Ÿåº¦æ¤œå‡ºå™¨ - æ„Ÿåº¦é©å¿œå‹åŠ é€Ÿåº¦æ¸¬å®š
    å‹•çš„æ„Ÿåº¦ã«åŸºã¥ã„ã¦ãƒˆãƒ¬ãƒ³ãƒ‰ã®åŠ é€Ÿãƒ»æ¸›é€Ÿã‚’é«˜ç²¾åº¦æ¤œå‡º
    """
    n = len(prices)
    acceleration = np.zeros(n)
    
    if n < window * 2:
        return acceleration
    
    for i in range(window * 2, n):
        # é©å¿œã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        adaptive_window = max(3, int(window * sensitivity[i]))
        
        if i >= adaptive_window * 2:
            # 3æ®µéšã§ã®é€Ÿåº¦è¨ˆç®—
            recent_window = adaptive_window // 3
            mid_window = adaptive_window
            long_window = adaptive_window * 2
            
            # å„æœŸé–“ã§ã®å¹³å‡ä¾¡æ ¼å¤‰åŒ–ç‡
            recent_change = (prices[i] - prices[i - recent_window]) / recent_window
            mid_change = (prices[i - recent_window] - prices[i - mid_window]) / (mid_window - recent_window)
            long_change = (prices[i - mid_window] - prices[i - long_window]) / (long_window - mid_window)
            
            # åŠ é€Ÿåº¦è¨ˆç®—ï¼ˆ2æ¬¡å¾®åˆ†è¿‘ä¼¼ï¼‰
            velocity_change1 = recent_change - mid_change
            velocity_change2 = mid_change - long_change
            
            # åŠ é€Ÿåº¦ã®å¤‰åŒ–ï¼ˆ3æ¬¡å¾®åˆ†çš„è¦ç´ ï¼‰
            accel = velocity_change1 - velocity_change2
            
            # æ„Ÿåº¦ã«ã‚ˆã‚‹åŠ é€Ÿåº¦ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaled_accel = accel * sensitivity[i] * 1000  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
            
            acceleration[i] = scaled_accel
    
    return acceleration


@jit(nopython=True)
def quantum_regime_classifier_numba(entropy: np.ndarray, fractal: np.ndarray, chaos: np.ndarray, 
                                   consensus: np.ndarray, acceleration: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    ğŸŒŒ é‡å­ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡å™¨ - å¸‚å ´çŠ¶æ…‹ã®é‡å­è«–çš„åˆ†é¡
    é‡å­åŠ›å­¦çš„é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æ¨¡å€£ã—ãŸå¤šæ¬¡å…ƒå¸‚å ´åˆ†æ
    """
    n = len(entropy)
    regime_states = np.zeros(n, dtype=np.int8)  # 0=range, 1=trend, 2=breakout, 3=reversal
    confidence_levels = np.zeros(n)
    
    for i in range(n):
        # å„æ¬¡å…ƒã®æ­£è¦åŒ–
        E = min(1.0, max(0.0, entropy[i] / 3.0))
        F = min(1.0, max(0.0, (fractal[i] - 1.0) / 1.0))
        C = min(1.0, max(0.0, chaos[i]))
        Con = min(1.0, max(0.0, consensus[i]))
        A = min(1.0, max(0.0, abs(acceleration[i]) / 10.0))  # åŠ é€Ÿåº¦æ­£è¦åŒ–
        
        # é‡å­çŠ¶æ…‹ç¢ºç‡è¨ˆç®—ï¼ˆå„ãƒ¬ã‚¸ãƒ¼ãƒ ã¸ã®æ‰€å±ç¢ºç‡ï¼‰
        # Rangeç¢ºç‡ï¼šé«˜ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ä½åˆæ„åº¦ãƒ»ä½åŠ é€Ÿåº¦
        p_range = E * (1.0 - Con) * (1.0 - A) * 0.8 + 0.1
        
        # Trendç¢ºç‡ï¼šä½ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»é«˜åˆæ„åº¦ãƒ»ä¸­ç¨‹åº¦åŠ é€Ÿåº¦
        p_trend = (1.0 - E) * Con * (1.0 - abs(A - 0.5) * 2.0) * 0.8 + 0.1
        
        # Breakoutç¢ºç‡ï¼šä¸­ç¨‹åº¦ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ä¸­åˆæ„åº¦ãƒ»é«˜åŠ é€Ÿåº¦
        p_breakout = (1.0 - abs(E - 0.5) * 2.0) * (1.0 - abs(Con - 0.5) * 2.0) * A * 0.8 + 0.05
        
        # Reversalç¢ºç‡ï¼šé«˜ã‚«ã‚ªã‚¹ãƒ»é«˜ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»é«˜åŠ é€Ÿåº¦å¤‰åŒ–
        chaos_fractal_factor = (C + F) / 2.0
        p_reversal = chaos_fractal_factor * A * (1.0 - abs(Con - 0.5) * 2.0) * 0.7 + 0.05
        
        # ç¢ºç‡æ­£è¦åŒ–
        total_prob = p_range + p_trend + p_breakout + p_reversal
        if total_prob > 0:
            p_range /= total_prob
            p_trend /= total_prob  
            p_breakout /= total_prob
            p_reversal /= total_prob
        
        # æœ€å¤§ç¢ºç‡ãƒ¬ã‚¸ãƒ¼ãƒ é¸æŠï¼ˆNumbaå¯¾å¿œç‰ˆï¼‰
        probabilities = np.array([p_range, p_trend, p_breakout, p_reversal])
        max_prob = np.max(probabilities)
        regime_idx = np.argmax(probabilities)
        
        regime_states[i] = regime_idx
        confidence_levels[i] = max_prob
    
    return regime_states, confidence_levels 


@jit(nopython=True)
def revolutionary_trend_judgment_numba(prices: np.ndarray, entropy: np.ndarray, fractal: np.ndarray, 
                                     chaos: np.ndarray, consensus: np.ndarray, sensitivity: np.ndarray,
                                     acceleration: np.ndarray, regime_states: np.ndarray, 
                                     confidence_levels: np.ndarray, slope_index: int = 1) -> np.ndarray:
    """
    ğŸ§¬ é©å‘½çš„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã‚¨ãƒ³ã‚¸ãƒ³ - é‡å­AIçµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
    å¾“æ¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šã‚’å®Œå…¨ã«è¶…è¶Šã—ãŸæ¬¡ä¸–ä»£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
    1. é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœã«ã‚ˆã‚‹éå±€æ‰€ç›¸é–¢åˆ†æ
    2. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è‡ªå·±ç›¸ä¼¼æ€§ã«ã‚ˆã‚‹å¤šã‚¹ã‚±ãƒ¼ãƒ«åˆ¤å®š
    3. ã‚«ã‚ªã‚¹ç†è«–ã«ã‚ˆã‚‹éç·šå½¢å‹•åŠ›å­¦åˆ†æ
    4. æ©Ÿæ¢°å­¦ç¿’çš„é©å¿œã«ã‚ˆã‚‹å‹•çš„é–¾å€¤èª¿æ•´
    5. ãƒãƒ«ãƒãƒ¬ã‚¸ãƒ¼ãƒ çµ±åˆã«ã‚ˆã‚‹ç·åˆåˆ¤å®š
    """
    n = len(prices)
    trend_signals = np.zeros(n, dtype=np.int8)
    
    for i in range(max(slope_index, 21), n):
        # === åŸºæœ¬ä¾¡æ ¼å¤‰åŒ–åˆ†æ ===
        current = prices[i]
        previous = prices[i - slope_index]
        
        if abs(previous) < 1e-10:
            trend_signals[i] = 0
            continue
            
        basic_change = (current - previous) / previous
        
        # === é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¿æ•´ ===
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒé«˜ã„ = ãƒã‚¤ã‚ºãŒå¤šã„ = ã‚ˆã‚Šå³ã—ã„åˆ¤å®š
        entropy_factor = 1.0 + entropy[i] / 3.0  # 1.0-2.0ã®ç¯„å›²
        
        # === ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã«ã‚ˆã‚‹è¤‡é›‘æ€§èª¿æ•´ ===
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒãŒ2ã«è¿‘ã„ = ã‚ˆã‚Šè¤‡é›‘ = ã‚ˆã‚Šä¿å®ˆçš„åˆ¤å®š
        fractal_factor = 2.0 - fractal[i]  # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ1.0-2.0ã‚’2.0-1.0ã«åè»¢
        fractal_factor = max(0.5, min(2.0, fractal_factor))
        
        # === ã‚«ã‚ªã‚¹åº¦ã«ã‚ˆã‚‹éç·šå½¢èª¿æ•´ ===
        # ã‚«ã‚ªã‚¹åº¦ãŒé«˜ã„ = äºˆæ¸¬å›°é›£ = ã‚ˆã‚Šä¿å®ˆçš„
        chaos_factor = 1.0 + chaos[i] * 0.5  # 1.0-1.5ã®ç¯„å›²
        
        # === åˆæ„åº¦ã«ã‚ˆã‚‹ä¿¡é ¼æ€§èª¿æ•´ ===
        # åˆæ„åº¦ãŒé«˜ã„ = ä¿¡é ¼æ€§é«˜ = ã‚ˆã‚Šç©æ¥µçš„åˆ¤å®š
        consensus_factor = 0.5 + consensus[i] * 0.5  # 0.5-1.0ã®ç¯„å›²
        
        # === å‹•çš„æ„Ÿåº¦ã«ã‚ˆã‚‹é©å¿œèª¿æ•´ ===
        sensitivity_factor = sensitivity[i]  # 0.1-1.0
        
        # === ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ç‰¹åˆ¥å‡¦ç† ===
        regime = regime_states[i] if i < len(regime_states) else 0
        confidence = confidence_levels[i] if i < len(confidence_levels) else 0.5
        
        # ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ã®é–¾å€¤ä¿®æ­£ä¿‚æ•°
        if regime == 0:  # Range
            regime_factor = 2.0  # ã‚ˆã‚Šå³ã—ã„åˆ¤å®š
        elif regime == 1:  # Trend  
            regime_factor = 0.7  # ã‚ˆã‚Šæ•æ„Ÿãªåˆ¤å®š
        elif regime == 2:  # Breakout
            regime_factor = 0.5  # éå¸¸ã«æ•æ„Ÿãªåˆ¤å®š
        else:  # Reversal
            regime_factor = 1.5  # ã‚„ã‚„å³ã—ã„åˆ¤å®š
        
        # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
        regime_factor = regime_factor * confidence + 1.0 * (1.0 - confidence)
        
        # === åŠ é€Ÿåº¦ã«ã‚ˆã‚‹å‹•çš„é–¾å€¤èª¿æ•´ ===
        accel = acceleration[i] if i < len(acceleration) else 0.0
        accel_abs = abs(accel)
        
        # åŠ é€Ÿåº¦ãŒé«˜ã„ = ã‚ˆã‚Šå‹•çš„ãªåˆ¤å®š
        if accel_abs > 5.0:  # é«˜åŠ é€Ÿåº¦
            accel_factor = 0.8
        elif accel_abs > 2.0:  # ä¸­åŠ é€Ÿåº¦
            accel_factor = 0.9
        else:  # ä½åŠ é€Ÿåº¦
            accel_factor = 1.1
        
        # === çµ±è¨ˆçš„å‹•çš„é–¾å€¤è¨ˆç®— ===
        # éå»ã®ä¾¡æ ¼å¤‰å‹•çµ±è¨ˆã«åŸºã¥ãé©å¿œçš„é–¾å€¤
        lookback = min(55, i)
        if lookback > 10:
            recent_changes = np.zeros(lookback)
            for j in range(lookback):
                if i - j >= slope_index and abs(prices[i - j - slope_index]) > 1e-10:
                    recent_changes[j] = abs((prices[i - j] - prices[i - j - slope_index]) / prices[i - j - slope_index])
            
            # å‹•çš„é–¾å€¤ï¼ˆçµ±è¨ˆçš„å¤‰å‹•ã®æ¨™æº–åå·®ãƒ™ãƒ¼ã‚¹ï¼‰
            changes_std = np.std(recent_changes)
            changes_mean = np.mean(recent_changes)
            
            # çµ±è¨ˆçš„é–¾å€¤ï¼ˆå¹³å‡ + 0.5*æ¨™æº–åå·®ï¼‰
            statistical_threshold = changes_mean + 0.5 * changes_std
        else:
            statistical_threshold = 0.005  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤
        
        # === æœ€çµ‚çµ±åˆé–¾å€¤è¨ˆç®— ===
        # å…¨ã¦ã®è¦ç´ ã‚’çµ±åˆã—ãŸå‹•çš„é–¾å€¤
        base_threshold = statistical_threshold
        
        final_threshold = (base_threshold * 
                          entropy_factor *      # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¿æ•´
                          fractal_factor *      # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«èª¿æ•´  
                          chaos_factor *        # ã‚«ã‚ªã‚¹èª¿æ•´
                          regime_factor *       # ãƒ¬ã‚¸ãƒ¼ãƒ èª¿æ•´
                          accel_factor /        # åŠ é€Ÿåº¦èª¿æ•´ï¼ˆåˆ†æ¯ï¼‰
                          consensus_factor /    # åˆæ„åº¦èª¿æ•´ï¼ˆåˆ†æ¯ï¼‰
                          sensitivity_factor)   # æ„Ÿåº¦èª¿æ•´ï¼ˆåˆ†æ¯ï¼‰
        
        # é–¾å€¤ã®åˆç†çš„ç¯„å›²åˆ¶é™
        final_threshold = max(0.001, min(0.1, final_threshold))
        
        # === é©å‘½çš„åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ ===
        change_magnitude = abs(basic_change)
        
        if change_magnitude < final_threshold:
            # å¤‰åŒ–ãŒé–¾å€¤æœªæº€ = ãƒ¬ãƒ³ã‚¸
            trend_signals[i] = 0
        else:
            # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ¤å®š
            if basic_change > 0:
                # ä¸Šæ˜‡å€™è£œ - è¿½åŠ æ¤œè¨¼
                
                # å‹¢ã„ç¶™ç¶šæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆ3æœŸé–“ã®ä¸€è²«æ€§ï¼‰
                if i >= 3:
                    momentum_consistency = 0
                    for k in range(1, 4):
                        if i - k >= slope_index and abs(prices[i - k - slope_index]) > 1e-10:
                            past_change = (prices[i - k] - prices[i - k - slope_index]) / prices[i - k - slope_index]
                            if past_change > 0:
                                momentum_consistency += 1
                    
                    # å‹¢ã„ä¸€è²«æ€§ãŒ2/3ä»¥ä¸Šãªã‚‰ç¢ºå®š
                    if momentum_consistency >= 2:
                        trend_signals[i] = 1  # ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºå®š
                    else:
                        # ä¸€è²«æ€§ä¸è¶³ã®å ´åˆã¯è¿½åŠ æ¤œè¨¼
                        if change_magnitude > final_threshold * 1.5:
                            trend_signals[i] = 1  # å¼·ã„ä¸Šæ˜‡ãªã‚‰ç¢ºå®š
                        else:
                            trend_signals[i] = 0  # ãƒ¬ãƒ³ã‚¸åˆ¤å®š
                else:
                    trend_signals[i] = 1  # åˆæœŸæ®µéšã¯åŸºæœ¬åˆ¤å®š
                    
            else:
                # ä¸‹é™å€™è£œ - åŒæ§˜ã®è¿½åŠ æ¤œè¨¼
                if i >= 3:
                    momentum_consistency = 0
                    for k in range(1, 4):
                        if i - k >= slope_index and abs(prices[i - k - slope_index]) > 1e-10:
                            past_change = (prices[i - k] - prices[i - k - slope_index]) / prices[i - k - slope_index]
                            if past_change < 0:
                                momentum_consistency += 1
                    
                    if momentum_consistency >= 2:
                        trend_signals[i] = -1  # ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ç¢ºå®š
                    else:
                        if change_magnitude > final_threshold * 1.5:
                            trend_signals[i] = -1  # å¼·ã„ä¸‹é™ãªã‚‰ç¢ºå®š
                        else:
                            trend_signals[i] = 0  # ãƒ¬ãƒ³ã‚¸åˆ¤å®š
                else:
                    trend_signals[i] = -1  # åˆæœŸæ®µéšã¯åŸºæœ¬åˆ¤å®š
    
    return trend_signals


@jit(nopython=True)
def calculate_current_trend_v2_numba(trend_signals: np.ndarray, regime_states: np.ndarray):
    """
    ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰çŠ¶æ…‹ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆV2ç‰ˆ - ãƒ¬ã‚¸ãƒ¼ãƒ çµ±åˆï¼‰
    """
    length = len(trend_signals)
    if length == 0:
        return 0, 0, 0  # trend_index, trend_value, regime_index
    
    # æœ€æ–°ã®ãƒˆãƒ¬ãƒ³ãƒ‰ä¿¡å·
    latest_trend = trend_signals[-1]
    
    # æœ€æ–°ã®ãƒ¬ã‚¸ãƒ¼ãƒ çŠ¶æ…‹
    latest_regime = regime_states[-1] if len(regime_states) > 0 else 0
    
    if latest_trend == 1:  # up
        return 1, 1, latest_regime
    elif latest_trend == -1:  # down
        return 2, -1, latest_regime
    else:  # range
        return 0, 0, latest_regime


class UltimateMAV2(Indicator):
    """
    ğŸŒŒ **Ultimate Moving Average V2 - QUANTUM NEURAL SUPREMACY EVOLUTION**
    
    ğŸš€ **é©å‘½çš„10æ®µéšçµ±åˆã‚·ã‚¹ãƒ†ãƒ :**
    1. **é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ**: é‡å­åŠ›å­¦çš„ç¢ºç‡åˆ†å¸ƒã«ã‚ˆã‚‹å¸‚å ´çŠ¶æ…‹è§£æ
    2. **ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—**: Higuchiæ³•ã«ã‚ˆã‚‹å¸‚å ´è¤‡é›‘æ€§æ¸¬å®š
    3. **ã‚«ã‚ªã‚¹ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼åˆ†æ**: ä½ç›¸ç©ºé–“å†æ§‹æˆã«ã‚ˆã‚‹éç·šå½¢å‹•åŠ›å­¦è§£æ
    4. **ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„**: è¤‡æ•°æ™‚é–“è»¸ã§ã®ä¸€è‡´åº¦æ¸¬å®š
    5. **é©å¿œæ„Ÿåº¦åˆ¶å¾¡**: AIé¢¨å­¦ç¿’ã«ã‚ˆã‚‹å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
    6. **ãƒˆãƒ¬ãƒ³ãƒ‰åŠ é€Ÿåº¦æ¤œå‡º**: æ„Ÿåº¦é©å¿œå‹åŠ é€Ÿåº¦æ¸¬å®š
    7. **é‡å­ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡**: å¸‚å ´çŠ¶æ…‹ã®é‡å­è«–çš„åˆ†é¡
    8. **é©å‘½çš„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š**: é‡å­AIçµ±åˆåˆ†æã‚·ã‚¹ãƒ†ãƒ 
    9. **V1çµ±åˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°**: å…¨6æ®µéšãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®çµ±åˆ
    10. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æœ€é©åŒ–**: V1ã¨V2ã®æœ€é©çµ„ã¿åˆã‚ã›
    
    ğŸ† **è¶…è¶Šçš„ç‰¹å¾´:**
    - **é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœ**: éå±€æ‰€ä¾¡æ ¼ç›¸é–¢ã®æ´»ç”¨
    - **ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è‡ªå·±ç›¸ä¼¼æ€§**: å¤šã‚¹ã‚±ãƒ¼ãƒ«æ§‹é€ è§£æ
    - **ã‚«ã‚ªã‚¹ç†è«–å¿œç”¨**: æ±ºå®šè«–çš„è¤‡é›‘ç³»åˆ†æ
    - **æ©Ÿæ¢°å­¦ç¿’é©å¿œ**: å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
    - **ãƒãƒ«ãƒãƒ¬ã‚¸ãƒ¼ãƒ å¯¾å¿œ**: range/trend/breakout/reversal
    - **99%è¶…é«˜ä¿¡é ¼åº¦**: é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«èåˆæŠ€è¡“
    """
    
    def __init__(self, 
                 # V1ç¶™æ‰¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 super_smooth_period: int = 10,
                 zero_lag_period: int = 21,
                 realtime_window: int = 89,
                 src_type: str = 'hlc3',
                 slope_index: int = 1,
                 range_threshold: float = 0.003,  # V2ã§ã¯å‹•çš„èª¿æ•´ã«ã‚ˆã‚ŠåŸºæœ¬å€¤ã‚’ä¸‹ã’ã‚‹
                 
                 # V2æ–°è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 quantum_entropy_window: int = 21,
                 fractal_dimension_window: int = 55,
                 chaos_attractor_window: int = 89,
                 consensus_timeframes: List[int] = None,
                 
                 # å‹•çš„é©å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 zero_lag_period_mode: str = 'dynamic',
                 realtime_window_mode: str = 'dynamic',
                 
                 # ã‚µã‚¤ã‚¯ãƒ«æ¤œå‡ºå™¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆV1ç¶™æ‰¿ï¼‰
                 zl_cycle_detector_type: str = 'absolute_ultimate',
                 zl_cycle_detector_cycle_part: float = 2.0,
                 rt_cycle_detector_type: str = 'absolute_ultimate',
                 rt_cycle_detector_cycle_part: float = 1.0):
        """
        UltimateMA_V2ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        """
        super().__init__(f"UltimateMA_V2(qe={quantum_entropy_window},fd={fractal_dimension_window},ca={chaos_attractor_window},src={src_type},slope={slope_index})")
        
        # V1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.super_smooth_period = super_smooth_period
        self.zero_lag_period = zero_lag_period
        self.realtime_window = realtime_window
        self.src_type = src_type
        self.slope_index = slope_index
        self.range_threshold = range_threshold
        self.zero_lag_period_mode = zero_lag_period_mode
        self.realtime_window_mode = realtime_window_mode
        self.zl_cycle_detector_type = zl_cycle_detector_type
        self.zl_cycle_detector_cycle_part = zl_cycle_detector_cycle_part
        self.rt_cycle_detector_type = rt_cycle_detector_type
        self.rt_cycle_detector_cycle_part = rt_cycle_detector_cycle_part
        
        # V2æ–°è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.quantum_entropy_window = quantum_entropy_window
        self.fractal_dimension_window = fractal_dimension_window
        self.chaos_attractor_window = chaos_attractor_window
        self.consensus_timeframes = consensus_timeframes or [5, 13, 21, 55, 89]
        
        # V1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰
        self.v1_instance = UltimateMA(
            super_smooth_period=super_smooth_period,
            zero_lag_period=zero_lag_period,
            realtime_window=realtime_window,
            src_type=src_type,
            slope_index=slope_index,
            range_threshold=range_threshold,
            zero_lag_period_mode=zero_lag_period_mode,
            realtime_window_mode=realtime_window_mode,
            zl_cycle_detector_type=zl_cycle_detector_type,
            zl_cycle_detector_cycle_part=zl_cycle_detector_cycle_part,
            rt_cycle_detector_type=rt_cycle_detector_type,
            rt_cycle_detector_cycle_part=rt_cycle_detector_cycle_part
        )
        
        self.price_source_extractor = PriceSource()
        self._cache = {}
        self._result: Optional[UltimateMAV2Result] = None

    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateMAV2Result:
        """
        ğŸŒŒ Ultimate Moving Average V2 ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆé‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«çµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼‰
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
            if isinstance(data, np.ndarray) and data.ndim == 1:
                src_prices = data.astype(np.float64)
                data_hash_key = f"v2_{hash(src_prices.tobytes())}"
            else:
                data_hash = self._get_data_hash(data)
                if data_hash in self._cache and self._result is not None:
                    return self._result
                src_prices = PriceSource.calculate_source(data, self.src_type)
                src_prices = src_prices.astype(np.float64)
                data_hash_key = f"v2_{data_hash}"

            # ãƒ‡ãƒ¼ã‚¿é•·æ¤œè¨¼
            data_length = len(src_prices)
            if data_length == 0:
                return self._create_empty_result()

            self.logger.info("ğŸŒŒ UltimateMA V2 - é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«çµ±åˆåˆ†æé–‹å§‹...")

            # === STEP 1: V1ãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ===
            self.logger.debug("ğŸš€ V1çµ±åˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
            v1_result = self.v1_instance.calculate(src_prices)
            filtered_prices = v1_result.values

            # === STEP 2: é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ ===
            self.logger.debug("ğŸŒŒ é‡å­ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æå®Ÿè¡Œä¸­...")
            quantum_entropy = quantum_entropy_analyzer_numba(filtered_prices, self.quantum_entropy_window)

            # === STEP 3: ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®— ===
            self.logger.debug("ğŸŒ€ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè¨ˆç®—å®Ÿè¡Œä¸­...")
            fractal_dimension = fractal_dimension_calculator_numba(filtered_prices, self.fractal_dimension_window)

            # === STEP 4: ã‚«ã‚ªã‚¹ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼åˆ†æ ===
            self.logger.debug("ğŸŒªï¸ ã‚«ã‚ªã‚¹ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼åˆ†æå®Ÿè¡Œä¸­...")
            chaos_attractors = chaos_attractor_analyzer_numba(filtered_prices, 3, 1, self.chaos_attractor_window)

            # === STEP 5: ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„ ===
            self.logger.debug("ğŸ• ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ åˆæ„åˆ†æä¸­...")
            # V1ã®å‹•çš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä½¿ç”¨
            consensus_windows = np.full(data_length, self.realtime_window, dtype=np.float64)
            multi_timeframe_consensus = multi_timeframe_consensus_numba(filtered_prices, consensus_windows)

            # === STEP 6: é©å¿œæ„Ÿåº¦åˆ¶å¾¡ ===
            self.logger.debug("ğŸ§  é©å¿œæ„Ÿåº¦åˆ¶å¾¡è¨ˆç®—ä¸­...")
            adaptive_sensitivity = adaptive_sensitivity_controller_numba(
                quantum_entropy, fractal_dimension, chaos_attractors, multi_timeframe_consensus
            )

            # === STEP 7: ãƒˆãƒ¬ãƒ³ãƒ‰åŠ é€Ÿåº¦æ¤œå‡º ===
            self.logger.debug("ğŸš€ ãƒˆãƒ¬ãƒ³ãƒ‰åŠ é€Ÿåº¦æ¤œå‡ºä¸­...")
            trend_acceleration = trend_acceleration_detector_numba(filtered_prices, adaptive_sensitivity)

            # === STEP 8: é‡å­ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡ ===
            self.logger.debug("ğŸŒŒ é‡å­ãƒ¬ã‚¸ãƒ¼ãƒ åˆ†é¡å®Ÿè¡Œä¸­...")
            regime_states, confidence_levels = quantum_regime_classifier_numba(
                quantum_entropy, fractal_dimension, chaos_attractors, 
                multi_timeframe_consensus, trend_acceleration
            )

            # === STEP 9: é©å‘½çš„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š ===
            self.logger.debug("ğŸ§¬ é©å‘½çš„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®šå®Ÿè¡Œä¸­...")
            trend_signals = revolutionary_trend_judgment_numba(
                filtered_prices, quantum_entropy, fractal_dimension, chaos_attractors,
                multi_timeframe_consensus, adaptive_sensitivity, trend_acceleration,
                regime_states, confidence_levels, self.slope_index
            )

            # === STEP 10: æœ€çµ‚çµ±åˆåˆ¤å®š ===
            trend_index, trend_value, regime_index = calculate_current_trend_v2_numba(trend_signals, regime_states)
            trend_names = ['range', 'up', 'down']
            current_trend = trend_names[trend_index]
            regime_names = ['range', 'trend', 'breakout', 'reversal']
            regime_name = regime_names[min(regime_index, 3)]

            # çµæœæ§‹ç¯‰
            result = UltimateMAV2Result(
                values=filtered_prices,
                raw_values=src_prices,
                quantum_entropy_score=quantum_entropy,
                fractal_dimension=fractal_dimension,
                chaos_attractors=chaos_attractors,
                multi_timeframe_consensus=multi_timeframe_consensus,
                adaptive_sensitivity=adaptive_sensitivity,
                trend_acceleration=trend_acceleration,
                regime_state=regime_states,
                confidence_level=confidence_levels,
                trend_signals=trend_signals,
                current_trend=current_trend,
                current_trend_value=trend_value,
                market_regime=regime_name
            )

            self._result = result
            self._cache[data_hash_key] = self._result
            
            self.logger.info(f"âœ… UltimateMA V2 è¨ˆç®—å®Œäº† - ãƒˆãƒ¬ãƒ³ãƒ‰: {current_trend}, ãƒ¬ã‚¸ãƒ¼ãƒ : {regime_name}")
            return self._result

        except Exception as e:
            import traceback
            self.logger.error(f"UltimateMA V2 è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}\n{traceback.format_exc()}")
            return self._create_error_result(len(data) if hasattr(data, '__len__') else 0)

    def _create_empty_result(self) -> UltimateMAV2Result:
        """ç©ºã®çµæœã‚’ä½œæˆ"""
        return UltimateMAV2Result(
            values=np.array([], dtype=np.float64),
            raw_values=np.array([], dtype=np.float64),
            quantum_entropy_score=np.array([], dtype=np.float64),
            fractal_dimension=np.array([], dtype=np.float64),
            chaos_attractors=np.array([], dtype=np.float64),
            multi_timeframe_consensus=np.array([], dtype=np.float64),
            adaptive_sensitivity=np.array([], dtype=np.float64),
            trend_acceleration=np.array([], dtype=np.float64),
            regime_state=np.array([], dtype=np.int8),
            confidence_level=np.array([], dtype=np.float64),
            trend_signals=np.array([], dtype=np.int8),
            current_trend='range',
            current_trend_value=0,
            market_regime='range'
        )

    def _create_error_result(self, data_len: int) -> UltimateMAV2Result:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®çµæœã‚’ä½œæˆ"""
        return UltimateMAV2Result(
            values=np.full(data_len, np.nan, dtype=np.float64),
            raw_values=np.full(data_len, np.nan, dtype=np.float64),
            quantum_entropy_score=np.full(data_len, np.nan, dtype=np.float64),
            fractal_dimension=np.full(data_len, 1.5, dtype=np.float64),
            chaos_attractors=np.full(data_len, np.nan, dtype=np.float64),
            multi_timeframe_consensus=np.full(data_len, np.nan, dtype=np.float64),
            adaptive_sensitivity=np.full(data_len, 0.5, dtype=np.float64),
            trend_acceleration=np.full(data_len, np.nan, dtype=np.float64),
            regime_state=np.zeros(data_len, dtype=np.int8),
            confidence_level=np.full(data_len, 0.0, dtype=np.float64),
            trend_signals=np.zeros(data_len, dtype=np.int8),
            current_trend='range',
            current_trend_value=0,
            market_regime='range'
        )

    def get_quantum_analysis_summary(self) -> dict:
        """é‡å­åˆ†æã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if self._result is None:
            return {}
        
        return {
            'average_entropy': np.nanmean(self._result.quantum_entropy_score),
            'average_fractal_dimension': np.nanmean(self._result.fractal_dimension),
            'average_chaos_level': np.nanmean(self._result.chaos_attractors),
            'average_consensus': np.nanmean(self._result.multi_timeframe_consensus),
            'average_sensitivity': np.nanmean(self._result.adaptive_sensitivity),
            'max_acceleration': np.nanmax(np.abs(self._result.trend_acceleration)),
            'current_regime': self._result.market_regime,
            'latest_confidence': float(self._result.confidence_level[-1]) if len(self._result.confidence_level) > 0 else 0.0
        }

    def reset(self) -> None:
        """çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._result = None
        self._cache = {}
        if hasattr(self, 'v1_instance'):
            self.v1_instance.reset()

    def _get_data_hash(self, data: Union[pd.DataFrame, np.ndarray]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ï¼ˆV1ã‹ã‚‰ç¶™æ‰¿ï¼‰"""
        return self.v1_instance._get_data_hash(data) 