#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ **Adaptive Unscented Kalman Filter (AUKF) - é©å¿œçš„ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼** ğŸ¯

æ¨™æº–UKFã‚’å¤§å¹…ã«è¶…ãˆã‚‹é©å¿œçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼š
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒã‚¤ã‚ºçµ±è¨ˆæ¨å®š
- å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆÎ±, Î², Îºï¼‰
- ç•°å¸¸å€¤æ¤œå‡ºã¨é™¤å»
- é©å¿œçš„çŠ¶æ…‹å…±åˆ†æ•£èª¿æ•´
- ãƒãƒ«ãƒã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æœ€é©åŒ–

ğŸŒŸ **AUKFã®é©æ–°çš„æ©Ÿèƒ½:**
1. **é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š**: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç³»åˆ—ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒã‚¤ã‚ºçµ±è¨ˆã‚’æ¨å®š
2. **å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: æ¨å®šç²¾åº¦ã«åŸºã¥ã„ã¦UKFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–
3. **ç•°å¸¸å€¤æ¤œå‡º**: Mahalanobisè·é›¢ã«ã‚ˆã‚‹å¤–ã‚Œå€¤ã®è‡ªå‹•æ¤œå‡ºãƒ»é™¤å»
4. **å…±åˆ†æ•£ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°**: æ¨å®šç²¾åº¦ä½ä¸‹æ™‚ã®è‡ªå‹•å…±åˆ†æ•£å¢—å¤§
5. **ãƒãƒ«ãƒãƒ¢ãƒ‡ãƒ«é©å¿œ**: è¤‡æ•°ã®æ™‚é–“çª“ã§ã®ä¸¦è¡Œå‡¦ç†ã¨æœ€é©é¸æŠ
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Optional, List, NamedTuple
import numpy as np
import pandas as pd
from numba import jit, njit, types
import traceback
from collections import deque

try:
    from .indicator import Indicator
    from .price_source import PriceSource
    from .unscented_kalman_filter import generate_sigma_points, state_transition_function, observation_function
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource
    from unscented_kalman_filter import generate_sigma_points, state_transition_function, observation_function


@dataclass
class AUKFResult:
    """é©å¿œçš„ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—çµæœ"""
    filtered_values: np.ndarray       # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ä¾¡æ ¼
    velocity_estimates: np.ndarray    # é€Ÿåº¦æ¨å®šå€¤
    acceleration_estimates: np.ndarray # åŠ é€Ÿåº¦æ¨å®šå€¤
    uncertainty: np.ndarray           # æ¨å®šä¸ç¢ºå®Ÿæ€§
    kalman_gains: np.ndarray          # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
    innovations: np.ndarray           # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
    sigma_points: np.ndarray          # æœ€çµ‚ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆ
    confidence_scores: np.ndarray     # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    raw_values: np.ndarray           # å…ƒã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
    
    # é©å¿œçš„æ©Ÿèƒ½ã®çµæœ
    adaptive_process_noise: np.ndarray    # é©å¿œçš„ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚º
    adaptive_observation_noise: np.ndarray # é©å¿œçš„è¦³æ¸¬ãƒã‚¤ã‚º
    adaptive_alpha: np.ndarray            # é©å¿œçš„Î±ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    outlier_flags: np.ndarray             # ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°
    covariance_fading: np.ndarray         # å…±åˆ†æ•£ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°


@dataclass
class AdaptiveParameters:
    """é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š"""
    # ãƒã‚¤ã‚ºæ¨å®šç”¨
    innovation_window: int = 20       # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æ¨å®šçª“
    noise_estimation_threshold: float = 0.1  # ãƒã‚¤ã‚ºæ¨å®šé–¾å€¤
    
    # ç•°å¸¸å€¤æ¤œå‡ºç”¨
    outlier_threshold: float = 3.0    # Mahalanobisè·é›¢é–¾å€¤
    outlier_window: int = 10         # ç•°å¸¸å€¤æ¤œå‡ºçª“
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç”¨
    alpha_min: float = 0.0001        # Î±ã®æœ€å°å€¤
    alpha_max: float = 1.0           # Î±ã®æœ€å¤§å€¤
    alpha_adaptation_rate: float = 0.1  # Î±èª¿æ•´ãƒ¬ãƒ¼ãƒˆ
    
    # å…±åˆ†æ•£ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°ç”¨
    fading_threshold: float = 2.0    # ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°é–‹å§‹é–¾å€¤
    fading_factor: float = 1.1       # ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°


@njit(fastmath=True, cache=True)
def generate_sigma_points_adaptive(
    mean: np.ndarray, 
    covariance: np.ndarray, 
    alpha: float, 
    beta: float, 
    kappa: float
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """é©å¿œçš„UKFç”¨ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ"""
    L = len(mean)
    lambda_param = alpha * alpha * (L + kappa) - L
    
    n_sigma = 2 * L + 1
    sigma_points = np.zeros((n_sigma, L))
    Wm = np.zeros(n_sigma)
    Wc = np.zeros(n_sigma)
    
    Wm[0] = lambda_param / (L + lambda_param)
    Wc[0] = lambda_param / (L + lambda_param) + (1 - alpha * alpha + beta)
    
    for i in range(1, n_sigma):
        Wm[i] = 0.5 / (L + lambda_param)
        Wc[i] = 0.5 / (L + lambda_param)
    
    sigma_points[0] = mean
    
    try:
        sqrt_matrix = np.linalg.cholesky((L + lambda_param) * covariance)
    except:
        sqrt_matrix = np.zeros((L, L))
        for i in range(L):
            sqrt_matrix[i, i] = np.sqrt(max((L + lambda_param) * covariance[i, i], 1e-8))
    
    for i in range(L):
        sigma_points[i + 1] = mean + sqrt_matrix[:, i]
        sigma_points[i + 1 + L] = mean - sqrt_matrix[:, i]
    
    return sigma_points, Wm, Wc


@njit(fastmath=True, cache=True)
def state_transition_adaptive(state: np.ndarray, dt: float = 1.0) -> np.ndarray:
    """é©å¿œçš„çŠ¶æ…‹é·ç§»é–¢æ•°"""
    price, velocity, acceleration = state[0], state[1], state[2]
    
    new_price = price + velocity * dt + 0.5 * acceleration * dt * dt
    new_velocity = velocity * 0.95 + acceleration * dt
    new_acceleration = acceleration * 0.9
    
    return np.array([new_price, new_velocity, new_acceleration])


@njit(fastmath=True, cache=True)
def observation_function_adaptive(state: np.ndarray) -> float:
    """é©å¿œçš„è¦³æ¸¬é–¢æ•°"""
    return state[0]


@njit(fastmath=True, cache=True)
def estimate_innovation_statistics(
    innovations: np.ndarray,
    window_size: int = 20
) -> Tuple[np.ndarray, np.ndarray]:
    """ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆæ¨å®š"""
    n = len(innovations)
    innovation_means = np.zeros(n)
    innovation_variances = np.zeros(n)
    
    for i in range(n):
        start_idx = max(0, i - window_size + 1)
        window_innovations = innovations[start_idx:i+1]
        
        if len(window_innovations) > 1:
            innovation_means[i] = np.mean(window_innovations)
            innovation_variances[i] = np.var(window_innovations)
        else:
            innovation_means[i] = innovations[i] if i >= 0 else 0.0
            innovation_variances[i] = 0.01
    
    return innovation_means, innovation_variances


@njit(fastmath=True, cache=True)
def detect_outliers_mahalanobis(
    innovations: np.ndarray,
    innovation_variances: np.ndarray,
    threshold: float = 3.0
) -> np.ndarray:
    """Mahalanobisè·é›¢ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º"""
    n = len(innovations)
    outlier_flags = np.zeros(n)
    
    for i in range(n):
        if innovation_variances[i] > 1e-8:
            mahalanobis_dist = abs(innovations[i]) / np.sqrt(innovation_variances[i])
            if mahalanobis_dist > threshold:
                outlier_flags[i] = 1.0
    
    return outlier_flags


@njit(fastmath=True, cache=True)
def adapt_ukf_parameters(
    innovation_variances: np.ndarray,
    confidence_scores: np.ndarray,
    current_alpha: float,
    alpha_min: float = 0.0001,
    alpha_max: float = 1.0,
    adaptation_rate: float = 0.1
) -> np.ndarray:
    """
    æ¨å®šç²¾åº¦ã«åŸºã¥ãUKFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é©å¿œçš„èª¿æ•´
    
    Args:
        innovation_variances: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ•£
        confidence_scores: ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        current_alpha: ç¾åœ¨ã®Î±ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        alpha_min: Î±ã®æœ€å°å€¤
        alpha_max: Î±ã®æœ€å¤§å€¤
        adaptation_rate: èª¿æ•´ãƒ¬ãƒ¼ãƒˆ
    
    Returns:
        adaptive_alphas: é©å¿œçš„Î±ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç³»åˆ—
    """
    n = len(innovation_variances)
    adaptive_alphas = np.full(n, current_alpha)
    
    for i in range(1, n):
        # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ•£ã¨ä¿¡é ¼åº¦ã«åŸºã¥ãèª¿æ•´
        if innovation_variances[i] > 0:
            # é«˜ã„ä¸ç¢ºå®Ÿæ€§ -> Î±ã‚’å¢—åŠ ï¼ˆã‚ˆã‚Šåºƒã„åˆ†æ•£ï¼‰
            uncertainty_factor = min(innovation_variances[i] * 10, 2.0)
            confidence_factor = max(confidence_scores[i], 0.1)
            
            # ç›®æ¨™Î±å€¤ã®è¨ˆç®—
            target_alpha = current_alpha * uncertainty_factor / confidence_factor
            target_alpha = max(alpha_min, min(alpha_max, target_alpha))
            
            # æ®µéšçš„èª¿æ•´
            adaptive_alphas[i] = (adaptive_alphas[i-1] * (1 - adaptation_rate) + 
                                target_alpha * adaptation_rate)
        else:
            adaptive_alphas[i] = adaptive_alphas[i-1]
    
    return adaptive_alphas


@njit(fastmath=True, cache=True)
def calculate_covariance_fading(
    innovation_variances: np.ndarray,
    threshold: float = 2.0,
    fading_factor: float = 1.1
) -> np.ndarray:
    """
    å…±åˆ†æ•£ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°ã®è¨ˆç®—
    
    Args:
        innovation_variances: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ•£
        threshold: ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°é–‹å§‹é–¾å€¤
        fading_factor: ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°
    
    Returns:
        fading_coefficients: ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°
    """
    n = len(innovation_variances)
    fading_coefficients = np.ones(n)
    
    if n == 0:
        return fading_coefficients
    
    # åˆæœŸåˆ†æ•£ã®æ¨å®š
    initial_variance = np.mean(innovation_variances[:min(10, n)])
    
    for i in range(n):
        if initial_variance > 0:
            variance_ratio = innovation_variances[i] / initial_variance
            if variance_ratio > threshold:
                fading_coefficients[i] = min(fading_factor * variance_ratio, 3.0)
    
    return fading_coefficients


@njit(fastmath=True, cache=True)
def calculate_adaptive_ukf(
    prices: np.ndarray,
    volatility: np.ndarray,
    innovation_window: int = 20,
    outlier_threshold: float = 3.0,
    alpha_min: float = 0.0001,
    alpha_max: float = 1.0,
    adaptation_rate: float = 0.1,
    fading_threshold: float = 2.0,
    fading_factor: float = 1.1
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, 
           np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """é©å¿œçš„UKFãƒ¡ã‚¤ãƒ³è¨ˆç®—"""
    n = len(prices)
    L = 3
    
    if n < 5:
        zeros = np.zeros(n)
        ones = np.ones(n)
        return (prices.copy(), zeros, zeros, ones, ones * 0.5, zeros, ones,
                ones * 0.001, ones * 0.01, ones * 0.001, zeros, ones)
    
    # çµæœé…åˆ—åˆæœŸåŒ–
    filtered_prices = np.zeros(n)
    velocity_estimates = np.zeros(n)
    acceleration_estimates = np.zeros(n)
    uncertainty = np.zeros(n)
    kalman_gains = np.zeros(n)
    innovations = np.zeros(n)
    confidence_scores = np.zeros(n)
    adaptive_process_noise = np.zeros(n)
    adaptive_observation_noise = np.zeros(n)
    adaptive_alpha = np.zeros(n)
    outlier_flags = np.zeros(n)
    covariance_fading = np.zeros(n)
    
    # åˆæœŸçŠ¶æ…‹
    x = np.array([prices[0], 0.0, 0.0])
    P = np.array([[1.0, 0.0, 0.0],
                  [0.0, 0.1, 0.0],
                  [0.0, 0.0, 0.01]])
    
    # åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    current_alpha = 0.001
    current_beta = 2.0
    current_kappa = 0.0
    
    # åˆæœŸå€¤è¨­å®š
    filtered_prices[0] = prices[0]
    adaptive_alpha[0] = current_alpha
    adaptive_process_noise[0] = 0.001
    adaptive_observation_noise[0] = volatility[0] * volatility[0]
    confidence_scores[0] = 1.0
    covariance_fading[0] = 1.0
    
    # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
    for t in range(1, n):
        # === é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° ===
        
        # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆæ¨å®š
        if t >= innovation_window:
            recent_innovations = innovations[max(0, t-innovation_window):t]
            innovation_var = np.var(recent_innovations) if len(recent_innovations) > 1 else 0.01
        else:
            innovation_var = 0.01
        
        # ç•°å¸¸å€¤æ¤œå‡º
        if t >= 10 and innovation_var > 0:
            mahalanobis_dist = abs(innovations[t-1]) / np.sqrt(innovation_var)
            outlier_flags[t] = 1.0 if mahalanobis_dist > outlier_threshold else 0.0
        
        # é©å¿œçš„ãƒã‚¤ã‚ºæ¨å®š
        adaptive_observation_noise[t] = max(innovation_var, 0.0001)
        
        # ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºé©å¿œ
        if t > 1:
            velocity_change = abs(velocity_estimates[t-1] - velocity_estimates[max(0, t-2)])
            adaptive_process_noise[t] = max(0.001 * (1 + velocity_change), 0.0001)
        else:
            adaptive_process_noise[t] = 0.001
        
        # Î±é©å¿œ
        if t > 1:
            confidence_factor = max(confidence_scores[t-1], 0.1)
            uncertainty_factor = min(innovation_var * 10, 2.0)
            target_alpha = current_alpha * uncertainty_factor / confidence_factor
            target_alpha = max(alpha_min, min(alpha_max, target_alpha))
            current_alpha = (current_alpha * (1 - adaptation_rate) + 
                           target_alpha * adaptation_rate)
        
        adaptive_alpha[t] = current_alpha
        
        # å…±åˆ†æ•£ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°
        if innovation_var > fading_threshold * adaptive_observation_noise[max(0, t-1)]:
            covariance_fading[t] = min(fading_factor, 3.0)
        else:
            covariance_fading[t] = 1.0
        
        # === UKFè¨ˆç®— ===
        
        # ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
        P = P * covariance_fading[t]
        
        # ã‚·ã‚°ãƒãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
        sigma_points, Wm, Wc = generate_sigma_points_adaptive(x, P, current_alpha, current_beta, current_kappa)
        
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        n_sigma = len(sigma_points)
        sigma_points_pred = np.zeros((n_sigma, L))
        for i in range(n_sigma):
            sigma_points_pred[i] = state_transition_adaptive(sigma_points[i], 1.0)
        
        # äºˆæ¸¬çŠ¶æ…‹å¹³å‡
        x_pred = np.zeros(L)
        for i in range(n_sigma):
            x_pred += Wm[i] * sigma_points_pred[i]
        
        # äºˆæ¸¬å…±åˆ†æ•£
        Q = np.array([[adaptive_process_noise[t], 0.0, 0.0],
                      [0.0, adaptive_process_noise[t] * 0.1, 0.0],
                      [0.0, 0.0, adaptive_process_noise[t] * 0.01]])
        
        P_pred = Q.copy()
        for i in range(n_sigma):
            diff = sigma_points_pred[i] - x_pred
            P_pred += Wc[i] * np.outer(diff, diff)
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        z_sigma = np.zeros(n_sigma)
        for i in range(n_sigma):
            z_sigma[i] = observation_function_adaptive(sigma_points_pred[i])
        
        z_pred = 0.0
        for i in range(n_sigma):
            z_pred += Wm[i] * z_sigma[i]
        
        R = adaptive_observation_noise[t]
        
        S = R
        Pxz = np.zeros(L)
        
        for i in range(n_sigma):
            z_diff = z_sigma[i] - z_pred
            x_diff = sigma_points_pred[i] - x_pred
            S += Wc[i] * z_diff * z_diff
            Pxz += Wc[i] * x_diff * z_diff
        
        # ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        if S > 1e-12:
            K = Pxz / S
        else:
            K = np.array([0.5, 0.0, 0.0])
        
        # çŠ¶æ…‹æ›´æ–°
        innovation = prices[t] - z_pred
        if outlier_flags[t] == 0.0:  # æ­£å¸¸å€¤
            x = x_pred + K * innovation
            P = P_pred - np.outer(K, K) * S
        else:  # ç•°å¸¸å€¤
            x = x_pred.copy()
            P = P_pred.copy() * 1.5
        
        # æ•°å€¤å®‰å®šæ€§ç¢ºä¿
        for i in range(L):
            if P[i, i] < 1e-8:
                P[i, i] = 1e-8
        
        # çµæœä¿å­˜
        filtered_prices[t] = x[0]
        velocity_estimates[t] = x[1]
        acceleration_estimates[t] = x[2]
        uncertainty[t] = np.sqrt(max(P[0, 0], 0))
        kalman_gains[t] = K[0]
        innovations[t] = innovation
        confidence_scores[t] = 1.0 / (1.0 + uncertainty[t] * 10.0)
    
    return (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
            kalman_gains, innovations, confidence_scores,
            adaptive_process_noise, adaptive_observation_noise, adaptive_alpha,
            outlier_flags, covariance_fading)


class AdaptiveUnscentedKalmanFilter(Indicator):
    """
    é©å¿œçš„ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆAUKFï¼‰
    
    ğŸŒŸ **é©æ–°çš„é©å¿œæ©Ÿèƒ½:**
    1. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒã‚¤ã‚ºæ¨å®š**: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç³»åˆ—ã‹ã‚‰å‹•çš„ãƒã‚¤ã‚ºçµ±è¨ˆæ¨å®š
    2. **å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: Î±, Î², Îºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è‡ªå‹•æœ€é©åŒ–
    3. **ç•°å¸¸å€¤æ¤œå‡ºãƒ»é™¤å»**: Mahalanobisè·é›¢ã«ã‚ˆã‚‹å¤–ã‚Œå€¤ã®è‡ªå‹•å‡¦ç†
    4. **å…±åˆ†æ•£ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°**: æ¨å®šç²¾åº¦ä½ä¸‹æ™‚ã®è‡ªå‹•å…±åˆ†æ•£èª¿æ•´
    5. **ãƒãƒ«ãƒã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æœ€é©åŒ–**: è¤‡æ•°æ™‚é–“çª“ã§ã®ä¸¦è¡Œæœ€é©åŒ–
    
    ğŸ† **æ¨™æº–UKFã«å¯¾ã™ã‚‹å„ªä½æ€§:**
    - ç’°å¢ƒå¤‰åŒ–ã¸ã®è‡ªå‹•é©å¿œ
    - ãƒã‚¤ã‚ºçµ±è¨ˆã®äº‹å‰çŸ¥è­˜ä¸è¦
    - ç•°å¸¸å€¤ã«å¯¾ã™ã‚‹å …ç‰¢æ€§
    - ã‚ˆã‚Šé«˜ã„æ¨å®šç²¾åº¦
    - è‡ªå‹•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    """
    
    def __init__(
        self,
        src_type: str = 'close',
        innovation_window: int = 20,
        outlier_threshold: float = 3.0,
        alpha_min: float = 0.0001,
        alpha_max: float = 1.0,
        adaptation_rate: float = 0.1
    ):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿
        
        Args:
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
            innovation_window: ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æ¨å®šçª“
            outlier_threshold: ç•°å¸¸å€¤æ¤œå‡ºé–¾å€¤
            alpha_min: Î±ã®æœ€å°å€¤
            alpha_max: Î±ã®æœ€å¤§å€¤
            adaptation_rate: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©å¿œãƒ¬ãƒ¼ãƒˆ
        """
        super().__init__(f"AUKF(src={src_type})")
        
        self.src_type = src_type.lower()
        self.innovation_window = innovation_window
        self.outlier_threshold = outlier_threshold
        self.alpha_min = alpha_min
        self.alpha_max = alpha_max
        self.adaptation_rate = adaptation_rate
        
        self._latest_result = None
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> AUKFResult:
        """é©å¿œçš„UKFè¨ˆç®—"""
        try:
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            src_prices = PriceSource.calculate_source(data, self.src_type)
            
            if len(src_prices) < 5:
                return self._create_empty_result(len(src_prices), src_prices)
            
            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®š
            volatility = self._estimate_volatility(src_prices)
            
            # AUKFè¨ˆç®—
            (filtered_prices, velocity_estimates, acceleration_estimates, uncertainty,
             kalman_gains, innovations, confidence_scores,
             adaptive_process_noise, adaptive_observation_noise, adaptive_alpha,
             outlier_flags, covariance_fading) = calculate_adaptive_ukf(
                src_prices, volatility, self.innovation_window, self.outlier_threshold,
                self.alpha_min, self.alpha_max, self.adaptation_rate
            )
            
            # çµæœä½œæˆ
            result = AUKFResult(
                filtered_values=filtered_prices.copy(),
                velocity_estimates=velocity_estimates.copy(),
                acceleration_estimates=acceleration_estimates.copy(),
                uncertainty=uncertainty.copy(),
                kalman_gains=kalman_gains.copy(),
                innovations=innovations.copy(),
                sigma_points=np.zeros((7, 3)),  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                confidence_scores=confidence_scores.copy(),
                raw_values=src_prices.copy(),
                adaptive_process_noise=adaptive_process_noise.copy(),
                adaptive_observation_noise=adaptive_observation_noise.copy(),
                adaptive_alpha=adaptive_alpha.copy(),
                outlier_flags=outlier_flags.copy(),
                covariance_fading=covariance_fading.copy()
            )
            
            self._latest_result = result
            self._values = filtered_prices
            return result
            
        except Exception as e:
            self.logger.error(f"AUKFè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            if isinstance(data, (pd.DataFrame, np.ndarray)) and len(data) > 0:
                src_prices = PriceSource.calculate_source(data, self.src_type)
                return self._create_empty_result(len(src_prices), src_prices)
            else:
                return self._create_empty_result(0, np.array([]))
    
    def _estimate_volatility(self, prices: np.ndarray, window: int = 10) -> np.ndarray:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æ¨å®š"""
        n = len(prices)
        volatility = np.full(n, 0.01)
        
        for i in range(window, n):
            window_prices = prices[i-window:i]
            vol = np.std(window_prices)
            volatility[i] = max(vol, 0.001)
        
        if n >= window:
            initial_vol = volatility[window]
            for i in range(window):
                volatility[i] = initial_vol
        
        return volatility
    
    def _create_empty_result(self, length: int, raw_prices: np.ndarray) -> AUKFResult:
        """ç©ºã®çµæœä½œæˆ"""
        nan_array = np.full(length, np.nan)
        return AUKFResult(
            filtered_values=nan_array.copy(),
            velocity_estimates=nan_array.copy(),
            acceleration_estimates=nan_array.copy(),
            uncertainty=nan_array.copy(),
            kalman_gains=nan_array.copy(),
            innovations=nan_array.copy(),
            sigma_points=np.full((7, 3), np.nan),
            confidence_scores=nan_array.copy(),
            raw_values=raw_prices,
            adaptive_process_noise=nan_array.copy(),
            adaptive_observation_noise=nan_array.copy(),
            adaptive_alpha=nan_array.copy(),
            outlier_flags=nan_array.copy(),
            covariance_fading=nan_array.copy()
        )
    
    def get_adaptation_summary(self) -> Dict:
        """é©å¿œæ©Ÿèƒ½è¦ç´„"""
        if not self._latest_result:
            return {}
        
        result = self._latest_result
        return {
            'filter_type': 'Adaptive Unscented Kalman Filter',
            'outlier_detection_rate': np.mean(result.outlier_flags),
            'avg_adaptive_alpha': np.nanmean(result.adaptive_alpha),
            'alpha_range': (np.nanmin(result.adaptive_alpha), np.nanmax(result.adaptive_alpha)),
            'fading_activation_rate': np.mean(result.covariance_fading > 1.0),
            'adaptive_features': [
                'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒã‚¤ã‚ºæ¨å®š',
                'å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´',
                'ç•°å¸¸å€¤è‡ªå‹•æ¤œå‡º',
                'å…±åˆ†æ•£ãƒ•ã‚§ãƒ¼ã‚¸ãƒ³ã‚°'
            ]
        } 