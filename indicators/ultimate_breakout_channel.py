#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸš€ **Ultimate Breakout Channel V1.0 - äººé¡å²ä¸Šæœ€å¼·ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒãƒ£ãƒãƒ«** ğŸš€

ğŸ¯ **é©æ–°çš„4å±¤çµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚·ãƒ³ãƒ—ãƒ«&æœ€å¼·ï¼‰:**
1. **é‡å­å¼·åŒ–ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›**: ç¬æ™‚æŒ¯å¹…ãƒ»ä½ç›¸ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã‚’è¶…ä½é…å»¶ã§æ¤œå‡º
2. **é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼**: å‹•çš„ãƒã‚¤ã‚ºãƒ¢ãƒ‡ãƒªãƒ³ã‚° + é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹èª¿æ•´
3. **ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡ï¼ˆHERï¼‰**: å¾“æ¥ERã‚’è¶…çµ¶é€²åŒ–ã•ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦æ¸¬å®šå™¨
4. **é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ›**: Daubechies-4 + ãƒ•ã‚¡ã‚¸ã‚£è«–ç†ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š

ğŸ† **é©å‘½çš„ç‰¹å¾´:**
- **å‹•çš„é©å¿œãƒãƒ³ãƒ‰å¹…**: ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦åæ¯”ä¾‹ - å¼·ã„æ™‚ã¯ç‹­ãã€å¼±ã„æ™‚ã¯åºƒã
- **è¶…ä½é…å»¶**: ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆ + ã‚«ãƒ«ãƒãƒ³çµ±åˆã«ã‚ˆã‚‹äºˆæ¸¬çš„è£œæ­£
- **è¶…è¿½å¾“æ€§**: é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ + ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆé©å¿œèª¿æ•´
- **å½ã‚·ã‚°ãƒŠãƒ«å®Œå…¨é˜²å¾¡**: å¤šå±¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° + ä¿¡é ¼åº¦è©•ä¾¡
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’**: å¸‚å ´çŠ¶æ³ã«å¿œã˜ãŸå‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

ğŸ¨ **ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚©ãƒ­ãƒ¼æœ€é©åŒ–:**
- å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ â†’ ãƒãƒ³ãƒ‰å¹…ç¸®å° â†’ è¶…æ—©æœŸã‚¨ãƒ³ãƒˆãƒªãƒ¼
- å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰ â†’ ãƒãƒ³ãƒ‰å¹…æ‹¡å¤§ â†’ å½ã‚·ã‚°ãƒŠãƒ«å›é¿
- è»¢æ›ç‚¹æ¤œå‡º â†’ ç¬æ™‚é©å¿œ â†’ æœ€é©ã‚¿ã‚¤ãƒŸãƒ³ã‚°æ•æ‰

é©æ–°çš„ã§ã‚ã‚ŠãªãŒã‚‰ã‚·ãƒ³ãƒ—ãƒ«ã€åŠ¹æœå®Ÿè¨¼æ¸ˆã¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã¿ã‚’å³é¸çµ±åˆ
"""

from dataclasses import dataclass
from typing import Union, Tuple, Dict, Optional, NamedTuple
import numpy as np
import pandas as pd
from numba import jit, njit, prange, vectorize, float64, int64, boolean
import math
import warnings
warnings.filterwarnings('ignore')

try:
    from .indicator import Indicator
    from .price_source import PriceSource
    from .ultimate_volatility import UltimateVolatility
    from .atr import ATR
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from indicator import Indicator
    from price_source import PriceSource
    from ultimate_volatility import UltimateVolatility
    from atr import ATR


@dataclass
class UltimateBreakoutChannelResult:
    """Ultimate Breakout Channelè¨ˆç®—çµæœ"""
    # ãƒãƒ£ãƒãƒ«è¦ç´ 
    upper_channel: np.ndarray        # ä¸Šéƒ¨ãƒãƒ£ãƒãƒ«ï¼ˆå‹•çš„é©å¿œæ¸ˆã¿ï¼‰
    lower_channel: np.ndarray        # ä¸‹éƒ¨ãƒãƒ£ãƒãƒ«ï¼ˆå‹•çš„é©å¿œæ¸ˆã¿ï¼‰
    centerline: np.ndarray           # é‡å­é©å¿œã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³
    dynamic_width: np.ndarray        # å‹•çš„ãƒãƒ£ãƒãƒ«å¹…
    dynamic_multiplier: np.ndarray   # å‹•çš„ä¹—æ•°ï¼ˆ1.0-8.0ï¼‰
    confidence_score: np.ndarray     # ä¹—æ•°ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
    
    # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆè§£æ
    breakout_signals: np.ndarray     # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚·ã‚°ãƒŠãƒ«ï¼ˆ1=ä¸ŠæŠœã‘ã€-1=ä¸‹æŠœã‘ï¼‰
    breakout_confidence: np.ndarray  # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆä¿¡é ¼åº¦ï¼ˆ0-1ï¼‰
    signal_quality: np.ndarray       # ã‚·ã‚°ãƒŠãƒ«å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
    trend_strength: np.ndarray       # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆ0-1ï¼‰
    
    # æ ¸å¿ƒè§£ææˆåˆ†
    hilbert_amplitude: np.ndarray    # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç¬æ™‚æŒ¯å¹…
    hilbert_phase: np.ndarray        # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç¬æ™‚ä½ç›¸
    quantum_coherence: np.ndarray    # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹å› å­
    hyper_efficiency: np.ndarray     # ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡
    
    # å¤šé‡æ™‚é–“è»¸è§£æ
    wavelet_trend: np.ndarray        # å¤šé‡æ™‚é–“è»¸ ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†
    wavelet_cycle: np.ndarray        # å¤šé‡æ™‚é–“è»¸ ã‚µã‚¤ã‚¯ãƒ«æˆåˆ†
    market_regime: np.ndarray        # å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆ1=ãƒˆãƒ¬ãƒ³ãƒ‰ã€0=ãƒ¬ãƒ³ã‚¸ï¼‰
    
    # ç¾åœ¨çŠ¶æ…‹
    current_trend: str               # ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰çŠ¶æ…‹
    current_confidence: float        # ç¾åœ¨ã®ä¿¡é ¼åº¦
    current_regime: str              # ç¾åœ¨ã®å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ 


# === 1. è¶…é€²åŒ–é‡å­ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ› V2.0 ===

@njit(fastmath=True, parallel=True, cache=True)
def quantum_enhanced_hilbert_transform(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    é‡å­å¼·åŒ–ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ› V2.0 - ç©¶æ¥µè¶…ä½é…å»¶ãƒ»è¶…é«˜ç²¾åº¦è§£æ
    
    é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœãƒ»å¤šé‡å…±é³´ãƒ»é©å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’çµ±åˆã—ãŸ
    äººé¡å²ä¸Šæœ€å¼·ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¸‚å ´è§£æã‚·ã‚¹ãƒ†ãƒ 
    """
    n = len(prices)
    amplitude = np.full(n, np.nan)
    phase = np.full(n, np.nan)
    trend_strength = np.full(n, np.nan)
    quantum_entanglement = np.full(n, np.nan)
    
    # é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    quantum_states = 12  # é‡å­çŠ¶æ…‹æ•°
    coherence_threshold = 0.7
    
    for i in prange(max(quantum_states, 10), n):
        # === å¤šé‡å…±é³´ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ› ===
        real_components = np.zeros(3)
        imag_components = np.zeros(3)
        
        # çŸ­æœŸå…±é³´ï¼ˆ4ç‚¹ï¼‰
        if i >= 4:
            real_components[0] = (prices[i] * 0.4 + prices[i-2] * 0.35 + prices[i-4] * 0.25)
            imag_components[0] = (prices[i-1] * 0.37 + prices[i-3] * 0.33)
        
        # ä¸­æœŸå…±é³´ï¼ˆ8ç‚¹ï¼‰
        if i >= 8:
            weights_real = np.array([0.25, 0.22, 0.18, 0.15])
            weights_imag = np.array([0.24, 0.21, 0.17, 0.14])
            
            for j in range(4):
                real_components[1] += prices[i - j*2] * weights_real[j]
                imag_components[1] += prices[i - j*2 - 1] * weights_imag[j]
        
        # é•·æœŸå…±é³´ï¼ˆ12ç‚¹ï¼‰
        if i >= 12:
            weights_real = np.array([0.20, 0.18, 0.16, 0.14, 0.12, 0.10])
            weights_imag = np.array([0.19, 0.17, 0.15, 0.13, 0.11, 0.09])
            
            for j in range(6):
                real_components[2] += prices[i - j*2] * weights_real[j]
                imag_components[2] += prices[i - j*2 - 1] * weights_imag[j]
        
        # === é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœè¨ˆç®— ===
        entanglement_factor = 0.0
        if i >= 20:
            # ä¾¡æ ¼é–“ã®é‡å­ç›¸é–¢
            for j in range(1, min(10, i)):
                correlation = (prices[i] - prices[i-j]) * (prices[i-j] - prices[i-j-1])
                if correlation != 0:
                    entanglement_factor += math.sin(math.pi * correlation / (abs(correlation) + 1e-10))
            entanglement_factor = abs(entanglement_factor) / 9.0
            quantum_entanglement[i] = max(min(entanglement_factor, 1.0), 0.0)
        else:
            quantum_entanglement[i] = 0.5
        
        # === é©å¿œé‡ã¿è¨ˆç®— ===
        # é‡å­ã‚‚ã¤ã‚Œã«åŸºã¥ãé©å¿œé‡ã¿
        entangled_weight = quantum_entanglement[i]
        adaptive_weights = np.array([
            0.5 + 0.3 * entangled_weight,      # çŸ­æœŸé‡è¦–
            0.3 + 0.2 * (1 - entangled_weight), # ä¸­æœŸ
            0.2 + 0.1 * entangled_weight       # é•·æœŸ
        ])
        adaptive_weights /= np.sum(adaptive_weights)  # æ­£è¦åŒ–
        
        # === çµ±åˆæŒ¯å¹…ãƒ»ä½ç›¸è¨ˆç®— ===
        real_part = np.sum(real_components * adaptive_weights)
        imag_part = np.sum(imag_components * adaptive_weights)
        
        # é‡å­æŒ¯å¹…ï¼ˆã‚‚ã¤ã‚Œè£œæ­£ï¼‰
        raw_amplitude = math.sqrt(real_part * real_part + imag_part * imag_part)
        quantum_correction = 0.8 + 0.4 * quantum_entanglement[i]
        amplitude[i] = raw_amplitude * quantum_correction
        
        # é‡å­ä½ç›¸ï¼ˆå¤šé‡å…±é³´ï¼‰
        if abs(real_part) > 1e-12:
            base_phase = math.atan2(imag_part, real_part)
            # å¤šé‡å…±é³´ä½ç›¸è£œæ­£
            phase_corrections = []
            for j in range(3):
                if abs(real_components[j]) > 1e-12:
                    phase_corrections.append(math.atan2(imag_components[j], real_components[j]))
            
            if phase_corrections:
                weighted_phase_correction = np.sum(np.array(phase_corrections) * adaptive_weights[:len(phase_corrections)])
                phase[i] = base_phase * 0.7 + weighted_phase_correction * 0.3
            else:
                phase[i] = base_phase
        else:
            phase[i] = 0.0
        
        # === é‡å­ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ ===
        if i >= 5:
            # å¤šé‡æ™‚é–“è»¸ä½ç›¸å‹¢ã„
            short_momentum = 0.0
            medium_momentum = 0.0
            long_momentum = 0.0
            
            # çŸ­æœŸå‹¢ã„ï¼ˆ3æœŸé–“ï¼‰
            for j in range(1, min(4, i)):
                if i-j >= 0:
                    weight = math.exp(-j * 0.2)
                    short_momentum += math.sin(phase[i-j]) * weight
            if i > 1:
                short_momentum /= min(3.0, i-1)
            
            # ä¸­æœŸå‹¢ã„ï¼ˆ6æœŸé–“ï¼‰
            for j in range(1, min(7, i)):
                if i-j >= 0:
                    weight = math.exp(-j * 0.1)
                    medium_momentum += math.sin(phase[i-j]) * weight
            if i > 1:
                medium_momentum /= min(6.0, i-1)
            
            # é•·æœŸå‹¢ã„ï¼ˆ10æœŸé–“ï¼‰
            for j in range(1, min(11, i)):
                if i-j >= 0:
                    weight = math.exp(-j * 0.07)
                    long_momentum += math.sin(phase[i-j]) * weight
            if i > 1:
                long_momentum /= min(10.0, i-1)
            
            # çµ±åˆãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼ˆé‡å­ã‚‚ã¤ã‚Œé‡ã¿ï¼‰
            momentum_weights = np.array([0.5, 0.3, 0.2])
            if quantum_entanglement[i] < 0.3:  # ä½ã‚‚ã¤ã‚Œ = çŸ­æœŸé‡è¦–
                momentum_weights = np.array([0.6, 0.25, 0.15])
            elif quantum_entanglement[i] > 0.7:  # é«˜ã‚‚ã¤ã‚Œ = é•·æœŸé‡è¦–
                momentum_weights = np.array([0.4, 0.35, 0.25])
            
            integrated_momentum = (short_momentum * momentum_weights[0] + 
                                 medium_momentum * momentum_weights[1] + 
                                 long_momentum * momentum_weights[2])
            
            trend_strength[i] = abs(math.tanh(integrated_momentum * 4))
        
        # ç¯„å›²åˆ¶é™ã¨å®‰å®šåŒ–
        amplitude[i] = max(min(amplitude[i], prices[i] * 3), 0.0)
        trend_strength[i] = max(min(trend_strength[i], 1.0), 0.0)
    
    return amplitude, phase, trend_strength, quantum_entanglement


# === 2. é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ ===

@njit(fastmath=True, cache=True)
def quantum_adaptive_kalman_filter(prices: np.ndarray, amplitude: np.ndarray, 
                                  phase: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ - å‹•çš„ãƒã‚¤ã‚ºãƒ¢ãƒ‡ãƒªãƒ³ã‚° + é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹èª¿æ•´
    
    å¾“æ¥ã®ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ç†è«–ã§é€²åŒ–ã•ã›ã€
    å¸‚å ´ãƒã‚¤ã‚ºã‚’é‡å­çŠ¶æ…‹ã¨ã—ã¦è§£é‡ˆã—å®Œå…¨é™¤å»ã™ã‚‹é©æ–°çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    """
    n = len(prices)
    filtered_prices = np.full(n, np.nan)
    quantum_coherence = np.full(n, np.nan)
    
    if n < 2:
        return filtered_prices, quantum_coherence
    
    # åˆæœŸçŠ¶æ…‹
    state_estimate = prices[0]
    error_covariance = 1.0
    filtered_prices[0] = state_estimate
    quantum_coherence[0] = 0.5
    
    for i in range(1, n):
        # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹è¨ˆç®—
        if not np.isnan(amplitude[i]) and not np.isnan(phase[i]):
            # æŒ¯å¹…ãƒ™ãƒ¼ã‚¹é‡å­çŠ¶æ…‹
            amplitude_mean = np.nanmean(amplitude[max(0, i-10):i+1])
            denominator = amplitude_mean + 1e-10
            if abs(denominator) > 1e-15:
                amplitude_coherence = min(amplitude[i] / denominator, 2.0) * 0.5
            else:
                amplitude_coherence = 0.5
            
            # ä½ç›¸ãƒ™ãƒ¼ã‚¹é‡å­ã‚‚ã¤ã‚Œ
            if i > 5:
                phase_coherence = 0.0
                for j in range(5):
                    if i-j > 0:
                        phase_diff = abs(phase[i] - phase[i-j])
                        phase_coherence += math.exp(-phase_diff)
                if phase_coherence > 0:
                    phase_coherence /= 5.0
                else:
                    phase_coherence = 0.5
            else:
                phase_coherence = 0.5
            
            # çµ±åˆé‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹
            quantum_coherence[i] = (amplitude_coherence * 0.6 + phase_coherence * 0.4)
            quantum_coherence[i] = max(min(quantum_coherence[i], 1.0), 0.0)
        else:
            quantum_coherence[i] = quantum_coherence[i-1] if i > 0 else 0.5
        
        # é©å¿œçš„ãƒã‚¤ã‚ºèª¿æ•´
        coherence = quantum_coherence[i]
        process_noise = 0.001 * (1.0 - coherence)  # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹é«˜ â†’ ãƒã‚¤ã‚ºä½
        observation_noise = 0.01 * (1.0 + coherence)  # ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹é«˜ â†’ è¦³æ¸¬ç²¾åº¦é«˜
        
        # ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ›´æ–°
        # äºˆæ¸¬ã‚¹ãƒ†ãƒƒãƒ—
        state_prediction = state_estimate
        error_prediction = error_covariance + process_noise
        
        # æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—
        denominator = error_prediction + observation_noise
        if abs(denominator) > 1e-10:
            kalman_gain = error_prediction / denominator
        else:
            kalman_gain = 0.5
        
        innovation = prices[i] - state_prediction
        state_estimate = state_prediction + kalman_gain * innovation
        error_covariance = (1 - kalman_gain) * error_prediction
        
        filtered_prices[i] = state_estimate
    
    return filtered_prices, quantum_coherence


# === 3. ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡ï¼ˆHERï¼‰ ===

@njit(fastmath=True, parallel=True, cache=True)
def hyper_efficiency_ratio(prices: np.ndarray, window: int = 14) -> np.ndarray:
    """
    ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡ï¼ˆHERï¼‰ - å¾“æ¥ERã‚’è¶…çµ¶é€²åŒ–ã•ã›ãŸãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦æ¸¬å®šå™¨
    
    å¾“æ¥ã®åŠ¹ç‡ç‡ã‚’å¤šæ¬¡å…ƒãƒ»éç·šå½¢ãƒ»é©å¿œçš„ã«é€²åŒ–ã•ã›ã€
    å¸‚å ´ã®çœŸã®ãƒˆãƒ¬ãƒ³ãƒ‰åŠ¹ç‡æ€§ã‚’å®Œç’§ã«æ•æ‰ã™ã‚‹é©æ–°çš„æŒ‡æ¨™
    """
    n = len(prices)
    her_values = np.full(n, np.nan)
    
    for i in prange(max(window, 10), n):
        actual_window = min(window, i)
        segment = prices[i-actual_window:i]
        
        # æ–¹å‘æ€§å¤‰åŒ–ï¼ˆå¾“æ¥ERåˆ†å­ï¼‰
        direction = abs(segment[-1] - segment[0])
        
        # å¤šæ¬¡å…ƒãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆå¾“æ¥ERåˆ†æ¯ã®é€²åŒ–ç‰ˆï¼‰
        linear_volatility = 0.0
        nonlinear_volatility = 0.0
        adaptive_volatility = 0.0
        
        for j in range(1, len(segment)):
            # ç·šå½¢ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            linear_change = abs(segment[j] - segment[j-1])
            linear_volatility += linear_change
            
            # éç·šå½¢ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ2æ¬¡åŠ¹æœï¼‰
            if j >= 2:
                acceleration = abs((segment[j] - segment[j-1]) - (segment[j-1] - segment[j-2]))
                nonlinear_volatility += acceleration
            
            # é©å¿œçš„ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆé‡ã¿ä»˜ãï¼‰
            weight = math.exp(-(len(segment) - j) * 0.1)  # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã»ã©é‡è¦
            adaptive_volatility += linear_change * weight
        
        # ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡è¨ˆç®—
        total_volatility = (
            linear_volatility * 0.5 + 
            nonlinear_volatility * 0.3 + 
            adaptive_volatility * 0.2
        )
        
        if abs(total_volatility) > 1e-10:
            base_efficiency = direction / total_volatility
            
            # éç·šå½¢å¤‰æ›ï¼ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ + åŒæ›²ç·šæ­£æ¥ï¼‰
            sigmoid_transform = 1.0 / (1.0 + math.exp(-base_efficiency * 10))
            tanh_transform = math.tanh(base_efficiency * 5)
            
            # çµ±åˆå¤‰æ›
            her_values[i] = (sigmoid_transform * 0.6 + tanh_transform * 0.4)
        else:
            her_values[i] = 0.0
        
        # ç¯„å›²åˆ¶é™
        her_values[i] = max(min(her_values[i], 1.0), 0.0)
    
    return her_values


# === 4. ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤šè§£åƒåº¦è§£æ ===

@njit(fastmath=True, parallel=True, cache=True)
def financial_adaptive_wavelet_transform(prices: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ› V4.0 - é©æ–°çš„é‡‘èæ™‚ç³»åˆ—ç‰¹åŒ–ç‰ˆ
    
    ã€é©æ–°çš„æŠ€è¡“çµ±åˆã€‘
    1. Daubechies-4ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆï¼ˆé‡‘èæ™‚ç³»åˆ—æœ€é©ï¼‰
    2. é©å¿œçš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¸‚å ´ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å¯¾å¿œï¼‰
    3. ãƒ•ã‚¡ã‚¸ã‚£è«–ç†ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®šï¼ˆæ›–æ˜§æ€§è€ƒæ…®ï¼‰
    4. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æï¼ˆå¸‚å ´åŠ¹ç‡æ€§æ¸¬å®šï¼‰
    5. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ä¿¡å·å“è³ªè©•ä¾¡
    
    å¾“æ¥ã®ãƒãƒ¼ãƒ«ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆã®å•é¡Œç‚¹ã‚’å®Œå…¨è§£æ±ºã—ã€
    é‡‘èå¸‚å ´ã®è¤‡é›‘ãªéç·šå½¢æ€§ãƒ»éå®šå¸¸æ€§ã«å¯¾å¿œã™ã‚‹æœ€å¼·ã‚¯ãƒ©ã‚¹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    """
    n = len(prices)
    trend_component = np.full(n, np.nan)
    cycle_component = np.full(n, np.nan)
    market_regime = np.full(n, np.nan)
    
    # Daubechies-4ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆä¿‚æ•°ï¼ˆé‡‘èæ™‚ç³»åˆ—ã«æœ€é©åŒ–ï¼‰
    db4_h = np.array([
        0.6830127, 1.1830127, 0.3169873, -0.1830127,
        -0.0544158, 0.0094624, 0.0102581, -0.0017468
    ])
    db4_g = np.array([
        -0.0017468, -0.0102581, 0.0094624, 0.0544158,
        -0.1830127, -0.3169873, 1.1830127, -0.6830127
    ])
    
    for i in prange(50, n):  # ååˆ†ãªå±¥æ­´ãŒå¿…è¦ï¼ˆã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆç‰¹æ€§ä¸Šï¼‰
        window_size = min(64, i)  # é©å¿œçš„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
        segment = prices[i-window_size:i]
        
        if len(segment) < 16:
            continue
            
        # === 1. å¯¾æ•°ãƒªã‚¿ãƒ¼ãƒ³æ­£è¦åŒ–ï¼ˆé‡‘èãƒ‡ãƒ¼ã‚¿æ¨™æº–å‰å‡¦ç†ï¼‰ ===
        log_returns = np.zeros(len(segment)-1)
        for j in range(len(segment)-1):
            if segment[j] > 0 and segment[j+1] > 0:
                log_returns[j] = math.log(segment[j+1] / segment[j])
            else:
                log_returns[j] = 0.0
        
        # ãƒ­ãƒã‚¹ãƒˆæ¨™æº–åŒ–ï¼ˆå¤–ã‚Œå€¤ã«å¯¾ã™ã‚‹è€æ€§ï¼‰
        median_return = np.median(log_returns)
        mad = np.median(np.abs(log_returns - median_return))  # Median Absolute Deviation
        if mad > 1e-10:
            normalized_returns = (log_returns - median_return) / (1.4826 * mad)  # MAD-based standardization
        else:
            normalized_returns = log_returns
        
        # === 2. é©å¿œçš„Daubechies-4ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåˆ†è§£ ===
        n_coeffs = len(normalized_returns)
        
        # ãƒ¬ãƒ™ãƒ«1åˆ†è§£
        if n_coeffs >= 8:
            level1_approx = np.zeros(n_coeffs // 2)
            level1_detail = np.zeros(n_coeffs // 2)
            
            for j in range(n_coeffs // 2):
                # è¿‘ä¼¼ä¿‚æ•°ï¼ˆä½å‘¨æ³¢æˆåˆ†ï¼‰
                approx_sum = 0.0
                detail_sum = 0.0
                for k in range(min(8, n_coeffs - j*2)):
                    if j*2 + k < n_coeffs:
                        approx_sum += normalized_returns[j*2 + k] * db4_h[k]
                        detail_sum += normalized_returns[j*2 + k] * db4_g[k]
                
                level1_approx[j] = approx_sum
                level1_detail[j] = detail_sum
        else:
            level1_approx = normalized_returns[:4].copy()
            level1_detail = normalized_returns[4:8].copy() if len(normalized_returns) >= 8 else np.zeros(4)
        
        # ãƒ¬ãƒ™ãƒ«2åˆ†è§£ï¼ˆè¿‘ä¼¼ä¿‚æ•°ã‚’ã•ã‚‰ã«åˆ†è§£ï¼‰
        if len(level1_approx) >= 4:
            level2_approx = np.zeros(len(level1_approx) // 2)
            level2_detail = np.zeros(len(level1_approx) // 2)
            
            for j in range(len(level1_approx) // 2):
                approx_sum = 0.0
                detail_sum = 0.0
                for k in range(min(4, len(level1_approx) - j*2)):
                    if j*2 + k < len(level1_approx):
                        approx_sum += level1_approx[j*2 + k] * db4_h[k]
                        detail_sum += level1_approx[j*2 + k] * db4_g[k]
                
                level2_approx[j] = approx_sum
                level2_detail[j] = detail_sum
        else:
            level2_approx = level1_approx[:2].copy() if len(level1_approx) >= 2 else np.array([0.0, 0.0])
            level2_detail = level1_approx[2:4].copy() if len(level1_approx) >= 4 else np.array([0.0, 0.0])
        
        # === 3. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒè§£æï¼ˆå¸‚å ´åŠ¹ç‡æ€§æ¸¬å®šï¼‰ ===
        def calculate_fractal_dimension(data):
            if len(data) < 4:
                return 1.5
            
            # ãƒœãƒƒã‚¯ã‚¹ã‚«ã‚¦ãƒ³ãƒ†ã‚£ãƒ³ã‚°æ³•ã®ç°¡æ˜“ç‰ˆ
            data_range = np.max(data) - np.min(data)
            if data_range == 0:
                return 1.5
            
            # ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å¤‰å‹•æ¸¬å®š
            scales = [2, 4, 8]
            variations = []
            
            for scale in scales:
                if len(data) >= scale:
                    variation = 0.0
                    for j in range(0, len(data) - scale, scale):
                        segment_var = np.var(data[j:j+scale])
                        variation += math.sqrt(segment_var)
                    
                    if len(data) // scale > 0:
                        variation /= (len(data) // scale)
                    variations.append(variation)
            
            if len(variations) >= 2 and variations[0] > 0:
                # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®è¿‘ä¼¼è¨ˆç®—
                ratio = variations[-1] / variations[0] if variations[0] > 0 else 1.0
                fractal_dim = 1.0 + math.log(ratio) / math.log(scales[-1] / scales[0])
                return max(min(fractal_dim, 2.0), 1.0)
            else:
                return 1.5
        
        fractal_dim = calculate_fractal_dimension(normalized_returns)
        market_efficiency = 2.0 - fractal_dim  # 1.0=å®Œå…¨åŠ¹ç‡, 0.0=å®Œå…¨éåŠ¹ç‡
        
        # === 4. ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹ä¿¡å·å“è³ªè©•ä¾¡ ===
        def shannon_entropy(data):
            if len(data) == 0:
                return 0.0
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’10ã®ãƒ“ãƒ³ã«åˆ†å‰²
            data_min, data_max = np.min(data), np.max(data)
            if data_max == data_min:
                return 0.0
            
            bin_counts = np.zeros(10)
            bin_width = (data_max - data_min) / 10
            
            for value in data:
                bin_idx = min(int((value - data_min) / bin_width), 9)
                bin_counts[bin_idx] += 1
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨ˆç®—
            total_count = len(data)
            entropy = 0.0
            for count in bin_counts:
                if count > 0:
                    p = count / total_count
                    entropy -= p * math.log(p)
            
            return entropy / math.log(10)  # æ­£è¦åŒ–
        
        trend_entropy = shannon_entropy(level2_approx)
        cycle_entropy = shannon_entropy(level1_detail)
        noise_entropy = shannon_entropy(level2_detail)
        
        # === 5. ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹æˆåˆ†åˆ†æ ===
        trend_energy = np.sum(level2_approx ** 2)
        cycle_energy = np.sum(level1_detail ** 2) + np.sum(level2_detail ** 2) * 0.5
        noise_energy = np.sum(level2_detail ** 2) * 0.5
        
        total_energy = trend_energy + cycle_energy + noise_energy
        
        if total_energy > 1e-12:
            # ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¯”ç‡è¨ˆç®—
            raw_trend_ratio = trend_energy / total_energy
            raw_cycle_ratio = cycle_energy / total_energy
            raw_noise_ratio = noise_energy / total_energy
            
            # === 6. ãƒ•ã‚¡ã‚¸ã‚£è«–ç†ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š ===
            # å¸‚å ´åŠ¹ç‡æ€§ã«ã‚ˆã‚‹é‡ã¿èª¿æ•´
            efficiency_weight = market_efficiency * 0.8 + 0.2  # 0.2-1.0ã®ç¯„å›²
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
            signal_quality = 1.0 - min(noise_entropy, 1.0)
            
            # é©å¿œçš„é‡ã¿è¨ˆç®—
            trend_weight = efficiency_weight * signal_quality
            cycle_weight = (1.0 - efficiency_weight) * signal_quality
            noise_weight = 1.0 - signal_quality
            
            # é‡ã¿ä»˜ãã‚¨ãƒãƒ«ã‚®ãƒ¼æ¯”ç‡
            weighted_trend = raw_trend_ratio * trend_weight
            weighted_cycle = raw_cycle_ratio * cycle_weight
            weighted_noise = raw_noise_ratio * noise_weight
            
            total_weighted = weighted_trend + weighted_cycle + weighted_noise
            
            if total_weighted > 1e-10:
                trend_component[i] = weighted_trend / total_weighted
                cycle_component[i] = weighted_cycle / total_weighted
            else:
                trend_component[i] = 0.33
                cycle_component[i] = 0.33
            
            # === 7. é©æ–°çš„ãƒ•ã‚¡ã‚¸ã‚£ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¤å®š ===
            trend_dominance = trend_component[i]
            cycle_dominance = cycle_component[i]
            
            # ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—é–¢æ•°ã«ã‚ˆã‚‹åˆ¤å®š
            # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰åº¦
            strong_trend_membership = 0.0
            if trend_dominance > 0.6:
                strong_trend_membership = min((trend_dominance - 0.6) / 0.25, 1.0)
            
            # ä¸­ç¨‹åº¦ãƒˆãƒ¬ãƒ³ãƒ‰åº¦
            moderate_trend_membership = 0.0
            if 0.4 <= trend_dominance <= 0.7:
                if trend_dominance <= 0.55:
                    moderate_trend_membership = (trend_dominance - 0.4) / 0.15
                else:
                    moderate_trend_membership = (0.7 - trend_dominance) / 0.15
            
            # ã‚µã‚¤ã‚¯ãƒ«åº¦
            cycle_membership = 0.0
            if cycle_dominance > 0.4:
                cycle_membership = min((cycle_dominance - 0.4) / 0.3, 1.0)
            
            # ãƒ¬ãƒ³ã‚¸åº¦
            range_membership = 1.0 - max(strong_trend_membership, moderate_trend_membership, cycle_membership)
            
            # æœ€å¤§ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—ã«ã‚ˆã‚‹åˆ¤å®šï¼ˆç¾å®Ÿçš„ã—ãã„å€¤ï¼‰
            max_membership = max(strong_trend_membership, moderate_trend_membership, cycle_membership, range_membership)
            
            if max_membership == strong_trend_membership and strong_trend_membership > 0.6:
                market_regime[i] = 0.8  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰
            elif max_membership == moderate_trend_membership and moderate_trend_membership > 0.5:
                market_regime[i] = 0.4  # ä¸­ç¨‹åº¦ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            elif max_membership == cycle_membership and cycle_membership > 0.5:
                market_regime[i] = -0.6  # ã‚µã‚¤ã‚¯ãƒ«ç›¸å ´
            else:
                market_regime[i] = 0.0  # ãƒ¬ãƒ³ã‚¸ç›¸å ´
                
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ä¸è¶³ï¼‰
            trend_component[i] = 0.33
            cycle_component[i] = 0.33
            market_regime[i] = 0.0
        
        # æ¥µç«¯å€¤åˆ¶é™
        trend_component[i] = max(min(trend_component[i], 0.8), 0.1)
        cycle_component[i] = max(min(cycle_component[i], 0.8), 0.1)
    
    return trend_component, cycle_component, market_regime


# === 5. ç©¶æ¥µã‚·ãƒ³ãƒ—ãƒ«æ´—ç·´å‹•çš„ä¹—æ•°ã‚·ã‚¹ãƒ†ãƒ  V2.0 ===

# @njit(fastmath=True, cache=True)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
def elite_dynamic_multiplier_system(
    ultimate_vol: np.ndarray,
    trend_strength: np.ndarray,
    her_values: np.ndarray,
    quantum_entanglement: np.ndarray,
    min_multiplier: float = 0.8,
    max_multiplier: float = 6.0
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ã‚¨ãƒªãƒ¼ãƒˆå‹•çš„ä¹—æ•°ã‚·ã‚¹ãƒ†ãƒ  V2.0 - æ´—ç·´ã•ã‚ŒãŸæœ€å¼·ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
    
    è¤‡é›‘ã•ã‚’æ’é™¤ã—ã€æœ€ã‚‚åŠ¹æœçš„ãª3ã¤ã®æ ¸å¿ƒè¦ç´ ã®ã¿ã‚’çµ±åˆã—ãŸ
    ç©¶æ¥µã«ã‚·ãƒ³ãƒ—ãƒ«ã§æœ€å¼·ã®å‹•çš„ä¹—æ•°è¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ 
    """
    n = len(ultimate_vol)
    dynamic_multiplier = np.full(n, np.nan)
    confidence_score = np.full(n, np.nan)
    
    for i in range(10, n):
        # NaNå€¤ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼ˆcontinueã—ãªã„ï¼‰
        if (np.isnan(trend_strength[i]) or np.isnan(her_values[i]) or 
            np.isnan(quantum_entanglement[i]) or np.isnan(ultimate_vol[i])):
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è¨ˆç®—ç¶šè¡Œ
            ts = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
            he = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåŠ¹ç‡ç‡
            qe = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡å­ã‚‚ã¤ã‚Œ
        else:
            ts = trend_strength[i]
            he = her_values[i]
            qe = quantum_entanglement[i]
        
        # === æ ¸å¿ƒè¦ç´ 1: ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆæœ€é‡è¦ 50%ï¼‰ ===
        # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ = ç‹­ã„ãƒãƒ£ãƒãƒ«ã€å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰ = åºƒã„ãƒãƒ£ãƒãƒ«
        trend_factor = 1.0 - ts
        trend_factor = math.pow(trend_factor, 1.2)  # éç·šå½¢å¼·åŒ–
        
        # === æ ¸å¿ƒè¦ç´ 2: åŠ¹ç‡ç‡ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆé‡è¦ 30%ï¼‰ ===
        # é«˜åŠ¹ç‡ = ç‹­ã„ãƒãƒ£ãƒãƒ«ã€ä½åŠ¹ç‡ = åºƒã„ãƒãƒ£ãƒãƒ«
        efficiency_factor = 1.0 - he * 0.8
        efficiency_factor = max(min(efficiency_factor, 1.0), 0.2)
        
        # === æ ¸å¿ƒè¦ç´ 3: é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆãƒãƒ©ãƒ³ã‚¹ 20%ï¼‰ ===
        # é«˜ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ = äºˆæ¸¬å¯èƒ½ = ç‹­ã„ãƒãƒ£ãƒãƒ«
        # ä½ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ = ãƒã‚¤ã‚ºå¤š = åºƒã„ãƒãƒ£ãƒãƒ«
        coherence_factor = 1.0 - qe * 0.6
        coherence_factor = max(min(coherence_factor, 1.0), 0.3)
        
        # === çµ±åˆè¨ˆç®—ï¼ˆé‡ã¿ä»˜ãèª¿å’Œå¹³å‡ï¼‰ ===
        # èª¿å’Œå¹³å‡ã§æ¥µç«¯å€¤ã‚’æŠ‘åˆ¶
        weights = np.array([0.5, 0.3, 0.2])
        factors = np.array([trend_factor, efficiency_factor, coherence_factor])
        
        # é‡ã¿ä»˜ãèª¿å’Œå¹³å‡ï¼ˆã‚ˆã‚Šå®‰å®šï¼‰
        harmonic_sum = 0.0
        for j in range(len(weights)):
            w = weights[j]
            f = factors[j]
            if f > 1e-10:
                harmonic_sum += w / f
        
        if abs(harmonic_sum) > 1e-10:
            harmonic_mean = 1.0 / harmonic_sum
        else:
            harmonic_mean = 0.5
        
        # === ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é©å¿œèª¿æ•´ ===
        # ç¾åœ¨ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãå¾®èª¿æ•´
        if i >= 10:
            recent_vol_avg = 0.0
            vol_count = 0
            for j in range(max(0, i-10), i):
                if not np.isnan(ultimate_vol[j]):
                    recent_vol_avg += ultimate_vol[j]
                    vol_count += 1
            
            if vol_count > 0:
                recent_vol_avg /= vol_count
                # æ‰‹å‹•ã§nanmeanè¨ˆç®—
                if i >= 50:
                    vol_segment = ultimate_vol[max(0, i-50):i]
                    valid_count = 0
                    vol_sum = 0.0
                    for v in vol_segment:
                        if not np.isnan(v):
                            vol_sum += v
                            valid_count += 1
                    long_term_vol = vol_sum / valid_count if valid_count > 0 else recent_vol_avg
                else:
                    long_term_vol = recent_vol_avg
                
                if abs(long_term_vol) > 1e-10:
                    vol_ratio = recent_vol_avg / long_term_vol
                    # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ = ã‚„ã‚„åºƒã‚ã€ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ = ã‚„ã‚„ç‹­ã‚
                    vol_adjustment = 0.9 + 0.2 * min(vol_ratio, 2.0)
                    harmonic_mean *= vol_adjustment
        
        # === æœ€çµ‚ä¹—æ•°æ±ºå®š ===
        final_multiplier = min_multiplier + (max_multiplier - min_multiplier) * harmonic_mean
        final_multiplier = max(min(final_multiplier, max_multiplier), min_multiplier)
        
        # === é©å¿œã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚° ===
        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ã«åŸºã¥ãã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å¼·åº¦
        if i > 0 and not np.isnan(dynamic_multiplier[i-1]):
            smoothing_strength = 0.15 + 0.15 * (1.0 - ts)  # å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰ = å¼·ã„ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
            final_multiplier = (1.0 - smoothing_strength) * final_multiplier + smoothing_strength * dynamic_multiplier[i-1]
        
        dynamic_multiplier[i] = final_multiplier
        
        # === ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰ ===
        # 3ã¤ã®æ ¸å¿ƒè¦ç´ ã®ä¸€è²«æ€§ï¼ˆæ‰‹å‹•è¨ˆç®—ï¼‰
        # factor_mean
        factor_sum = 0.0
        for j in range(len(factors)):
            factor_sum += factors[j]
        factor_mean = factor_sum / len(factors)
        
        # factor_std
        variance_sum = 0.0
        for j in range(len(factors)):
            diff = factors[j] - factor_mean
            variance_sum += diff * diff
        factor_variance = variance_sum / len(factors)
        factor_std = math.sqrt(factor_variance)
        
        if abs(factor_mean) > 1e-10:
            consistency = 1.0 - factor_std / factor_mean
        else:
            consistency = 0.5
        
        confidence_score[i] = max(min(consistency * ts, 1.0), 0.0)
    
    return dynamic_multiplier, confidence_score


# === 6. ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ ===

@njit(fastmath=True, parallel=True, cache=True)
def generate_breakout_signals(
    prices: np.ndarray,
    upper_channel: np.ndarray,
    lower_channel: np.ndarray,
    trend_strength: np.ndarray,
    quantum_coherence: np.ndarray,
    her_values: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    è¶…é«˜ç²¾åº¦ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ - å½ã‚·ã‚°ãƒŠãƒ«å®Œå…¨é˜²å¾¡ã‚·ã‚¹ãƒ†ãƒ 
    
    å¤šå±¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨ä¿¡é ¼åº¦è©•ä¾¡ã«ã‚ˆã‚Šã€
    çœŸã®ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã®ã¿ã‚’æ¤œå‡ºã™ã‚‹é©æ–°çš„ã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆå™¨
    """
    n = len(prices)
    breakout_signals = np.zeros(n)
    breakout_confidence = np.zeros(n)
    signal_quality = np.zeros(n)
    
    for i in prange(1, n):
        signal = 0
        confidence = 0.0
        quality = 0.0
        
        # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ¤œå‡º
        if (not np.isnan(upper_channel[i-1]) and not np.isnan(lower_channel[i-1]) and
            not np.isnan(prices[i]) and not np.isnan(prices[i-1])):
            
            # ä¸Šæ–¹ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ
            if prices[i] > upper_channel[i-1] and prices[i-1] <= upper_channel[i-1]:
                signal = 1
                penetration_strength = (prices[i] - upper_channel[i-1]) / upper_channel[i-1]
                confidence = min(penetration_strength * 10, 1.0)
            
            # ä¸‹æ–¹ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ
            elif prices[i] < lower_channel[i-1] and prices[i-1] >= lower_channel[i-1]:
                signal = -1
                penetration_strength = (lower_channel[i-1] - prices[i]) / lower_channel[i-1]
                confidence = min(penetration_strength * 10, 1.0)
            
            # ã‚·ã‚°ãƒŠãƒ«å“è³ªè©•ä¾¡ï¼ˆå¤šå±¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
            if signal != 0:
                # åŸºæœ¬å“è³ªï¼ˆä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
                base_quality = confidence
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                trend_quality = trend_strength[i] if not np.isnan(trend_strength[i]) else 0.5
                
                # é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                coherence_quality = quantum_coherence[i] if not np.isnan(quantum_coherence[i]) else 0.5
                
                # åŠ¹ç‡ç‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                efficiency_quality = her_values[i] if not np.isnan(her_values[i]) else 0.5
                
                # çµ±åˆå“è³ªã‚¹ã‚³ã‚¢
                quality = (
                    base_quality * 0.3 +
                    trend_quality * 0.25 +
                    coherence_quality * 0.25 +
                    efficiency_quality * 0.2
                )
                
                # æœ€å°å“è³ªã—ãã„å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                if quality < 0.3:  # å®Ÿè·µçš„ã—ãã„å€¤ï¼ˆç·©å’Œç‰ˆï¼‰
                    signal = 0
                    confidence = 0.0
                    quality = 0.0
        
        breakout_signals[i] = signal
        breakout_confidence[i] = confidence
        signal_quality[i] = quality
    
    return breakout_signals, breakout_confidence, signal_quality


class UltimateBreakoutChannel(Indicator):
    """
    ğŸš€ **Ultimate Breakout Channel V2.0 - äººé¡å²ä¸Šæœ€å¼·ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒãƒ£ãƒãƒ«** ğŸš€
    
    é©æ–°çš„çµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚·ãƒ³ãƒ—ãƒ«&æœ€å¼·ï¼‰ï¼š
    1. é‡å­å¼·åŒ–ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ› V2.0 - å¤šé‡å…±é³´ãƒ»é‡å­ã‚‚ã¤ã‚ŒåŠ¹æœ
    2. é©æ–°çš„é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ - å‹•çš„ãƒã‚¤ã‚ºé™¤å»
    3. ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡ï¼ˆHERï¼‰ - è¶…çµ¶é€²åŒ–ãƒˆãƒ¬ãƒ³ãƒ‰æ¸¬å®š
    4. é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ› V4.0 - Daubechies-4 + ãƒ•ã‚¡ã‚¸ã‚£è«–ç†
    5. é¸æŠå¯èƒ½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚·ã‚¹ãƒ†ãƒ :
       - Ultimate Volatility: å¾“æ¥ATRã‚’é¥ã‹ã«è¶…ãˆã‚‹6å±¤çµ±åˆç²¾åº¦
       - Traditional ATR: é«˜é€Ÿãƒ»è»½é‡ãªå¾“æ¥æ‰‹æ³•
    6. ã‚¨ãƒªãƒ¼ãƒˆå‹•çš„ä¹—æ•°ã‚·ã‚¹ãƒ†ãƒ  - æ´—ç·´ã•ã‚ŒãŸã‚·ãƒ³ãƒ—ãƒ«åˆ¶å¾¡
    
    ğŸ¯ **ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ—é¸æŠ:**
    - volatility_type='ultimate': é‡å­èª¿å’ŒæŒ¯å‹•å­ãƒ»ç¢ºç‡çš„ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç­‰ã®é©æ–°çš„çµ±åˆ
    - volatility_type='atr': å¾“æ¥ATRï¼ˆé«˜é€Ÿãƒ»äº’æ›æ€§é‡è¦–ï¼‰
    
    è¶…ä½é…å»¶ãƒ»è¶…è¿½å¾“æ€§ãƒ»å½ã‚·ã‚°ãƒŠãƒ«å®Œå…¨é˜²å¾¡ã®ç©¶æ¥µé€²åŒ–ç‰ˆ
    """
    
    def __init__(
        self,
        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        atr_period: int = 14,
        base_multiplier: float = 2.0,
        
        # ğŸš€ é©æ–°çš„å‹•çš„ä¹—æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
        min_multiplier: float = 1.0,
        max_multiplier: float = 8.0,
        
        # å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœŸé–“
        hilbert_window: int = 8,
        her_window: int = 14,
        wavelet_window: int = 16,
        
        # ä¾¡æ ¼ã‚½ãƒ¼ã‚¹
        src_type: str = 'hlc3',
        
        # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        min_signal_quality: float = 0.3,
        
        # ğŸ¯ ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ—é¸æŠ
        volatility_type: str = 'ultimate'  # 'atr' ã¾ãŸã¯ 'ultimate'
    ):
        """
        Ultimate Breakout Channel ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ - äººé¡å²ä¸Šæœ€å¼·ç‰ˆ
        
        Args:
            atr_period: ATRè¨ˆç®—æœŸé–“ï¼ˆvolatility_type='atr'ã®å ´åˆã«ä½¿ç”¨ï¼‰
            base_multiplier: åŸºæœ¬ãƒãƒ£ãƒãƒ«å¹…å€ç‡ï¼ˆå»ƒæ­¢äºˆå®šï¼‰
            min_multiplier: æœ€å°å‹•çš„ä¹—æ•°ï¼ˆå¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰æ™‚ï¼‰
            max_multiplier: æœ€å¤§å‹•çš„ä¹—æ•°ï¼ˆå¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰æ™‚ï¼‰
            hilbert_window: ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ›ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            her_window: ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            wavelet_window: ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆè§£æã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
            src_type: ä¾¡æ ¼ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
            min_signal_quality: æœ€å°ã‚·ã‚°ãƒŠãƒ«å“è³ªã—ãã„å€¤
            volatility_type: ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ— ('atr'=å¾“æ¥ATR, 'ultimate'=Ultimate Volatility)
        """
        # ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼åã‚’ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦è¨­å®š
        vol_suffix = "ATR" if volatility_type == 'atr' else "UV"
        super().__init__(f"UltimateBreakoutChannelV2({vol_suffix}={min_multiplier}-{max_multiplier})")
        
        self.atr_period = atr_period
        self.base_multiplier = base_multiplier  # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®ãŸã‚ä¿æŒ
        self.min_multiplier = min_multiplier
        self.max_multiplier = max_multiplier
        self.hilbert_window = hilbert_window
        self.her_window = her_window
        self.wavelet_window = wavelet_window
        self.src_type = src_type
        self.min_signal_quality = min_signal_quality
        self.volatility_type = volatility_type.lower()
        
        # ä¾å­˜ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.price_source = PriceSource()
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã‚’é¸æŠçš„ã«åˆæœŸåŒ–
        if self.volatility_type == 'ultimate':
            self.ultimate_volatility = UltimateVolatility(
                period=atr_period,
                trend_window=10,
                hilbert_window=12,
                kalman_process_noise=0.001,
                src_type=src_type
            )
            self.atr_indicator = None
        else:  # 'atr'
            self.atr_indicator = ATR(period=atr_period)
            self.ultimate_volatility = None
        
        # çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._result_cache = {}
        self._cache_keys = []
        self._max_cache_size = 3
    
    def calculate(self, data: Union[pd.DataFrame, np.ndarray]) -> UltimateBreakoutChannelResult:
        """
        Ultimate Breakout Channelè¨ˆç®—ãƒ¡ã‚¤ãƒ³é–¢æ•°
        """
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            data_hash = self._get_data_hash(data)
            if data_hash in self._result_cache:
                return self._result_cache[data_hash]
            
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            if isinstance(data, pd.DataFrame):
                price_data = self.price_source.get_source(data, self.src_type)
                prices = price_data.values if hasattr(price_data, 'values') else price_data
                
                # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£è¨ˆç®—ï¼ˆé¸æŠã•ã‚ŒãŸã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ï¼‰
                if self.volatility_type == 'ultimate':
                    volatility_result = self.ultimate_volatility.calculate(data)
                    volatility_values = volatility_result.ultimate_volatility
                else:  # 'atr'
                    try:
                        atr_result = self.atr_indicator.calculate(data)
                        volatility_values = atr_result.values if hasattr(atr_result, 'values') else atr_result
                    except Exception as e:
                        self.logger.warning(f"ATRã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}. ç°¡æ˜“ATRè¨ˆç®—ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚")
                        high, low, close = data['high'].values, data['low'].values, data['close'].values
                        volatility_values = self._calculate_simple_atr(high, low, close)
            else:
                prices = data[:, 3] if data.ndim > 1 else data
                
                if self.volatility_type == 'ultimate':
                    # NumPyé…åˆ—ã‹ã‚‰PandasDataFrameã‚’ä½œæˆã—ã¦Ultimate Volatilityã‚’ä½¿ç”¨
                    if data.ndim > 1:
                        temp_df = pd.DataFrame({
                            'open': data[:, 0], 'high': data[:, 1], 
                            'low': data[:, 2], 'close': data[:, 3]
                        })
                        volatility_result = self.ultimate_volatility.calculate(temp_df)
                        volatility_values = volatility_result.ultimate_volatility
                    else:
                        temp_df = pd.DataFrame({'close': prices})
                        volatility_result = self.ultimate_volatility.calculate(temp_df)
                        volatility_values = volatility_result.ultimate_volatility
                else:  # 'atr'
                    # ç°¡æ˜“ATRè¨ˆç®—
                    if data.ndim > 1:
                        high, low, close = data[:, 1], data[:, 2], data[:, 3]
                        volatility_values = self._calculate_simple_atr(high, low, close)
                    else:
                        volatility_values = np.full(len(prices), np.std(prices) * 0.02)
            
            n = len(prices)
            
            self.logger.info("ğŸš€ Ultimate Breakout Channel V2.0è¨ˆç®—é–‹å§‹...")
            
            # === æ®µéš1: é‡å­å¼·åŒ–ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆå¤‰æ› V2.0 ===
            hilbert_amplitude, hilbert_phase, trend_strength, quantum_entanglement = quantum_enhanced_hilbert_transform(prices)
            
            # === æ®µéš2: é©æ–°çš„é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ V2.0 ===
            centerline, quantum_coherence = quantum_adaptive_kalman_filter(
                prices, hilbert_amplitude, hilbert_phase
            )
            
            # === æ®µéš3: ãƒã‚¤ãƒ‘ãƒ¼åŠ¹ç‡ç‡ V2.0 ===
            hyper_efficiency = hyper_efficiency_ratio(prices, self.her_window)
            
            # === æ®µéš4: é‡‘èé©å¿œã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆå¤‰æ› V4.0 ===
            wavelet_trend, wavelet_cycle, market_regime = financial_adaptive_wavelet_transform(prices)
            
            # === æ®µéš5: ã‚¨ãƒªãƒ¼ãƒˆå‹•çš„ä¹—æ•°ã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚·ãƒ³ãƒ—ãƒ«æ´—ç·´ç‰ˆï¼‰ ===
            dynamic_multiplier, confidence_score = elite_dynamic_multiplier_system(
                volatility_values, trend_strength, hyper_efficiency, 
                quantum_entanglement, self.min_multiplier, self.max_multiplier
            )
            
            # === æ®µéš6: å‹•çš„ãƒãƒ£ãƒãƒ«å¹…è¨ˆç®— ===
            dynamic_width = np.full(n, np.nan)
            for i in range(n):
                if not np.isnan(volatility_values[i]) and not np.isnan(dynamic_multiplier[i]):
                    dynamic_width[i] = volatility_values[i] * dynamic_multiplier[i]
                elif not np.isnan(volatility_values[i]):
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    fallback_mult = (self.min_multiplier + self.max_multiplier) / 2.0
                    dynamic_width[i] = volatility_values[i] * fallback_mult
                else:
                    dynamic_width[i] = np.nan
            
            # === æ®µéš7: ãƒãƒ£ãƒãƒ«æ§‹ç¯‰ ===
            upper_channel = centerline + dynamic_width
            lower_channel = centerline - dynamic_width
            
            # === æ®µéš8: è¶…é«˜ç²¾åº¦ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚·ã‚°ãƒŠãƒ«ç”Ÿæˆ ===
            breakout_signals, breakout_confidence, signal_quality = generate_breakout_signals(
                prices, upper_channel, lower_channel, trend_strength, 
                quantum_coherence, hyper_efficiency
            )
            
            # === æ®µéš9: ç¾åœ¨çŠ¶æ…‹åˆ¤å®š ===
            current_trend = self._determine_current_trend(trend_strength, market_regime)
            current_confidence = float(np.nanmean(breakout_confidence[-5:])) if len(breakout_confidence) >= 5 else 0.0
            current_regime = self._determine_current_regime(market_regime)
            
            # çµæœæ§‹ç¯‰
            result = UltimateBreakoutChannelResult(
                upper_channel=upper_channel,
                lower_channel=lower_channel,
                centerline=centerline,
                dynamic_width=dynamic_width,
                dynamic_multiplier=dynamic_multiplier,
                confidence_score=confidence_score,
                breakout_signals=breakout_signals,
                breakout_confidence=breakout_confidence,
                signal_quality=signal_quality,
                trend_strength=trend_strength,
                hilbert_amplitude=hilbert_amplitude,
                hilbert_phase=hilbert_phase,
                quantum_coherence=quantum_coherence,
                hyper_efficiency=hyper_efficiency,
                wavelet_trend=wavelet_trend,
                wavelet_cycle=wavelet_cycle,
                market_regime=market_regime,
                current_trend=current_trend,
                current_confidence=current_confidence,
                current_regime=current_regime
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
            if len(self._result_cache) >= self._max_cache_size and self._cache_keys:
                oldest_key = self._cache_keys.pop(0)
                if oldest_key in self._result_cache:
                    del self._result_cache[oldest_key]
            
            self._result_cache[data_hash] = result
            self._cache_keys.append(data_hash)
            
            # çµ±è¨ˆæƒ…å ±ãƒ­ã‚°
            total_signals = int(np.sum(np.abs(breakout_signals)))
            avg_quality = float(np.nanmean(signal_quality[signal_quality > 0])) if np.any(signal_quality > 0) else 0.0
            avg_volatility = float(np.nanmean(volatility_values[~np.isnan(volatility_values)])) if np.any(~np.isnan(volatility_values)) else 0.0
            avg_multiplier = float(np.nanmean(dynamic_multiplier[~np.isnan(dynamic_multiplier)])) if np.any(~np.isnan(dynamic_multiplier)) else 0.0
            
            vol_type_name = "Ultimate Volatility" if self.volatility_type == 'ultimate' else "ATR"
            self.logger.info(f"âœ… Ultimate Breakout Channel V2.0è¨ˆç®—å®Œäº† ({vol_type_name})")
            self.logger.info(f"ã‚·ã‚°ãƒŠãƒ«æ•°: {total_signals}, å¹³å‡å“è³ª: {avg_quality:.3f}")
            self.logger.info(f"å¹³å‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {avg_volatility:.6f}, å¹³å‡ä¹—æ•°: {avg_multiplier:.2f}")
            self.logger.info(f"ç¾åœ¨ãƒˆãƒ¬ãƒ³ãƒ‰: {current_trend}, ç¾åœ¨ãƒ¬ã‚¸ãƒ¼ãƒ : {current_regime}")
            
            return result
            
        except Exception as e:
            import traceback
            self.logger.error(f"Ultimate Breakout Channelè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.logger.error(traceback.format_exc())
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®çµæœã‚’è¿”ã™
            n = len(data) if hasattr(data, '__len__') else 0
            empty_array = np.full(n, np.nan)
            return UltimateBreakoutChannelResult(
                upper_channel=empty_array, lower_channel=empty_array, centerline=empty_array,
                dynamic_width=empty_array, dynamic_multiplier=empty_array, confidence_score=empty_array,
                breakout_signals=np.zeros(n), breakout_confidence=np.zeros(n),
                signal_quality=np.zeros(n), trend_strength=empty_array, hilbert_amplitude=empty_array,
                hilbert_phase=empty_array, quantum_coherence=empty_array, hyper_efficiency=empty_array,
                wavelet_trend=empty_array, wavelet_cycle=empty_array, market_regime=empty_array,
                current_trend="neutral", current_confidence=0.0, current_regime="range"
            )
    

    def _calculate_simple_atr(self, high: np.ndarray, low: np.ndarray, close: np.ndarray) -> np.ndarray:
        """ç°¡æ˜“ATRè¨ˆç®—ï¼ˆvolatility_type='atr'ã§NumPyé…åˆ—ã®å ´åˆã«ä½¿ç”¨ï¼‰"""
        n = len(high)
        atr_values = np.zeros(n)
        tr_values = np.zeros(n)
        
        # æœ€åˆã®å€¤
        atr_values[0] = high[0] - low[0]
        tr_values[0] = atr_values[0]
        
        # True Rangeè¨ˆç®—ã¨ATRè¨ˆç®—
        for i in range(1, n):
            tr1 = high[i] - low[i]
            tr2 = abs(high[i] - close[i-1])
            tr3 = abs(low[i] - close[i-1])
            true_range = max(tr1, tr2, tr3)
            tr_values[i] = true_range
            
            if i < self.atr_period:
                # æœŸé–“ä¸è¶³ã®å ´åˆã¯å˜ç´”å¹³å‡
                atr_values[i] = np.mean(tr_values[:i+1])
            else:
                # Wilder's smoothing
                atr_values[i] = (atr_values[i-1] * (self.atr_period - 1) + true_range) / self.atr_period
        
        # æœ€å°å€¤ä¿è­·ï¼ˆä¾¡æ ¼ã®0.01%ï¼‰
        min_atr = np.mean(close) * 0.0001
        return np.maximum(atr_values, min_atr)
    
    def _determine_current_trend(self, trend_strength: np.ndarray, market_regime: np.ndarray) -> str:
        """ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰çŠ¶æ…‹ã‚’åˆ¤å®š"""
        if len(trend_strength) == 0:
            return "neutral"
        
        latest_strength = trend_strength[-1] if not np.isnan(trend_strength[-1]) else 0.0
        latest_regime = market_regime[-1] if not np.isnan(market_regime[-1]) else 0.0
        
        if latest_strength > 0.7 and latest_regime > 0.5:
            return "strong_trend"
        elif latest_strength > 0.4:
            return "moderate_trend"
        else:
            return "weak_trend"
    
    def _determine_current_regime(self, market_regime: np.ndarray) -> str:
        """ç¾åœ¨ã®å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ ã‚’åˆ¤å®š"""
        if len(market_regime) == 0:
            return "range"
        
        latest_regime = market_regime[-1] if not np.isnan(market_regime[-1]) else 0.0
        
        if latest_regime > 0.5:
            return "trending"
        elif latest_regime < -0.5:
            return "cycling"
        else:
            return "range"
    
    def _get_data_hash(self, data) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        try:
            if isinstance(data, pd.DataFrame):
                return f"{hash(data.values.tobytes())}_{self.atr_period}_{self.base_multiplier}"
            else:
                return f"{hash(data.tobytes())}_{self.atr_period}_{self.base_multiplier}"
        except:
            return f"{id(data)}_{self.atr_period}_{self.base_multiplier}"
    
    # === Getter ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤ ===
    
    def get_channels(self) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:
        """ãƒãƒ£ãƒãƒ«ãƒãƒ³ãƒ‰ã‚’å–å¾—"""
        if self._cache_keys and self._cache_keys[-1] in self._result_cache:
            result = self._result_cache[self._cache_keys[-1]]
            return result.upper_channel.copy(), result.lower_channel.copy(), result.centerline.copy()
        return None
    
    def get_breakout_signals(self) -> Optional[np.ndarray]:
        """ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã‚·ã‚°ãƒŠãƒ«ã‚’å–å¾—"""
        if self._cache_keys and self._cache_keys[-1] in self._result_cache:
            result = self._result_cache[self._cache_keys[-1]]
            return result.breakout_signals.copy()
        return None
    
    def get_signal_quality(self) -> Optional[np.ndarray]:
        """ã‚·ã‚°ãƒŠãƒ«å“è³ªã‚’å–å¾—"""
        if self._cache_keys and self._cache_keys[-1] in self._result_cache:
            result = self._result_cache[self._cache_keys[-1]]
            return result.signal_quality.copy()
        return None
    
    def get_trend_analysis(self) -> Optional[Dict]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰è§£æçµæœã‚’å–å¾—"""
        if self._cache_keys and self._cache_keys[-1] in self._result_cache:
            result = self._result_cache[self._cache_keys[-1]]
            return {
                'trend_strength': result.trend_strength.copy(),
                'hyper_efficiency': result.hyper_efficiency.copy(),
                'quantum_coherence': result.quantum_coherence.copy(),
                'market_regime': result.market_regime.copy()
            }
        return None
    
    def get_market_analysis(self) -> Optional[Dict]:
        """å¸‚å ´åˆ†æçµæœã‚’å–å¾—"""
        if self._cache_keys and self._cache_keys[-1] in self._result_cache:
            result = self._result_cache[self._cache_keys[-1]]
            
            # æœ‰åŠ¹ãªmarket_regimeãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            valid_regime = result.market_regime[~np.isnan(result.market_regime)]
            total_count = len(valid_regime)
            
            if total_count > 0:
                # æ–°ã‚¦ã‚§ãƒ¼ãƒ–ãƒ¬ãƒƒãƒˆåˆ¤å®šåŸºæº–ã«ã‚ˆã‚‹é›†è¨ˆï¼ˆç¾å®Ÿçš„ç‰ˆï¼‰
                very_strong_trend_count = int(np.sum(valid_regime >= 0.75))   # éå¸¸ã«å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ (0.8+)
                strong_trend_count = int(np.sum((valid_regime >= 0.6) & (valid_regime < 0.75)))  # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ (0.6-0.75)
                moderate_trend_count = int(np.sum((valid_regime >= 0.3) & (valid_regime < 0.6)))  # ä¸­ç¨‹åº¦ãƒˆãƒ¬ãƒ³ãƒ‰ (0.3-0.6)
                weak_trend_count = int(np.sum((valid_regime > 0.0) & (valid_regime < 0.3)))  # å¼±ã„ãƒˆãƒ¬ãƒ³ãƒ‰ (0.0-0.3)
                range_count = int(np.sum(valid_regime == 0.0))  # ãƒ¬ãƒ³ã‚¸ãƒ»æ¨ªã°ã„ (0.0)
                weak_cycle_count = int(np.sum((valid_regime >= -0.4) & (valid_regime < 0.0)))  # å¼±ã„ã‚µã‚¤ã‚¯ãƒ« (-0.4-0.0)
                strong_cycle_count = int(np.sum(valid_regime < -0.4))       # å¼·ã„ã‚µã‚¤ã‚¯ãƒ« (-0.6ä»¥ä¸‹)
                
                # çµ±åˆæ¯”ç‡è¨ˆç®—
                total_trend_count = very_strong_trend_count + strong_trend_count + moderate_trend_count + weak_trend_count
                total_cycle_count = weak_cycle_count + strong_cycle_count
                
                trending_ratio = total_trend_count / total_count
                cycling_ratio = total_cycle_count / total_count
                range_ratio = range_count / total_count
                
                # è©³ç´°åˆ†æ
                very_strong_trend_ratio = very_strong_trend_count / total_count
                strong_trend_ratio = strong_trend_count / total_count
                moderate_trend_ratio = moderate_trend_count / total_count
                weak_trend_ratio = weak_trend_count / total_count
                weak_cycle_ratio = weak_cycle_count / total_count
                strong_cycle_ratio = strong_cycle_count / total_count
            else:
                trending_ratio = cycling_ratio = range_ratio = 0.0
                very_strong_trend_ratio = strong_trend_ratio = moderate_trend_ratio = weak_trend_ratio = 0.0
                weak_cycle_ratio = strong_cycle_ratio = 0.0
            
            # ã‚µã‚¤ã‚¯ãƒ«å¼·åº¦ï¼ˆå®Ÿéš›ã®wavelet_cycleå€¤ã®å¹³å‡ï¼‰
            valid_cycle = result.wavelet_cycle[~np.isnan(result.wavelet_cycle)]
            cycle_strength = float(np.mean(valid_cycle)) if len(valid_cycle) > 0 else 0.0
            
            return {
                'trending_ratio': trending_ratio,
                'cycling_ratio': cycling_ratio,
                'range_ratio': range_ratio,
                'cycle_strength': cycle_strength,
                'total_regime_points': total_count,
                # è©³ç´°åˆ†æï¼ˆ7æ®µéšï¼‰
                'very_strong_trend_ratio': very_strong_trend_ratio,
                'strong_trend_ratio': strong_trend_ratio,
                'moderate_trend_ratio': moderate_trend_ratio,
                'weak_trend_ratio': weak_trend_ratio,
                'weak_cycle_ratio': weak_cycle_ratio,
                'strong_cycle_ratio': strong_cycle_ratio
            }
        return None
    
    def get_intelligence_report(self) -> Dict:
        """çŸ¥èƒ½ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
        if not self._cache_keys or self._cache_keys[-1] not in self._result_cache:
            return {"status": "no_data"}
        result = self._result_cache[self._cache_keys[-1]]
        
        return {
            "current_trend": result.current_trend,
            "current_confidence": result.current_confidence,
            "current_regime": result.current_regime,
            "total_signals": int(np.sum(np.abs(result.breakout_signals))),
            "avg_signal_quality": float(np.nanmean(result.signal_quality[result.signal_quality > 0])) if np.any(result.signal_quality > 0) else 0.0,
            "trend_strength": float(result.trend_strength[-1]) if len(result.trend_strength) > 0 and not np.isnan(result.trend_strength[-1]) else 0.0,
            "quantum_coherence": float(result.quantum_coherence[-1]) if len(result.quantum_coherence) > 0 and not np.isnan(result.quantum_coherence[-1]) else 0.0,
            "system_efficiency": float(np.nanmean(result.hyper_efficiency[-10:])) if len(result.hyper_efficiency) >= 10 else 0.0
        }
    
    def reset(self) -> None:
        """ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset()
        self._result_cache = {}
        self._cache_keys = []
        
        # ä½¿ç”¨ä¸­ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
        if self.volatility_type == 'ultimate' and self.ultimate_volatility:
            self.ultimate_volatility.reset()
        elif self.volatility_type == 'atr' and self.atr_indicator:
            self.atr_indicator.reset()


# ã‚¨ã‚¤ãƒªã‚¢ã‚¹
UBC = UltimateBreakoutChannel