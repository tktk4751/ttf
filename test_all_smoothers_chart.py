#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ **çµ±åˆã‚¹ãƒ ãƒ¼ã‚µãƒ¼å…¨ç¨®é¡ãƒãƒ£ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ** ğŸ¯

indicators/smoother/unified_smoother.py ã«ã‚ã‚‹å…¨ã¦ã®ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã‚’
ãƒãƒ£ãƒ¼ãƒˆã«æç”»ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ç‰¹æ€§ã‚’æ¯”è¼ƒã™ã‚‹ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰

ğŸ“Š **æ©Ÿèƒ½:**
- å…¨ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã®è‡ªå‹•å–å¾—ã¨å®Ÿè¡Œ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒ¼ãƒˆæç”»
- çµ±è¨ˆæƒ…å ±ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨è©³ç´°ãƒ­ã‚°
- ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœã®å®šé‡è©•ä¾¡

ğŸ”§ **å¯¾è±¡ã‚¹ãƒ ãƒ¼ã‚µãƒ¼:**
- FRAMA (ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«é©å¿œç§»å‹•å¹³å‡)
- Super Smoother (ã‚¨ãƒ¼ãƒ©ãƒ¼ã‚º2æ¥µãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)
- Ultimate Smoother (ç©¶æ¥µã‚¹ãƒ ãƒ¼ã‚µãƒ¼)
- Zero Lag EMA (ã‚¼ãƒ­ãƒ©ã‚°æŒ‡æ•°ç§»å‹•å¹³å‡)
- Laguerre Filter (ãƒ©ã‚²ãƒ¼ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import traceback
import time
from typing import Dict, List, Tuple, Any
import warnings

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings('ignore')

try:
    from indicators.smoother import UnifiedSmoother
    from indicators.price_source import PriceSource
except ImportError as e:
    print(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„")
    exit(1)


def generate_test_data(length: int = 200, complexity: str = 'medium') -> pd.DataFrame:
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    
    Args:
        length: ãƒ‡ãƒ¼ã‚¿é•·
        complexity: è¤‡é›‘ã• ('simple', 'medium', 'complex')
        
    Returns:
        OHLC ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­... (é•·ã•: {length}, è¤‡é›‘ã•: {complexity})")
    
    np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
    
    # è¤‡é›‘ã•ã«å¿œã˜ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    complexity_params = {
        'simple': {'trend': 0.0005, 'volatility': 0.015, 'noise_factor': 0.1, 'jump_probability': 0.01},
        'medium': {'trend': 0.001, 'volatility': 0.025, 'noise_factor': 0.3, 'jump_probability': 0.02},
        'complex': {'trend': 0.002, 'volatility': 0.04, 'noise_factor': 0.5, 'jump_probability': 0.05}
    }
    
    params = complexity_params.get(complexity, complexity_params['medium'])
    
    base_price = 100.0
    trend = params['trend']
    volatility = params['volatility']
    noise_factor = params['noise_factor']
    jump_prob = params['jump_probability']
    
    # ä¾¡æ ¼ç³»åˆ—ç”Ÿæˆ
    prices = [base_price]
    
    for i in range(1, length):
        # åŸºæœ¬ãƒˆãƒ¬ãƒ³ãƒ‰
        base_change = trend
        
        # å‘¨æœŸçš„å¤‰å‹•ï¼ˆè¤‡æ•°ã®å‘¨æœŸï¼‰
        cycle1 = 0.001 * np.sin(2 * np.pi * i / 20)  # 20æœŸé–“å‘¨æœŸ
        cycle2 = 0.0005 * np.sin(2 * np.pi * i / 50)  # 50æœŸé–“å‘¨æœŸ
        cycle3 = 0.0003 * np.sin(2 * np.pi * i / 100)  # 100æœŸé–“å‘¨æœŸ
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º
        noise = np.random.normal(0, volatility * noise_factor)
        
        # æ™‚ã€…ç™ºç”Ÿã™ã‚‹ã‚¸ãƒ£ãƒ³ãƒ—
        if np.random.random() < jump_prob:
            jump = np.random.choice([-1, 1]) * volatility * 3
        else:
            jump = 0
        
        # ç·å¤‰åŒ–é‡
        total_change = base_change + cycle1 + cycle2 + cycle3 + noise + jump
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åŠ¹æœ
        if i > 10:
            recent_vol = np.std(np.diff(prices[-10:]))
            vol_adjust = recent_vol / volatility
            total_change *= (0.7 + 0.6 * vol_adjust)  # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®æŒç¶šæ€§
        
        new_price = prices[-1] * (1 + total_change)
        prices.append(max(new_price, 0.1))  # è² ã®ä¾¡æ ¼ã‚’é˜²ã
    
    # OHLC ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
    data = []
    for i, close in enumerate(prices):
        # æ—¥ä¸­å¤‰å‹•ã®ç”Ÿæˆ
        daily_range = abs(np.random.normal(0, volatility * close * 0.4))
        
        high = close + daily_range * np.random.uniform(0.2, 1.0)
        low = close - daily_range * np.random.uniform(0.2, 1.0)
        
        if i == 0:
            open_price = close
        else:
            # ã‚®ãƒ£ãƒƒãƒ—ã®ç”Ÿæˆ
            gap = np.random.normal(0, volatility * close * 0.15)
            open_price = prices[i-1] + gap
        
        # OHLC ã®è«–ç†çš„æ•´åˆæ€§ç¢ºä¿
        high = max(high, open_price, close)
        low = min(low, open_price, close)
        
        data.append({
            'open': open_price,
            'high': high,
            'low': low,
            'close': close,
            'volume': np.random.uniform(1000, 20000)
        })
    
    df = pd.DataFrame(data)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¨­å®š
    start_date = datetime.now() - timedelta(days=length)
    df.index = [start_date + timedelta(hours=i*4) for i in range(length)]  # 4æ™‚é–“è¶³
    
    print(f"ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(df)}ãƒã‚¤ãƒ³ãƒˆ")
    print(f"ä¾¡æ ¼ç¯„å›²: {df['close'].min():.2f} - {df['close'].max():.2f}")
    print(f"å¹³å‡ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£: {np.std(df['close'].pct_change().dropna())*100:.2f}%")
    
    return df


def calculate_smoother_statistics(original: np.ndarray, smoothed: np.ndarray) -> Dict[str, float]:
    """
    ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã®çµ±è¨ˆæŒ‡æ¨™ã‚’è¨ˆç®—
    
    Args:
        original: å…ƒãƒ‡ãƒ¼ã‚¿
        smoothed: ã‚¹ãƒ ãƒ¼ã‚ºæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        çµ±è¨ˆæŒ‡æ¨™ã®è¾æ›¸
    """
    valid_mask = ~(np.isnan(original) | np.isnan(smoothed))
    if np.sum(valid_mask) < 2:
        return {'error': True}
    
    orig_valid = original[valid_mask]
    smooth_valid = smoothed[valid_mask]
    
    # åŸºæœ¬çµ±è¨ˆ
    correlation = np.corrcoef(orig_valid, smooth_valid)[0, 1] if len(orig_valid) > 1 else 0
    
    # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœ (ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å‰Šæ¸›ç‡)
    orig_vol = np.std(np.diff(orig_valid)) if len(orig_valid) > 1 else 0
    smooth_vol = np.std(np.diff(smooth_valid)) if len(smooth_valid) > 1 else 0
    smoothing_effect = (1 - smooth_vol / orig_vol) * 100 if orig_vol > 0 else 0
    
    # é…å»¶æ¨å®š (ã‚¯ãƒ­ã‚¹ã‚³ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
    if len(orig_valid) > 10:
        max_lag = min(20, len(orig_valid) // 4)
        correlations = []
        for lag in range(-max_lag, max_lag + 1):
            if lag < 0:
                corr = np.corrcoef(orig_valid[:lag], smooth_valid[-lag:])[0, 1]
            elif lag > 0:
                corr = np.corrcoef(orig_valid[lag:], smooth_valid[:-lag])[0, 1]
            else:
                corr = np.corrcoef(orig_valid, smooth_valid)[0, 1]
            correlations.append((lag, corr))
        
        best_lag = max(correlations, key=lambda x: abs(x[1]))[0]
        lag_estimate = best_lag
    else:
        lag_estimate = 0
    
    # å¹³å‡çµ¶å¯¾èª¤å·®
    mae = np.mean(np.abs(orig_valid - smooth_valid))
    
    # æœ€å¤§åå·®
    max_deviation = np.max(np.abs(orig_valid - smooth_valid))
    
    return {
        'correlation': correlation,
        'smoothing_effect': smoothing_effect,
        'lag_estimate': lag_estimate,
        'mae': mae,
        'max_deviation': max_deviation,
        'valid_ratio': np.sum(valid_mask) / len(original) * 100,
        'error': False
    }


def test_all_smoothers(data: pd.DataFrame) -> Dict[str, Any]:
    """
    å…¨ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    
    Args:
        data: OHLCãƒ‡ãƒ¼ã‚¿
        
    Returns:
        çµæœè¾æ›¸
    """
    print("\n=== å…¨ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===")
    
    # åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã‚’å–å¾—
    available_smoothers = UnifiedSmoother.get_available_smoothers()
    
    # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’é™¤å¤–ã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã®ã¿å–å¾—
    unique_smoothers = {}
    seen_classes = set()
    
    for name, description in available_smoothers.items():
        smoother_key = (description, name)
        if smoother_key not in seen_classes:
            unique_smoothers[name] = description
            seen_classes.add(smoother_key)
    
    print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚¹ãƒ ãƒ¼ã‚µãƒ¼: {len(unique_smoothers)}ç¨®é¡")
    for name, desc in unique_smoothers.items():
        print(f"  {name}: {desc}")
    
    results = {}
    src_prices = PriceSource.calculate_source(data, 'close')
    
    for smoother_name in unique_smoothers.keys():
        print(f"\n{smoother_name} ã‚’å®Ÿè¡Œä¸­...")
        start_time = time.time()
        
        try:
            # ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ä½œæˆ
            smoother = UnifiedSmoother(smoother_type=smoother_name, src_type='close')
            
            # è¨ˆç®—å®Ÿè¡Œ
            result = smoother.calculate(data)
            
            # çµ±è¨ˆè¨ˆç®—
            stats = calculate_smoother_statistics(src_prices, result.values)
            
            execution_time = time.time() - start_time
            
            results[smoother_name] = {
                'values': result.values,
                'description': unique_smoothers[smoother_name],
                'parameters': result.parameters,
                'additional_data': result.additional_data,
                'statistics': stats,
                'execution_time': execution_time,
                'success': True,
                'error_message': None
            }
            
            print(f"  âœ“ æˆåŠŸ ({execution_time:.3f}ç§’)")
            if not stats.get('error', False):
                print(f"    ç›¸é–¢ä¿‚æ•°: {stats['correlation']:.3f}")
                print(f"    ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœ: {stats['smoothing_effect']:.1f}%")
                print(f"    æ¨å®šé…å»¶: {stats['lag_estimate']:.0f}æœŸé–“")
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = str(e)
            
            results[smoother_name] = {
                'values': np.full(len(data), np.nan),
                'description': unique_smoothers[smoother_name],
                'parameters': {},
                'additional_data': {},
                'statistics': {'error': True},
                'execution_time': execution_time,
                'success': False,
                'error_message': error_msg
            }
            
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {error_msg}")
    
    # æˆåŠŸã—ãŸã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã®æ•°
    successful_count = sum(1 for r in results.values() if r['success'])
    print(f"\nå®Ÿè¡Œçµæœ: {successful_count}/{len(unique_smoothers)} æˆåŠŸ")
    
    return results


def create_comprehensive_chart(data: pd.DataFrame, smoother_results: Dict[str, Any]) -> None:
    """
    åŒ…æ‹¬çš„ãªãƒãƒ£ãƒ¼ãƒˆä½œæˆ
    
    Args:
        data: å…ƒã®OHLCãƒ‡ãƒ¼ã‚¿
        smoother_results: ã‚¹ãƒ ãƒ¼ã‚µãƒ¼çµæœ
    """
    print("\n=== ãƒãƒ£ãƒ¼ãƒˆä½œæˆä¸­ ===")
    
    # æˆåŠŸã—ãŸã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã®ã¿æŠ½å‡º
    successful_results = {k: v for k, v in smoother_results.items() if v['success']}
    
    if not successful_results:
        print("æˆåŠŸã—ãŸã‚¹ãƒ ãƒ¼ã‚µãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # å›³ã®ã‚µã‚¤ã‚ºã¨è‰²è¨­å®š
    fig_width = 16
    fig_height = 12
    colors = plt.cm.tab10(np.linspace(0, 1, len(successful_results)))
    
    # ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒ¼ãƒˆä½œæˆ
    fig, axes = plt.subplots(3, 1, figsize=(fig_width, fig_height))
    fig.suptitle('çµ±åˆã‚¹ãƒ ãƒ¼ã‚µãƒ¼å…¨ç¨®é¡æ¯”è¼ƒãƒ†ã‚¹ãƒˆ', fontsize=16, fontweight='bold')
    
    # æ™‚é–“è»¸
    time_index = data.index
    
    # 1. ä¾¡æ ¼ã¨ã‚¹ãƒ ãƒ¼ã‚µãƒ¼æ¯”è¼ƒ
    ax1 = axes[0]
    
    # å…ƒä¾¡æ ¼ï¼ˆè–„ã„ã‚°ãƒ¬ãƒ¼ï¼‰
    ax1.plot(time_index, data['close'], color='lightgray', linewidth=1, alpha=0.7, label='å…ƒä¾¡æ ¼')
    
    # å„ã‚¹ãƒ ãƒ¼ã‚µãƒ¼
    for i, (name, result) in enumerate(successful_results.items()):
        ax1.plot(time_index, result['values'], color=colors[i], linewidth=2, 
                label=f"{name}", alpha=0.8)
    
    ax1.set_title('ä¾¡æ ¼ã¨ã‚¹ãƒ ãƒ¼ã‚µãƒ¼æ¯”è¼ƒ', fontsize=14, fontweight='bold')
    ax1.set_ylabel('ä¾¡æ ¼')
    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # 2. ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœæ¯”è¼ƒ (æ¨™æº–åå·®æ¯”è¼ƒ)
    ax2 = axes[1]
    
    smoother_names = []
    smoothing_effects = []
    correlations = []
    
    for name, result in successful_results.items():
        stats = result['statistics']
        if not stats.get('error', False):
            smoother_names.append(name)
            smoothing_effects.append(stats['smoothing_effect'])
            correlations.append(stats['correlation'])
    
    if smoother_names:
        # ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœãƒãƒ¼
        bars = ax2.bar(range(len(smoother_names)), smoothing_effects, 
                      color=colors[:len(smoother_names)], alpha=0.7)
        ax2.set_title('ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœ (ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å‰Šæ¸›ç‡)', fontsize=14, fontweight='bold')
        ax2.set_ylabel('å‰Šæ¸›ç‡ (%)')
        ax2.set_xticks(range(len(smoother_names)))
        ax2.set_xticklabels(smoother_names, rotation=45, ha='right')
        ax2.grid(True, alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for i, (bar, effect) in enumerate(zip(bars, smoothing_effects)):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    f'{effect:.1f}%', ha='center', va='bottom', fontsize=10)
    
    # 3. ç›¸é–¢ä¿‚æ•°ã¨é…å»¶
    ax3 = axes[2]
    
    if smoother_names:
        # ç›¸é–¢ä¿‚æ•°
        ax3_twin = ax3.twinx()
        
        line1 = ax3.plot(range(len(smoother_names)), correlations, 
                        'o-', color='blue', linewidth=2, markersize=8, label='ç›¸é–¢ä¿‚æ•°')
        ax3.set_ylabel('ç›¸é–¢ä¿‚æ•°', color='blue')
        ax3.tick_params(axis='y', labelcolor='blue')
        
        # é…å»¶
        lags = [successful_results[name]['statistics']['lag_estimate'] 
               for name in smoother_names if not successful_results[name]['statistics'].get('error', False)]
        
        if len(lags) == len(smoother_names):
            line2 = ax3_twin.plot(range(len(smoother_names)), lags, 
                                 's-', color='red', linewidth=2, markersize=8, label='æ¨å®šé…å»¶')
            ax3_twin.set_ylabel('æ¨å®šé…å»¶ (æœŸé–“)', color='red')
            ax3_twin.tick_params(axis='y', labelcolor='red')
        
        ax3.set_title('ç›¸é–¢ä¿‚æ•°ã¨æ¨å®šé…å»¶', fontsize=14, fontweight='bold')
        ax3.set_xticks(range(len(smoother_names)))
        ax3.set_xticklabels(smoother_names, rotation=45, ha='right')
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 1)
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´
    plt.tight_layout()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"all_smoothers_comparison_{timestamp}.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    print(f"ãƒãƒ£ãƒ¼ãƒˆä¿å­˜: {filename}")
    
    # è¡¨ç¤º
    plt.show()


def print_detailed_statistics(smoother_results: Dict[str, Any]) -> None:
    """
    è©³ç´°çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    
    Args:
        smoother_results: ã‚¹ãƒ ãƒ¼ã‚µãƒ¼çµæœ
    """
    print("\n" + "="*80)
    print("ğŸ“Š è©³ç´°çµ±è¨ˆæƒ…å ±")
    print("="*80)
    
    successful_results = {k: v for k, v in smoother_results.items() if v['success']}
    
    if not successful_results:
        print("æˆåŠŸã—ãŸã‚¹ãƒ ãƒ¼ã‚µãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
    print(f"{'ã‚¹ãƒ ãƒ¼ã‚µãƒ¼':<20} {'ç›¸é–¢':<8} {'åŠ¹æœ%':<8} {'é…å»¶':<6} {'MAE':<10} {'æ™‚é–“(s)':<8}")
    print("-" * 80)
    
    # å„ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ã®çµ±è¨ˆ
    for name, result in successful_results.items():
        stats = result['statistics']
        
        if not stats.get('error', False):
            correlation = stats['correlation']
            smoothing = stats['smoothing_effect']
            lag = stats['lag_estimate']
            mae = stats['mae']
            exec_time = result['execution_time']
            
            print(f"{name:<20} {correlation:<8.3f} {smoothing:<8.1f} {lag:<6.0f} {mae:<10.4f} {exec_time:<8.3f}")
        else:
            print(f"{name:<20} {'ERROR':<8} {'ERROR':<8} {'ERROR':<6} {'ERROR':<10} {result['execution_time']:<8.3f}")
    
    print("-" * 80)
    
    # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    valid_stats = [r['statistics'] for r in successful_results.values() 
                  if not r['statistics'].get('error', False)]
    
    if valid_stats:
        avg_correlation = np.mean([s['correlation'] for s in valid_stats])
        avg_smoothing = np.mean([s['smoothing_effect'] for s in valid_stats])
        avg_lag = np.mean([s['lag_estimate'] for s in valid_stats])
        total_time = sum(r['execution_time'] for r in successful_results.values())
        
        print(f"å¹³å‡çµ±è¨ˆå€¤:")
        print(f"  ç›¸é–¢ä¿‚æ•°: {avg_correlation:.3f}")
        print(f"  ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°åŠ¹æœ: {avg_smoothing:.1f}%")
        print(f"  å¹³å‡é…å»¶: {avg_lag:.1f}æœŸé–“")
        print(f"  ç·å®Ÿè¡Œæ™‚é–“: {total_time:.3f}ç§’")
    
    # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ãŸå ´åˆã®æƒ…å ±
    failed_results = {k: v for k, v in smoother_results.items() if not v['success']}
    if failed_results:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚¹ãƒ ãƒ¼ã‚µãƒ¼ ({len(failed_results)}å€‹):")
        for name, result in failed_results.items():
            print(f"  {name}: {result['error_message']}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ çµ±åˆã‚¹ãƒ ãƒ¼ã‚µãƒ¼å…¨ç¨®é¡ãƒãƒ£ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    try:
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        data = generate_test_data(length=150, complexity='medium')
        
        # å…¨ã‚¹ãƒ ãƒ¼ã‚µãƒ¼ãƒ†ã‚¹ãƒˆ
        smoother_results = test_all_smoothers(data)
        
        # è©³ç´°çµ±è¨ˆè¡¨ç¤º
        print_detailed_statistics(smoother_results)
        
        # ãƒãƒ£ãƒ¼ãƒˆä½œæˆ
        create_comprehensive_chart(data, smoother_results)
        
        print("\nâœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("\nã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
        traceback.print_exc()


if __name__ == "__main__":
    main()