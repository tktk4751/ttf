#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import sys
import time
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from indicators.ultimate_ma_v3 import UltimateMAV3
from indicators.ultimate_ma import UltimateMA

def load_binance_data(symbol='BTC', market_type='spot', timeframe='4h', data_dir='data/binance'):
    """
    Binanceãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥èª­ã¿è¾¼ã‚€
    """
    file_path = f"{data_dir}/{symbol}/{market_type}/{timeframe}/historical_data.csv"
    
    print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return None
    
    try:
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        df = pd.read_csv(file_path)
        
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¨­å®š
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df.set_index('timestamp', inplace=True)
        
        # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        required_columns = ['open', 'high', 'low', 'close']
        missing_columns = [col for col in required_columns if col not in df.columns]
        
        if missing_columns:
            print(f"âŒ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_columns}")
            return None
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ•°å€¤ã«å¤‰æ›
        for col in required_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # NaNã‚’é™¤å»
        df = df.dropna()
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {symbol} {market_type} {timeframe}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df.index.min()} - {df.index.max()}")
        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶")
        
        return df
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def generate_advanced_test_data(n_points=2000, scenario='mixed'):
    """
    é«˜åº¦ãªãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    
    Args:
        n_points: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°
        scenario: ã‚·ãƒŠãƒªã‚ª ('trend', 'range', 'volatile', 'mixed')
    """
    np.random.seed(42)
    
    if scenario == 'trend':
        # å¼·ã„ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿
        trend = np.cumsum(np.random.randn(n_points) * 0.005 + 0.002)
        noise = np.random.randn(n_points) * 0.01
    elif scenario == 'range':
        # ãƒ¬ãƒ³ã‚¸ç›¸å ´ãƒ‡ãƒ¼ã‚¿
        trend = np.sin(np.linspace(0, 4*np.pi, n_points)) * 2
        noise = np.random.randn(n_points) * 0.02
    elif scenario == 'volatile':
        # é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿
        trend = np.cumsum(np.random.randn(n_points) * 0.001)
        noise = np.random.randn(n_points) * 0.05
    else:  # mixed
        # æ··åˆã‚·ãƒŠãƒªã‚ª
        trend_part = np.cumsum(np.random.randn(n_points//2) * 0.003)
        range_part = np.sin(np.linspace(0, 2*np.pi, n_points//2)) * 1.5
        trend = np.concatenate([trend_part, range_part])
        noise = np.random.randn(n_points) * 0.015
    
    base_price = 50000 + trend * 1000 + noise * 100
    
    # OHLCä½œæˆ
    high = base_price + np.abs(np.random.randn(n_points) * 50)
    low = base_price - np.abs(np.random.randn(n_points) * 50)
    open_price = base_price + np.random.randn(n_points) * 20
    close_price = base_price + np.random.randn(n_points) * 20
    
    # æ—¥ä»˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    dates = pd.date_range(start='2023-01-01', periods=n_points, freq='4H')
    
    df = pd.DataFrame({
        'open': open_price,
        'high': high,
        'low': low,
        'close': close_price
    }, index=dates)
    
    return df

def comprehensive_performance_test():
    """åŒ…æ‹¬çš„æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ UltimateMA V3 åŒ…æ‹¬çš„æ€§èƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("="*60)
    
    scenarios = ['trend', 'range', 'volatile', 'mixed']
    results = {}
    
    for scenario in scenarios:
        print(f"\nğŸ“Š ã‚·ãƒŠãƒªã‚ª: {scenario.upper()}")
        print("-" * 40)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        data = generate_advanced_test_data(1500, scenario)
        
        # UltimateMA V3
        uma_v3 = UltimateMAV3(
            super_smooth_period=8,
            zero_lag_period=16,
            realtime_window=34,
            quantum_window=16,
            fractal_window=16,
            entropy_window=16,
            src_type='hlc3',
            slope_index=2,
            base_threshold=0.002,
            min_confidence=0.15
        )
        
        # è¨ˆç®—å®Ÿè¡Œ
        start_time = time.time()
        result = uma_v3.calculate(data)
        calc_time = time.time() - start_time
        
        # çµ±è¨ˆè¨ˆç®—
        up_signals = np.sum(result.trend_signals == 1)
        down_signals = np.sum(result.trend_signals == -1)
        range_signals = np.sum(result.trend_signals == 0)
        total_signals = len(result.trend_signals)
        
        avg_confidence = np.mean(result.trend_confidence[result.trend_confidence > 0])
        max_confidence = np.max(result.trend_confidence)
        
        # é‡å­åˆ†æçµ±è¨ˆ
        quantum_analysis = uma_v3.get_quantum_analysis()
        avg_quantum_strength = np.mean(np.abs(quantum_analysis['quantum_state']))
        avg_mtf_consensus = np.mean(quantum_analysis['multi_timeframe_consensus'])
        avg_fractal_dim = np.mean(quantum_analysis['fractal_dimension'])
        avg_entropy = np.mean(quantum_analysis['entropy_level'])
        
        results[scenario] = {
            'calc_time': calc_time,
            'data_points': len(data),
            'current_trend': result.current_trend,
            'current_confidence': result.current_confidence,
            'signal_distribution': {
                'up': up_signals / total_signals * 100,
                'down': down_signals / total_signals * 100,
                'range': range_signals / total_signals * 100
            },
            'confidence_stats': {
                'avg': avg_confidence,
                'max': max_confidence
            },
            'quantum_stats': {
                'quantum_strength': avg_quantum_strength,
                'mtf_consensus': avg_mtf_consensus,
                'fractal_dimension': avg_fractal_dim,
                'entropy_level': avg_entropy
            },
            'result': result,
            'data': data
        }
        
        print(f"âš¡ è¨ˆç®—æ™‚é–“: {calc_time:.3f}ç§’ ({len(data)}ãƒã‚¤ãƒ³ãƒˆ)")
        print(f"ğŸ¯ ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰: {result.current_trend} (ä¿¡é ¼åº¦: {result.current_confidence:.3f})")
        print(f"ğŸ“Š ä¿¡å·åˆ†å¸ƒ: ä¸Šæ˜‡{up_signals/total_signals*100:.1f}% | ä¸‹é™{down_signals/total_signals*100:.1f}% | ãƒ¬ãƒ³ã‚¸{range_signals/total_signals*100:.1f}%")
        print(f"ğŸ”¥ å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}")
        print(f"ğŸŒŒ é‡å­å¼·åº¦: {avg_quantum_strength:.3f} | MTFåˆæ„åº¦: {avg_mtf_consensus:.3f}")
    
    return results

def create_comprehensive_visualization(results):
    """åŒ…æ‹¬çš„ãªå¯è¦–åŒ–"""
    print("\nğŸ“ˆ åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒãƒ£ãƒ¼ãƒˆä½œæˆä¸­...")
    
    fig, axes = plt.subplots(3, 2, figsize=(20, 15))
    fig.suptitle('ğŸš€ UltimateMA V3 - åŒ…æ‹¬çš„æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
    
    scenarios = list(results.keys())
    colors = ['red', 'blue', 'green', 'orange']
    
    # 1. å„ã‚·ãƒŠãƒªã‚ªã®ä¾¡æ ¼ãƒãƒ£ãƒ¼ãƒˆ
    ax1 = axes[0, 0]
    for i, (scenario, data) in enumerate(results.items()):
        df = data['data']
        result = data['result']
        ax1.plot(df.index, df['close'], alpha=0.6, color=colors[i], label=f'{scenario} Price')
        ax1.plot(df.index, result.values, color=colors[i], linewidth=2, linestyle='--', 
                label=f'{scenario} UMA V3')
    ax1.set_title('ğŸ’° ä¾¡æ ¼ãƒãƒ£ãƒ¼ãƒˆæ¯”è¼ƒ')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ä¿¡å·åˆ†å¸ƒæ¯”è¼ƒ
    ax2 = axes[0, 1]
    x = np.arange(len(scenarios))
    width = 0.25
    
    up_ratios = [results[s]['signal_distribution']['up'] for s in scenarios]
    down_ratios = [results[s]['signal_distribution']['down'] for s in scenarios]
    range_ratios = [results[s]['signal_distribution']['range'] for s in scenarios]
    
    ax2.bar(x - width, up_ratios, width, label='ä¸Šæ˜‡', color='green', alpha=0.7)
    ax2.bar(x, down_ratios, width, label='ä¸‹é™', color='red', alpha=0.7)
    ax2.bar(x + width, range_ratios, width, label='ãƒ¬ãƒ³ã‚¸', color='blue', alpha=0.7)
    
    ax2.set_title('ğŸ“Š ä¿¡å·åˆ†å¸ƒæ¯”è¼ƒ')
    ax2.set_ylabel('å‰²åˆ (%)')
    ax2.set_xticks(x)
    ax2.set_xticklabels(scenarios)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. ä¿¡é ¼åº¦çµ±è¨ˆ
    ax3 = axes[1, 0]
    avg_confidences = [results[s]['confidence_stats']['avg'] for s in scenarios]
    max_confidences = [results[s]['confidence_stats']['max'] for s in scenarios]
    
    ax3.bar(x - width/2, avg_confidences, width, label='å¹³å‡ä¿¡é ¼åº¦', color='orange', alpha=0.7)
    ax3.bar(x + width/2, max_confidences, width, label='æœ€å¤§ä¿¡é ¼åº¦', color='purple', alpha=0.7)
    
    ax3.set_title('ğŸ”¥ ä¿¡é ¼åº¦çµ±è¨ˆ')
    ax3.set_ylabel('ä¿¡é ¼åº¦')
    ax3.set_xticks(x)
    ax3.set_xticklabels(scenarios)
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. é‡å­åˆ†ææŒ‡æ¨™
    ax4 = axes[1, 1]
    quantum_strengths = [results[s]['quantum_stats']['quantum_strength'] for s in scenarios]
    mtf_consensus = [results[s]['quantum_stats']['mtf_consensus'] for s in scenarios]
    
    ax4.bar(x - width/2, quantum_strengths, width, label='é‡å­å¼·åº¦', color='purple', alpha=0.7)
    ax4.bar(x + width/2, mtf_consensus, width, label='MTFåˆæ„åº¦', color='cyan', alpha=0.7)
    
    ax4.set_title('ğŸŒŒ é‡å­åˆ†ææŒ‡æ¨™')
    ax4.set_ylabel('å€¤')
    ax4.set_xticks(x)
    ax4.set_xticklabels(scenarios)
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. è¨ˆç®—æ€§èƒ½
    ax5 = axes[2, 0]
    calc_times = [results[s]['calc_time'] * 1000 for s in scenarios]  # ãƒŸãƒªç§’
    data_points = [results[s]['data_points'] for s in scenarios]
    
    bars = ax5.bar(scenarios, calc_times, color=colors, alpha=0.7)
    ax5.set_title('âš¡ è¨ˆç®—æ™‚é–“')
    ax5.set_ylabel('æ™‚é–“ (ms)')
    
    # ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’è¡¨ç¤º
    for bar, points in zip(bars, data_points):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{points}pts', ha='center', va='bottom', fontsize=8)
    
    ax5.grid(True, alpha=0.3)
    
    # 6. ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ
    ax6 = axes[2, 1]
    fractal_dims = [results[s]['quantum_stats']['fractal_dimension'] for s in scenarios]
    entropy_levels = [results[s]['quantum_stats']['entropy_level'] for s in scenarios]
    
    ax6.bar(x - width/2, fractal_dims, width, label='ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒ', color='green', alpha=0.7)
    ax6.bar(x + width/2, entropy_levels, width, label='ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼', color='red', alpha=0.7)
    
    ax6.set_title('ğŸ”¬ ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ãƒ»ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ')
    ax6.set_ylabel('å€¤')
    ax6.set_xticks(x)
    ax6.set_xticklabels(scenarios)
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    filename = "tests/ultimate_ma_v3_comprehensive_analysis.png"
    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"âœ… åŒ…æ‹¬çš„åˆ†æãƒãƒ£ãƒ¼ãƒˆä¿å­˜å®Œäº†: {filename}")
    
    plt.show()
    plt.close()

def real_data_test():
    """å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸŒ å®Ÿéš›ã®Binanceãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ†ã‚¹ãƒˆ")
    print("="*50)
    
    symbols = ['BTC', 'ETH', 'ADA']
    real_results = {}
    
    for symbol in symbols:
        print(f"\nğŸ“Š {symbol} ãƒ†ã‚¹ãƒˆä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        data = load_binance_data(symbol=symbol, market_type='spot', timeframe='4h')
        
        if data is None:
            print(f"âŒ {symbol}ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—")
            continue
        
        # æœ€æ–°1000ä»¶ã‚’ä½¿ç”¨
        if len(data) > 1000:
            data = data.tail(1000)
        
        # UltimateMA V3
        uma_v3 = UltimateMAV3(
            super_smooth_period=8,
            zero_lag_period=16,
            realtime_window=34,
            quantum_window=16,
            fractal_window=16,
            entropy_window=16,
            src_type='hlc3',
            slope_index=2,
            base_threshold=0.002,
            min_confidence=0.15
        )
        
        # è¨ˆç®—å®Ÿè¡Œ
        start_time = time.time()
        result = uma_v3.calculate(data)
        calc_time = time.time() - start_time
        
        # çµ±è¨ˆ
        up_signals = np.sum(result.trend_signals == 1)
        down_signals = np.sum(result.trend_signals == -1)
        range_signals = np.sum(result.trend_signals == 0)
        total_signals = len(result.trend_signals)
        
        avg_confidence = np.mean(result.trend_confidence[result.trend_confidence > 0])
        
        real_results[symbol] = {
            'calc_time': calc_time,
            'current_trend': result.current_trend,
            'current_confidence': result.current_confidence,
            'avg_confidence': avg_confidence,
            'signal_distribution': {
                'up': up_signals / total_signals * 100,
                'down': down_signals / total_signals * 100,
                'range': range_signals / total_signals * 100
            }
        }
        
        print(f"âœ… {symbol} å®Œäº†:")
        print(f"  âš¡ è¨ˆç®—æ™‚é–“: {calc_time:.3f}ç§’")
        print(f"  ğŸ¯ ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰: {result.current_trend} (ä¿¡é ¼åº¦: {result.current_confidence:.3f})")
        print(f"  ğŸ“Š ä¿¡å·åˆ†å¸ƒ: ä¸Šæ˜‡{up_signals/total_signals*100:.1f}% | ä¸‹é™{down_signals/total_signals*100:.1f}% | ãƒ¬ãƒ³ã‚¸{range_signals/total_signals*100:.1f}%")
        print(f"  ğŸ”¥ å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}")
    
    return real_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ UltimateMA V3 æ”¹è‰¯ç‰ˆåŒ…æ‹¬ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ")
    print("="*60)
    
    # 1. åŒ…æ‹¬çš„æ€§èƒ½ãƒ†ã‚¹ãƒˆ
    synthetic_results = comprehensive_performance_test()
    
    # 2. åŒ…æ‹¬çš„å¯è¦–åŒ–
    create_comprehensive_visualization(synthetic_results)
    
    # 3. å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ
    real_results = real_data_test()
    
    # 4. ç·åˆã‚µãƒãƒªãƒ¼
    print(f"\n{'='*60}")
    print("ğŸ† ç·åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    
    print("\nğŸ“Š åˆæˆãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆçµæœ:")
    for scenario, data in synthetic_results.items():
        print(f"  {scenario.upper()}: {data['current_trend']} (ä¿¡é ¼åº¦: {data['current_confidence']:.3f})")
    
    if real_results:
        print("\nğŸŒ å®Ÿãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆçµæœ:")
        for symbol, data in real_results.items():
            print(f"  {symbol}: {data['current_trend']} (ä¿¡é ¼åº¦: {data['current_confidence']:.3f})")
    
    print(f"\nâœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print("ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸãƒãƒ£ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
    print("ğŸŒŸ UltimateMA V3ã¯æ§˜ã€…ãªå¸‚å ´ç’°å¢ƒã§å„ªç§€ãªæ€§èƒ½ã‚’ç™ºæ®ã—ã¦ã„ã¾ã™ï¼")

if __name__ == "__main__":
    main() 