#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ğŸ¯ **Kalman Filter Unified Analyzer - çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ** ğŸ¯

kalman_filter_unified.py ã®å…¨ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã§åˆ†æã—ã€
åŒ…æ‹¬çš„ãªæ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

ğŸš€ **åˆ†æå¯¾è±¡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:**
- adaptive: åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
- quantum_adaptive: é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼  
- unscented: ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUKFï¼‰
- extended: æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆEKFï¼‰
- hyper_quantum: ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
- triple_ensemble: ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
- neural_supreme: ğŸ§ ğŸš€ Neural Adaptive Quantum Supreme
- market_adaptive_unscented: ğŸ¯ å¸‚å ´é©å¿œç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

ğŸ“Š **åŒ…æ‹¬çš„åˆ†æ:**
- å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æ€§èƒ½æ¯”è¼ƒ
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢åˆ†æ
- ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³å‹•å‘
- äºˆæ¸¬ç²¾åº¦è©•ä¾¡
- å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ æ¤œå‡ºï¼ˆå¯¾å¿œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼‰
"""

import argparse
import sys
import numpy as np
import pandas as pd
import yaml
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from matplotlib.patches import Rectangle
import seaborn as sns
from typing import Dict, List, Any
import warnings
warnings.filterwarnings('ignore')

from data.data_loader import DataLoader, CSVDataSource
from data.data_processor import DataProcessor
from data.binance_data_source import BinanceDataSource
from indicators.kalman_filter_unified import KalmanFilterUnified
from logger import get_logger

# è‰²è¨­å®š
FILTER_COLORS = {
    'adaptive': '#2E86AB',           # é’
    'quantum_adaptive': '#A23B72',   # ç´«
    'unscented': '#F18F01',          # ã‚ªãƒ¬ãƒ³ã‚¸
    'extended': '#C73E1D',           # èµ¤
    'hyper_quantum': '#7209B7',      # ç´«
    'triple_ensemble': '#2A9D8F',    # é’ç·‘
    'neural_supreme': '#E63946',     # èµ¤
    'market_adaptive_unscented': '#FF6B35'  # ã‚ªãƒ¬ãƒ³ã‚¸èµ¤
}

class KalmanFilterUnifiedAnalyzer:
    """
    ğŸ¯ çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    
    å…¨ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã§åˆ†æã—ã€
    åŒ…æ‹¬çš„ãªæ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        self.logger = get_logger(__name__)
        self.config = self._load_config(config_path)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–
        binance_config = self.config.get('binance_data', {})
        data_dir = binance_config.get('data_dir', 'data/binance')
        binance_data_source = BinanceDataSource(data_dir)
        
        dummy_csv_source = CSVDataSource("dummy")
        self.data_loader = DataLoader(
            data_source=dummy_csv_source,
            binance_data_source=binance_data_source
        )
        
        self.data_processor = DataProcessor()
        
        # å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
        self.filters = {}
        available_filters = KalmanFilterUnified.get_available_filters()
        
        for filter_type in available_filters.keys():
            try:
                self.filters[filter_type] = KalmanFilterUnified(
                    filter_type=filter_type,
                    src_type='hlc3',
                    ukf_alpha=0.001,
                    ukf_beta=2.0,
                    ukf_kappa=0.0,
                    quantum_scale=0.5
                )
                self.logger.info(f"âœ… {filter_type} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–")
            except Exception as e:
                self.logger.error(f"âŒ {filter_type} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
        
        self.logger.info(f"ğŸ¯ KalmanFilterUnifiedAnalyzer initialized with {len(self.filters)} filters")
    
    def _load_config(self, config_path: str) -> dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            self.logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            raise
    
    def load_market_data(self) -> pd.DataFrame:
        """å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        try:
            self.logger.info("ğŸ“Š å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            
            raw_data = self.data_loader.load_data_from_config(self.config)
            
            if not raw_data:
                raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            
            first_symbol = next(iter(raw_data))
            symbol_data = raw_data[first_symbol]
            
            if symbol_data.empty:
                raise ValueError(f"ã‚·ãƒ³ãƒœãƒ« {first_symbol} ã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            
            processed_data = self.data_processor.process(symbol_data)
            
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {first_symbol}")
            self.logger.info(f"æœŸé–“: {processed_data.index.min()} â†’ {processed_data.index.max()}")
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(processed_data)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            
            return processed_data
            
        except Exception as e:
            self.logger.error(f"å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            raise
    
    def run_comprehensive_analysis(self, show_chart: bool = True) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æã®å®Ÿè¡Œ"""
        try:
            self.logger.info("ğŸš€ çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æã‚’é–‹å§‹...")
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            data = self.load_market_data()
            
            # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—
            filter_results = {}
            filter_metadata = {}
            
            for filter_name, filter_obj in self.filters.items():
                try:
                    self.logger.info(f"ğŸ” {filter_name} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨ˆç®—ä¸­...")
                    result = filter_obj.calculate(data)
                    filter_results[filter_name] = result
                    filter_metadata[filter_name] = filter_obj.get_filter_metadata()
                    self.logger.info(f"âœ… {filter_name} è¨ˆç®—å®Œäº†")
                except Exception as e:
                    self.logger.error(f"âŒ {filter_name} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è¨ˆç®—ã«å¤±æ•—: {e}")
            
            # çµ±è¨ˆåˆ†æ
            stats = self._calculate_comprehensive_stats(data, filter_results)
            
            # çµæœã®è¡¨ç¤º
            self._display_comprehensive_results(stats, filter_metadata)
            
            # ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
            if show_chart:
                self._create_comprehensive_charts(data, filter_results, filter_metadata, stats)
            
            return {
                'data': data,
                'filter_results': filter_results,
                'filter_metadata': filter_metadata,
                'stats': stats
            }
            
        except Exception as e:
            self.logger.error(f"çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æã®å®Ÿè¡Œã«å¤±æ•—: {e}")
            raise
    
    def _calculate_comprehensive_stats(self, data: pd.DataFrame, filter_results: Dict) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„çµ±è¨ˆåˆ†æ"""
        
        def safe_mean(arr):
            if arr is None or len(arr) == 0:
                return 0.0
            valid_values = arr[np.isfinite(arr)]
            return np.mean(valid_values) if len(valid_values) > 0 else 0.0
        
        def safe_std(arr):
            if arr is None or len(arr) == 0:
                return 0.0
            valid_values = arr[np.isfinite(arr)]
            return np.std(valid_values) if len(valid_values) > 0 else 0.0
        
        def calculate_tracking_error(filtered_values, actual_values):
            """è¿½è·¡èª¤å·®ã‚’è¨ˆç®—"""
            if len(filtered_values) == 0 or len(actual_values) == 0:
                return float('inf')
            
            min_len = min(len(filtered_values), len(actual_values))
            filtered_values = filtered_values[:min_len]
            actual_values = actual_values[:min_len]
            
            valid_mask = np.isfinite(filtered_values) & np.isfinite(actual_values)
            if np.sum(valid_mask) == 0:
                return float('inf')
            
            error = np.sqrt(np.mean((filtered_values[valid_mask] - actual_values[valid_mask])**2))
            return float(error)
        
        def calculate_prediction_accuracy(filtered_values, actual_values):
            """äºˆæ¸¬ç²¾åº¦ã‚’è¨ˆç®—ï¼ˆæ–¹å‘æ€§ã®ä¸€è‡´ç‡ï¼‰"""
            if len(filtered_values) < 2 or len(actual_values) < 2:
                return 0.0
            
            min_len = min(len(filtered_values), len(actual_values))
            filtered_values = filtered_values[:min_len]
            actual_values = actual_values[:min_len]
            
            # æ–¹å‘æ€§ã®è¨ˆç®—
            filtered_direction = np.diff(filtered_values) > 0
            actual_direction = np.diff(actual_values) > 0
            
            valid_mask = np.isfinite(filtered_direction) & np.isfinite(actual_direction)
            if np.sum(valid_mask) == 0:
                return 0.0
            
            accuracy = np.mean(filtered_direction[valid_mask] == actual_direction[valid_mask])
            return float(accuracy)
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾— - åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã‚’ç¢ºèª
        if 'hlc3' in data.columns:
            price_values = data['hlc3'].values
        elif 'close' in data.columns:
            price_values = data['close'].values
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®æ•°å€¤ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                price_values = data[numeric_columns[0]].values
            else:
                raise ValueError("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        stats = {}
        
        # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®çµ±è¨ˆè¨ˆç®—
        for filter_name, result in filter_results.items():
            try:
                filtered_values = result.filtered_values
                confidence_scores = result.confidence_scores
                kalman_gains = result.kalman_gains
                innovations = result.innovation
                
                filter_stats = {
                    'tracking_error': calculate_tracking_error(filtered_values, price_values),
                    'prediction_accuracy': calculate_prediction_accuracy(filtered_values, price_values),
                    'avg_confidence': safe_mean(confidence_scores),
                    'std_confidence': safe_std(confidence_scores),
                    'avg_kalman_gain': safe_mean(kalman_gains),
                    'std_kalman_gain': safe_std(kalman_gains),
                    'avg_innovation': safe_mean(np.abs(innovations)),
                    'std_innovation': safe_std(innovations),
                    'smoothness': safe_std(np.diff(filtered_values)) if len(filtered_values) > 1 else 0.0,
                    'responsiveness': safe_mean(np.abs(np.diff(filtered_values))) if len(filtered_values) > 1 else 0.0
                }
                
                # é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨ã®è¿½åŠ çµ±è¨ˆ
                if hasattr(result, 'quantum_coherence') and result.quantum_coherence is not None:
                    filter_stats['avg_quantum_coherence'] = safe_mean(result.quantum_coherence)
                    filter_stats['std_quantum_coherence'] = safe_std(result.quantum_coherence)
                
                if hasattr(result, 'uncertainty') and result.uncertainty is not None:
                    filter_stats['avg_uncertainty'] = safe_mean(result.uncertainty)
                    filter_stats['std_uncertainty'] = safe_std(result.uncertainty)
                
                if hasattr(result, 'trend_estimate') and result.trend_estimate is not None:
                    filter_stats['avg_trend_estimate'] = safe_mean(result.trend_estimate)
                    filter_stats['std_trend_estimate'] = safe_std(result.trend_estimate)
                
                stats[filter_name] = filter_stats
                
            except Exception as e:
                self.logger.error(f"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ {filter_name} ã®çµ±è¨ˆè¨ˆç®—ã«å¤±æ•—: {e}")
                stats[filter_name] = {
                    'tracking_error': float('inf'),
                    'prediction_accuracy': 0.0,
                    'avg_confidence': 0.0,
                    'avg_kalman_gain': 0.0,
                    'smoothness': 0.0,
                    'responsiveness': 0.0
                }
        
        # ç·åˆãƒ©ãƒ³ã‚­ãƒ³ã‚°
        ranking = self._calculate_filter_ranking(stats)
        stats['ranking'] = ranking
        
        return stats
    
    def _calculate_filter_ranking(self, stats: Dict) -> Dict[str, int]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¨ˆç®—"""
        ranking_scores = {}
        
        for filter_name, filter_stats in stats.items():
            if filter_name == 'ranking':
                continue
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆä½ã„æ–¹ãŒè‰¯ã„æŒ‡æ¨™ã¯é€†è»¢ï¼‰
            score = 0
            
            # è¿½è·¡èª¤å·®ï¼ˆä½ã„æ–¹ãŒè‰¯ã„ï¼‰
            if filter_stats['tracking_error'] != float('inf'):
                score += (1.0 / (1.0 + filter_stats['tracking_error'])) * 30
            
            # äºˆæ¸¬ç²¾åº¦ï¼ˆé«˜ã„æ–¹ãŒè‰¯ã„ï¼‰
            score += filter_stats['prediction_accuracy'] * 25
            
            # ä¿¡é ¼åº¦ï¼ˆé«˜ã„æ–¹ãŒè‰¯ã„ï¼‰
            score += filter_stats['avg_confidence'] * 20
            
            # å¹³æ»‘æ€§ï¼ˆé©åº¦ãŒè‰¯ã„ï¼‰
            smoothness = filter_stats['smoothness']
            if smoothness > 0:
                score += min(1.0 / smoothness, 10.0) * 15
            
            # å¿œç­”æ€§ï¼ˆé©åº¦ãŒè‰¯ã„ï¼‰
            responsiveness = filter_stats['responsiveness']
            if responsiveness > 0:
                score += min(responsiveness, 10.0) * 10
            
            ranking_scores[filter_name] = score
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
        sorted_filters = sorted(ranking_scores.items(), key=lambda x: x[1], reverse=True)
        ranking = {filter_name: rank + 1 for rank, (filter_name, _) in enumerate(sorted_filters)}
        
        return ranking
    
    def _display_comprehensive_results(self, stats: Dict, metadata: Dict) -> None:
        """åŒ…æ‹¬çš„çµæœã®è¡¨ç¤º"""
        self.logger.info("\n" + "="*100)
        self.logger.info("ğŸ¯ çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åŒ…æ‹¬åˆ†æçµæœ")
        self.logger.info("="*100)
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤º
        ranking = stats.get('ranking', {})
        if ranking:
            self.logger.info(f"\nğŸ† ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
            for filter_name, rank in sorted(ranking.items(), key=lambda x: x[1]):
                description = KalmanFilterUnified.get_available_filters().get(filter_name, filter_name)
                self.logger.info(f"   {rank}ä½: {filter_name} - {description}")
        
        # å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è©³ç´°çµæœ
        for filter_name in sorted(stats.keys()):
            if filter_name == 'ranking':
                continue
            
            filter_stats = stats[filter_name]
            rank = ranking.get(filter_name, 0)
            
            self.logger.info(f"\nğŸ“Š {filter_name} (ãƒ©ãƒ³ã‚¯: {rank}ä½):")
            self.logger.info(f"   è¿½è·¡èª¤å·®: {filter_stats['tracking_error']:.6f}")
            self.logger.info(f"   äºˆæ¸¬ç²¾åº¦: {filter_stats['prediction_accuracy']:.3f} ({filter_stats['prediction_accuracy']*100:.1f}%)")
            self.logger.info(f"   å¹³å‡ä¿¡é ¼åº¦: {filter_stats['avg_confidence']:.3f}")
            self.logger.info(f"   å¹³å‡ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³: {filter_stats['avg_kalman_gain']:.3f}")
            self.logger.info(f"   å¹³æ»‘æ€§: {filter_stats['smoothness']:.6f}")
            self.logger.info(f"   å¿œç­”æ€§: {filter_stats['responsiveness']:.6f}")
            
            # é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨ã®è¿½åŠ æƒ…å ±
            if 'avg_quantum_coherence' in filter_stats:
                self.logger.info(f"   å¹³å‡é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹: {filter_stats['avg_quantum_coherence']:.3f}")
            if 'avg_uncertainty' in filter_stats:
                self.logger.info(f"   å¹³å‡ä¸ç¢ºå®Ÿæ€§: {filter_stats['avg_uncertainty']:.3f}")
            if 'avg_trend_estimate' in filter_stats:
                self.logger.info(f"   å¹³å‡ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®š: {filter_stats['avg_trend_estimate']:.3f}")
        
        # ç·åˆè©•ä¾¡
        if ranking:
            top_filter = min(ranking.items(), key=lambda x: x[1])[0]
            self.logger.info(f"\nğŸ¯ ç·åˆè©•ä¾¡: {top_filter} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒæœ€é«˜æ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸï¼")
    
    def _create_comprehensive_charts(self, data: pd.DataFrame, filter_results: Dict, 
                                   filter_metadata: Dict, stats: Dict) -> None:
        """åŒ…æ‹¬çš„æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        try:
            symbol = self.config.get('binance_data', {}).get('symbol', 'Unknown')
            timeframe = self.config.get('binance_data', {}).get('timeframe', 'Unknown')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã®æº–å‚™
            valid_filters = {name: result for name, result in filter_results.items() 
                           if len(result.filtered_values) > 0}
            
            if not valid_filters:
                self.logger.error("æœ‰åŠ¹ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                return
            
            # 1. ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ
            self._create_main_comparison_chart(data, valid_filters, symbol, timeframe)
            
            # 2. è©³ç´°åˆ†æãƒãƒ£ãƒ¼ãƒˆ
            self._create_detailed_analysis_chart(data, valid_filters, stats, symbol, timeframe)
            
            # 3. çµ±è¨ˆæ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆ
            self._create_statistical_comparison_chart(stats, symbol, timeframe)
            
            # 4. é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æãƒãƒ£ãƒ¼ãƒˆ
            self._create_advanced_filters_chart(data, valid_filters, symbol, timeframe)
            
        except Exception as e:
            self.logger.error(f"ãƒãƒ£ãƒ¼ãƒˆä½œæˆã«å¤±æ•—: {e}")
    
    def _create_main_comparison_chart(self, data: pd.DataFrame, filter_results: Dict, 
                                    symbol: str, timeframe: str) -> None:
        """ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        fig, axes = plt.subplots(2, 1, figsize=(20, 12))
        
        # ä¸Šæ®µï¼šä¾¡æ ¼ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
        ax1 = axes[0]
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾— - åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã‚’ç¢ºèª
        if 'hlc3' in data.columns:
            price_data = data['hlc3']
            price_label = 'Price (HLC3)'
        elif 'close' in data.columns:
            price_data = data['close']
            price_label = 'Price (Close)'
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®æ•°å€¤ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                price_data = data[numeric_columns[0]]
                price_label = f'Price ({numeric_columns[0]})'
            else:
                raise ValueError("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        ax1.plot(data.index, price_data, linewidth=1, color='black', label=price_label, alpha=0.7)
        
        for filter_name, result in filter_results.items():
            color = FILTER_COLORS.get(filter_name, '#666666')
            ax1.plot(data.index, result.filtered_values, 
                    linewidth=1.5, color=color, label=f'{filter_name}', alpha=0.8)
        
        ax1.set_title(f'ğŸ¯ çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¯”è¼ƒ - {symbol} ({timeframe})')
        ax1.set_ylabel('Price')
        ax1.grid(True, alpha=0.3)
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        # ä¸‹æ®µï¼šä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        ax2 = axes[1]
        for filter_name, result in filter_results.items():
            if hasattr(result, 'confidence_scores') and result.confidence_scores is not None:
                color = FILTER_COLORS.get(filter_name, '#666666')
                ax2.plot(data.index, result.confidence_scores, 
                        linewidth=1, color=color, label=f'{filter_name}', alpha=0.7)
        
        ax2.set_title('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢æ¯”è¼ƒ')
        ax2.set_ylabel('Confidence Score')
        ax2.set_xlabel('Time')
        ax2.grid(True, alpha=0.3)
        ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        
        plt.tight_layout()
        filename = f"kalman_unified_main_{symbol}_{timeframe}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        self.logger.info(f"ğŸ“Š ãƒ¡ã‚¤ãƒ³æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜: {filename}")
        plt.show()
    
    def _create_detailed_analysis_chart(self, data: pd.DataFrame, filter_results: Dict, 
                                      stats: Dict, symbol: str, timeframe: str) -> None:
        """è©³ç´°åˆ†æãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        fig = plt.figure(figsize=(24, 16))
        gs = GridSpec(4, 2, height_ratios=[2, 1, 1, 1], hspace=0.3, wspace=0.3)
        
        # 1. ä¾¡æ ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸Šéƒ¨å…¨ä½“ï¼‰
        ax1 = fig.add_subplot(gs[0, :])
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        if 'hlc3' in data.columns:
            price_data = data['hlc3']
            price_label = 'Price'
        elif 'close' in data.columns:
            price_data = data['close']
            price_label = 'Price'
        else:
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                price_data = data[numeric_columns[0]]
                price_label = 'Price'
            else:
                raise ValueError("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        ax1.plot(data.index, price_data, linewidth=2, color='black', label=price_label, alpha=0.8)
        
        # ä¸Šä½3ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã¿è¡¨ç¤º
        ranking = stats.get('ranking', {})
        top_filters = sorted(ranking.items(), key=lambda x: x[1])[:3]
        
        for filter_name, rank in top_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                color = FILTER_COLORS.get(filter_name, '#666666')
                ax1.plot(data.index, result.filtered_values, 
                        linewidth=2, color=color, label=f'{filter_name} (#{rank})', alpha=0.9)
        
        ax1.set_title(f'ğŸ† ãƒˆãƒƒãƒ—3ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è©³ç´°åˆ†æ - {symbol} ({timeframe})')
        ax1.set_ylabel('Price')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # 2. ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³æ¯”è¼ƒï¼ˆå·¦ä¸‹ï¼‰
        ax2 = fig.add_subplot(gs[1, 0])
        for filter_name, rank in top_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                if hasattr(result, 'kalman_gains') and result.kalman_gains is not None:
                    color = FILTER_COLORS.get(filter_name, '#666666')
                    ax2.plot(data.index, result.kalman_gains, 
                            linewidth=1, color=color, label=f'{filter_name}', alpha=0.8)
        
        ax2.set_title('ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³æ¯”è¼ƒ')
        ax2.set_ylabel('Kalman Gain')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        # 3. ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æ¯”è¼ƒï¼ˆå³ä¸‹ï¼‰
        ax3 = fig.add_subplot(gs[1, 1])
        for filter_name, rank in top_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                if hasattr(result, 'innovation') and result.innovation is not None:
                    color = FILTER_COLORS.get(filter_name, '#666666')
                    ax3.plot(data.index, np.abs(result.innovation), 
                            linewidth=1, color=color, label=f'{filter_name}', alpha=0.8)
        
        ax3.set_title('ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆäºˆæ¸¬èª¤å·®ï¼‰æ¯”è¼ƒ')
        ax3.set_ylabel('|Innovation|')
        ax3.grid(True, alpha=0.3)
        ax3.legend()
        
        # 4. è¿½è·¡èª¤å·®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆä¸‹éƒ¨å·¦ï¼‰
        ax4 = fig.add_subplot(gs[2, 0])
        tracking_errors = []
        filter_names = []
        
        for filter_name in sorted(stats.keys()):
            if filter_name != 'ranking':
                tracking_errors.append(stats[filter_name]['tracking_error'])
                filter_names.append(filter_name)
        
        y_pos = np.arange(len(filter_names))
        bars = ax4.barh(y_pos, tracking_errors, color=[FILTER_COLORS.get(name, '#666666') for name in filter_names])
        ax4.set_yticks(y_pos)
        ax4.set_yticklabels(filter_names)
        ax4.set_xlabel('Tracking Error')
        ax4.set_title('è¿½è·¡èª¤å·®æ¯”è¼ƒ')
        ax4.grid(True, alpha=0.3)
        
        # 5. äºˆæ¸¬ç²¾åº¦æ¯”è¼ƒï¼ˆä¸‹éƒ¨å³ï¼‰
        ax5 = fig.add_subplot(gs[2, 1])
        accuracies = []
        
        for filter_name in filter_names:
            accuracies.append(stats[filter_name]['prediction_accuracy'] * 100)
        
        bars = ax5.barh(y_pos, accuracies, color=[FILTER_COLORS.get(name, '#666666') for name in filter_names])
        ax5.set_yticks(y_pos)
        ax5.set_yticklabels(filter_names)
        ax5.set_xlabel('Prediction Accuracy (%)')
        ax5.set_title('äºˆæ¸¬ç²¾åº¦æ¯”è¼ƒ')
        ax5.grid(True, alpha=0.3)
        
        # 6. ç·åˆã‚¹ã‚³ã‚¢æ¯”è¼ƒï¼ˆæœ€ä¸‹éƒ¨ï¼‰
        ax6 = fig.add_subplot(gs[3, :])
        ranking_scores = []
        
        for filter_name in filter_names:
            rank = ranking.get(filter_name, len(filter_names))
            ranking_scores.append(len(filter_names) + 1 - rank)  # é€†é †ã‚¹ã‚³ã‚¢
        
        bars = ax6.bar(filter_names, ranking_scores, color=[FILTER_COLORS.get(name, '#666666') for name in filter_names])
        ax6.set_ylabel('Performance Score')
        ax6.set_title('ç·åˆæ€§èƒ½ã‚¹ã‚³ã‚¢')
        ax6.grid(True, alpha=0.3)
        plt.setp(ax6.get_xticklabels(), rotation=45, ha='right')
        
        plt.tight_layout()
        filename = f"kalman_unified_detailed_{symbol}_{timeframe}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        self.logger.info(f"ğŸ“Š è©³ç´°åˆ†æãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜: {filename}")
        plt.show()
    
    def _create_statistical_comparison_chart(self, stats: Dict, symbol: str, timeframe: str) -> None:
        """çµ±è¨ˆæ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        filter_names = [name for name in stats.keys() if name != 'ranking']
        
        # 1. è¿½è·¡èª¤å·® vs äºˆæ¸¬ç²¾åº¦
        ax1 = axes[0, 0]
        for filter_name in filter_names:
            filter_stats = stats[filter_name]
            x = filter_stats['tracking_error']
            y = filter_stats['prediction_accuracy']
            color = FILTER_COLORS.get(filter_name, '#666666')
            ax1.scatter(x, y, s=100, color=color, alpha=0.7, label=filter_name)
        
        ax1.set_xlabel('Tracking Error')
        ax1.set_ylabel('Prediction Accuracy')
        ax1.set_title('è¿½è·¡èª¤å·® vs äºˆæ¸¬ç²¾åº¦')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # 2. ä¿¡é ¼åº¦ vs ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³
        ax2 = axes[0, 1]
        for filter_name in filter_names:
            filter_stats = stats[filter_name]
            x = filter_stats['avg_confidence']
            y = filter_stats['avg_kalman_gain']
            color = FILTER_COLORS.get(filter_name, '#666666')
            ax2.scatter(x, y, s=100, color=color, alpha=0.7, label=filter_name)
        
        ax2.set_xlabel('Average Confidence')
        ax2.set_ylabel('Average Kalman Gain')
        ax2.set_title('ä¿¡é ¼åº¦ vs ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        # 3. å¹³æ»‘æ€§ vs å¿œç­”æ€§
        ax3 = axes[1, 0]
        for filter_name in filter_names:
            filter_stats = stats[filter_name]
            x = filter_stats['smoothness']
            y = filter_stats['responsiveness']
            color = FILTER_COLORS.get(filter_name, '#666666')
            ax3.scatter(x, y, s=100, color=color, alpha=0.7, label=filter_name)
        
        ax3.set_xlabel('Smoothness')
        ax3.set_ylabel('Responsiveness')
        ax3.set_title('å¹³æ»‘æ€§ vs å¿œç­”æ€§')
        ax3.grid(True, alpha=0.3)
        ax3.legend()
        
        # 4. ç·åˆè©•ä¾¡ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
        ax4 = axes[1, 1]
        ranking = stats.get('ranking', {})
        top_3_filters = sorted(ranking.items(), key=lambda x: x[1])[:3]
        
        categories = ['Accuracy', 'Confidence', 'Smoothness', 'Responsiveness']
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]  # å††ã‚’é–‰ã˜ã‚‹
        
        for filter_name, rank in top_3_filters:
            filter_stats = stats[filter_name]
            values = [
                filter_stats['prediction_accuracy'],
                filter_stats['avg_confidence'],
                min(1.0, 1.0 / (filter_stats['smoothness'] + 0.001)),
                min(1.0, filter_stats['responsiveness'] / 10.0)
            ]
            values += values[:1]  # å††ã‚’é–‰ã˜ã‚‹
            
            color = FILTER_COLORS.get(filter_name, '#666666')
            ax4.plot(angles, values, 'o-', linewidth=2, color=color, label=f'{filter_name} (#{rank})')
            ax4.fill(angles, values, alpha=0.25, color=color)
        
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(categories)
        ax4.set_ylim(0, 1)
        ax4.set_title('ãƒˆãƒƒãƒ—3ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç·åˆè©•ä¾¡')
        ax4.grid(True)
        ax4.legend()
        
        plt.tight_layout()
        filename = f"kalman_unified_stats_{symbol}_{timeframe}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        self.logger.info(f"ğŸ“Š çµ±è¨ˆæ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜: {filename}")
        plt.show()
    
    def _create_advanced_filters_chart(self, data: pd.DataFrame, filter_results: Dict, 
                                     symbol: str, timeframe: str) -> None:
        """é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
        fig = plt.figure(figsize=(20, 16))
        gs = GridSpec(5, 2, height_ratios=[2, 1, 1, 1, 1], hspace=0.3, wspace=0.3)
        
        # 1. ä¾¡æ ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸Šéƒ¨å…¨ä½“ï¼‰
        ax1 = fig.add_subplot(gs[0, :])
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        if 'hlc3' in data.columns:
            price_data = data['hlc3']
            price_label = 'Price'
        elif 'close' in data.columns:
            price_data = data['close']
            price_label = 'Price'
        else:
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                price_data = data[numeric_columns[0]]
                price_label = 'Price'
            else:
                raise ValueError("ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        ax1.plot(data.index, price_data, linewidth=2, color='black', label=price_label, alpha=0.8)
        
        # é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆneural_supreme, market_adaptive_unscentedï¼‰
        advanced_filters = ['neural_supreme', 'market_adaptive_unscented']
        for filter_name in advanced_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                color = FILTER_COLORS.get(filter_name, '#666666')
                ax1.plot(data.index, result.filtered_values, 
                        linewidth=2, color=color, label=filter_name, alpha=0.9)
        
        ax1.set_title(f'ğŸš€ é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æ - {symbol} ({timeframe})')
        ax1.set_ylabel('Price')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # 2. Neural Supreme ã®é‡å­ä½ç›¸ï¼ˆå·¦ï¼‰
        ax2 = fig.add_subplot(gs[1, 0])
        if 'neural_supreme' in filter_results:
            result = filter_results['neural_supreme']
            if hasattr(result, 'quantum_coherence') and result.quantum_coherence is not None:
                ax2.plot(data.index, result.quantum_coherence, 
                        linewidth=1, color=FILTER_COLORS.get('neural_supreme', '#666666'), alpha=0.8)
        ax2.set_title('Neural Supreme: é‡å­ä½ç›¸')
        ax2.set_ylabel('Quantum Phase')
        ax2.grid(True, alpha=0.3)
        
        # 3. Market Adaptive ã®å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ ï¼ˆå³ï¼‰
        ax3 = fig.add_subplot(gs[1, 1])
        if 'market_adaptive_unscented' in filter_results:
            result = filter_results['market_adaptive_unscented']
            if hasattr(result, 'quantum_coherence') and result.quantum_coherence is not None:
                ax3.plot(data.index, result.quantum_coherence, 
                        linewidth=1, color=FILTER_COLORS.get('market_adaptive_unscented', '#666666'), alpha=0.8)
                ax3.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
                ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)
                ax3.axhline(y=-0.5, color='gray', linestyle='--', alpha=0.5)
        ax3.set_title('Market Adaptive: å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ ')
        ax3.set_ylabel('Market Regime')
        ax3.grid(True, alpha=0.3)
        
        # 4. ä¸ç¢ºå®Ÿæ€§æ¯”è¼ƒï¼ˆå·¦ï¼‰
        ax4 = fig.add_subplot(gs[2, 0])
        for filter_name in advanced_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                if hasattr(result, 'uncertainty') and result.uncertainty is not None:
                    color = FILTER_COLORS.get(filter_name, '#666666')
                    ax4.plot(data.index, result.uncertainty, 
                            linewidth=1, color=color, label=filter_name, alpha=0.8)
        ax4.set_title('ä¸ç¢ºå®Ÿæ€§æ¨å®šæ¯”è¼ƒ')
        ax4.set_ylabel('Uncertainty')
        ax4.grid(True, alpha=0.3)
        ax4.legend()
        
        # 5. ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®šæ¯”è¼ƒï¼ˆå³ï¼‰
        ax5 = fig.add_subplot(gs[2, 1])
        for filter_name in advanced_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                if hasattr(result, 'trend_estimate') and result.trend_estimate is not None:
                    color = FILTER_COLORS.get(filter_name, '#666666')
                    ax5.plot(data.index, result.trend_estimate, 
                            linewidth=1, color=color, label=filter_name, alpha=0.8)
        ax5.set_title('ãƒˆãƒ¬ãƒ³ãƒ‰æ¨å®šæ¯”è¼ƒ')
        ax5.set_ylabel('Trend Estimate')
        ax5.grid(True, alpha=0.3)
        ax5.legend()
        
        # 6. é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ¯”è¼ƒï¼ˆå·¦ï¼‰
        ax6 = fig.add_subplot(gs[3, 0])
        quantum_filters = ['quantum_adaptive', 'hyper_quantum', 'neural_supreme']
        for filter_name in quantum_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                if hasattr(result, 'quantum_coherence') and result.quantum_coherence is not None:
                    color = FILTER_COLORS.get(filter_name, '#666666')
                    ax6.plot(data.index, result.quantum_coherence, 
                            linewidth=1, color=color, label=filter_name, alpha=0.8)
        ax6.set_title('é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹æ¯”è¼ƒ')
        ax6.set_ylabel('Quantum Coherence')
        ax6.grid(True, alpha=0.3)
        ax6.legend()
        
        # 7. ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºæ¯”è¼ƒï¼ˆå³ï¼‰
        ax7 = fig.add_subplot(gs[3, 1])
        for filter_name in advanced_filters:
            if filter_name in filter_results:
                result = filter_results[filter_name]
                if hasattr(result, 'process_noise') and result.process_noise is not None:
                    color = FILTER_COLORS.get(filter_name, '#666666')
                    ax7.plot(data.index, result.process_noise, 
                            linewidth=1, color=color, label=filter_name, alpha=0.8)
        ax7.set_title('ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚¤ã‚ºæ¯”è¼ƒ')
        ax7.set_ylabel('Process Noise')
        ax7.grid(True, alpha=0.3)
        ax7.legend()
        
        # 8. å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³åˆ†å¸ƒï¼ˆä¸‹éƒ¨å…¨ä½“ï¼‰
        ax8 = fig.add_subplot(gs[4, :])
        
        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
        gain_data = []
        labels = []
        colors = []
        
        for filter_name, result in filter_results.items():
            if hasattr(result, 'kalman_gains') and result.kalman_gains is not None:
                valid_gains = result.kalman_gains[np.isfinite(result.kalman_gains)]
                if len(valid_gains) > 0:
                    gain_data.append(valid_gains)
                    labels.append(filter_name)
                    colors.append(FILTER_COLORS.get(filter_name, '#666666'))
        
        if gain_data:
            ax8.hist(gain_data, bins=50, alpha=0.7, label=labels, color=colors, density=True)
            ax8.set_xlabel('Kalman Gain')
            ax8.set_ylabel('Density')
            ax8.set_title('ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³åˆ†å¸ƒæ¯”è¼ƒ')
            ax8.grid(True, alpha=0.3)
            ax8.legend()
        
        plt.tight_layout()
        filename = f"kalman_unified_advanced_{symbol}_{timeframe}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        self.logger.info(f"ğŸ“Š é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜: {filename}")
        plt.show()


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='ğŸ¯ çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ğŸš€ **åˆ†æå¯¾è±¡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:**
- adaptive: åŸºæœ¬é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
- quantum_adaptive: é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼  
- unscented: ç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆUKFï¼‰
- extended: æ‹¡å¼µã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆEKFï¼‰
- hyper_quantum: ãƒã‚¤ãƒ‘ãƒ¼é‡å­é©å¿œã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
- triple_ensemble: ä¸‰é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
- neural_supreme: ğŸ§ ğŸš€ Neural Adaptive Quantum Supreme
- market_adaptive_unscented: ğŸ¯ å¸‚å ´é©å¿œç„¡é¦™æ–™ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

ğŸ“Š **åŒ…æ‹¬çš„åˆ†æ:**
- å„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®æ€§èƒ½æ¯”è¼ƒ
- è¿½è·¡èª¤å·®ãƒ»äºˆæ¸¬ç²¾åº¦è©•ä¾¡
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒ»ã‚«ãƒ«ãƒãƒ³ã‚²ã‚¤ãƒ³åˆ†æ
- å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ ãƒ»é‡å­ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹å¯è¦–åŒ–
- çµ±è¨ˆçš„æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        """
    )
    
    parser.add_argument('--config', '-c', type=str, default='config.yaml', help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--no-show', action='store_true', help='ãƒãƒ£ãƒ¼ãƒˆéè¡¨ç¤º')
    
    args = parser.parse_args()
    
    try:
        print("ğŸ¯ çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ä¸­...")
        print("   ğŸ“Š å…¨ã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’å®Ÿéš›ã®ç›¸å ´ãƒ‡ãƒ¼ã‚¿ã§åŒ…æ‹¬åˆ†æ")
        
        analyzer = KalmanFilterUnifiedAnalyzer(args.config)
        
        # åˆ†æå®Ÿè¡Œ
        results = analyzer.run_comprehensive_analysis(show_chart=not args.no_show)
        
        print("\nâœ… çµ±åˆã‚«ãƒ«ãƒãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        # æœ€çµ‚è©•ä¾¡
        ranking = results['stats'].get('ranking', {})
        if ranking:
            top_filter = min(ranking.items(), key=lambda x: x[1])[0]
            print(f"ğŸ† æœ€é«˜æ€§èƒ½: {top_filter} ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
            print("ğŸ“Š è©³ç´°ãªåˆ†æçµæœã¨ãƒãƒ£ãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ åˆ†æãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()